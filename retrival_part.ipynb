{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "import ast\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\immah\\anaconda3\\envs\\rag\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/isaac-sim/IsaacLab.git\"\n",
    "local_path = \"./isaaclab_local\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Repo cloned.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Clone the GitHub repo ---\n",
    "if not os.path.exists(local_path):\n",
    "    git.Repo.clone_from(repo_url, local_path)\n",
    "    print(\"✅ Repo cloned.\")\n",
    "else:\n",
    "    print(\"📁 Repo already exists. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Set source folder path ---\n",
    "SOURCE_PATH = \"./isaaclab_local\"\n",
    "VALID_CODE_EXTENSIONS = {'.py', '.json', '.yaml', '.yml', '.toml', '.md'}\n",
    "EXCLUDE_DIRS = {\"_static\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. AST Chunking for Python files ---\n",
    "def chunk_python_ast(code, file_path):\n",
    "    chunks = []\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n",
    "                text = ast.get_source_segment(code, node)\n",
    "                if text:\n",
    "                    chunks.append(Document(page_content=text, metadata={\"source\": file_path}))\n",
    "    except Exception as e:\n",
    "        print(f\"AST parse error in {file_path}: {e}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Fallback chunker for other files ---\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "def chunk_fallback(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        return text_splitter.create_documents([text], metadatas=[{\"source\": file_path}])\n",
    "    except Exception as e:\n",
    "        print(f\"Error chunking {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Traverse all valid files and chunk ---\n",
    "def ingest_repo_files():\n",
    "    all_chunks = []\n",
    "    for root, dirs, files in os.walk(SOURCE_PATH):\n",
    "        if any(skip in root for skip in EXCLUDE_DIRS):\n",
    "            continue\n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1]\n",
    "            if ext not in VALID_CODE_EXTENSIONS:\n",
    "                continue\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"📄 Ingesting: {file_path}\")\n",
    "            if ext == '.py':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    code = f.read()\n",
    "                chunks = chunk_python_ast(code, file_path)\n",
    "                print(\"Used AST chunker\")\n",
    "            else:\n",
    "                chunks = chunk_fallback(file_path)\n",
    "                print(\"Used fallback chunker\")\n",
    "            all_chunks.extend(chunks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Ingesting: ./isaaclab_local\\.pre-commit-config.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\CONTRIBUTING.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\CONTRIBUTORS.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\environment.yml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\pyproject.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\SECURITY.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.aws\\mirror-buildspec.yml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.aws\\postmerge-ci-buildspec.yml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.aws\\premerge-ci-buildspec.yml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.github\\PULL_REQUEST_TEMPLATE.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.github\\stale.yml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.github\\ISSUE_TEMPLATE\\bug.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.github\\ISSUE_TEMPLATE\\proposal.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.github\\ISSUE_TEMPLATE\\question.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.github\\workflows\\docs.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.github\\workflows\\pre-commit.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.vscode\\extensions.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.vscode\\tasks.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.vscode\\tools\\launch.template.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.vscode\\tools\\settings.template.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\.vscode\\tools\\setup_vscode.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\container.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\docker-compose.cloudxr-runtime.patch.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\docker-compose.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\x11.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\test\\test_docker.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\utils\\container_interface.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\utils\\state_file.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\utils\\x11_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docker\\utils\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docs\\conf.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docs\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\docs\\source\\refs\\snippets\\tutorial_modify_direct_rl_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\benchmarks\\benchmark_cameras.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\benchmarks\\benchmark_load_robot.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\benchmarks\\benchmark_non_rl.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\benchmarks\\benchmark_rlgames.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\benchmarks\\benchmark_rsl_rl.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\benchmarks\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\arms.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\bipeds.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\deformables.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\h1_locomotion.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\hands.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\markers.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\multi_asset.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\procedural_terrain.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\quadcopter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\quadrupeds.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\sensors\\cameras.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\sensors\\contact_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\sensors\\frame_transformer_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\sensors\\imu_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\demos\\sensors\\raycaster_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\environments\\list_envs.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\environments\\random_agent.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\environments\\zero_agent.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\environments\\state_machine\\lift_cube_sm.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\environments\\state_machine\\lift_teddy_bear.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\environments\\state_machine\\open_cabinet_sm.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\environments\\teleoperation\\teleop_se3_agent.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\imitation_learning\\isaaclab_mimic\\annotate_demos.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\imitation_learning\\isaaclab_mimic\\consolidated_demo.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\imitation_learning\\isaaclab_mimic\\generate_dataset.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\imitation_learning\\robomimic\\play.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\imitation_learning\\robomimic\\train.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\grok_cluster_with_kubectl.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\launch.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\mlflow_to_local_tensorboard.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\submit_job.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\tuner.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\util.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\wrap_resources.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\hyperparameter_tuning\\vision_cartpole_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\ray\\hyperparameter_tuning\\vision_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\rl_games\\play.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\rl_games\\train.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\rsl_rl\\cli_args.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\rsl_rl\\play.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\rsl_rl\\train.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\sb3\\play.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\sb3\\train.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\skrl\\play.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\reinforcement_learning\\skrl\\train.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\blender_obj.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\check_instanceable.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\convert_instanceable.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\convert_mesh.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\convert_mjcf.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\convert_urdf.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\merge_hdf5_datasets.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\pretrained_checkpoint.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\process_meshes_to_obj.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\record_demos.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tools\\replay_demos.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\00_sim\\create_empty.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\00_sim\\launch_app.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\00_sim\\log_time.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\00_sim\\set_rendering_mode.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\00_sim\\spawn_prims.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\01_assets\\add_new_robot.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\01_assets\\run_articulation.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\01_assets\\run_deformable_object.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\01_assets\\run_rigid_object.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\02_scene\\create_scene.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\03_envs\\create_cartpole_base_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\03_envs\\create_cube_base_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\03_envs\\create_quadruped_base_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\03_envs\\policy_inference_in_usd.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\03_envs\\run_cartpole_rl_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\04_sensors\\add_sensors_on_robot.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\04_sensors\\run_frame_transformer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\04_sensors\\run_ray_caster.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\04_sensors\\run_ray_caster_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\04_sensors\\run_usd_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\05_controllers\\run_diff_ik.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\scripts\\tutorials\\05_controllers\\run_osc.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\pyproject.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\setup.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\config\\extension.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\docs\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\actuators\\actuator_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\actuators\\actuator_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\actuators\\actuator_net.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\actuators\\actuator_pd.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\actuators\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\app\\app_launcher.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\app\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\asset_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\asset_base_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\articulation\\articulation.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\articulation\\articulation_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\articulation\\articulation_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\articulation\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\deformable_object\\deformable_object.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\deformable_object\\deformable_object_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\deformable_object\\deformable_object_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\deformable_object\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object\\rigid_object.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object\\rigid_object_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object\\rigid_object_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object_collection\\rigid_object_collection.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object_collection\\rigid_object_collection_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object_collection\\rigid_object_collection_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\assets\\rigid_object_collection\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\differential_ik.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\differential_ik_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\joint_impedance.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\operational_space.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\operational_space_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\pink_ik.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\pink_ik_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\rmp_flow.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\config\\rmp_flow.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\controllers\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\device_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\retargeter_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\gamepad\\se2_gamepad.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\gamepad\\se3_gamepad.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\gamepad\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\keyboard\\se2_keyboard.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\keyboard\\se3_keyboard.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\keyboard\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\common.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\openxr_device.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\xr_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\dex\\dex_retargeter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\humanoid\\fourier\\gr1t2_retargeter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\humanoid\\fourier\\gr1_t2_dex_retargeting_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\humanoid\\fourier\\data\\configs\\dex-retargeting\\fourier_hand_left_dexpilot.yml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\humanoid\\fourier\\data\\configs\\dex-retargeting\\fourier_hand_right_dexpilot.yml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\manipulator\\gripper_retargeter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\manipulator\\se3_abs_retargeter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\manipulator\\se3_rel_retargeter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\openxr\\retargeters\\manipulator\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\spacemouse\\se2_spacemouse.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\spacemouse\\se3_spacemouse.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\spacemouse\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\devices\\spacemouse\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\common.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\direct_marl_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\direct_marl_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\direct_rl_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\direct_rl_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\manager_based_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\manager_based_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\manager_based_rl_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\manager_based_rl_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\manager_based_rl_mimic_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mimic_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\curriculums.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\events.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\observations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\terminations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\actions_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\binary_joint_actions.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\joint_actions.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\joint_actions_to_limits.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\non_holonomic_actions.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\pink_actions_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\pink_task_space_actions.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\task_space_actions.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\actions\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\commands\\commands_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\commands\\null_command.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\commands\\pose_2d_command.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\commands\\pose_command.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\commands\\velocity_command.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\commands\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\recorders\\recorders.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\recorders\\recorders_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\mdp\\recorders\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\ui\\base_env_window.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\ui\\empty_window.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\ui\\manager_based_rl_env_window.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\ui\\viewport_camera_controller.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\ui\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\utils\\marl.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\utils\\spaces.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\envs\\utils\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\action_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\command_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\curriculum_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\event_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\manager_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\manager_term_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\observation_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\recorder_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\reward_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\scene_entity_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\termination_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\managers\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\markers\\visualization_markers.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\markers\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\markers\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\scene\\interactive_scene.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\scene\\interactive_scene_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\scene\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\sensor_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\sensor_base_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\camera\\camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\camera\\camera_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\camera\\camera_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\camera\\tiled_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\camera\\tiled_camera_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\camera\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\camera\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\contact_sensor\\contact_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\contact_sensor\\contact_sensor_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\contact_sensor\\contact_sensor_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\contact_sensor\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\frame_transformer\\frame_transformer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\frame_transformer\\frame_transformer_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\frame_transformer\\frame_transformer_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\frame_transformer\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\imu\\imu.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\imu\\imu_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\imu\\imu_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\imu\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\ray_caster.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\ray_caster_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\ray_caster_camera_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\ray_caster_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\ray_caster_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\patterns\\patterns.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\patterns\\patterns_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sensors\\ray_caster\\patterns\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\simulation_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\simulation_context.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\asset_converter_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\asset_converter_base_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\mesh_converter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\mesh_converter_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\mjcf_converter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\mjcf_converter_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\urdf_converter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\urdf_converter_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\converters\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\schemas\\schemas.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\schemas\\schemas_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\schemas\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\spawner_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\from_files\\from_files.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\from_files\\from_files_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\from_files\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\lights\\lights.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\lights\\lights_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\lights\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\materials\\physics_materials.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\materials\\physics_materials_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\materials\\visual_materials.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\materials\\visual_materials_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\materials\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\meshes\\meshes.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\meshes\\meshes_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\meshes\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\sensors\\sensors.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\sensors\\sensors_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\sensors\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\shapes\\shapes.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\shapes\\shapes_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\shapes\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\wrappers\\wrappers.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\wrappers\\wrappers_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\sim\\spawners\\wrappers\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\terrain_generator.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\terrain_generator_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\terrain_importer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\terrain_importer_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\config\\rough.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\height_field\\hf_terrains.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\height_field\\hf_terrains_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\height_field\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\height_field\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\trimesh\\mesh_terrains.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\trimesh\\mesh_terrains_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\trimesh\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\terrains\\trimesh\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\widgets\\image_plot.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\widgets\\line_plot.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\widgets\\manager_live_visualizer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\widgets\\ui_visualizer_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\widgets\\ui_widget_wrapper.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\widgets\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\xr_widgets\\instruction_widget.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\ui\\xr_widgets\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\array.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\assets.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\configclass.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\dict.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\math.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\pretrained_checkpoint.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\sensors.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\string.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\timer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\types.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\buffers\\circular_buffer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\buffers\\delay_buffer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\buffers\\timestamped_buffer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\buffers\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\datasets\\dataset_file_handler_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\datasets\\episode_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\datasets\\hdf5_dataset_file_handler.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\datasets\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\interpolation\\linear_interpolation.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\interpolation\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\io\\pkl.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\io\\yaml.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\io\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\modifiers\\modifier.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\modifiers\\modifier_base.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\modifiers\\modifier_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\modifiers\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\noise\\noise_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\noise\\noise_model.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\noise\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\warp\\kernels.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\warp\\ops.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\isaaclab\\utils\\warp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\app\\test_argparser_launch.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\app\\test_env_var_launch.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\app\\test_kwarg_launch.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\assets\\check_external_force.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\assets\\check_fixed_base_assets.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\assets\\check_ridgeback_franka.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\assets\\test_articulation.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\assets\\test_deformable_object.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\assets\\test_rigid_object.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\assets\\test_rigid_object_collection.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\controllers\\test_differential_ik.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\controllers\\test_operational_space.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\controllers\\test_pink_ik.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\deps\\test_scipy.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\deps\\test_torch.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\deps\\isaacsim\\check_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\deps\\isaacsim\\check_floating_base_made_fixed.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\deps\\isaacsim\\check_legged_robot_clone.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\deps\\isaacsim\\check_ref_count.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\deps\\isaacsim\\check_rep_texture_randomizer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\devices\\check_keyboard.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\devices\\test_oxr_device.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\check_manager_based_env_anymal_locomotion.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\check_manager_based_env_floating_cube.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_action_state_recorder_term.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_direct_marl_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_env_rendering_logic.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_manager_based_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_manager_based_rl_env_ui.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_null_command_term.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_scale_randomization.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_spaces_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\envs\\test_texture_randomization.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\managers\\test_event_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\managers\\test_observation_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\managers\\test_recorder_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\managers\\test_reward_manager.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\markers\\check_markers_visibility.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\markers\\test_visualization_markers.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\performance\\test_kit_startup_performance.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\performance\\test_robot_load_performance.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\scene\\check_interactive_scene.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\scene\\test_interactive_scene.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\check_contact_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\check_imu_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\check_ray_caster.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_contact_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_frame_transformer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_imu.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_multi_tiled_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_outdated_sensor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_ray_caster_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_tiled_camera.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sensors\\test_tiled_camera_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\check_meshes.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_build_simulation_context_headless.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_build_simulation_context_nonheadless.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_mesh_converter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_mjcf_converter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_schemas.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_simulation_context.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_simulation_render_config.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_spawn_from_files.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_spawn_lights.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_spawn_materials.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_spawn_meshes.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_spawn_sensors.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_spawn_shapes.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_spawn_wrappers.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_urdf_converter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\sim\\test_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\terrains\\check_height_field_subterrains.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\terrains\\check_mesh_subterrains.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\terrains\\check_terrain_importer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\terrains\\test_terrain_generator.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\terrains\\test_terrain_importer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_assets.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_circular_buffer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_configclass.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_delay_buffer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_dict.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_episode_data.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_hdf5_dataset_file_handler.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_math.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_modifiers.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_noise.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_string.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab\\test\\utils\\test_timer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\pyproject.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\setup.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\config\\extension.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\docs\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\allegro.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\ant.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\anymal.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\cartpole.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\cart_double_pendulum.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\cassie.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\fourier.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\franka.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\humanoid.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\humanoid_28.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\kinova.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\quadcopter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\ridgeback_franka.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\sawyer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\shadow_hand.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\spot.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\unitree.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\universal_robots.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\robots\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\sensors\\velodyne.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\isaaclab_assets\\sensors\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_assets\\test\\test_valid_configs.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\pyproject.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\setup.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\config\\extension.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\datagen_info.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\datagen_info_pool.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\data_generator.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\generation.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\selection_strategy.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\waypoint.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\datagen\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\franka_stack_ik_abs_mimic_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\franka_stack_ik_abs_mimic_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\franka_stack_ik_rel_blueprint_mimic_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\franka_stack_ik_rel_mimic_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\franka_stack_ik_rel_mimic_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\franka_stack_ik_rel_visuomotor_mimic_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\pinocchio_envs\\pickplace_gr1t2_mimic_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\pinocchio_envs\\pickplace_gr1t2_mimic_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\envs\\pinocchio_envs\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\isaaclab_mimic\\ui\\instruction_display.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\test\\test_generate_dataset.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_mimic\\test\\test_selection_strategy.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\pyproject.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\setup.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\config\\extension.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rl_games.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\sb3.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\skrl.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rsl_rl\\distillation_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rsl_rl\\exporter.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rsl_rl\\rl_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rsl_rl\\rnd_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rsl_rl\\symmetry_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rsl_rl\\vecenv_wrapper.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\isaaclab_rl\\rsl_rl\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\test\\test_rl_games_wrapper.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\test\\test_rsl_rl_wrapper.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\test\\test_sb3_wrapper.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_rl\\test\\test_skrl_wrapper.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\pyproject.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\setup.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\config\\extension.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\docs\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\allegro_hand\\allegro_hand_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\allegro_hand\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\allegro_hand\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\allegro_hand\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\allegro_hand\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\allegro_hand\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\ant\\ant_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\ant\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\ant\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\ant\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\ant\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\ant\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\anymal_c_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\anymal_c_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\agents\\rl_games_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\agents\\rl_games_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\anymal_c\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\assembly_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\assembly_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\assembly_tasks_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\automate_algo_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\automate_log_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\disassembly_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\disassembly_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\disassembly_tasks_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\factory_control.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\industreal_algo_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\run_disassembly_w_id.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\run_w_id.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\soft_dtw_cuda.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\assembly\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\cartpole_camera_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\cartpole_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\agents\\rl_games_camera_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\agents\\sb3_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\agents\\skrl_camera_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\cartpole_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\cartpole_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_box_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_box_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_box_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_dict_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_dict_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_dict_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_discrete_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_discrete_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_discrete_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_multidiscrete_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_multidiscrete_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_multidiscrete_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_tuple_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_tuple_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\skrl_tuple_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\cartpole_camera_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\cartpole_camera_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_box_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_box_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_box_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_dict_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_dict_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_dict_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_tuple_box_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_tuple_discrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\skrl_tuple_multidiscrete_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cartpole_showcase\\cartpole_camera\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cart_double_pendulum\\cart_double_pendulum_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cart_double_pendulum\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cart_double_pendulum\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cart_double_pendulum\\agents\\skrl_ippo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cart_double_pendulum\\agents\\skrl_mappo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cart_double_pendulum\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\cart_double_pendulum\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\factory\\factory_control.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\factory\\factory_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\factory\\factory_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\factory\\factory_tasks_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\factory\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\factory\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\factory\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\franka_cabinet\\franka_cabinet_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\franka_cabinet\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\franka_cabinet\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\franka_cabinet\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\franka_cabinet\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\franka_cabinet\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid\\humanoid_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\humanoid_amp_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\humanoid_amp_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\agents\\skrl_dance_amp_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\agents\\skrl_run_amp_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\agents\\skrl_walk_amp_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\motions\\motion_loader.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\motions\\motion_viewer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\motions\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\humanoid_amp\\motions\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\inhand_manipulation\\inhand_manipulation_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\inhand_manipulation\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\locomotion\\locomotion_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\locomotion\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\quadcopter\\quadcopter_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\quadcopter\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\quadcopter\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\quadcopter\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\quadcopter\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\quadcopter\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\feature_extractor.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\shadow_hand_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\shadow_hand_vision_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\rl_games_ppo_ff_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\rl_games_ppo_lstm_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\rl_games_ppo_vision_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\skrl_ff_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\shadow_hand_over_env.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\shadow_hand_over_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\agents\\skrl_ippo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\agents\\skrl_mappo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\direct\\shadow_hand_over\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\ant\\ant_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\ant\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\ant\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\ant\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\ant\\agents\\sb3_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\ant\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\ant\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\cartpole_camera_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\cartpole_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\agents\\rl_games_camera_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\agents\\rl_games_feature_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\agents\\sb3_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\cartpole\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\humanoid_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\agents\\sb3_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\mdp\\observations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\classic\\humanoid\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\velocity_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\a1\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\a1\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\a1\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\a1\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\a1\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\a1\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\a1\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_b\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_b\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_b\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_b\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_b\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_b\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_b\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\agents\\rl_games_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\agents\\rl_games_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_c\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_d\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_d\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_d\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_d\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_d\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_d\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\anymal_d\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\cassie\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\cassie\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\cassie\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\cassie\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\cassie\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\cassie\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\cassie\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\g1\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\g1\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\g1\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\g1\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\g1\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\g1\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\g1\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go1\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go1\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go1\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go1\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go1\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go1\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go1\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go2\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go2\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go2\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go2\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go2\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go2\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\go2\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\h1\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\h1\\rough_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\h1\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\h1\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\h1\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\h1\\agents\\skrl_rough_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\h1\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\flat_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\mdp\\events.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\config\\spot\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\mdp\\curriculums.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\mdp\\terminations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\locomotion\\velocity\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\cabinet_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\ik_abs_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\ik_rel_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\joint_pos_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\config\\franka\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\mdp\\observations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\cabinet\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\inhand_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\config\\allegro_hand\\allegro_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\config\\allegro_hand\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\config\\allegro_hand\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\config\\allegro_hand\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\config\\allegro_hand\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\config\\allegro_hand\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\events.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\observations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\terminations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\commands\\commands_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\commands\\orientation_command.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\inhand\\mdp\\commands\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\lift_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\ik_abs_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\ik_rel_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\joint_pos_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\agents\\sb3_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\agents\\robomimic\\bc.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\config\\franka\\agents\\robomimic\\bcq.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\mdp\\observations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\mdp\\terminations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\lift\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\pick_place\\pickplace_gr1t2_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\pick_place\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\pick_place\\agents\\robomimic\\bc_rnn_low_dim.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\pick_place\\mdp\\observations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\pick_place\\mdp\\terminations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\pick_place\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\reach_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\ik_abs_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\ik_rel_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\joint_pos_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\osc_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\franka\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\ur_10\\joint_pos_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\ur_10\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\ur_10\\agents\\rl_games_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\ur_10\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\ur_10\\agents\\skrl_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\config\\ur_10\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\reach\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\stack_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\stack_instance_randomize_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\stack_ik_abs_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\stack_ik_rel_blueprint_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\stack_ik_rel_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\stack_ik_rel_instance_randomize_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\stack_ik_rel_visuomotor_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\stack_joint_pos_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\stack_joint_pos_instance_randomize_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\agents\\robomimic\\bc_rnn_image_84.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\config\\franka\\agents\\robomimic\\bc_rnn_low_dim.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\mdp\\franka_stack_events.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\mdp\\observations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\mdp\\terminations.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\manipulation\\stack\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\config\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\config\\anymal_c\\navigation_env_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\config\\anymal_c\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\config\\anymal_c\\agents\\rsl_rl_ppo_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\config\\anymal_c\\agents\\skrl_flat_ppo_cfg.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\config\\anymal_c\\agents\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\mdp\\pre_trained_policy_action.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\manager_based\\navigation\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\utils\\hydra.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\utils\\importer.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\utils\\parse_cfg.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\isaaclab_tasks\\utils\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\test_environments.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\test_environment_determinism.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\test_factory_environments.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\test_hydra.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\test_multi_agent_environments.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\test_record_video.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\benchmarking\\configs.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\benchmarking\\conftest.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\benchmarking\\test_environments_training.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\source\\isaaclab_tasks\\test\\benchmarking\\test_utils.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\conftest.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\install_deps.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\run_all_tests.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\run_train_envs.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\test_settings.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\cli.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\common.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\generator.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\__init__.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\extension\\pyproject.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\extension\\setup.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\extension\\ui_extension_example.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\extension\\config\\extension.toml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\external\\README.md\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\external\\.vscode\\extensions.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\external\\.vscode\\tasks.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\external\\.vscode\\tools\\launch.template.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\external\\.vscode\\tools\\settings.template.json\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\external\\.vscode\\tools\\setup_vscode.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\external\\docker\\docker-compose.yaml\n",
      "Used fallback chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\tasks\\manager-based_single-agent\\mdp\\rewards.py\n",
      "Used AST chunker\n",
      "📄 Ingesting: ./isaaclab_local\\tools\\template\\templates\\tasks\\manager-based_single-agent\\mdp\\__init__.py\n",
      "Used AST chunker\n",
      "✅ Total chunks ingested: 3336\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Run it ---\n",
    "documents = ingest_repo_files()\n",
    "print(f\"✅ Total chunks ingested: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='repos:\\n  - repo: https://github.com/python/black\\n    rev: 24.3.0\\n    hooks:\\n      - id: black\\n        args: [\"--line-length\", \"120\", \"--unstable\"]\\n  - repo: https://github.com/pycqa/flake8\\n    rev: 7.0.0\\n    hooks:\\n      - id: flake8\\n        additional_dependencies: [flake8-simplify, flake8-return]\\n  - repo: https://github.com/pre-commit/pre-commit-hooks\\n    rev: v4.5.0\\n    hooks:\\n      - id: trailing-whitespace\\n      - id: check-symlinks\\n      - id: destroyed-symlinks'),\n",
       " Document(metadata={}, page_content='- id: destroyed-symlinks\\n      - id: check-added-large-files\\n        args: [\"--maxkb=2000\"]  # restrict files more than 2 MB. Should use git-lfs instead.\\n      - id: check-yaml\\n      - id: check-merge-conflict\\n      - id: check-case-conflict\\n      - id: check-executables-have-shebangs\\n      - id: check-toml\\n      - id: end-of-file-fixer\\n      - id: check-shebang-scripts-are-executable\\n      - id: detect-private-key\\n      - id: debug-statements\\n  - repo: https://github.com/pycqa/isort'),\n",
       " Document(metadata={}, page_content='- repo: https://github.com/pycqa/isort\\n    rev: 5.13.2\\n    hooks:\\n      - id: isort\\n        name: isort (python)\\n        args: [\"--profile\", \"black\", \"--filter-files\"]\\n  - repo: https://github.com/asottile/pyupgrade\\n    rev: v3.15.1\\n    hooks:\\n      - id: pyupgrade\\n        args: [\"--py310-plus\"]\\n        # FIXME: This is a hack because Pytorch does not like: torch.Tensor | dict aliasing'),\n",
       " Document(metadata={}, page_content='exclude: \"source/isaaclab/isaaclab/envs/common.py|source/isaaclab/isaaclab/ui/widgets/image_plot.py|source/isaaclab_tasks/isaaclab_tasks/direct/humanoid_amp/motions/motion_loader.py\"\\n  - repo: https://github.com/codespell-project/codespell\\n    rev: v2.2.6\\n    hooks:\\n      - id: codespell\\n        additional_dependencies:\\n        - tomli\\n        exclude: \"CONTRIBUTORS.md\"\\n  # FIXME: Figure out why this is getting stuck under VPN.\\n  # - repo: https://github.com/RobertCraigie/pyright-python'),\n",
       " Document(metadata={}, page_content='#   rev: v1.1.315\\n  #   hooks:\\n  #   - id: pyright\\n  - repo: https://github.com/Lucas-C/pre-commit-hooks\\n    rev: v1.5.1\\n    hooks:\\n      - id: insert-license\\n        files: \\\\.py$\\n        args:\\n          # - --remove-header    # Remove existing license headers. Useful when updating license.\\n          - --license-filepath\\n          - .github/LICENSE_HEADER.txt\\n          - --use-current-year\\n        exclude: \"source/isaaclab_mimic/|scripts/imitation_learning/isaaclab_mimic/\"'),\n",
       " Document(metadata={}, page_content='# Apache 2.0 license for mimic files\\n  - repo: https://github.com/Lucas-C/pre-commit-hooks\\n    rev: v1.5.1\\n    hooks:\\n      - id: insert-license\\n        files: ^(source/isaaclab_mimic|scripts/imitation_learning/isaaclab_mimic)/.*\\\\.py$\\n        args:\\n          # - --remove-header    # Remove existing license headers. Useful when updating license.\\n          - --license-filepath\\n          - .github/LICENSE_HEADER_MIMIC.txt\\n          - --use-current-year'),\n",
       " Document(metadata={}, page_content='- --use-current-year\\n  - repo: https://github.com/pre-commit/pygrep-hooks\\n    rev: v1.10.0\\n    hooks:\\n      - id: rst-backticks\\n      - id: rst-directive-colons\\n      - id: rst-inline-touching-normal'),\n",
       " Document(metadata={}, page_content='# Contribution Guidelines\\n\\nIsaac Lab is a community maintained project. We wholeheartedly welcome contributions to the project to make\\nthe framework more mature and useful for everyone. These may happen in forms of bug reports, feature requests,\\ndesign proposals and more.\\n\\nFor general information on how to contribute see\\n<https://isaac-sim.github.io/IsaacLab/main/source/refs/contributing.html>.'),\n",
       " Document(metadata={}, page_content='# Isaac Lab Developers and Contributors\\n\\nThis is the official list of Isaac Lab Project developers and contributors.\\n\\nTo see the full list of contributors, please check the revision history in the source control.\\n\\nGuidelines for modifications:\\n\\n* Please keep the **lists sorted alphabetically**.\\n* Names should be added to this file as: *individual names* or *organizations*.\\n* E-mail addresses are tracked elsewhere to avoid spam.\\n\\n## Developers'),\n",
       " Document(metadata={}, page_content='## Developers\\n\\n* Boston Dynamics AI Institute, Inc.\\n* ETH Zurich\\n* NVIDIA Corporation & Affiliates\\n* University of Toronto\\n\\n---\\n\\n* Antonio Serrano-Muñoz\\n* David Hoeller\\n* Farbod Farshidian\\n* Hunter Hansen\\n* James Smith\\n* James Tigue\\n* Kelly (Yunrong) Guo\\n* Matthew Trepte\\n* Mayank Mittal\\n* Nikita Rudin\\n* Pascal Roth\\n* Sheikh Dawood\\n* Ossama Ahmed\\n\\n## Contributors'),\n",
       " Document(metadata={}, page_content='* Alessandro Assirelli\\n* Alice Zhou\\n* Amr Mousa\\n* Andrej Orsula\\n* Anton Bjørndahl Mortensen\\n* Arjun Bhardwaj\\n* Ashwin Varghese Kuruttukulam\\n* Bikram Pandit\\n* Bingjie Tang\\n* Brayden Zhang\\n* Cameron Upright\\n* Calvin Yu\\n* Cheng-Rong Lai\\n* Chenyu Yang\\n* Clemens Schwarke\\n* Connor Smith\\n* CY (Chien-Ying) Chen\\n* David Yang\\n* Dhananjay Shendre\\n* Dorsa Rohani\\n* Felipe Mohr\\n* Felix Yu\\n* Gary Lvov\\n* Giulio Romualdi\\n* Haoran Zhou\\n* HoJin Jeon\\n* Hongwei Xiong\\n* Hongyu Li\\n* Iretiayo Akinola\\n* Jack Zeng'),\n",
       " Document(metadata={}, page_content='* Hongyu Li\\n* Iretiayo Akinola\\n* Jack Zeng\\n* Jan Kerner\\n* Jean Tampon\\n* Jia Lin Yuan\\n* Jiakai Zhang\\n* Jinghuan Shang\\n* Jingzhou Liu\\n* Jinqi Wei\\n* Johnson Sun\\n* Kaixi Bao\\n* Kourosh Darvish\\n* Kousheek Chakraborty\\n* Lionel Gulich\\n* Louis Le Lay\\n* Lorenz Wellhausen\\n* Manuel Schweiger\\n* Masoud Moghani\\n* Michael Gussert\\n* Michael Noseworthy\\n* Miguel Alonso Jr\\n* Muhong Guo\\n* Nicola Loi\\n* Nuoyan Chen (Alvin)\\n* Nuralem Abizov\\n* Ori Gadot\\n* Oyindamola Omotuyi\\n* Özhan Özen\\n* Peter Du\\n* Pulkit Goyal'),\n",
       " Document(metadata={}, page_content='* Özhan Özen\\n* Peter Du\\n* Pulkit Goyal\\n* Qian Wan\\n* Qinxi Yu\\n* Rafael Wiltz\\n* Renaud Poncelet\\n* René Zurbrügg\\n* Ritvik Singh\\n* Rosario Scalise\\n* Ryley McCarroll\\n* Shafeef Omar\\n* Shundo Kishi\\n* Stefan Van de Mosselaer\\n* Stephan Pleines\\n* Tyler Lum\\n* Victor Khaustov\\n* Virgilio Gómez Lambo\\n* Vladimir Fokow\\n* Wei Yang\\n* Xavier Nal\\n* Yang Jin\\n* Yanzi Zhu\\n* Yijie Guo\\n* Yujian Zhang\\n* Yun Liu\\n* Zhengyu Zhang\\n* Ziqi Fan\\n* Zoe McCarthy'),\n",
       " Document(metadata={}, page_content='## Acknowledgements\\n\\n* Ajay Mandlekar\\n* Animesh Garg\\n* Buck Babich\\n* Gavriel State\\n* Hammad Mazhar\\n* Marco Hutter\\n* Yashraj Narang'),\n",
       " Document(metadata={}, page_content='channels:\\n  - conda-forge\\n  - defaults\\ndependencies:\\n  - python=3.10\\n  - importlib_metadata'),\n",
       " Document(metadata={}, page_content='[tool.isort]\\n\\npy_version = 310\\nline_length = 120\\ngroup_by_package = true\\n\\n# Files to skip\\nskip_glob = [\"docs/*\", \"logs/*\", \"_isaac_sim/*\", \".vscode/*\"]\\n\\n# Order of imports\\nsections = [\\n    \"FUTURE\",\\n    \"STDLIB\",\\n    \"THIRDPARTY\",\\n    \"ASSETS_FIRSTPARTY\",\\n    \"FIRSTPARTY\",\\n    \"EXTRA_FIRSTPARTY\",\\n    \"TASK_FIRSTPARTY\",\\n    \"LOCALFOLDER\",\\n]'),\n",
       " Document(metadata={}, page_content='# Extra standard libraries considered as part of python (permissive licenses\\nextra_standard_library = [\\n    \"numpy\",\\n    \"h5py\",\\n    \"open3d\",\\n    \"torch\",\\n    \"tensordict\",\\n    \"bpy\",\\n    \"matplotlib\",\\n    \"gymnasium\",\\n    \"gym\",\\n    \"scipy\",\\n    \"hid\",\\n    \"yaml\",\\n    \"prettytable\",\\n    \"toml\",\\n    \"trimesh\",\\n    \"tqdm\",\\n    \"torchvision\",\\n    \"transformers\",\\n    \"einops\" # Needed for transformers, doesn\\'t always auto-install\\n]\\n# Imports from Isaac Sim and Omniverse\\nknown_third_party = ['),\n",
       " Document(metadata={}, page_content='known_third_party = [\\n    \"isaacsim.core.api\",\\n    \"isaacsim.replicator.common\",\\n    \"omni.replicator.core\",\\n    \"pxr\",\\n    \"omni.kit.*\",\\n    \"warp\",\\n    \"carb\",\\n    \"Semantics\",\\n]\\n# Imports from this repository\\nknown_first_party = \"isaaclab\"\\nknown_assets_firstparty = \"isaaclab_assets\"\\nknown_extra_firstparty = [\\n    \"isaaclab_rl\",\\n    \"isaaclab_mimic\",\\n]\\nknown_task_firstparty = \"isaaclab_tasks\"\\n# Imports from the local folder\\nknown_local_folder = \"config\"'),\n",
       " Document(metadata={}, page_content='[tool.pyright]\\n\\ninclude = [\"source\", \"scripts\"]\\nexclude = [\\n    \"**/__pycache__\",\\n    \"**/_isaac_sim\",\\n    \"**/docs\",\\n    \"**/logs\",\\n    \".git\",\\n    \".vscode\",\\n]\\n\\ntypeCheckingMode = \"basic\"\\npythonVersion = \"3.10\"\\npythonPlatform = \"Linux\"\\nenableTypeIgnoreComments = true'),\n",
       " Document(metadata={}, page_content='# This is required as the CI pre-commit does not download the module (i.e. numpy, torch, prettytable)\\n# Therefore, we have to ignore missing imports\\nreportMissingImports = \"none\"\\n# This is required to ignore for type checks of modules with stubs missing.\\nreportMissingModuleSource = \"none\" # -> most common: prettytable in mdp managers'),\n",
       " Document(metadata={}, page_content='reportGeneralTypeIssues = \"none\"       # -> raises 218 errors (usage of literal MISSING in dataclasses)\\nreportOptionalMemberAccess = \"warning\" # -> raises 8 errors\\nreportPrivateUsage = \"warning\"'),\n",
       " Document(metadata={}, page_content='[tool.codespell]\\nskip = \\'*.usd,*.svg,*.png,_isaac_sim*,*.bib,*.css,*/_build\\'\\nquiet-level = 0\\n# the world list should always have words in lower case\\nignore-words-list = \"haa,slq,collapsable,buss\"\\n# todo: this is hack to deal with incorrect spelling of \"Environment\" in the Isaac Sim grid world asset\\nexclude-file = \"source/isaaclab/isaaclab/sim/spawners/from_files/from_files.py\"'),\n",
       " Document(metadata={}, page_content='![Isaac Lab](docs/source/_static/isaaclab.jpg)\\n\\n---\\n\\n# Isaac Lab'),\n",
       " Document(metadata={}, page_content='[![IsaacSim](https://img.shields.io/badge/IsaacSim-4.5.0-silver.svg)](https://docs.isaacsim.omniverse.nvidia.com/latest/index.html)\\n[![Python](https://img.shields.io/badge/python-3.10-blue.svg)](https://docs.python.org/3/whatsnew/3.10.html)\\n[![Linux platform](https://img.shields.io/badge/platform-linux--64-orange.svg)](https://releases.ubuntu.com/20.04/)\\n[![Windows platform](https://img.shields.io/badge/platform-windows--64-orange.svg)](https://www.microsoft.com/en-us/)'),\n",
       " Document(metadata={}, page_content='[![pre-commit](https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/pre-commit.yaml?logo=pre-commit&logoColor=white&label=pre-commit&color=brightgreen)](https://github.com/isaac-sim/IsaacLab/actions/workflows/pre-commit.yaml)\\n[![docs status](https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/docs.yaml?label=docs&color=brightgreen)](https://github.com/isaac-sim/IsaacLab/actions/workflows/docs.yaml)'),\n",
       " Document(metadata={}, page_content='[![License](https://img.shields.io/badge/license-BSD--3-yellow.svg)](https://opensource.org/licenses/BSD-3-Clause)\\n[![License](https://img.shields.io/badge/license-Apache--2.0-yellow.svg)](https://opensource.org/license/apache-2-0)'),\n",
       " Document(metadata={}, page_content='**Isaac Lab** is a GPU-accelerated, open-source framework designed to unify and simplify robotics research workflows, such as reinforcement learning, imitation learning, and motion planning. Built on [NVIDIA Isaac Sim](https://docs.isaacsim.omniverse.nvidia.com/latest/index.html), it combines fast and accurate physics and sensor simulation, making it an ideal choice for sim-to-real transfer in robotics.'),\n",
       " Document(metadata={}, page_content=\"Isaac Lab provides developers with a range of essential features for accurate sensor simulation, such as RTX-based cameras, LIDAR, or contact sensors. The framework's GPU acceleration enables users to run complex simulations and computations faster, which is key for iterative processes like reinforcement learning and data-intensive tasks. Moreover, Isaac Lab can run locally or be distributed across the cloud, offering flexibility for large-scale deployments.\\n\\n## Key Features\"),\n",
       " Document(metadata={}, page_content='Isaac Lab offers a comprehensive set of tools and environments designed to facilitate robot learning:\\n- **Robots**: A diverse collection of robots, from manipulators, quadrupeds, to humanoids, with 16 commonly available models.\\n- **Environments**: Ready-to-train implementations of more than 30 environments, which can be trained with popular reinforcement learning frameworks such as RSL RL, SKRL, RL Games, or Stable Baselines. We also support multi-agent reinforcement learning.'),\n",
       " Document(metadata={}, page_content='- **Physics**: Rigid bodies, articulated systems, deformable objects\\n- **Sensors**: RGB/depth/segmentation cameras, camera annotations, IMU, contact sensors, ray casters.'),\n",
       " Document(metadata={}, page_content='## Getting Started\\n\\nOur [documentation page](https://isaac-sim.github.io/IsaacLab) provides everything you need to get started, including detailed tutorials and step-by-step guides. Follow these links to learn more about:'),\n",
       " Document(metadata={}, page_content='- [Installation steps](https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html#local-installation)\\n- [Reinforcement learning](https://isaac-sim.github.io/IsaacLab/main/source/overview/reinforcement-learning/rl_existing_scripts.html)\\n- [Tutorials](https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html)\\n- [Available environments](https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html)\\n\\n\\n## Contributing to Isaac Lab'),\n",
       " Document(metadata={}, page_content='## Contributing to Isaac Lab\\n\\nWe wholeheartedly welcome contributions from the community to make this framework mature and useful for everyone.\\nThese may happen as bug reports, feature requests, or code contributions. For details, please check our\\n[contribution guidelines](https://isaac-sim.github.io/IsaacLab/main/source/refs/contributing.html).\\n\\n## Show & Tell: Share Your Inspiration'),\n",
       " Document(metadata={}, page_content=\"## Show & Tell: Share Your Inspiration\\n\\nWe encourage you to utilize our [Show & Tell](https://github.com/isaac-sim/IsaacLab/discussions/categories/show-and-tell) area in the\\n`Discussions` section of this repository. This space is designed for you to:\\n\\n* Share the tutorials you've created\\n* Showcase your learning content\\n* Present exciting projects you've developed\"),\n",
       " Document(metadata={}, page_content=\"By sharing your work, you'll inspire others and contribute to the collective knowledge\\nof our community. Your contributions can spark new ideas and collaborations, fostering\\ninnovation in robotics and simulation.\\n\\n## Troubleshooting\\n\\nPlease see the [troubleshooting](https://isaac-sim.github.io/IsaacLab/main/source/refs/troubleshooting.html) section for\\ncommon fixes or [submit an issue](https://github.com/isaac-sim/IsaacLab/issues).\"),\n",
       " Document(metadata={}, page_content='For issues related to Isaac Sim, we recommend checking its [documentation](https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/overview.html)\\nor opening a question on its [forums](https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67).\\n\\n## Support'),\n",
       " Document(metadata={}, page_content='## Support\\n\\n* Please use GitHub [Discussions](https://github.com/isaac-sim/IsaacLab/discussions) for discussing ideas, asking questions, and requests for new features.\\n* Github [Issues](https://github.com/isaac-sim/IsaacLab/issues) should only be used to track executable pieces of work with a definite scope and a clear deliverable. These can be fixing bugs, documentation issues, new features, or general updates.\\n\\n## Connect with the NVIDIA Omniverse Community'),\n",
       " Document(metadata={}, page_content=\"## Connect with the NVIDIA Omniverse Community\\n\\nHave a project or resource you'd like to share more widely? We'd love to hear from you! Reach out to the\\nNVIDIA Omniverse Community team at OmniverseCommunity@nvidia.com to discuss potential opportunities\\nfor broader dissemination of your work.\\n\\nJoin us in building a vibrant, collaborative ecosystem where creativity and technology intersect. Your\\ncontributions can make a significant impact on the Isaac Lab community and beyond!\\n\\n## License\"),\n",
       " Document(metadata={}, page_content='## License\\n\\nThe Isaac Lab framework is released under [BSD-3 License](LICENSE). The `isaaclab_mimic` extension and its corresponding standalone scripts are released under [Apache 2.0](LICENSE-mimic). The license files of its dependencies and assets are present in the [`docs/licenses`](docs/licenses) directory.\\n\\n## Acknowledgement'),\n",
       " Document(metadata={}, page_content='## Acknowledgement\\n\\nIsaac Lab development initiated from the [Orbit](https://isaac-orbit.github.io/) framework. We would appreciate if you would cite it in academic publications as well:'),\n",
       " Document(metadata={}, page_content='```\\n@article{mittal2023orbit,\\n   author={Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and Yuan, Jia Lin and Singh, Ritvik and Guo, Yunrong and Mazhar, Hammad and Mandlekar, Ajay and Babich, Buck and State, Gavriel and Hutter, Marco and Garg, Animesh},\\n   journal={IEEE Robotics and Automation Letters},\\n   title={Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments},\\n   year={2023},\\n   volume={8},\\n   number={6},'),\n",
       " Document(metadata={}, page_content='year={2023},\\n   volume={8},\\n   number={6},\\n   pages={3740-3747},\\n   doi={10.1109/LRA.2023.3270034}\\n}\\n```'),\n",
       " Document(metadata={}, page_content='# Security\\n\\nNVIDIA is dedicated to the security and trust of our software products and services, including all source code\\nrepositories managed through our organization.\\n\\nIf you need to report a security issue, please use the appropriate contact points outlined below. **Please do\\nnot report security vulnerabilities through GitHub.**\\n\\n## Reporting Potential Security Vulnerability in an NVIDIA Product\\n\\nTo report a potential security vulnerability in any NVIDIA product:'),\n",
       " Document(metadata={}, page_content='- Web: [Security Vulnerability Submission Form](https://www.nvidia.com/object/submit-security-vulnerability.html)\\n\\n- E-Mail: psirt@nvidia.com\\n\\n    - We encourage you to use the following PGP key for secure email communication: [NVIDIA public PGP Key for communication](https://www.nvidia.com/en-us/security/pgp-key)\\n\\n    - Please include the following information:\\n\\n    - Product/Driver name and version/branch that contains the vulnerability'),\n",
       " Document(metadata={}, page_content='- Type of vulnerability (code execution, denial of service, buffer overflow, etc.)\\n\\n    - Instructions to reproduce the vulnerability\\n\\n    - Proof-of-concept or exploit code\\n\\n    - Potential impact of the vulnerability, including how an attacker could exploit the vulnerability'),\n",
       " Document(metadata={}, page_content='While NVIDIA currently does not have a bug bounty program, we do offer acknowledgement when an\\nexternally reported security issue is addressed under our coordinated vulnerability disclosure policy. Please\\nvisit our [Product Security Incident Response Team (PSIRT)](https://www.nvidia.com/en-us/security/psirt-policies/)\\npolicies page for more information.\\n\\n## NVIDIA Product Security'),\n",
       " Document(metadata={}, page_content=\"## NVIDIA Product Security\\n\\nFor all security-related concerns, please visit NVIDIA's Product Security portal at: https://www.nvidia.com/en-us/security\"),\n",
       " Document(metadata={}, page_content='version: 0.2\\n\\nphases:\\n  install:\\n    runtime-versions:\\n      nodejs: 14\\n  pre_build:\\n    commands:\\n      - git config --global user.name \"Isaac LAB CI Bot\"\\n      - git config --global user.email \"isaac-lab-ci-bot@nvidia.com\"\\n  build:\\n    commands:\\n      - git remote set-url origin https://github.com/${TARGET_REPO}.git\\n      - git checkout $SOURCE_BRANCH\\n      - git push --force https://$GITHUB_TOKEN@github.com/${TARGET_REPO}.git $SOURCE_BRANCH:$TARGET_BRANCH'),\n",
       " Document(metadata={}, page_content='version: 0.2\\n\\nphases:\\n  build:\\n    commands:\\n      - echo \"Building and pushing Docker image\"\\n      - |\\n        # Determine branch name or use fallback\\n        if [ -n \"$CODEBUILD_WEBHOOK_HEAD_REF\" ]; then\\n          BRANCH_NAME=$(echo $CODEBUILD_WEBHOOK_HEAD_REF | sed \\'s/refs\\\\/heads\\\\///\\')\\n        elif [ -n \"$CODEBUILD_SOURCE_VERSION\" ]; then\\n          BRANCH_NAME=$CODEBUILD_SOURCE_VERSION\\n        else\\n          BRANCH_NAME=\"unknown\"\\n        fi'),\n",
       " Document(metadata={}, page_content='# Replace \\'/\\' with \\'-\\' and remove any invalid characters for Docker tag\\n        SAFE_BRANCH_NAME=$(echo $BRANCH_NAME | sed \\'s/[^a-zA-Z0-9._-]/-/g\\')\\n\\n        # Use \"latest\" if branch name is empty or only contains invalid characters\\n        if [ -z \"$SAFE_BRANCH_NAME\" ]; then\\n          SAFE_BRANCH_NAME=\"latest\"\\n        fi'),\n",
       " Document(metadata={}, page_content='# Get the git repository short name\\n        REPO_SHORT_NAME=$(basename -s .git `git config --get remote.origin.url`)\\n        if [ -z \"$REPO_SHORT_NAME\" ]; then\\n          REPO_SHORT_NAME=\"verification\"\\n        fi'),\n",
       " Document(metadata={}, page_content='# Parse the env variable string into an array\\n        mapfile -d \\' \\' -t IMAGE_BASE_VERSIONS <<< \"$ISAACSIM_BASE_VERSIONS_STRING\"\\n        for IMAGE_BASE_VERSION in \"${IMAGE_BASE_VERSIONS[@]}\"; do\\n          IMAGE_BASE_VERSION=$(echo \"$IMAGE_BASE_VERSION\" | tr -d \\'[:space:]\\')\\n          # Combine repo short name and branch name for the tag\\n          COMBINED_TAG=\"${REPO_SHORT_NAME}-${SAFE_BRANCH_NAME}-${IMAGE_BASE_VERSION}\"'),\n",
       " Document(metadata={}, page_content='docker login -u \\\\$oauthtoken -p $NGC_TOKEN nvcr.io\\n          docker build -t $IMAGE_NAME:$COMBINED_TAG \\\\\\n            --build-arg ISAACSIM_BASE_IMAGE_ARG=$ISAACSIM_BASE_IMAGE \\\\\\n            --build-arg ISAACSIM_VERSION_ARG=$IMAGE_BASE_VERSION \\\\\\n            --build-arg ISAACSIM_ROOT_PATH_ARG=/isaac-sim \\\\\\n            --build-arg ISAACLAB_PATH_ARG=/workspace/isaaclab \\\\\\n            --build-arg DOCKER_USER_HOME_ARG=/root \\\\\\n            -f docker/Dockerfile.base .'),\n",
       " Document(metadata={}, page_content='-f docker/Dockerfile.base .\\n          docker push $IMAGE_NAME:$COMBINED_TAG\\n          docker tag $IMAGE_NAME:$COMBINED_TAG $IMAGE_NAME:$COMBINED_TAG-b$CODEBUILD_BUILD_NUMBER\\n          docker push $IMAGE_NAME:$COMBINED_TAG-b$CODEBUILD_BUILD_NUMBER\\n        done'),\n",
       " Document(metadata={}, page_content='# Copyright (c) 2022-2025, The Isaac Lab Project Developers.\\n# All rights reserved.\\n#\\n# SPDX-License-Identifier: BSD-3-Clause\\n#\\n# This buildspec file defines the CI/CD pipeline for IsaacLab.\\n# It runs tests on an EC2 instance with GPU support and uses Docker BuildKit\\n# for efficient builds with S3 caching.\\n#\\n# Required environment variables:\\n# - ISAACSIM_BASE_IMAGE: Base image for IsaacSim\\n# - ISAACSIM_BASE_VERSION: Version of IsaacSim to use\\n#\\n# Required AWS Secrets:'),\n",
       " Document(metadata={}, page_content='#\\n# Required AWS Secrets:\\n# - NGC_TOKEN: NVIDIA NGC authentication token\\n# - SSH_KEY: SSH private key for EC2 access\\n# - SSH_PUBLIC_KEY: SSH public key for EC2 access'),\n",
       " Document(metadata={}, page_content='version: 0.2\\n\\nenv:\\n  variables:\\n    # Build configuration\\n    MAX_RETRIES: \"5\"\\n    RETRY_WAIT_TIME: \"30\"\\n\\n    # EC2 configuration\\n    INSTANCE_TYPE: \"g5.2xlarge\"\\n    VOLUME_SIZE: \"500\"\\n    REGION: \"us-west-2\"\\n    AZ: \"us-west-2a\"\\n\\n    # Docker and cache configuration\\n    ECR_REPOSITORY: \"isaaclab-dev\"\\n    CACHE_BUCKET_PREFIX: \"isaaclab-build-cache\"\\n\\n    # Docker versions\\n    BUILDX_VERSION: \"0.11.2\"'),\n",
       " Document(metadata={}, page_content='secrets-manager:\\n    NGC_TOKEN: \"production/ngc/token\"\\n    SSH_KEY: \"production/ssh/isaaclab\"\\n    SSH_PUBLIC_KEY: \"production/ssh/isaaclab\"\\n\\nphases:\\n  install:\\n    runtime-versions:\\n      python: 3.9\\n    commands:\\n      - echo \"Installing required packages...\"\\n      - pip install awscli boto3'),\n",
       " Document(metadata={}, page_content='pre_build:\\n    commands:\\n      - |\\n        # Validate required environment variables\\n        if [ -z \"$ISAACSIM_BASE_IMAGE\" ]; then\\n          echo \"Error: Required environment variable ISAACSIM_BASE_IMAGE is not set\"\\n          exit 1\\n        fi\\n        if [ -z \"$ISAACSIM_BASE_VERSION\" ]; then\\n          echo \"Error: Required environment variable ISAACSIM_BASE_VERSION is not set\"\\n          exit 1\\n        fi'),\n",
       " Document(metadata={}, page_content='# Get AWS account ID\\n        AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\\n        if [ -z \"$AWS_ACCOUNT_ID\" ]; then\\n          echo \"Error: Failed to get AWS account ID\"\\n          exit 1\\n        fi\\n\\n        # Create ECR repository if it doesn\\'t exist\\n        aws ecr describe-repositories --repository-names $ECR_REPOSITORY || \\\\\\n          aws ecr create-repository --repository-name $ECR_REPOSITORY'),\n",
       " Document(metadata={}, page_content='# Configure ECR repository lifecycle policy\\n        aws ecr put-lifecycle-policy \\\\\\n          --repository-name $ECR_REPOSITORY \\\\\\n          --lifecycle-policy-text \\'{\\n            \"rules\": [\\n              {\\n                \"rulePriority\": 1,\\n                \"description\": \"Expire images older than 2 weeks\",\\n                \"selection\": {\\n                  \"tagStatus\": \"any\",\\n                  \"countType\": \"sinceImagePushed\",\\n                  \"countUnit\": \"days\",'),\n",
       " Document(metadata={}, page_content='\"countUnit\": \"days\",\\n                  \"countNumber\": 14\\n                },\\n                \"action\": {\\n                  \"type\": \"expire\"\\n                }\\n              }\\n            ]\\n          }\\''),\n",
       " Document(metadata={}, page_content='# Create S3 bucket for BuildKit cache if it doesn\\'t exist\\n        CACHE_BUCKET=\"${CACHE_BUCKET_PREFIX}-${AWS_ACCOUNT_ID}\"\\n        aws s3api head-bucket --bucket $CACHE_BUCKET || \\\\\\n          aws s3 mb s3://$CACHE_BUCKET --region $REGION'),\n",
       " Document(metadata={}, page_content='# Configure S3 bucket lifecycle rule for cache expiration\\n        aws s3api put-bucket-lifecycle-configuration \\\\\\n          --bucket $CACHE_BUCKET \\\\\\n          --lifecycle-configuration \\'{\\n            \"Rules\": [\\n              {\\n                \"ID\": \"ExpireBuildKitCache\",\\n                \"Status\": \"Enabled\",\\n                \"Filter\": {\\n                  \"Prefix\": \"\"\\n                },\\n                \"Expiration\": {\\n                  \"Days\": 14\\n                }\\n              }'),\n",
       " Document(metadata={}, page_content=\"}\\n              }\\n            ]\\n          }'\"),\n",
       " Document(metadata={}, page_content='echo \"Launching EC2 instance to run tests...\"\\n        INSTANCE_ID=$(aws ec2 run-instances \\\\\\n          --image-id ami-0e6cc441f9f4caab3 \\\\\\n          --count 1 \\\\\\n          --instance-type $INSTANCE_TYPE \\\\\\n          --key-name production/ssh/isaaclab \\\\\\n          --security-group-ids sg-02617e4b8916794c4 \\\\\\n          --subnet-id subnet-0907ceaeb40fd9eac \\\\\\n          --iam-instance-profile Name=\"IsaacLabBuildRole\" \\\\'),\n",
       " Document(metadata={}, page_content='--block-device-mappings \"[{\\\\\"DeviceName\\\\\":\\\\\"/dev/sda1\\\\\",\\\\\"Ebs\\\\\":{\\\\\"VolumeSize\\\\\":$VOLUME_SIZE}}]\" \\\\\\n          --output text \\\\\\n          --query \\'Instances[0].InstanceId\\')'),\n",
       " Document(metadata={}, page_content='echo \"Waiting for instance $INSTANCE_ID to be running...\"\\n        aws ec2 wait instance-running --instance-ids $INSTANCE_ID\\n\\n        echo \"Getting instance IP address...\"\\n        EC2_INSTANCE_IP=$(aws ec2 describe-instances \\\\\\n          --filters \"Name=instance-state-name,Values=running\" \"Name=instance-id,Values=$INSTANCE_ID\" \\\\\\n          --query \\'Reservations[*].Instances[*].[PrivateIpAddress]\\' \\\\\\n          --output text)'),\n",
       " Document(metadata={}, page_content='echo \"Setting up SSH configuration...\"\\n        mkdir -p ~/.ssh\\n        aws ec2 describe-key-pairs --include-public-key --key-name production/ssh/isaaclab \\\\\\n          --query \\'KeyPairs[0].PublicKey\\' --output text > ~/.ssh/id_rsa.pub\\n        echo \"$SSH_KEY\" > ~/.ssh/id_rsa\\n        chmod 400 ~/.ssh/id_*\\n        echo \"Host $EC2_INSTANCE_IP\\\\n\\\\tStrictHostKeyChecking no\\\\n\\\\tUserKnownHostsFile=/dev/null\\\\n\" >> ~/.ssh/config'),\n",
       " Document(metadata={}, page_content='echo \"Sending SSH public key to instance...\"\\n        aws ec2-instance-connect send-ssh-public-key \\\\\\n          --instance-id $INSTANCE_ID \\\\\\n          --availability-zone $AZ \\\\\\n          --ssh-public-key file://~/.ssh/id_rsa.pub \\\\\\n          --instance-os-user ubuntu\\n\\n  build:\\n    commands:\\n      - |\\n        #!/bin/sh\\n        set -e\\n\\n        echo \"Running tests on EC2 instance...\"\\n        SRC_DIR=$(basename $CODEBUILD_SRC_DIR)\\n        cd ..'),\n",
       " Document(metadata={}, page_content='# Retry SCP with exponential backoff\\n        retry_count=0\\n        wait_time=$RETRY_WAIT_TIME\\n\\n        while [ $retry_count -lt $MAX_RETRIES ]; do\\n          if [ $retry_count -gt 0 ]; then\\n            wait_time=$((wait_time * 2))\\n            echo \"Retry attempt $((retry_count + 1))/$MAX_RETRIES. Waiting $wait_time seconds...\"\\n            sleep $wait_time\\n          fi'),\n",
       " Document(metadata={}, page_content='if scp -o ConnectTimeout=10 -o StrictHostKeyChecking=no -r $SRC_DIR ubuntu@$EC2_INSTANCE_IP:~; then\\n            echo \"SCP command succeeded\"\\n            break\\n          fi\\n\\n          retry_count=$((retry_count + 1))\\n        done\\n\\n        if [ $retry_count -eq $MAX_RETRIES ]; then\\n          echo \"SCP command failed after $MAX_RETRIES attempts\"\\n          exit 1\\n        fi\\n\\n        # Get ECR login token\\n        ECR_LOGIN_TOKEN=$(aws ecr get-login-password --region $REGION)'),\n",
       " Document(metadata={}, page_content='# Run tests with proper error handling and Docker caching\\n        ssh -o ConnectTimeout=10 -o StrictHostKeyChecking=no ubuntu@$EC2_INSTANCE_IP \"\\n          set -e'),\n",
       " Document(metadata={}, page_content='# Install Docker with BuildKit support\\n          echo \\'Installing Docker with BuildKit support...\\'\\n          sudo apt-get update\\n          sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common\\n          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\\n          sudo add-apt-repository \\\\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\\$(lsb_release -cs) stable\\\\\"\\n          sudo apt-get update'),\n",
       " Document(metadata={}, page_content='sudo apt-get update\\n          sudo apt-get install -y docker-ce docker-ce-cli containerd.io'),\n",
       " Document(metadata={}, page_content='# Enable BuildKit at daemon level\\n          sudo mkdir -p /etc/docker\\n          echo \\'{\\\\\"features\\\\\":{\\\\\"buildkit\\\\\":true}}\\' | sudo tee /etc/docker/daemon.json'),\n",
       " Document(metadata={}, page_content=\"# Install Docker Buildx\\n          echo 'Installing Docker Buildx...'\\n          mkdir -p ~/.docker/cli-plugins/\\n          curl -SL https://github.com/docker/buildx/releases/download/v$BUILDX_VERSION/buildx-v$BUILDX_VERSION.linux-amd64 -o ~/.docker/cli-plugins/docker-buildx\\n          chmod a+x ~/.docker/cli-plugins/docker-buildx\\n\\n          # Add current user to docker group\\n          sudo usermod -aG docker ubuntu\\n          newgrp docker\"),\n",
       " Document(metadata={}, page_content='echo \\'Logging into NGC...\\'\\n          docker login -u \\\\\\\\\\\\$oauthtoken -p $NGC_TOKEN nvcr.io\\n\\n          # Login to ECR using token from CodeBuild\\n          echo \\\\\"$ECR_LOGIN_TOKEN\\\\\" | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\\n\\n          cd $SRC_DIR\\n          echo \\'Building Docker image with BuildKit caching...\\'\\n\\n          # Configure BuildKit environment\\n          export DOCKER_BUILDKIT=1\\n          export BUILDKIT_INLINE_CACHE=1'),\n",
       " Document(metadata={}, page_content='# Create a new builder instance with S3 cache support\\n          docker buildx create --name mybuilder --driver docker-container --bootstrap\\n          docker buildx use mybuilder'),\n",
       " Document(metadata={}, page_content='# Build with BuildKit and S3 cache\\n          if docker pull $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$ECR_REPOSITORY:latest 2>/dev/null; then\\n            echo \"Using existing image for cache...\"\\n            docker buildx build --progress=plain --platform linux/amd64 -t isaac-lab-dev \\\\\\n              --cache-from type=registry,ref=$AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$ECR_REPOSITORY:latest \\\\'),\n",
       " Document(metadata={}, page_content='--cache-to type=s3,region=$REGION,bucket=$CACHE_BUCKET,mode=max,ignore-error=true \\\\\\n              --build-arg ISAACSIM_BASE_IMAGE_ARG=$ISAACSIM_BASE_IMAGE \\\\\\n              --build-arg ISAACSIM_VERSION_ARG=$ISAACSIM_BASE_VERSION \\\\\\n              --build-arg ISAACSIM_ROOT_PATH_ARG=/isaac-sim \\\\\\n              --build-arg ISAACLAB_PATH_ARG=/workspace/isaaclab \\\\\\n              --build-arg DOCKER_USER_HOME_ARG=/root \\\\\\n              -f docker/Dockerfile.base \\\\\\n              --load .'),\n",
       " Document(metadata={}, page_content='--load .\\n          else\\n            echo \"No existing image found, building without cache-from...\"\\n            docker buildx build --progress=plain --platform linux/amd64 -t isaac-lab-dev \\\\\\n              --cache-to type=s3,region=$REGION,bucket=$CACHE_BUCKET,mode=max,ignore-error=true \\\\\\n              --build-arg ISAACSIM_BASE_IMAGE_ARG=$ISAACSIM_BASE_IMAGE \\\\\\n              --build-arg ISAACSIM_VERSION_ARG=$ISAACSIM_BASE_VERSION \\\\'),\n",
       " Document(metadata={}, page_content='--build-arg ISAACSIM_ROOT_PATH_ARG=/isaac-sim \\\\\\n              --build-arg ISAACLAB_PATH_ARG=/workspace/isaaclab \\\\\\n              --build-arg DOCKER_USER_HOME_ARG=/root \\\\\\n              -f docker/Dockerfile.base \\\\\\n              --load .\\n          fi'),\n",
       " Document(metadata={}, page_content='echo \\'Running tests...\\'\\n          TEST_EXIT_CODE=0\\n          docker run --rm --entrypoint bash --gpus all --network=host \\\\\\n            --name isaac-lab-test -v ~/$SRC_DIR/reports:/workspace/IsaacLab/tests isaac-lab-dev \\\\\\n            /isaac-sim/python.sh -m \\\\\\n            pytest tools -v || TEST_EXIT_CODE=$?\\n\\n          echo \"Test exit code: $TEST_EXIT_CODE\" > ~/$SRC_DIR/test_exit_code.txt\\n        \" || { echo \"Test execution failed\"; exit 1; }'),\n",
       " Document(metadata={}, page_content='echo \"Copying test reports...\"\\n        mkdir -p $CODEBUILD_SRC_DIR/reports\\n        scp -o ConnectTimeout=10 -o StrictHostKeyChecking=no -r ubuntu@$EC2_INSTANCE_IP:~/$SRC_DIR/reports/test-reports.xml $CODEBUILD_SRC_DIR/reports/\\n        scp -o ConnectTimeout=10 -o StrictHostKeyChecking=no ubuntu@$EC2_INSTANCE_IP:~/$SRC_DIR/test_exit_code.txt $CODEBUILD_SRC_DIR/'),\n",
       " Document(metadata={}, page_content='if [ \"$(cat $CODEBUILD_SRC_DIR/test_exit_code.txt)\" != \"0\" ]; then\\n          echo \"Tests failed with exit code $(cat $CODEBUILD_SRC_DIR/test_exit_code.txt)\"\\n          exit 1\\n        fi\\n\\n  post_build:\\n    commands:\\n      - |\\n        echo \"Cleaning up resources...\"\\n        if [ ! -z \"$INSTANCE_ID\" ]; then\\n          echo \"Terminating EC2 instance $INSTANCE_ID...\"\\n          aws ec2 terminate-instances --instance-ids $INSTANCE_ID || true\\n        fi'),\n",
       " Document(metadata={}, page_content=\"reports:\\n  pytest_reports:\\n    files:\\n      - 'reports/test-reports.xml'\\n    base-directory: '.'\\n    file-format: JUNITXML\\n\\ncache:\\n  paths:\\n    - '/root/.cache/pip/**/*'\\n    - '/root/.docker/**/*'\\n    - '/root/.aws/**/*'\"),\n",
       " Document(metadata={}, page_content='# Description\\n\\n<!--\\nThank you for your interest in sending a pull request. Please make sure to check the contribution guidelines.\\n\\nLink: https://isaac-sim.github.io/IsaacLab/main/source/refs/contributing.html\\n-->\\n\\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context.\\nList any dependencies that are required for this change.\\n\\nFixes # (issue)'),\n",
       " Document(metadata={}, page_content='Fixes # (issue)\\n\\n<!-- As a practice, it is recommended to open an issue to have discussions on the proposed pull request.\\nThis makes it easier for the community to keep track of what is being developed or added, and if a given feature\\nis demanded by more than one party. -->\\n\\n## Type of change\\n\\n<!-- As you go through the list, delete the ones that are not applicable. -->'),\n",
       " Document(metadata={}, page_content='- Bug fix (non-breaking change which fixes an issue)\\n- New feature (non-breaking change which adds functionality)\\n- Breaking change (fix or feature that would cause existing functionality to not work as expected)\\n- This change requires a documentation update\\n\\n## Screenshots\\n\\nPlease attach before and after screenshots of the change if applicable.\\n\\n<!--\\nExample:\\n\\n| Before | After |\\n| ------ | ----- |\\n| _gif/png before_ | _gif/png after_ |'),\n",
       " Document(metadata={}, page_content='To upload images to a PR -- simply drag and drop an image while in edit mode and it should upload the image directly. You can then paste that source into the above before/after sections.\\n-->\\n\\n## Checklist'),\n",
       " Document(metadata={}, page_content=\"## Checklist\\n\\n- [ ] I have run the [`pre-commit` checks](https://pre-commit.com/) with `./isaaclab.sh --format`\\n- [ ] I have made corresponding changes to the documentation\\n- [ ] My changes generate no new warnings\\n- [ ] I have added tests that prove my fix is effective or that my feature works\\n- [ ] I have updated the changelog and the corresponding version in the extension's `config/extension.toml` file\\n- [ ] I have added my name to the `CONTRIBUTORS.md` or my name already exists there\"),\n",
       " Document(metadata={}, page_content='<!--\\nAs you go through the checklist above, you can mark something as done by putting an x character in it\\n\\nFor example,\\n- [x] I have done this task\\n- [ ] I have not done this task\\n-->'),\n",
       " Document(metadata={}, page_content='# Configuration for probot-stale - https://github.com/probot/stale\\n\\n# Number of days of inactivity before an Issue or Pull Request becomes stale\\ndaysUntilStale: 60\\n\\n# Number of days of inactivity before an Issue or Pull Request with the stale label is closed.\\n# Set to false to disable. If disabled, issues still need to be closed manually, but will remain marked as stale.\\ndaysUntilClose: 14'),\n",
       " Document(metadata={}, page_content='# Only issues or pull requests with all of these labels are check if stale. Defaults to `[]` (disabled)\\nonlyLabels:\\n  - more-information-needed\\n\\n# Issues or Pull Requests with these labels will never be considered stale. Set to `[]` to disable\\nexemptLabels:\\n  - pinned\\n  - security\\n  - \"[Status] Maybe Later\"\\n\\n# Set to true to ignore issues in a project (defaults to false)\\nexemptProjects: true\\n\\n# Set to true to ignore issues in a milestone (defaults to false)\\nexemptMilestones: true'),\n",
       " Document(metadata={}, page_content='# Set to true to ignore issues with an assignee (defaults to false)\\nexemptAssignees: true\\n\\n# Label to use when marking as stale\\nstaleLabel: stale\\n\\n# Comment to post when marking as stale. Set to `false` to disable\\nmarkComment: >\\n  This issue has been automatically marked as stale because it has not had\\n  recent activity. It will be closed if no further activity occurs. Thank you\\n  for your contributions.'),\n",
       " Document(metadata={}, page_content='# Comment to post when removing the stale label.\\n# unmarkComment: >\\n#   Your comment here.\\n\\n# Comment to post when closing a stale Issue or Pull Request.\\n# closeComment: >\\n#   Your comment here.\\n\\n# Limit the number of actions per hour, from 1-30. Default is 30\\nlimitPerRun: 30\\n\\n# Limit to only `issues` or `pulls`\\nonly: issues'),\n",
       " Document(metadata={}, page_content=\"# Limit to only `issues` or `pulls`\\nonly: issues\\n\\n# Optionally, specify configuration settings that are specific to just 'issues' or 'pulls':\\n# pulls:\\n#   daysUntilStale: 30\\n#   markComment: >\\n#     This pull request has been automatically marked as stale because it has not had\\n#     recent activity. It will be closed if no further activity occurs. Thank you\\n#     for your contributions.\\n\\n# issues:\\n#   exemptLabels:\\n#     - confirmed\"),\n",
       " Document(metadata={}, page_content='---\\nname: Bug Report\\nabout: Submit a bug report\\ntitle: \"[Bug Report] Bug title\"\\n\\n---\\n\\nIf you are submitting a bug report, please fill in the following details and use the tag [bug].\\n\\n### Describe the bug\\n\\nA clear and concise description of what the bug is.\\n\\n### Steps to reproduce\\n\\nPlease try to provide a minimal example to reproduce the bug. Error messages and stack traces are also helpful.'),\n",
       " Document(metadata={}, page_content='<!-- Please post terminal logs, minimal example to reproduce, or command to run under three backticks (```) to allow code formatting.\\n\\n```\\nPaste your error here\\n```\\n\\nFor more information on this, check: https://www.markdownguide.org/extended-syntax/#fenced-code-blocks\\n\\n-->\\n\\n### System Info\\n\\nDescribe the characteristic of your environment:'),\n",
       " Document(metadata={}, page_content='Describe the characteristic of your environment:\\n\\n<!-- Please complete the following description. -->\\n- Commit: [e.g. 8f3b9ca]\\n- Isaac Sim Version: [e.g. 2022.2.0, this can be obtained by `cat ${ISAACSIM_PATH}/VERSION`]\\n- OS: [e.g. Ubuntu 20.04]\\n- GPU: [e.g. RTX 2060 Super]\\n- CUDA: [e.g. 11.4]\\n- GPU Driver: [e.g. 470.82.01, this can be seen by using `nvidia-smi` command.]\\n\\n### Additional context\\n\\nAdd any other context about the problem here.\\n\\n### Checklist'),\n",
       " Document(metadata={}, page_content='### Checklist\\n\\n- [ ] I have checked that there is no similar issue in the repo (**required**)\\n- [ ] I have checked that the issue is not in running Isaac Sim itself and is related to the repo\\n\\n### Acceptance Criteria\\n\\nAdd the criteria for which this task is considered **done**. If not known at issue creation time, you can add this once the issue is assigned.\\n\\n- [ ] Criteria 1\\n- [ ] Criteria 2'),\n",
       " Document(metadata={}, page_content='---\\nname: Proposal\\nabout: Propose changes that are not bug fixes\\ntitle: \"[Proposal] Proposal title\"\\n---\\n\\n\\n### Proposal\\n\\nA clear and concise description of the proposal. In a few sentences, describe the feature and its core capabilities.\\n\\n### Motivation\\n\\nPlease outline the motivation for the proposal. Summarize the core use cases and user problems and needs you are trying to solve.\\n\\nIs your feature request related to a problem? e.g.,\"I\\'m always frustrated when [...]\".'),\n",
       " Document(metadata={}, page_content=\"If this is related to another GitHub issue, please link here too.\\n\\n### Alternatives\\n\\nA clear and concise description of any alternative solutions or features you've considered, if any.\\n\\n### Additional context\\n\\nAdd any other context or screenshots about the feature request here.\\n\\n### Checklist\\n\\n- [ ] I have checked that there is no similar issue in the repo (**required**)\\n\\n### Acceptance Criteria\"),\n",
       " Document(metadata={}, page_content='### Acceptance Criteria\\n\\nAdd the criteria for which this task is considered **done**. If not known at issue creation time, you can add this once the issue is assigned.\\n\\n- [ ] Criteria 1\\n- [ ] Criteria 2'),\n",
       " Document(metadata={}, page_content='---\\nname: Question\\nabout: Ask a question\\ntitle: \"[Question] Question title\"\\n---\\n\\n### Question\\n\\nBasic questions, related to robot learning, that are not bugs or feature requests will be closed without reply, because GitHub issues are not an appropriate venue for these.\\n\\nAdvanced/nontrivial questions, especially in areas where documentation is lacking, are very much welcome.'),\n",
       " Document(metadata={}, page_content='For questions that are related to running and understanding Isaac Sim, please post them at the official [Isaac Sim forums](https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/isaac_sim_forums.html).'),\n",
       " Document(metadata={}, page_content='name: Build & deploy docs\\n\\non:\\n  push:\\n    branches:\\n      - main\\n      - devel\\n  pull_request:\\n    types: [opened, synchronize, reopened]\\n\\nconcurrency:\\n  group: ${{ github.workflow }}-${{ github.ref }}\\n  cancel-in-progress: true'),\n",
       " Document(metadata={}, page_content='jobs:\\n  check-secrets:\\n    name: Check secrets\\n    runs-on: ubuntu-latest\\n    outputs:\\n      trigger-deploy: ${{ steps.trigger-deploy.outputs.defined }}\\n    steps:\\n    - id: trigger-deploy\\n      env:\\n        REPO_NAME: ${{ secrets.REPO_NAME }}\\n        BRANCH_REF: ${{ secrets.BRANCH_REF }}\\n      if: \"${{ github.repository == env.REPO_NAME && github.ref == env.BRANCH_REF }}\"\\n      run: echo \"defined=true\" >> \"$GITHUB_OUTPUT\"'),\n",
       " Document(metadata={}, page_content='build-docs:\\n    name: Build Docs\\n    runs-on: ubuntu-latest\\n    needs: [check-secrets]\\n\\n    steps:\\n    - name: Checkout code\\n      uses: actions/checkout@v2\\n\\n    - name: Setup python\\n      uses: actions/setup-python@v2\\n      with:\\n        python-version: \"3.10\"\\n        architecture: x64\\n\\n    - name: Install dev requirements\\n      working-directory: ./docs\\n      run: pip install -r requirements.txt'),\n",
       " Document(metadata={}, page_content=\"- name: Check branch docs building\\n      working-directory: ./docs\\n      if: needs.check-secrets.outputs.trigger-deploy != 'true'\\n      run: make current-docs\\n\\n    - name: Generate multi-version docs\\n      working-directory: ./docs\\n      run: |\\n        git fetch --prune --unshallow --tags\\n        make multi-docs\\n\\n    - name: Upload docs artifact\\n      uses: actions/upload-artifact@v4\\n      with:\\n        name: docs-html\\n        path: ./docs/_build\"),\n",
       " Document(metadata={}, page_content=\"deploy-docs:\\n    name: Deploy Docs\\n    runs-on: ubuntu-latest\\n    needs: [check-secrets, build-docs]\\n    if: needs.check-secrets.outputs.trigger-deploy == 'true'\\n\\n    steps:\\n    - name: Download docs artifact\\n      uses: actions/download-artifact@v4\\n      with:\\n        name: docs-html\\n        path: ./docs/_build\\n\\n    - name: Deploy to gh-pages\\n      uses: peaceiris/actions-gh-pages@v3\\n      with:\\n        github_token: ${{ secrets.GITHUB_TOKEN }}\\n        publish_dir: ./docs/_build\"),\n",
       " Document(metadata={}, page_content='name: Run linters using pre-commit\\n\\non:\\n  pull_request:\\n  push:\\n    branches: [main]\\n\\njobs:\\n  pre-commit:\\n    runs-on: ubuntu-latest\\n    steps:\\n    - uses: actions/checkout@v3\\n    - uses: actions/setup-python@v3\\n    - uses: pre-commit/action@v3.0.0'),\n",
       " Document(metadata={}, page_content='{\\n\\t// See http://go.microsoft.com/fwlink/?LinkId=827846\\n\\t// for the documentation about the extensions.json format\\n\\t\"recommendations\": [\\n\\t\\t\"ms-vscode.cpptools\",\\n\\t\\t\"ms-python.python\",\\n        \"ms-python.vscode-pylance\",\\n\\t\\t\"ban.spellright\",\\n\\t\\t\"ms-iot.vscode-ros\",\\n\\t\\t\"ExecutableBookProject.myst-highlight\",\\n\\t]\\n}'),\n",
       " Document(metadata={}, page_content='{\\n    // See https://go.microsoft.com/fwlink/?LinkId=733558\\n    // for the documentation about the tasks.json format\\n    \"version\": \"2.0.0\",\\n    \"tasks\": [\\n        {\\n            // setup python env\\n            \"label\": \"setup_python_env\",\\n            \"type\": \"shell\",\\n            \"linux\": {\\n                \"command\": \"${workspaceFolder}/isaaclab.sh -p ${workspaceFolder}/.vscode/tools/setup_vscode.py\"\\n            },\\n            \"windows\": {'),\n",
       " Document(metadata={}, page_content='},\\n            \"windows\": {\\n                \"command\": \"${workspaceFolder}/isaaclab.bat -p ${workspaceFolder}/.vscode/tools/setup_vscode.py\"\\n            }\\n        },\\n        {\\n            // run formatter\\n            \"label\": \"run_formatter\",\\n            \"type\": \"shell\",\\n            \"linux\": {\\n                \"command\": \"${workspaceFolder}/isaaclab.sh --format\"\\n            },\\n            \"windows\": {\\n                \"command\": \"${workspaceFolder}/isaaclab.bat --format\"\\n            }'),\n",
       " Document(metadata={}, page_content='}\\n        }\\n    ]\\n}'),\n",
       " Document(metadata={}, page_content='{\\n    // Use IntelliSense to learn about possible attributes.\\n    // Hover to view descriptions of existing attributes.\\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\\n    \"version\": \"0.2.0\",\\n    \"configurations\": [\\n        {\\n            \"name\": \"Python: Current File\",\\n            \"type\": \"python\",\\n            \"request\": \"launch\",\\n            \"program\": \"${file}\",\\n            \"console\": \"integratedTerminal\"\\n        },\\n        {'),\n",
       " Document(metadata={}, page_content='},\\n        {\\n            \"name\": \"Python: Attach (windows-x86_64/linux-x86_64)\",\\n            \"type\": \"python\",\\n            \"request\": \"attach\",\\n            \"port\": 3000,\\n            \"host\": \"localhost\"\\n        },\\n        {\\n            \"name\": \"Python: Train Environment\",\\n            \"type\": \"python\",\\n            \"request\": \"launch\",\\n            \"args\" : [\"--task\", \"Isaac-Reach-Franka-v0\", \"--headless\"],'),\n",
       " Document(metadata={}, page_content='\"program\": \"${workspaceFolder}/scripts/reinforcement_learning/rsl_rl/train.py\",\\n            \"console\": \"integratedTerminal\"\\n        },\\n        {\\n            \"name\": \"Python: Play Environment\",\\n            \"type\": \"python\",\\n            \"request\": \"launch\",\\n            \"args\" : [\"--task\", \"Isaac-Reach-Franka-v0\", \"--num_envs\", \"32\"],\\n            \"program\": \"${workspaceFolder}/scripts/reinforcement_learning/rsl_rl/play.py\",\\n            \"console\": \"integratedTerminal\"\\n        },'),\n",
       " Document(metadata={}, page_content='},\\n        {\\n            \"name\": \"Python: SinglePytest\",\\n            \"type\": \"python\",\\n            \"request\": \"launch\",\\n            \"module\": \"pytest\",\\n            \"args\": [\\n              \"${file}\"\\n            ],\\n            \"console\": \"integratedTerminal\"\\n        },\\n        {\\n            \"name\": \"Python: ALL Pytest\",\\n            \"type\": \"python\",\\n            \"request\": \"launch\",\\n            \"module\": \"pytest\",\\n            \"args\": [\"source/isaaclab/test\"],'),\n",
       " Document(metadata={}, page_content='\"args\": [\"source/isaaclab/test\"],\\n            \"console\": \"integratedTerminal\",\\n            \"justMyCode\": false\\n        }\\n    ]\\n}'),\n",
       " Document(metadata={}, page_content='{\\n    \"files.exclude\": {\\n        \"**/.mypy_cache\": true,\\n        \"**/__pycache__\": true,\\n        \"**/*.egg-info\": true\\n    },\\n    \"files.associations\": {\\n        \"*.tpp\": \"cpp\",\\n        \"*.kit\": \"toml\",\\n        \"*.rst\": \"restructuredtext\"\\n    },\\n    \"editor.rulers\": [120],'),\n",
       " Document(metadata={}, page_content='// files to be ignored by the linter\\n    \"files.watcherExclude\": {\\n        \"**/.git/objects/**\": true,\\n        \"**/.git/subtree-cache/**\": true,\\n        \"**/node_modules/**\": true,\\n        \"**/_isaac_sim/**\": true,\\n        \"**/_compiler/**\": true\\n    },\\n    // Configuration for spelling checker\\n    \"spellright.language\": [\\n        \"en-US-10-1.\"\\n    ],\\n    \"spellright.documentTypes\": [\\n        \"markdown\",\\n        \"latex\",\\n        \"plaintext\",\\n        \"cpp\",\\n        \"asciidoc\",'),\n",
       " Document(metadata={}, page_content='\"cpp\",\\n        \"asciidoc\",\\n        \"python\",\\n        \"restructuredtext\"\\n    ],\\n    \"cSpell.words\": [\\n        \"literalinclude\",\\n        \"linenos\",\\n        \"instanceable\",\\n        \"isaacSim\",\\n        \"jacobians\",\\n        \"pointcloud\",\\n        \"ridgeback\",\\n        \"rllib\",\\n        \"robomimic\",\\n        \"teleoperation\",\\n        \"xform\",\\n        \"numpy\",\\n        \"flatcache\",\\n        \"physx\",\\n        \"dpad\",\\n        \"gamepad\",\\n        \"linspace\",\\n        \"upsampled\",\\n        \"downsampled\",'),\n",
       " Document(metadata={}, page_content='\"upsampled\",\\n        \"downsampled\",\\n        \"arange\",\\n        \"discretization\",\\n        \"trimesh\",\\n        \"uninstanceable\",\\n        \"coeff\",\\n        \"prestartup\"\\n    ],\\n    // This enables python language server. Seems to work slightly better than jedi:\\n    \"python.languageServer\": \"Pylance\",\\n    // We use \"black\" as a formatter:\\n    \"python.formatting.provider\": \"black\",\\n    \"python.formatting.blackArgs\": [\"--line-length\", \"120\"],\\n    // Use flake8 for linting'),\n",
       " Document(metadata={}, page_content='// Use flake8 for linting\\n    \"python.linting.pylintEnabled\": false,\\n    \"python.linting.flake8Enabled\": true,\\n    \"python.linting.flake8Args\": [\\n        \"--max-line-length=120\"\\n    ],\\n    // Use docstring generator\\n    \"autoDocstring.docstringFormat\": \"google\",\\n    \"autoDocstring.guessTypes\": true,\\n    // Python environment path\\n    // note: the default interpreter is overridden when user selects a workspace interpreter'),\n",
       " Document(metadata={}, page_content='//     in the status bar. For example, the virtual environment python interpreter\\n    \"python.defaultInterpreterPath\": \"${workspaceFolder}/_isaac_sim/python.sh\",\\n    // ROS distribution\\n    \"ros.distro\": \"noetic\",\\n    // Language specific settings\\n    \"[python]\": {\\n        \"editor.tabSize\": 4\\n    },\\n    \"[restructuredtext]\": {\\n        \"editor.tabSize\": 2\\n    },\\n    // Python extra paths\\n    // Note: this is filled up when \"./isaaclab.sh -i\" is run\\n    \"python.analysis.extraPaths\": []\\n}'),\n",
       " Document(metadata={}, page_content='def overwrite_python_analysis_extra_paths(isaaclab_settings: str) -> str:\\n    \"\"\"Overwrite the python.analysis.extraPaths in the Isaac Lab settings file.\\n\\n    The extraPaths are replaced with the path names from the isaac-sim settings file that exists in the\\n    \"{ISAACSIM_DIR}/.vscode/settings.json\" file.\\n\\n    If the isaac-sim settings file does not exist, the extraPaths are not overwritten.\\n\\n    Args:\\n        isaaclab_settings: The settings string to use as template.\\n\\n    Returns:\\n        The settings string with overwritten python analysis extra paths.\\n    \"\"\"\\n    # isaac-sim settings\\n    isaacsim_vscode_filename = os.path.join(ISAACSIM_DIR, \".vscode\", \"settings.json\")\\n\\n    # we use the isaac-sim settings file to get the python.analysis.extraPaths for kit extensions\\n    # if this file does not exist, we will not add any extra paths\\n    if os.path.exists(isaacsim_vscode_filename):\\n        # read the path names from the isaac-sim settings file\\n        with open(isaacsim_vscode_filename) as f:\\n            vscode_settings = f.read()\\n        # extract the path names\\n        # search for the python.analysis.extraPaths section and extract the contents\\n        settings = re.search(\\n            r\"\\\\\"python.analysis.extraPaths\\\\\": \\\\[.*?\\\\]\", vscode_settings, flags=re.MULTILINE | re.DOTALL\\n        )\\n        settings = settings.group(0)\\n        settings = settings.split(\\'\"python.analysis.extraPaths\": [\\')[-1]\\n        settings = settings.split(\"]\")[0]\\n\\n        # read the path names from the isaac-sim settings file\\n        path_names = settings.split(\",\")\\n        path_names = [path_name.strip().strip(\\'\"\\') for path_name in path_names]\\n        path_names = [path_name for path_name in path_names if len(path_name) > 0]\\n\\n        # change the path names to be relative to the Isaac Lab directory\\n        rel_path = os.path.relpath(ISAACSIM_DIR, ISAACLAB_DIR)\\n        path_names = [\\'\"${workspaceFolder}/\\' + rel_path + \"/\" + path_name + \\'\"\\' for path_name in path_names]\\n    else:\\n        path_names = []\\n        print(\\n            f\"[WARN] Could not find Isaac Sim VSCode settings: {isaacsim_vscode_filename}.\"\\n            \"\\\\n\\\\tThis will result in missing \\'python.analysis.extraPaths\\' in the VSCode\"\\n            \"\\\\n\\\\tsettings, which limits the functionality of the Python language server.\"\\n            \"\\\\n\\\\tHowever, it does not affect the functionality of the Isaac Lab project.\"\\n            \"\\\\n\\\\tWe are working on a fix for this issue with the Isaac Sim team.\"\\n        )\\n\\n    # add the path names that are in the Isaac Lab extensions directory\\n    isaaclab_extensions = os.listdir(os.path.join(ISAACLAB_DIR, \"source\"))\\n    path_names.extend([\\'\"${workspaceFolder}/source/\\' + ext + \\'\"\\' for ext in isaaclab_extensions])\\n\\n    # combine them into a single string\\n    path_names = \",\\\\n\\\\t\\\\t\".expandtabs(4).join(path_names)\\n    # deal with the path separator being different on Windows and Unix\\n    path_names = path_names.replace(\"\\\\\\\\\", \"/\")\\n\\n    # replace the path names in the Isaac Lab settings file with the path names parsed\\n    isaaclab_settings = re.sub(\\n        r\"\\\\\"python.analysis.extraPaths\\\\\": \\\\[.*?\\\\]\",\\n        \\'\"python.analysis.extraPaths\": [\\\\n\\\\t\\\\t\\'.expandtabs(4) + path_names + \"\\\\n\\\\t]\".expandtabs(4),\\n        isaaclab_settings,\\n        flags=re.DOTALL,\\n    )\\n    # return the Isaac Lab settings string\\n    return isaaclab_settings'),\n",
       " Document(metadata={}, page_content='def overwrite_default_python_interpreter(isaaclab_settings: str) -> str:\\n    \"\"\"Overwrite the default python interpreter in the Isaac Lab settings file.\\n\\n    The default python interpreter is replaced with the path to the python interpreter used by the\\n    isaac-sim project. This is necessary because the default python interpreter is the one shipped with\\n    isaac-sim.\\n\\n    Args:\\n        isaaclab_settings: The settings string to use as template.\\n\\n    Returns:\\n        The settings string with overwritten default python interpreter.\\n    \"\"\"\\n    # read executable name\\n    python_exe = sys.executable.replace(\"\\\\\\\\\", \"/\")\\n\\n    # We make an exception for replacing the default interpreter if the\\n    # path (/kit/python/bin/python3) indicates that we are using a local/container\\n    # installation of IsaacSim. We will preserve the calling script as the default, python.sh.\\n    # We want to use python.sh because it modifies LD_LIBRARY_PATH and PYTHONPATH\\n    # (among other envars) that we need for all of our dependencies to be accessible.\\n    if \"kit/python/bin/python3\" in python_exe:\\n        return isaaclab_settings\\n    # replace the default python interpreter in the Isaac Lab settings file with the path to the\\n    # python interpreter in the Isaac Lab directory\\n    isaaclab_settings = re.sub(\\n        r\"\\\\\"python.defaultInterpreterPath\\\\\": \\\\\".*?\\\\\"\",\\n        f\\'\"python.defaultInterpreterPath\": \"{python_exe}\"\\',\\n        isaaclab_settings,\\n        flags=re.DOTALL,\\n    )\\n    # return the Isaac Lab settings file\\n    return isaaclab_settings'),\n",
       " Document(metadata={}, page_content='def main():\\n    # Isaac Lab template settings\\n    isaaclab_vscode_template_filename = os.path.join(ISAACLAB_DIR, \".vscode\", \"tools\", \"settings.template.json\")\\n    # make sure the Isaac Lab template settings file exists\\n    if not os.path.exists(isaaclab_vscode_template_filename):\\n        raise FileNotFoundError(\\n            f\"Could not find the Isaac Lab template settings file: {isaaclab_vscode_template_filename}\"\\n        )\\n    # read the Isaac Lab template settings file\\n    with open(isaaclab_vscode_template_filename) as f:\\n        isaaclab_template_settings = f.read()\\n\\n    # overwrite the python.analysis.extraPaths in the Isaac Lab settings file with the path names\\n    isaaclab_settings = overwrite_python_analysis_extra_paths(isaaclab_template_settings)\\n    # overwrite the default python interpreter in the Isaac Lab settings file with the path to the\\n    # python interpreter used to call this script\\n    isaaclab_settings = overwrite_default_python_interpreter(isaaclab_settings)\\n\\n    # add template notice to the top of the file\\n    header_message = (\\n        \"// This file is a template and is automatically generated by the setup_vscode.py script.\\\\n\"\\n        \"// Do not edit this file directly.\\\\n\"\\n        \"// \\\\n\"\\n        f\"// Generated from: {isaaclab_vscode_template_filename}\\\\n\"\\n    )\\n    isaaclab_settings = header_message + isaaclab_settings\\n\\n    # write the Isaac Lab settings file\\n    isaaclab_vscode_filename = os.path.join(ISAACLAB_DIR, \".vscode\", \"settings.json\")\\n    with open(isaaclab_vscode_filename, \"w\") as f:\\n        f.write(isaaclab_settings)\\n\\n    # copy the launch.json file if it doesn\\'t exist\\n    isaaclab_vscode_launch_filename = os.path.join(ISAACLAB_DIR, \".vscode\", \"launch.json\")\\n    isaaclab_vscode_template_launch_filename = os.path.join(ISAACLAB_DIR, \".vscode\", \"tools\", \"launch.template.json\")\\n    if not os.path.exists(isaaclab_vscode_launch_filename):\\n        # read template launch settings\\n        with open(isaaclab_vscode_template_launch_filename) as f:\\n            isaaclab_template_launch_settings = f.read()\\n        # add header\\n        header_message = header_message.replace(\\n            isaaclab_vscode_template_filename, isaaclab_vscode_template_launch_filename\\n        )\\n        isaaclab_launch_settings = header_message + isaaclab_template_launch_settings\\n        # write the Isaac Lab launch settings file\\n        with open(isaaclab_vscode_launch_filename, \"w\") as f:\\n            f.write(isaaclab_launch_settings)'),\n",
       " Document(metadata={}, page_content='def parse_cli_args() -> argparse.Namespace:\\n    \"\"\"Parse command line arguments.\\n\\n    This function creates a parser object and adds subparsers for each command. The function then parses the\\n    command line arguments and returns the parsed arguments.\\n\\n    Returns:\\n        The parsed command line arguments.\\n    \"\"\"\\n    parser = argparse.ArgumentParser(description=\"Utility for using Docker with Isaac Lab.\")\\n\\n    # We have to create separate parent parsers for common options to our subparsers\\n    parent_parser = argparse.ArgumentParser(add_help=False)\\n    parent_parser.add_argument(\\n        \"profile\", nargs=\"?\", default=\"base\", help=\"Optional container profile specification. Example: \\'base\\' or \\'ros\\'.\"\\n    )\\n    parent_parser.add_argument(\\n        \"--files\",\\n        nargs=\"*\",\\n        default=None,\\n        help=(\\n            \"Allows additional \\'.yaml\\' files to be passed to the docker compose command. These files will be merged\"\\n            \" with \\'docker-compose.yaml\\' in their provided order.\"\\n        ),\\n    )\\n    parent_parser.add_argument(\\n        \"--env-files\",\\n        nargs=\"*\",\\n        default=None,\\n        help=(\\n            \"Allows additional \\'.env\\' files to be passed to the docker compose command. These files will be merged with\"\\n            \" \\'.env.base\\' in their provided order.\"\\n        ),\\n    )\\n    parent_parser.add_argument(\\n        \"--suffix\",\\n        nargs=\"?\",\\n        default=None,\\n        help=(\\n            \"Optional docker image and container name suffix.  Defaults to None, in which case, the docker name\"\\n            \" suffix is set to the empty string. A hyphen is inserted in between the profile and the suffix if\"\\n            \\' the suffix is a nonempty string.  For example, if \"base\" is passed to profile, and \"custom\" is\\'\\n            \" passed to suffix, then the produced docker image and container will be named ``isaac-lab-base-custom``.\"\\n        ),\\n    )\\n\\n    # Actual command definition begins here\\n    subparsers = parser.add_subparsers(dest=\"command\", required=True)\\n    subparsers.add_parser(\\n        \"start\",\\n        help=\"Build the docker image and create the container in detached mode.\",\\n        parents=[parent_parser],\\n    )\\n    subparsers.add_parser(\\n        \"enter\", help=\"Begin a new bash process within an existing Isaac Lab container.\", parents=[parent_parser]\\n    )\\n    config = subparsers.add_parser(\\n        \"config\",\\n        help=(\\n            \"Generate a docker-compose.yaml from the passed yamls, .envs, and either print to the terminal or create a\"\\n            \" yaml at output_yaml\"\\n        ),\\n        parents=[parent_parser],\\n    )\\n    config.add_argument(\\n        \"--output-yaml\", nargs=\"?\", default=None, help=\"Yaml file to write config output to. Defaults to None.\"\\n    )\\n    subparsers.add_parser(\\n        \"copy\", help=\"Copy build and logs artifacts from the container to the host machine.\", parents=[parent_parser]\\n    )\\n    subparsers.add_parser(\"stop\", help=\"Stop the docker container and remove it.\", parents=[parent_parser])\\n\\n    # parse the arguments to determine the command\\n    args = parser.parse_args()\\n\\n    return args'),\n",
       " Document(metadata={}, page_content='def main(args: argparse.Namespace):\\n    \"\"\"Main function for the Docker utility.\"\"\"\\n    # check if docker is installed\\n    if not shutil.which(\"docker\"):\\n        raise RuntimeError(\\n            \"Docker is not installed! Please check the \\'Docker Guide\\' for instruction: \"\\n            \"https://isaac-sim.github.io/IsaacLab/source/deployment/docker.html\"\\n        )\\n\\n    # creating container interface\\n    ci = ContainerInterface(\\n        context_dir=Path(__file__).resolve().parent,\\n        profile=args.profile,\\n        yamls=args.files,\\n        envs=args.env_files,\\n        suffix=args.suffix,\\n    )\\n\\n    print(f\"[INFO] Using container profile: {ci.profile}\")\\n    if args.command == \"start\":\\n        # check if x11 forwarding is enabled\\n        x11_outputs = x11_utils.x11_check(ci.statefile)\\n        # if x11 forwarding is enabled, add the x11 yaml and environment variables\\n        if x11_outputs is not None:\\n            (x11_yaml, x11_envar) = x11_outputs\\n            ci.add_yamls += x11_yaml\\n            ci.environ.update(x11_envar)\\n        # start the container\\n        ci.start()\\n    elif args.command == \"enter\":\\n        # refresh the x11 forwarding\\n        x11_utils.x11_refresh(ci.statefile)\\n        # enter the container\\n        ci.enter()\\n    elif args.command == \"config\":\\n        ci.config(args.output_yaml)\\n    elif args.command == \"copy\":\\n        ci.copy()\\n    elif args.command == \"stop\":\\n        # stop the container\\n        ci.stop()\\n        # cleanup the x11 forwarding\\n        x11_utils.x11_cleanup(ci.statefile)\\n    else:\\n        raise RuntimeError(f\"Invalid command provided: {args.command}. Please check the help message.\")'),\n",
       " Document(metadata={}, page_content='services:\\n  cloudxr-runtime:\\n    image: ${CLOUDXR_RUNTIME_BASE_IMAGE_ARG}:${CLOUDXR_RUNTIME_VERSION_ARG}\\n    ports:\\n      - \"48010:48010/tcp\" # signaling\\n      - \"47998:47998/udp\" # media\\n      - \"47999:47999/udp\" # media\\n      - \"48000:48000/udp\" # media\\n      - \"48005:48005/udp\" # media\\n      - \"48008:48008/udp\" # media\\n      - \"48012:48012/udp\" # media\\n    healthcheck:\\n      test: [\"CMD\", \"test\", \"-S\", \"/openxr/run/ipc_cloudxr\"]\\n      interval: 1s\\n      timeout: 1s\\n      retries: 10'),\n",
       " Document(metadata={}, page_content='timeout: 1s\\n      retries: 10\\n      start_period: 5s\\n    environment:\\n      - ACCEPT_EULA=${ACCEPT_EULA}\\n    volumes:\\n      - openxr-volume:/openxr:rw\\n    deploy:\\n      resources:\\n        reservations:\\n          devices:\\n            - driver: nvidia\\n              count: all\\n              capabilities: [ gpu ]'),\n",
       " Document(metadata={}, page_content='isaac-lab-base:\\n    environment:\\n      - XDG_RUNTIME_DIR=/openxr/run\\n      - XR_RUNTIME_JSON=/openxr/share/openxr/1/openxr_cloudxr.json\\n    volumes:\\n      - openxr-volume:/openxr:rw\\n    depends_on:\\n        - cloudxr-runtime\\n    deploy:\\n      resources:\\n        reservations:\\n          devices:\\n            - driver: nvidia\\n              count: all\\n              capabilities: [ gpu ]\\n\\nvolumes:\\n  openxr-volume:'),\n",
       " Document(metadata={}, page_content='# Here we set the parts that would\\n# be re-used between services to an\\n# extension field\\n# https://docs.docker.com/compose/compose-file/compose-file-v3/#extension-fields\\nx-default-isaac-lab-volumes: &default-isaac-lab-volumes\\n  # These volumes follow from this page\\n  # https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_faq.html#save-isaac-sim-configs-on-local-disk\\n  - type: volume\\n    source: isaac-cache-kit\\n    target: ${DOCKER_ISAACSIM_ROOT_PATH}/kit/cache\\n  - type: volume'),\n",
       " Document(metadata={}, page_content='- type: volume\\n    source: isaac-cache-ov\\n    target: ${DOCKER_USER_HOME}/.cache/ov\\n  - type: volume\\n    source: isaac-cache-pip\\n    target: ${DOCKER_USER_HOME}/.cache/pip\\n  - type: volume\\n    source: isaac-cache-gl\\n    target: ${DOCKER_USER_HOME}/.cache/nvidia/GLCache\\n  - type: volume\\n    source: isaac-cache-compute\\n    target: ${DOCKER_USER_HOME}/.nv/ComputeCache\\n  - type: volume\\n    source: isaac-logs\\n    target: ${DOCKER_USER_HOME}/.nvidia-omniverse/logs\\n  - type: volume'),\n",
       " Document(metadata={}, page_content='- type: volume\\n    source: isaac-carb-logs\\n    target: ${DOCKER_ISAACSIM_ROOT_PATH}/kit/logs/Kit/Isaac-Sim\\n  - type: volume\\n    source: isaac-data\\n    target: ${DOCKER_USER_HOME}/.local/share/ov/data\\n  - type: volume\\n    source: isaac-docs\\n    target: ${DOCKER_USER_HOME}/Documents\\n    # This overlay allows changes on the local files to\\n    # be reflected within the container immediately\\n  - type: bind\\n    source: ../source\\n    target: ${DOCKER_ISAACLAB_PATH}/source\\n  - type: bind'),\n",
       " Document(metadata={}, page_content='- type: bind\\n    source: ../scripts\\n    target: ${DOCKER_ISAACLAB_PATH}/scripts\\n  - type: bind\\n    source: ../docs\\n    target: ${DOCKER_ISAACLAB_PATH}/docs\\n  - type: bind\\n    source: ../tools\\n    target: ${DOCKER_ISAACLAB_PATH}/tools\\n    # The effect of these volumes is twofold:\\n    # 1. Prevent root-owned files from flooding the _build and logs dir\\n    #    on the host machine\\n    # 2. Preserve the artifacts in persistent volumes for later copying\\n    #    to the host machine'),\n",
       " Document(metadata={}, page_content='#    to the host machine\\n  - type: volume\\n    source: isaac-lab-docs\\n    target: ${DOCKER_ISAACLAB_PATH}/docs/_build\\n  - type: volume\\n    source: isaac-lab-logs\\n    target: ${DOCKER_ISAACLAB_PATH}/logs\\n  - type: volume\\n    source: isaac-lab-data\\n    target: ${DOCKER_ISAACLAB_PATH}/data_storage'),\n",
       " Document(metadata={}, page_content='x-default-isaac-lab-environment: &default-isaac-lab-environment\\n  - ISAACSIM_PATH=${DOCKER_ISAACLAB_PATH}/_isaac_sim\\n  - OMNI_KIT_ALLOW_ROOT=1\\n\\nx-default-isaac-lab-deploy: &default-isaac-lab-deploy\\n  resources:\\n    reservations:\\n      devices:\\n        - driver: nvidia\\n          count: all\\n          capabilities: [ gpu ]'),\n",
       " Document(metadata={}, page_content='services:\\n  # This service is the base Isaac Lab image\\n  isaac-lab-base:\\n    profiles: [ \"base\" ]\\n    env_file: .env.base\\n    build:\\n      context: ../\\n      dockerfile: docker/Dockerfile.base\\n      args:\\n        - ISAACSIM_BASE_IMAGE_ARG=${ISAACSIM_BASE_IMAGE}\\n        - ISAACSIM_VERSION_ARG=${ISAACSIM_VERSION}\\n        - ISAACSIM_ROOT_PATH_ARG=${DOCKER_ISAACSIM_ROOT_PATH}\\n        - ISAACLAB_PATH_ARG=${DOCKER_ISAACLAB_PATH}\\n        - DOCKER_USER_HOME_ARG=${DOCKER_USER_HOME}'),\n",
       " Document(metadata={}, page_content='image: isaac-lab-base${DOCKER_NAME_SUFFIX-}\\n    container_name: isaac-lab-base${DOCKER_NAME_SUFFIX-}\\n    environment: *default-isaac-lab-environment\\n    volumes: *default-isaac-lab-volumes\\n    network_mode: host\\n    deploy: *default-isaac-lab-deploy\\n    # This is the entrypoint for the container\\n    entrypoint: bash\\n    stdin_open: true\\n    tty: true'),\n",
       " Document(metadata={}, page_content='# This service adds a ROS2 Humble\\n  # installation on top of the base image\\n  isaac-lab-ros2:\\n    profiles: [ \"ros2\" ]\\n    env_file:\\n      - .env.base\\n      - .env.ros2\\n    build:\\n      context: ../\\n      dockerfile: docker/Dockerfile.ros2\\n      args:\\n        # ROS2_APT_PACKAGE will default to NONE. This is to\\n        # avoid a warning message when building only the base profile\\n        # with the .env.base file\\n        - ROS2_APT_PACKAGE=${ROS2_APT_PACKAGE:-NONE}'),\n",
       " Document(metadata={}, page_content='# Make sure that the correct Docker Name Suffix is being passed to the dockerfile, to know which base image to\\n        # start from.\\n        - DOCKER_NAME_SUFFIX=${DOCKER_NAME_SUFFIX-}\\n    image: isaac-lab-ros2${DOCKER_NAME_SUFFIX-}\\n    container_name: isaac-lab-ros2${DOCKER_NAME_SUFFIX-}\\n    environment: *default-isaac-lab-environment\\n    volumes: *default-isaac-lab-volumes\\n    network_mode: host\\n    deploy: *default-isaac-lab-deploy\\n    # This is the entrypoint for the container'),\n",
       " Document(metadata={}, page_content='# This is the entrypoint for the container\\n    entrypoint: bash\\n    stdin_open: true\\n    tty: true'),\n",
       " Document(metadata={}, page_content='volumes:\\n  # isaac-sim\\n  isaac-cache-kit:\\n  isaac-cache-ov:\\n  isaac-cache-pip:\\n  isaac-cache-gl:\\n  isaac-cache-compute:\\n  isaac-logs:\\n  isaac-carb-logs:\\n  isaac-data:\\n  isaac-docs:\\n  # isaac-lab\\n  isaac-lab-docs:\\n  isaac-lab-logs:\\n  isaac-lab-data:'),\n",
       " Document(metadata={}, page_content='services:\\n  isaac-lab-base:\\n    environment:\\n      - DISPLAY\\n      - TERM\\n      - QT_X11_NO_MITSHM=1\\n      - XAUTHORITY=${__ISAACLAB_TMP_XAUTH}\\n    volumes:\\n    - type: bind\\n      source: ${__ISAACLAB_TMP_DIR}\\n      target: ${__ISAACLAB_TMP_DIR}\\n    - type: bind\\n      source: /tmp/.X11-unix\\n      target: /tmp/.X11-unix\\n    - type: bind\\n      source: /etc/localtime\\n      target: /etc/localtime\\n      read_only: true'),\n",
       " Document(metadata={}, page_content='isaac-lab-ros2:\\n    environment:\\n      - DISPLAY\\n      - TERM\\n      - QT_X11_NO_MITSHM=1\\n      - XAUTHORITY=${__ISAACLAB_TMP_XAUTH}\\n    volumes:\\n    - type: bind\\n      source: ${__ISAACLAB_TMP_DIR}\\n      target: ${__ISAACLAB_TMP_DIR}\\n    - type: bind\\n      source: /tmp/.X11-unix\\n      target: /tmp/.X11-unix\\n    - type: bind\\n      source: /etc/localtime\\n      target: /etc/localtime\\n      read_only: true'),\n",
       " Document(metadata={}, page_content='class TestDocker(unittest.TestCase):\\n    \"\"\"Test starting and stopping of the docker container with both currently supported profiles and with and without\\n    a suffix.  This assumes that docker is installed and configured correctly so that the user can use the docker\\n    commands from the current shell.\"\"\"\\n\\n    def start_stop_docker(self, profile, suffix):\\n        \"\"\"Test starting and stopping docker profile with suffix.\"\"\"\\n        environ = os.environ\\n        context_dir = Path(__file__).resolve().parent.parent\\n\\n        # generate parameters for the arguments\\n        if suffix != \"\":\\n            container_name = f\"isaac-lab-{profile}-{suffix}\"\\n            suffix_args = [\"--suffix\", suffix]\\n        else:\\n            container_name = f\"isaac-lab-{profile}\"\\n            suffix_args = []\\n\\n        run_kwargs = {\\n            \"check\": False,\\n            \"capture_output\": True,\\n            \"text\": True,\\n            \"cwd\": context_dir,\\n            \"env\": environ,\\n        }\\n\\n        # start the container\\n        docker_start = subprocess.run([\"python\", \"container.py\", \"start\", profile] + suffix_args, **run_kwargs)\\n        self.assertEqual(docker_start.returncode, 0)\\n\\n        # verify that the container is running\\n        docker_running_true = subprocess.run([\"docker\", \"ps\"], **run_kwargs)\\n        self.assertEqual(docker_running_true.returncode, 0)\\n        self.assertIn(container_name, docker_running_true.stdout)\\n\\n        # stop the container\\n        docker_stop = subprocess.run([\"python\", \"container.py\", \"stop\", profile] + suffix_args, **run_kwargs)\\n        self.assertEqual(docker_stop.returncode, 0)\\n\\n        # verify that the container has stopped\\n        docker_running_false = subprocess.run([\"docker\", \"ps\"], **run_kwargs)\\n        self.assertEqual(docker_running_false.returncode, 0)\\n        self.assertNotIn(container_name, docker_running_false.stdout)\\n\\n    def test_docker_base(self):\\n        \"\"\"Test starting and stopping docker base.\"\"\"\\n        self.start_stop_docker(\"base\", \"\")\\n\\n    def test_docker_base_suffix(self):\\n        \"\"\"Test starting and stopping docker base with a test suffix.\"\"\"\\n        self.start_stop_docker(\"base\", \"test\")\\n\\n    def test_docker_ros2(self):\\n        \"\"\"Test starting and stopping docker ros2.\"\"\"\\n        self.start_stop_docker(\"ros2\", \"\")\\n\\n    def test_docker_ros2_suffix(self):\\n        \"\"\"Test starting and stopping docker ros2 with a test suffix.\"\"\"\\n        self.start_stop_docker(\"ros2\", \"test\")'),\n",
       " Document(metadata={}, page_content='class ContainerInterface:\\n    \"\"\"A helper class for managing Isaac Lab containers.\"\"\"\\n\\n    def __init__(\\n        self,\\n        context_dir: Path,\\n        profile: str = \"base\",\\n        yamls: list[str] | None = None,\\n        envs: list[str] | None = None,\\n        statefile: StateFile | None = None,\\n        suffix: str | None = None,\\n    ):\\n        \"\"\"Initialize the container interface with the given parameters.\\n\\n        Args:\\n            context_dir: The context directory for Docker operations.\\n            profile: The profile name for the container. Defaults to \"base\".\\n            yamls: A list of yaml files to extend ``docker-compose.yaml`` settings. These are extended in the order\\n                they are provided.\\n            envs: A list of environment variable files to extend the ``.env.base`` file. These are extended in the order\\n                they are provided.\\n            statefile: An instance of the :class:`Statefile` class to manage state variables. Defaults to None, in\\n                which case a new configuration object is created by reading the configuration file at the path\\n                ``context_dir/.container.cfg``.\\n            suffix: Optional docker image and container name suffix.  Defaults to None, in which case, the docker name\\n                suffix is set to the empty string. A hyphen is inserted in between the profile and the suffix if\\n                the suffix is a nonempty string.  For example, if \"base\" is passed to profile, and \"custom\" is\\n                passed to suffix, then the produced docker image and container will be named ``isaac-lab-base-custom``.\\n        \"\"\"\\n        # set the context directory\\n        self.context_dir = context_dir\\n\\n        # create a state-file if not provided\\n        # the state file is a manager of run-time state variables that are saved to a file\\n        if statefile is None:\\n            self.statefile = StateFile(path=self.context_dir / \".container.cfg\")\\n        else:\\n            self.statefile = statefile\\n\\n        # set the profile and container name\\n        self.profile = profile\\n        if self.profile == \"isaaclab\":\\n            # Silently correct from isaaclab to base, because isaaclab is a commonly passed arg\\n            # but not a real profile\\n            self.profile = \"base\"\\n\\n        # set the docker image and container name suffix\\n        if suffix is None or suffix == \"\":\\n            # if no name suffix is given, default to the empty string as the name suffix\\n            self.suffix = \"\"\\n        else:\\n            # insert a hyphen before the suffix if a suffix is given\\n            self.suffix = f\"-{suffix}\"\\n\\n        self.container_name = f\"isaac-lab-{self.profile}{self.suffix}\"\\n        self.image_name = f\"isaac-lab-{self.profile}{self.suffix}:latest\"\\n\\n        # keep the environment variables from the current environment,\\n        # except make sure that the docker name suffix is set from the script\\n        self.environ = os.environ.copy()\\n        self.environ[\"DOCKER_NAME_SUFFIX\"] = self.suffix\\n\\n        # resolve the image extension through the passed yamls and envs\\n        self._resolve_image_extension(yamls, envs)\\n        # load the environment variables from the .env files\\n        self._parse_dot_vars()\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def is_container_running(self) -> bool:\\n        \"\"\"Check if the container is running.\\n\\n        Returns:\\n            True if the container is running, otherwise False.\\n        \"\"\"\\n        status = subprocess.run(\\n            [\"docker\", \"container\", \"inspect\", \"-f\", \"{{.State.Status}}\", self.container_name],\\n            capture_output=True,\\n            text=True,\\n            check=False,\\n        ).stdout.strip()\\n        return status == \"running\"\\n\\n    def does_image_exist(self) -> bool:\\n        \"\"\"Check if the Docker image exists.\\n\\n        Returns:\\n            True if the image exists, otherwise False.\\n        \"\"\"\\n        result = subprocess.run([\"docker\", \"image\", \"inspect\", self.image_name], capture_output=True, text=True)\\n        return result.returncode == 0\\n\\n    def start(self):\\n        \"\"\"Build and start the Docker container using the Docker compose command.\"\"\"\\n        print(\\n            f\"[INFO] Building the docker image and starting the container \\'{self.container_name}\\' in the\"\\n            \" background...\\\\n\"\\n        )\\n\\n        # build the image for the base profile if not running base (up will build base already if profile is base)\\n        if self.profile != \"base\":\\n            subprocess.run(\\n                [\\n                    \"docker\",\\n                    \"compose\",\\n                    \"--file\",\\n                    \"docker-compose.yaml\",\\n                    \"--env-file\",\\n                    \".env.base\",\\n                    \"build\",\\n                    \"isaac-lab-base\",\\n                ],\\n                check=False,\\n                cwd=self.context_dir,\\n                env=self.environ,\\n            )\\n\\n        # build the image for the profile\\n        subprocess.run(\\n            [\"docker\", \"compose\"]\\n            + self.add_yamls\\n            + self.add_profiles\\n            + self.add_env_files\\n            + [\"up\", \"--detach\", \"--build\", \"--remove-orphans\"],\\n            check=False,\\n            cwd=self.context_dir,\\n            env=self.environ,\\n        )\\n\\n    def enter(self):\\n        \"\"\"Enter the running container by executing a bash shell.\\n\\n        Raises:\\n            RuntimeError: If the container is not running.\\n        \"\"\"\\n        if self.is_container_running():\\n            print(f\"[INFO] Entering the existing \\'{self.container_name}\\' container in a bash session...\\\\n\")\\n            subprocess.run([\\n                \"docker\",\\n                \"exec\",\\n                \"--interactive\",\\n                \"--tty\",\\n                *([\"-e\", f\"DISPLAY={os.environ[\\'DISPLAY\\']}\"] if \"DISPLAY\" in os.environ else []),\\n                f\"{self.container_name}\",\\n                \"bash\",\\n            ])\\n        else:\\n            raise RuntimeError(f\"The container \\'{self.container_name}\\' is not running.\")\\n\\n    def stop(self):\\n        \"\"\"Stop the running container using the Docker compose command.\\n\\n        Raises:\\n            RuntimeError: If the container is not running.\\n        \"\"\"\\n        if self.is_container_running():\\n            print(f\"[INFO] Stopping the launched docker container \\'{self.container_name}\\'...\\\\n\")\\n            subprocess.run(\\n                [\"docker\", \"compose\"] + self.add_yamls + self.add_profiles + self.add_env_files + [\"down\", \"--volumes\"],\\n                check=False,\\n                cwd=self.context_dir,\\n                env=self.environ,\\n            )\\n        else:\\n            raise RuntimeError(f\"Can\\'t stop container \\'{self.container_name}\\' as it is not running.\")\\n\\n    def copy(self, output_dir: Path | None = None):\\n        \"\"\"Copy artifacts from the running container to the host machine.\\n\\n        Args:\\n            output_dir: The directory to copy the artifacts to. Defaults to None, in which case\\n                the context directory is used.\\n\\n        Raises:\\n            RuntimeError: If the container is not running.\\n        \"\"\"\\n        if self.is_container_running():\\n            print(f\"[INFO] Copying artifacts from the \\'{self.container_name}\\' container...\\\\n\")\\n            if output_dir is None:\\n                output_dir = self.context_dir\\n\\n            # create a directory to store the artifacts\\n            output_dir = output_dir.joinpath(\"artifacts\")\\n            if not output_dir.is_dir():\\n                output_dir.mkdir()\\n\\n            # define dictionary of mapping from docker container path to host machine path\\n            docker_isaac_lab_path = Path(self.dot_vars[\"DOCKER_ISAACLAB_PATH\"])\\n            artifacts = {\\n                docker_isaac_lab_path.joinpath(\"logs\"): output_dir.joinpath(\"logs\"),\\n                docker_isaac_lab_path.joinpath(\"docs/_build\"): output_dir.joinpath(\"docs\"),\\n                docker_isaac_lab_path.joinpath(\"data_storage\"): output_dir.joinpath(\"data_storage\"),\\n            }\\n            # print the artifacts to be copied\\n            for container_path, host_path in artifacts.items():\\n                print(f\"\\\\t -{container_path} -> {host_path}\")\\n            # remove the existing artifacts\\n            for path in artifacts.values():\\n                shutil.rmtree(path, ignore_errors=True)\\n\\n            # copy the artifacts\\n            for container_path, host_path in artifacts.items():\\n                subprocess.run(\\n                    [\\n                        \"docker\",\\n                        \"cp\",\\n                        f\"isaac-lab-{self.profile}{self.suffix}:{container_path}/\",\\n                        f\"{host_path}\",\\n                    ],\\n                    check=False,\\n                )\\n            print(\"\\\\n[INFO] Finished copying the artifacts from the container.\")\\n        else:\\n            raise RuntimeError(f\"The container \\'{self.container_name}\\' is not running.\")\\n\\n    def config(self, output_yaml: Path | None = None):\\n        \"\"\"Process the Docker compose configuration based on the passed yamls and environment files.\\n\\n        If the :attr:`output_yaml` is not None, the configuration is written to the file. Otherwise, it is printed to\\n        the terminal.\\n\\n        Args:\\n            output_yaml: The path to the yaml file where the configuration is written to. Defaults\\n                to None, in which case the configuration is printed to the terminal.\\n        \"\"\"\\n        print(\"[INFO] Configuring the passed options into a yaml...\\\\n\")\\n\\n        # resolve the output argument\\n        if output_yaml is not None:\\n            output = [\"--output\", output_yaml]\\n        else:\\n            output = []\\n\\n        # run the docker compose config command to generate the configuration\\n        subprocess.run(\\n            [\"docker\", \"compose\"] + self.add_yamls + self.add_profiles + self.add_env_files + [\"config\"] + output,\\n            check=False,\\n            cwd=self.context_dir,\\n            env=self.environ,\\n        )\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _resolve_image_extension(self, yamls: list[str] | None = None, envs: list[str] | None = None):\\n        \"\"\"\\n        Resolve the image extension by setting up YAML files, profiles, and environment files for the Docker compose command.\\n\\n        Args:\\n            yamls: A list of yaml files to extend ``docker-compose.yaml`` settings. These are extended in the order\\n                they are provided.\\n            envs: A list of environment variable files to extend the ``.env.base`` file. These are extended in the order\\n                they are provided.\\n        \"\"\"\\n        self.add_yamls = [\"--file\", \"docker-compose.yaml\"]\\n        self.add_profiles = [\"--profile\", f\"{self.profile}\"]\\n        self.add_env_files = [\"--env-file\", \".env.base\"]\\n\\n        # extend env file based on profile\\n        if self.profile != \"base\":\\n            self.add_env_files += [\"--env-file\", f\".env.{self.profile}\"]\\n\\n        # extend the env file based on the passed envs\\n        if envs is not None:\\n            for env in envs:\\n                self.add_env_files += [\"--env-file\", env]\\n\\n        # extend the docker-compose.yaml based on the passed yamls\\n        if yamls is not None:\\n            for yaml in yamls:\\n                self.add_yamls += [\"--file\", yaml]\\n\\n    def _parse_dot_vars(self):\\n        \"\"\"Parse the environment variables from the .env files.\\n\\n        Based on the passed \".env\" files, this function reads the environment variables and stores them in a dictionary.\\n        The environment variables are read in order and overwritten if there are name conflicts, mimicking the behavior\\n        of Docker compose.\\n        \"\"\"\\n        self.dot_vars: dict[str, Any] = {}\\n\\n        # check if the number of arguments is even for the env files\\n        if len(self.add_env_files) % 2 != 0:\\n            raise RuntimeError(\\n                \"The parameters for env files are configured incorrectly. There should be an even number of arguments.\"\\n                f\" Received: {self.add_env_files}.\"\\n            )\\n\\n        # read the environment variables from the .env files\\n        for i in range(1, len(self.add_env_files), 2):\\n            with open(self.context_dir / self.add_env_files[i]) as f:\\n                self.dot_vars.update(dict(line.strip().split(\"=\", 1) for line in f if \"=\" in line))'),\n",
       " Document(metadata={}, page_content='class StateFile:\\n    \"\"\"A class to manage state variables parsed from a configuration file.\\n\\n    This class provides a simple interface to set, get, and delete variables from a configuration\\n    object. It also provides the ability to save the configuration object to a file.\\n\\n    It thinly wraps around the ConfigParser class from the configparser module.\\n    \"\"\"\\n\\n    def __init__(self, path: Path, namespace: str | None = None):\\n        \"\"\"Initialize the class instance and load the configuration file.\\n\\n        Args:\\n            path: The path to the configuration file.\\n            namespace: The default namespace to use when setting and getting variables.\\n                Namespace corresponds to a section in the configuration file. Defaults to None,\\n                meaning  all member functions will have to specify the section explicitly,\\n                or :attr:`StateFile.namespace` must be set manually.\\n        \"\"\"\\n        self.path = path\\n        self.namespace = namespace\\n\\n        # load the configuration file\\n        self.load()\\n\\n    def __del__(self):\\n        \"\"\"\\n        Save the loaded configuration to the initial file path upon deconstruction. This helps\\n        ensure that the configuration file is always up to date.\\n        \"\"\"\\n        # save the configuration file\\n        self.save()\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def set_variable(self, key: str, value: Any, section: str | None = None):\\n        \"\"\"Set a variable into the configuration object.\\n\\n        Note:\\n            Since we use the ConfigParser class, the section names are case-sensitive but the keys are not.\\n\\n        Args:\\n            key: The key of the variable to be set.\\n            value: The value of the variable to be set.\\n            section: The section of the configuration object to set the variable in.\\n                Defaults to None, in which case the default section is used.\\n\\n        Raises:\\n            configparser.Error: If no section is specified and the default section is None.\\n        \"\"\"\\n        # resolve the section\\n        if section is None:\\n            if self.namespace is None:\\n                raise configparser.Error(\"No section specified. Please specify a section or set StateFile.namespace.\")\\n            section = self.namespace\\n\\n        # create section if it does not exist\\n        if section not in self.loaded_cfg.sections():\\n            self.loaded_cfg.add_section(section)\\n        # set the variable\\n        self.loaded_cfg.set(section, key, value)\\n\\n    def get_variable(self, key: str, section: str | None = None) -> Any:\\n        \"\"\"Get a variable from the configuration object.\\n\\n        Note:\\n            Since we use the ConfigParser class, the section names are case-sensitive but the keys are not.\\n\\n        Args:\\n            key: The key of the variable to be loaded.\\n            section: The section of the configuration object to read the variable from.\\n                Defaults to None, in which case the default section is used.\\n\\n        Returns:\\n            The value of the variable. It is None if the key does not exist.\\n\\n        Raises:\\n            configparser.Error: If no section is specified and the default section is None.\\n        \"\"\"\\n        # resolve the section\\n        if section is None:\\n            if self.namespace is None:\\n                raise configparser.Error(\"No section specified. Please specify a section or set StateFile.namespace.\")\\n            section = self.namespace\\n\\n        return self.loaded_cfg.get(section, key, fallback=None)\\n\\n    def delete_variable(self, key: str, section: str | None = None):\\n        \"\"\"Delete a variable from the configuration object.\\n\\n        Note:\\n            Since we use the ConfigParser class, the section names are case-sensitive but the keys are not.\\n\\n        Args:\\n            key: The key of the variable to be deleted.\\n            section: The section of the configuration object to remove the variable from.\\n                Defaults to None, in which case the default section is used.\\n\\n        Raises:\\n            configparser.Error: If no section is specified and the default section is None.\\n            configparser.NoSectionError: If the section does not exist in the configuration object.\\n            configparser.NoOptionError: If the key does not exist in the section.\\n        \"\"\"\\n        # resolve the section\\n        if section is None:\\n            if self.namespace is None:\\n                raise configparser.Error(\"No section specified. Please specify a section or set StateFile.namespace.\")\\n            section = self.namespace\\n\\n        # check if the section exists\\n        if section not in self.loaded_cfg.sections():\\n            raise configparser.NoSectionError(f\"Section \\'{section}\\' does not exist in the file: {self.path}\")\\n\\n        # check if the key exists\\n        if self.loaded_cfg.has_option(section, key):\\n            self.loaded_cfg.remove_option(section, key)\\n        else:\\n            raise configparser.NoOptionError(option=key, section=section)\\n\\n    \"\"\"\\n    Operations - File I/O.\\n    \"\"\"\\n\\n    def load(self):\\n        \"\"\"Load the configuration file into memory.\\n\\n        This function reads the contents of the configuration file into memory.\\n        If the file does not exist, it creates an empty file.\\n        \"\"\"\\n        self.loaded_cfg = ConfigParser()\\n        self.loaded_cfg.read(self.path)\\n\\n    def save(self):\\n        \"\"\"Save the configuration file to disk.\"\"\"\\n        with open(self.path, \"w+\") as f:\\n            self.loaded_cfg.write(f)'),\n",
       " Document(metadata={}, page_content='def configure_x11(statefile: StateFile) -> dict[str, str]:\\n    \"\"\"Configure X11 forwarding by creating and managing a temporary .xauth file.\\n\\n    If xauth is not installed, the function prints an error message and exits. The message\\n    instructs the user to install xauth with \\'apt install xauth\\'.\\n\\n    If the .xauth file does not exist, the function creates it and configures it with the necessary\\n    xauth cookie.\\n\\n    Args:\\n        statefile: An instance of the configuration file class.\\n\\n    Returns:\\n        A dictionary with two key-value pairs:\\n\\n        - \"__ISAACLAB_TMP_XAUTH\": The path to the temporary .xauth file.\\n        - \"__ISAACLAB_TMP_DIR\": The path to the directory where the temporary .xauth file is stored.\\n\\n    \"\"\"\\n    # check if xauth is installed\\n    if not shutil.which(\"xauth\"):\\n        print(\"[INFO] xauth is not installed.\")\\n        print(\"[INFO] Please install it with \\'apt install xauth\\'\")\\n        exit(1)\\n\\n    # set the namespace to X11 for the statefile\\n    statefile.namespace = \"X11\"\\n    # load the value of the temporary xauth file\\n    tmp_xauth_value = statefile.get_variable(\"__ISAACLAB_TMP_XAUTH\")\\n\\n    if tmp_xauth_value is None or not Path(tmp_xauth_value).exists():\\n        # create a temporary directory to store the .xauth file\\n        tmp_dir = subprocess.run([\"mktemp\", \"-d\"], capture_output=True, text=True, check=True).stdout.strip()\\n        # create the .xauth file\\n        tmp_xauth_value = create_x11_tmpfile(tmpdir=Path(tmp_dir))\\n        # set the statefile variable\\n        statefile.set_variable(\"__ISAACLAB_TMP_XAUTH\", str(tmp_xauth_value))\\n    else:\\n        tmp_dir = Path(tmp_xauth_value).parent\\n\\n    return {\"__ISAACLAB_TMP_XAUTH\": str(tmp_xauth_value), \"__ISAACLAB_TMP_DIR\": str(tmp_dir)}'),\n",
       " Document(metadata={}, page_content='def x11_check(statefile: StateFile) -> tuple[list[str], dict[str, str]] | None:\\n    \"\"\"Check and configure X11 forwarding based on user input and existing state.\\n\\n    This function checks if X11 forwarding is enabled in the configuration file. If it is not configured,\\n    the function prompts the user to enable or disable X11 forwarding. If X11 forwarding is enabled, the function\\n    configures X11 forwarding by creating a temporary .xauth file.\\n\\n    Args:\\n        statefile: An instance of the configuration file class.\\n\\n    Returns:\\n        If X11 forwarding is enabled, the function returns a tuple containing the following:\\n\\n        - A list containing the x11.yaml file configuration option for docker-compose.\\n        - A dictionary containing the environment variables for the container.\\n\\n        If X11 forwarding is disabled, the function returns None.\\n    \"\"\"\\n    # set the namespace to X11 for the statefile\\n    statefile.namespace = \"X11\"\\n    # check if X11 forwarding is enabled\\n    is_x11_forwarding_enabled = statefile.get_variable(\"X11_FORWARDING_ENABLED\")\\n\\n    if is_x11_forwarding_enabled is None:\\n        print(\"[INFO] X11 forwarding from the Isaac Lab container is disabled by default.\")\\n        print(\\n            \"[INFO] It will fail if there is no display, or this script is being run via ssh without proper\"\\n            \" configuration.\"\\n        )\\n        x11_answer = input(\"Would you like to enable it? (y/N) \")\\n\\n        # parse the user\\'s input\\n        if x11_answer.lower() == \"y\":\\n            is_x11_forwarding_enabled = \"1\"\\n            print(\"[INFO] X11 forwarding is enabled from the container.\")\\n        else:\\n            is_x11_forwarding_enabled = \"0\"\\n            print(\"[INFO] X11 forwarding is disabled from the container.\")\\n\\n        # remember the user\\'s choice and set the statefile variable\\n        statefile.set_variable(\"X11_FORWARDING_ENABLED\", is_x11_forwarding_enabled)\\n    else:\\n        # print the current configuration\\n        print(f\"[INFO] X11 Forwarding is configured as \\'{is_x11_forwarding_enabled}\\' in \\'.container.cfg\\'.\")\\n\\n        # print help message to enable/disable X11 forwarding\\n        if is_x11_forwarding_enabled == \"1\":\\n            print(\"\\\\tTo disable X11 forwarding, set \\'X11_FORWARDING_ENABLED=0\\' in \\'.container.cfg\\'.\")\\n        else:\\n            print(\"\\\\tTo enable X11 forwarding, set \\'X11_FORWARDING_ENABLED=1\\' in \\'.container.cfg\\'.\")\\n\\n    if is_x11_forwarding_enabled == \"1\":\\n        x11_envars = configure_x11(statefile)\\n        # If X11 forwarding is enabled, return the proper args to\\n        # compose the x11.yaml file. Else, return an empty string.\\n        return [\"--file\", \"x11.yaml\"], x11_envars\\n\\n    return None'),\n",
       " Document(metadata={}, page_content='def x11_cleanup(statefile: StateFile):\\n    \"\"\"Clean up the temporary .xauth file used for X11 forwarding.\\n\\n    If the .xauth file exists, this function deletes it and remove the corresponding state variable.\\n\\n    Args:\\n        statefile: An instance of the configuration file class.\\n    \"\"\"\\n    # set the namespace to X11 for the statefile\\n    statefile.namespace = \"X11\"\\n\\n    # load the value of the temporary xauth file\\n    tmp_xauth_value = statefile.get_variable(\"__ISAACLAB_TMP_XAUTH\")\\n\\n    # if the file exists, delete it and remove the state variable\\n    if tmp_xauth_value is not None and Path(tmp_xauth_value).exists():\\n        print(f\"[INFO] Removing temporary Isaac Lab \\'.xauth\\' file: {tmp_xauth_value}.\")\\n        Path(tmp_xauth_value).unlink()\\n        statefile.delete_variable(\"__ISAACLAB_TMP_XAUTH\")'),\n",
       " Document(metadata={}, page_content='def create_x11_tmpfile(tmpfile: Path | None = None, tmpdir: Path | None = None) -> Path:\\n    \"\"\"Creates an .xauth file with an MIT-MAGIC-COOKIE derived from the current ``DISPLAY`` environment variable.\\n\\n    Args:\\n        tmpfile: A Path to a file which will be filled with the correct .xauth info.\\n        tmpdir: A Path to the directory where a random tmp file will be made.\\n            This is used as an ``--tmpdir arg`` to ``mktemp`` bash command.\\n\\n    Returns:\\n        The Path to the .xauth file.\\n    \"\"\"\\n    if tmpfile is None:\\n        if tmpdir is None:\\n            add_tmpdir = \"\"\\n        else:\\n            add_tmpdir = f\"--tmpdir={tmpdir}\"\\n        # Create .tmp file with .xauth suffix\\n        tmp_xauth = Path(\\n            subprocess.run(\\n                [\"mktemp\", \"--suffix=.xauth\", f\"{add_tmpdir}\"], capture_output=True, text=True, check=True\\n            ).stdout.strip()\\n        )\\n    else:\\n        tmpfile.touch()\\n        tmp_xauth = tmpfile\\n\\n    # Derive current MIT-MAGIC-COOKIE and make it universally addressable\\n    xauth_cookie = subprocess.run(\\n        [\"xauth\", \"nlist\", os.environ[\"DISPLAY\"]], capture_output=True, text=True, check=True\\n    ).stdout.replace(\"ffff\", \"\")\\n\\n    # Merge the new cookie into the create .tmp file\\n    subprocess.run([\"xauth\", \"-f\", tmp_xauth, \"nmerge\", \"-\"], input=xauth_cookie, text=True, check=True)\\n\\n    return tmp_xauth'),\n",
       " Document(metadata={}, page_content='def x11_refresh(statefile: StateFile):\\n    \"\"\"Refresh the temporary .xauth file used for X11 forwarding.\\n\\n    If x11 is enabled, this function generates a new .xauth file with the current MIT-MAGIC-COOKIE-1.\\n    The new file uses the same filename so that the bind-mount and ``XAUTHORITY`` var from build-time\\n    still work.\\n\\n    As the envar ``DISPLAY` informs the contents of the MIT-MAGIC-COOKIE-1, that value within the container\\n    will also need to be updated to the current value on the host. Currently, this done automatically in\\n    :meth:`ContainerInterface.enter` method.\\n\\n    The function exits if X11 forwarding is enabled but the temporary .xauth file does not exist. In this case,\\n    the user must rebuild the container.\\n\\n    Args:\\n        statefile: An instance of the configuration file class.\\n    \"\"\"\\n    # set the namespace to X11 for the statefile\\n    statefile.namespace = \"X11\"\\n\\n    # check if X11 forwarding is enabled\\n    is_x11_forwarding_enabled = statefile.get_variable(\"X11_FORWARDING_ENABLED\")\\n    # load the value of the temporary xauth file\\n    tmp_xauth_value = statefile.get_variable(\"__ISAACLAB_TMP_XAUTH\")\\n\\n    # print the current configuration\\n    if is_x11_forwarding_enabled is not None:\\n        status = \"enabled\" if is_x11_forwarding_enabled == \"1\" else \"disabled\"\\n        print(f\"[INFO] X11 Forwarding is {status} from the settings in \\'.container.cfg\\'\")\\n\\n    # if the file exists, delete it and create a new one\\n    if tmp_xauth_value is not None and Path(tmp_xauth_value).exists():\\n        # remove the file and create a new one\\n        Path(tmp_xauth_value).unlink()\\n        create_x11_tmpfile(tmpfile=Path(tmp_xauth_value))\\n        # update the statefile with the new path\\n        statefile.set_variable(\"__ISAACLAB_TMP_XAUTH\", str(tmp_xauth_value))\\n    elif tmp_xauth_value is None:\\n        if is_x11_forwarding_enabled is not None and is_x11_forwarding_enabled == \"1\":\\n            print(\\n                \"[ERROR] X11 forwarding is enabled but the temporary .xauth file does not exist.\"\\n                \" Please rebuild the container by running: \\'./docker/container.py start\\'\"\\n            )\\n            sys.exit(1)\\n        else:\\n            print(\"[INFO] X11 forwarding is disabled. No action taken.\")'),\n",
       " Document(metadata={}, page_content='def skip_member(app, what, name, obj, skip, options):\\n    # List the names of the functions you want to skip here\\n    exclusions = [\"from_dict\", \"to_dict\", \"replace\", \"copy\", \"validate\", \"__post_init__\"]\\n    if name in exclusions:\\n        return True\\n    return None'),\n",
       " Document(metadata={}, page_content='def setup(app):\\n    app.connect(\"autodoc-skip-member\", skip_member)'),\n",
       " Document(metadata={}, page_content=\"# Building Documentation\\n\\nWe use [Sphinx](https://www.sphinx-doc.org/en/master/) with the [Book Theme](https://sphinx-book-theme.readthedocs.io/en/stable/) for maintaining and generating our documentation.\\n\\n> **Note:** To avoid dependency conflicts, we strongly recommend using a Python virtual environment to isolate the required dependencies from your system's global Python environment.\\n\\n## Current-Version Documentation\"),\n",
       " Document(metadata={}, page_content='## Current-Version Documentation\\n\\nThis section describes how to build the documentation for the current version of the project.\\n\\n<details open>\\n<summary><strong>Linux</strong></summary>\\n\\n```bash\\n# 1. Navigate to the docs directory and install dependencies\\ncd docs\\npip install -r requirements.txt\\n\\n# 2. Build the current documentation\\nmake current-docs\\n\\n# 3. Open the current docs\\nxdg-open _build/current/index.html\\n```\\n</details>\\n\\n<details> <summary><strong>Windows</strong></summary>'),\n",
       " Document(metadata={}, page_content='```batch\\n:: 1. Navigate to the docs directory and install dependencies\\ncd docs\\npip install -r requirements.txt\\n\\n:: 2. Build the current documentation\\nmake current-docs\\n\\n:: 3. Open the current docs\\nstart _build\\\\current\\\\index.html\\n```\\n</details>\\n\\n\\n## Multi-Version Documentation\\n\\nThis section describes how to build the multi-version documentation, which includes previous tags and the main branch.\\n\\n<details open> <summary><strong>Linux</strong></summary>'),\n",
       " Document(metadata={}, page_content='```bash\\n# 1. Navigate to the docs directory and install dependencies\\ncd docs\\npip install -r requirements.txt\\n\\n# 2. Build the multi-version documentation\\nmake multi-docs\\n\\n# 3. Open the multi-version docs\\nxdg-open _build/index.html\\n```\\n</details>\\n\\n<details> <summary><strong>Windows</strong></summary>\\n\\n```batch\\n:: 1. Navigate to the docs directory and install dependencies\\ncd docs\\npip install -r requirements.txt\\n\\n:: 2. Build the multi-version documentation\\nmake multi-docs'),\n",
       " Document(metadata={}, page_content=':: 3. Open the multi-version docs\\nstart _build\\\\index.html\\n```\\n</details>'),\n",
       " Document(metadata={}, page_content='def create_camera_base(\\n    camera_cfg: type[CameraCfg | TiledCameraCfg],\\n    num_cams: int,\\n    data_types: list[str],\\n    height: int,\\n    width: int,\\n    prim_path: str | None = None,\\n    instantiate: bool = True,\\n) -> Camera | TiledCamera | CameraCfg | TiledCameraCfg | None:\\n    \"\"\"Generalized function to create a camera or tiled camera sensor.\"\"\"\\n    # Determine prim prefix based on the camera class\\n    name = camera_cfg.class_type.__name__\\n\\n    if instantiate:\\n        # Create the necessary prims\\n        for idx in range(num_cams):\\n            prim_utils.create_prim(f\"/World/{name}_{idx:02d}\", \"Xform\")\\n    if prim_path is None:\\n        prim_path = f\"/World/{name}_.*/{name}\"\\n    # If valid camera settings are provided, create the camera\\n    if num_cams > 0 and len(data_types) > 0 and height > 0 and width > 0:\\n        cfg = camera_cfg(\\n            prim_path=prim_path,\\n            update_period=0,\\n            height=height,\\n            width=width,\\n            data_types=data_types,\\n            spawn=sim_utils.PinholeCameraCfg(\\n                focal_length=24, focus_distance=400.0, horizontal_aperture=20.955, clipping_range=(0.1, 1e4)\\n            ),\\n        )\\n        if instantiate:\\n            return camera_cfg.class_type(cfg=cfg)\\n        else:\\n            return cfg\\n    else:\\n        return None'),\n",
       " Document(metadata={}, page_content='def create_tiled_cameras(\\n    num_cams: int = 2, data_types: list[str] | None = None, height: int = 100, width: int = 120\\n) -> TiledCamera | None:\\n    if data_types is None:\\n        data_types = [\"rgb\", \"depth\"]\\n    \"\"\"Defines the tiled camera sensor to add to the scene.\"\"\"\\n    return create_camera_base(\\n        camera_cfg=TiledCameraCfg,\\n        num_cams=num_cams,\\n        data_types=data_types,\\n        height=height,\\n        width=width,\\n    )'),\n",
       " Document(metadata={}, page_content='def create_cameras(\\n    num_cams: int = 2, data_types: list[str] | None = None, height: int = 100, width: int = 120\\n) -> Camera | None:\\n    \"\"\"Defines the Standard cameras.\"\"\"\\n    if data_types is None:\\n        data_types = [\"rgb\", \"depth\"]\\n    return create_camera_base(\\n        camera_cfg=CameraCfg, num_cams=num_cams, data_types=data_types, height=height, width=width\\n    )'),\n",
       " Document(metadata={}, page_content='def create_ray_caster_cameras(\\n    num_cams: int = 2,\\n    data_types: list[str] = [\"distance_to_image_plane\"],\\n    mesh_prim_paths: list[str] = [\"/World/ground\"],\\n    height: int = 100,\\n    width: int = 120,\\n    prim_path: str = \"/World/RayCasterCamera_.*/RayCaster\",\\n    instantiate: bool = True,\\n) -> RayCasterCamera | RayCasterCameraCfg | None:\\n    \"\"\"Create the raycaster cameras; different configuration than Standard/Tiled camera\"\"\"\\n    for idx in range(num_cams):\\n        prim_utils.create_prim(f\"/World/RayCasterCamera_{idx:02d}/RayCaster\", \"Xform\")\\n\\n    if num_cams > 0 and len(data_types) > 0 and height > 0 and width > 0:\\n        cam_cfg = RayCasterCameraCfg(\\n            prim_path=prim_path,\\n            mesh_prim_paths=mesh_prim_paths,\\n            update_period=0,\\n            offset=RayCasterCameraCfg.OffsetCfg(pos=(0.0, 0.0, 0.0), rot=(1.0, 0.0, 0.0, 0.0)),\\n            data_types=data_types,\\n            debug_vis=False,\\n            pattern_cfg=patterns.PinholeCameraPatternCfg(\\n                focal_length=24.0,\\n                horizontal_aperture=20.955,\\n                height=480,\\n                width=640,\\n            ),\\n        )\\n        if instantiate:\\n            return RayCasterCamera(cfg=cam_cfg)\\n        else:\\n            return cam_cfg\\n\\n    else:\\n        return None'),\n",
       " Document(metadata={}, page_content='def create_tiled_camera_cfg(prim_path: str) -> TiledCameraCfg:\\n    \"\"\"Grab a simple tiled camera config for injecting into task environments.\"\"\"\\n    return create_camera_base(\\n        TiledCameraCfg,\\n        num_cams=args_cli.num_tiled_cameras,\\n        data_types=args_cli.tiled_camera_data_types,\\n        width=args_cli.width,\\n        height=args_cli.height,\\n        prim_path=\"{ENV_REGEX_NS}/\" + prim_path,\\n        instantiate=False,\\n    )'),\n",
       " Document(metadata={}, page_content='def create_standard_camera_cfg(prim_path: str) -> CameraCfg:\\n    \"\"\"Grab a simple standard camera config for injecting into task environments.\"\"\"\\n    return create_camera_base(\\n        CameraCfg,\\n        num_cams=args_cli.num_standard_cameras,\\n        data_types=args_cli.standard_camera_data_types,\\n        width=args_cli.width,\\n        height=args_cli.height,\\n        prim_path=\"{ENV_REGEX_NS}/\" + prim_path,\\n        instantiate=False,\\n    )'),\n",
       " Document(metadata={}, page_content='def create_ray_caster_camera_cfg(prim_path: str) -> RayCasterCameraCfg:\\n    \"\"\"Grab a simple ray caster config for injecting into task environments.\"\"\"\\n    return create_ray_caster_cameras(\\n        num_cams=args_cli.num_ray_caster_cameras,\\n        data_types=args_cli.ray_caster_camera_data_types,\\n        width=args_cli.width,\\n        height=args_cli.height,\\n        prim_path=\"{ENV_REGEX_NS}/\" + prim_path,\\n    )'),\n",
       " Document(metadata={}, page_content='def design_scene(\\n    num_tiled_cams: int = 2,\\n    num_standard_cams: int = 0,\\n    num_ray_caster_cams: int = 0,\\n    tiled_camera_data_types: list[str] | None = None,\\n    standard_camera_data_types: list[str] | None = None,\\n    ray_caster_camera_data_types: list[str] | None = None,\\n    height: int = 100,\\n    width: int = 200,\\n    num_objects: int = 20,\\n    mesh_prim_paths: list[str] = [\"/World/ground\"],\\n) -> dict:\\n    \"\"\"Design the scene.\"\"\"\\n    if tiled_camera_data_types is None:\\n        tiled_camera_data_types = [\"rgb\"]\\n    if standard_camera_data_types is None:\\n        standard_camera_data_types = [\"rgb\"]\\n    if ray_caster_camera_data_types is None:\\n        ray_caster_camera_data_types = [\"distance_to_image_plane\"]\\n\\n    # Populate scene\\n    # -- Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/ground\", cfg)\\n    # -- Lights\\n    cfg = sim_utils.DistantLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create a dictionary for the scene entities\\n    scene_entities = {}\\n\\n    # Xform to hold objects\\n    prim_utils.create_prim(\"/World/Objects\", \"Xform\")\\n    # Random objects\\n    for i in range(num_objects):\\n        # sample random position\\n        position = np.random.rand(3) - np.asarray([0.05, 0.05, -1.0])\\n        position *= np.asarray([1.5, 1.5, 0.5])\\n        # sample random color\\n        color = (random.random(), random.random(), random.random())\\n        # choose random prim type\\n        prim_type = random.choice([\"Cube\", \"Cone\", \"Cylinder\"])\\n        common_properties = {\\n            \"rigid_props\": sim_utils.RigidBodyPropertiesCfg(),\\n            \"mass_props\": sim_utils.MassPropertiesCfg(mass=5.0),\\n            \"collision_props\": sim_utils.CollisionPropertiesCfg(),\\n            \"visual_material\": sim_utils.PreviewSurfaceCfg(diffuse_color=color, metallic=0.5),\\n            \"semantic_tags\": [(\"class\", prim_type)],\\n        }\\n        if prim_type == \"Cube\":\\n            shape_cfg = sim_utils.CuboidCfg(size=(0.25, 0.25, 0.25), **common_properties)\\n        elif prim_type == \"Cone\":\\n            shape_cfg = sim_utils.ConeCfg(radius=0.1, height=0.25, **common_properties)\\n        elif prim_type == \"Cylinder\":\\n            shape_cfg = sim_utils.CylinderCfg(radius=0.25, height=0.25, **common_properties)\\n        # Rigid Object\\n        obj_cfg = RigidObjectCfg(\\n            prim_path=f\"/World/Objects/Obj_{i:02d}\",\\n            spawn=shape_cfg,\\n            init_state=RigidObjectCfg.InitialStateCfg(pos=position),\\n        )\\n        scene_entities[f\"rigid_object{i}\"] = RigidObject(cfg=obj_cfg)\\n\\n    # Sensors\\n    standard_camera = create_cameras(\\n        num_cams=num_standard_cams, data_types=standard_camera_data_types, height=height, width=width\\n    )\\n    tiled_camera = create_tiled_cameras(\\n        num_cams=num_tiled_cams, data_types=tiled_camera_data_types, height=height, width=width\\n    )\\n    ray_caster_camera = create_ray_caster_cameras(\\n        num_cams=num_ray_caster_cams,\\n        data_types=ray_caster_camera_data_types,\\n        mesh_prim_paths=mesh_prim_paths,\\n        height=height,\\n        width=width,\\n    )\\n    # return the scene information\\n    if tiled_camera is not None:\\n        scene_entities[\"tiled_camera\"] = tiled_camera\\n    if standard_camera is not None:\\n        scene_entities[\"standard_camera\"] = standard_camera\\n    if ray_caster_camera is not None:\\n        scene_entities[\"ray_caster_camera\"] = ray_caster_camera\\n    return scene_entities'),\n",
       " Document(metadata={}, page_content='def inject_cameras_into_task(\\n    task: str,\\n    num_cams: int,\\n    camera_name_prefix: str,\\n    camera_creation_callable: Callable,\\n    num_cameras_per_env: int = 1,\\n) -> gym.Env:\\n    \"\"\"Loads the task, sticks cameras into the config, and creates the environment.\"\"\"\\n    cfg = load_cfg_from_registry(task, \"env_cfg_entry_point\")\\n    cfg.sim.device = args_cli.device\\n    cfg.sim.use_fabric = args_cli.use_fabric\\n    scene_cfg = cfg.scene\\n\\n    num_envs = int(num_cams / num_cameras_per_env)\\n    scene_cfg.num_envs = num_envs\\n\\n    for idx in range(num_cameras_per_env):\\n        suffix = \"\" if idx == 0 else str(idx)\\n        name = camera_name_prefix + suffix\\n        setattr(scene_cfg, name, camera_creation_callable(name))\\n    cfg.scene = scene_cfg\\n    env = gym.make(task, cfg=cfg)\\n    return env'),\n",
       " Document(metadata={}, page_content='def get_utilization_percentages(reset: bool = False, max_values: list[float] = [0.0, 0.0, 0.0, 0.0]) -> list[float]:\\n    \"\"\"Get the maximum CPU, RAM, GPU utilization (processing), and\\n    GPU memory usage percentages since the last time reset was true.\"\"\"\\n    if reset:\\n        max_values[:] = [0, 0, 0, 0]  # Reset the max values\\n\\n    # CPU utilization\\n    cpu_usage = psutil.cpu_percent(interval=0.1)\\n    max_values[0] = max(max_values[0], cpu_usage)\\n\\n    # RAM utilization\\n    memory_info = psutil.virtual_memory()\\n    ram_usage = memory_info.percent\\n    max_values[1] = max(max_values[1], ram_usage)\\n\\n    # GPU utilization using pynvml\\n    if torch.cuda.is_available():\\n\\n        if args_cli.autotune:\\n            pynvml.nvmlInit()  # Initialize NVML\\n            for i in range(torch.cuda.device_count()):\\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\\n\\n                # GPU Utilization\\n                gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\\n                gpu_processing_utilization_percent = gpu_utilization.gpu  # GPU core utilization\\n                max_values[2] = max(max_values[2], gpu_processing_utilization_percent)\\n\\n                # GPU Memory Usage\\n                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\\n                gpu_memory_total = memory_info.total\\n                gpu_memory_used = memory_info.used\\n                gpu_memory_utilization_percent = (gpu_memory_used / gpu_memory_total) * 100\\n                max_values[3] = max(max_values[3], gpu_memory_utilization_percent)\\n\\n            pynvml.nvmlShutdown()  # Shutdown NVML after usage\\n    else:\\n        gpu_processing_utilization_percent = None\\n        gpu_memory_utilization_percent = None\\n    return max_values'),\n",
       " Document(metadata={}, page_content='def run_simulator(\\n    sim: sim_utils.SimulationContext | None,\\n    scene_entities: dict | InteractiveScene,\\n    warm_start_length: int = 10,\\n    experiment_length: int = 100,\\n    tiled_camera_data_types: list[str] | None = None,\\n    standard_camera_data_types: list[str] | None = None,\\n    ray_caster_camera_data_types: list[str] | None = None,\\n    depth_predicate: Callable = lambda x: \"to\" in x or x == \"depth\",\\n    perspective_depth_predicate: Callable = lambda x: x == \"distance_to_camera\",\\n    convert_depth_to_camera_to_image_plane: bool = True,\\n    max_cameras_per_env: int = 1,\\n    env: gym.Env | None = None,\\n) -> dict:\\n    \"\"\"Run the simulator with all cameras, and return timing analytics. Visualize if desired.\"\"\"\\n\\n    if tiled_camera_data_types is None:\\n        tiled_camera_data_types = [\"rgb\"]\\n    if standard_camera_data_types is None:\\n        standard_camera_data_types = [\"rgb\"]\\n    if ray_caster_camera_data_types is None:\\n        ray_caster_camera_data_types = [\"distance_to_image_plane\"]\\n\\n    # Initialize camera lists\\n    tiled_cameras = []\\n    standard_cameras = []\\n    ray_caster_cameras = []\\n\\n    # Dynamically extract cameras from the scene entities up to max_cameras_per_env\\n    for i in range(max_cameras_per_env):\\n        # Extract tiled cameras\\n        tiled_camera_key = f\"tiled_camera{i}\" if i > 0 else \"tiled_camera\"\\n        standard_camera_key = f\"standard_camera{i}\" if i > 0 else \"standard_camera\"\\n        ray_caster_camera_key = f\"ray_caster_camera{i}\" if i > 0 else \"ray_caster_camera\"\\n\\n        try:  # if instead you checked ... if key is in scene_entities... # errors out always even if key present\\n            tiled_cameras.append(scene_entities[tiled_camera_key])\\n            standard_cameras.append(scene_entities[standard_camera_key])\\n            ray_caster_cameras.append(scene_entities[ray_caster_camera_key])\\n        except KeyError:\\n            break\\n\\n    # Initialize camera counts\\n    camera_lists = [tiled_cameras, standard_cameras, ray_caster_cameras]\\n    camera_data_types = [tiled_camera_data_types, standard_camera_data_types, ray_caster_camera_data_types]\\n    labels = [\"tiled\", \"standard\", \"ray_caster\"]\\n\\n    if sim is not None:\\n        # Set camera world poses\\n        for camera_list in camera_lists:\\n            for camera in camera_list:\\n                num_cameras = camera.data.intrinsic_matrices.size(0)\\n                positions = torch.tensor([[2.5, 2.5, 2.5]], device=sim.device).repeat(num_cameras, 1)\\n                targets = torch.tensor([[0.0, 0.0, 0.0]], device=sim.device).repeat(num_cameras, 1)\\n                camera.set_world_poses_from_view(positions, targets)\\n\\n    # Initialize timing variables\\n    timestep = 0\\n    total_time = 0.0\\n    valid_timesteps = 0\\n    sim_step_time = 0.0\\n\\n    while simulation_app.is_running() and timestep < experiment_length:\\n        print(f\"On timestep {timestep} of {experiment_length}, with warm start of {warm_start_length}\")\\n        get_utilization_percentages()\\n\\n        # Measure the total simulation step time\\n        step_start_time = time.time()\\n\\n        if sim is not None:\\n            sim.step()\\n\\n        if env is not None:\\n            with torch.inference_mode():\\n                # compute zero actions\\n                actions = torch.zeros(env.action_space.shape, device=env.unwrapped.device)\\n                # apply actions\\n                env.step(actions)\\n\\n        # Update cameras and process vision data within the simulation step\\n        clouds = {}\\n        images = {}\\n        depth_images = {}\\n\\n        # Loop through all camera lists and their data_types\\n        for camera_list, data_types, label in zip(camera_lists, camera_data_types, labels):\\n            for cam_idx, camera in enumerate(camera_list):\\n\\n                if env is None:  # No env, need to step cams manually\\n                    # Only update the camera if it hasn\\'t been updated as part of scene_entities.update ...\\n                    camera.update(dt=sim.get_physics_dt())\\n\\n                for data_type in data_types:\\n                    data_label = f\"{label}_{cam_idx}_{data_type}\"\\n\\n                    if depth_predicate(data_type):  # is a depth image, want to create cloud\\n                        depth = camera.data.output[data_type]\\n                        depth_images[data_label + \"_raw\"] = depth\\n                        if perspective_depth_predicate(data_type) and convert_depth_to_camera_to_image_plane:\\n                            depth = orthogonalize_perspective_depth(\\n                                camera.data.output[data_type], camera.data.intrinsic_matrices\\n                            )\\n                            depth_images[data_label + \"_undistorted\"] = depth\\n\\n                        pointcloud = unproject_depth(depth=depth, intrinsics=camera.data.intrinsic_matrices)\\n                        clouds[data_label] = pointcloud\\n                    else:  # rgb image, just save it\\n                        image = camera.data.output[data_type]\\n                        images[data_label] = image\\n\\n        # End timing for the step\\n        step_end_time = time.time()\\n        sim_step_time += step_end_time - step_start_time\\n\\n        if timestep > warm_start_length:\\n            get_utilization_percentages(reset=True)\\n            total_time += step_end_time - step_start_time\\n            valid_timesteps += 1\\n\\n        timestep += 1\\n\\n    # Calculate average timings\\n    if valid_timesteps > 0:\\n        avg_timestep_duration = total_time / valid_timesteps\\n        avg_sim_step_duration = sim_step_time / experiment_length\\n    else:\\n        avg_timestep_duration = 0.0\\n        avg_sim_step_duration = 0.0\\n\\n    # Package timing analytics in a dictionary\\n    timing_analytics = {\\n        \"average_timestep_duration\": avg_timestep_duration,\\n        \"average_sim_step_duration\": avg_sim_step_duration,\\n        \"total_simulation_time\": sim_step_time,\\n        \"total_experiment_duration\": sim_step_time,\\n    }\\n\\n    system_utilization_analytics = get_utilization_percentages()\\n\\n    print(\"--- Benchmark Results ---\")\\n    print(f\"Average timestep duration: {avg_timestep_duration:.6f} seconds\")\\n    print(f\"Average simulation step duration: {avg_sim_step_duration:.6f} seconds\")\\n    print(f\"Total simulation time: {sim_step_time:.6f} seconds\")\\n    print(\"\\\\nSystem Utilization Statistics:\")\\n    print(\\n        f\"| CPU:{system_utilization_analytics[0]}% | \"\\n        f\"RAM:{system_utilization_analytics[1]}% | \"\\n        f\"GPU Compute:{system_utilization_analytics[2]}% | \"\\n        f\" GPU Memory: {system_utilization_analytics[3]:.2f}% |\"\\n    )\\n\\n    return {\"timing_analytics\": timing_analytics, \"system_utilization_analytics\": system_utilization_analytics}'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load simulation context\\n    if args_cli.num_tiled_cameras + args_cli.num_standard_cameras + args_cli.num_ray_caster_cameras <= 0:\\n        raise ValueError(\"You must select at least one camera.\")\\n    if (\\n        (args_cli.num_tiled_cameras > 0 and args_cli.num_standard_cameras > 0)\\n        or (args_cli.num_ray_caster_cameras > 0 and args_cli.num_standard_cameras > 0)\\n        or (args_cli.num_ray_caster_cameras > 0 and args_cli.num_tiled_cameras > 0)\\n    ):\\n        print(\"[WARNING]: You have elected to use more than one camera type.\")\\n        print(\"[WARNING]: For a benchmark to be meaningful, use ONLY ONE camera type at a time.\")\\n        print(\\n            \"[WARNING]: For example, if num_tiled_cameras=100, for a meaningful benchmark,\"\\n            \"num_standard_cameras should be 0, and num_ray_caster_cameras should be 0\"\\n        )\\n        raise ValueError(\"Benchmark one camera at a time.\")\\n\\n    print(\"[INFO]: Designing the scene\")\\n    if args_cli.task is None:\\n        print(\"[INFO]: No task environment provided, creating random scene.\")\\n        sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n        sim = sim_utils.SimulationContext(sim_cfg)\\n        # Set main camera\\n        sim.set_camera_view([2.5, 2.5, 2.5], [0.0, 0.0, 0.0])\\n        scene_entities = design_scene(\\n            num_tiled_cams=args_cli.num_tiled_cameras,\\n            num_standard_cams=args_cli.num_standard_cameras,\\n            num_ray_caster_cams=args_cli.num_ray_caster_cameras,\\n            tiled_camera_data_types=args_cli.tiled_camera_data_types,\\n            standard_camera_data_types=args_cli.standard_camera_data_types,\\n            ray_caster_camera_data_types=args_cli.ray_caster_camera_data_types,\\n            height=args_cli.height,\\n            width=args_cli.width,\\n            num_objects=args_cli.num_objects,\\n            mesh_prim_paths=args_cli.ray_caster_visible_mesh_prim_paths,\\n        )\\n        # Play simulator\\n        sim.reset()\\n        # Now we are ready!\\n        print(\"[INFO]: Setup complete...\")\\n        # Run simulator\\n        run_simulator(\\n            sim=sim,\\n            scene_entities=scene_entities,\\n            warm_start_length=args_cli.warm_start_length,\\n            experiment_length=args_cli.experiment_length,\\n            tiled_camera_data_types=args_cli.tiled_camera_data_types,\\n            standard_camera_data_types=args_cli.standard_camera_data_types,\\n            ray_caster_camera_data_types=args_cli.ray_caster_camera_data_types,\\n            convert_depth_to_camera_to_image_plane=args_cli.convert_depth_to_camera_to_image_plane,\\n        )\\n    else:\\n        print(\"[INFO]: Using known task environment, injecting cameras.\")\\n        autotune_iter = 0\\n        max_sys_util_thresh = [0.0, 0.0, 0.0]\\n        max_num_cams = max(args_cli.num_tiled_cameras, args_cli.num_standard_cameras, args_cli.num_ray_caster_cameras)\\n        cur_num_cams = max_num_cams\\n        cur_sys_util = max_sys_util_thresh\\n        interval = args_cli.autotune_camera_count_interval\\n\\n        if args_cli.autotune:\\n            max_sys_util_thresh = args_cli.autotune_max_percentage_util\\n            max_num_cams = args_cli.autotune_max_camera_count\\n            print(\"[INFO]: Auto tuning until any of the following threshold are met\")\\n            print(f\"|CPU: {max_sys_util_thresh[0]}% | RAM {max_sys_util_thresh[1]}% | GPU: {max_sys_util_thresh[2]}% |\")\\n            print(f\"[INFO]: Maximum number of cameras allowed: {max_num_cams}\")\\n        # Determine which camera is being tested...\\n        tiled_camera_cfg = create_tiled_camera_cfg(\"tiled_camera\")\\n        standard_camera_cfg = create_standard_camera_cfg(\"standard_camera\")\\n        ray_caster_camera_cfg = create_ray_caster_camera_cfg(\"ray_caster_camera\")\\n        camera_name_prefix = \"\"\\n        camera_creation_callable = None\\n        num_cams = 0\\n        if tiled_camera_cfg is not None:\\n            camera_name_prefix = \"tiled_camera\"\\n            camera_creation_callable = create_tiled_camera_cfg\\n            num_cams = args_cli.num_tiled_cameras\\n        elif standard_camera_cfg is not None:\\n            camera_name_prefix = \"standard_camera\"\\n            camera_creation_callable = create_standard_camera_cfg\\n            num_cams = args_cli.num_standard_cameras\\n        elif ray_caster_camera_cfg is not None:\\n            camera_name_prefix = \"ray_caster_camera\"\\n            camera_creation_callable = create_ray_caster_camera_cfg\\n            num_cams = args_cli.num_ray_caster_cameras\\n\\n        while (\\n            all(cur <= max_thresh for cur, max_thresh in zip(cur_sys_util, max_sys_util_thresh))\\n            and cur_num_cams <= max_num_cams\\n        ):\\n            cur_num_cams = num_cams + interval * autotune_iter\\n            autotune_iter += 1\\n\\n            env = inject_cameras_into_task(\\n                task=args_cli.task,\\n                num_cams=cur_num_cams,\\n                camera_name_prefix=camera_name_prefix,\\n                camera_creation_callable=camera_creation_callable,\\n                num_cameras_per_env=args_cli.task_num_cameras_per_env,\\n            )\\n            env.reset()\\n            print(f\"Testing with {cur_num_cams} {camera_name_prefix}\")\\n            analysis = run_simulator(\\n                sim=None,\\n                scene_entities=env.unwrapped.scene,\\n                warm_start_length=args_cli.warm_start_length,\\n                experiment_length=args_cli.experiment_length,\\n                tiled_camera_data_types=args_cli.tiled_camera_data_types,\\n                standard_camera_data_types=args_cli.standard_camera_data_types,\\n                ray_caster_camera_data_types=args_cli.ray_caster_camera_data_types,\\n                convert_depth_to_camera_to_image_plane=args_cli.convert_depth_to_camera_to_image_plane,\\n                max_cameras_per_env=args_cli.task_num_cameras_per_env,\\n                env=env,\\n            )\\n\\n            cur_sys_util = analysis[\"system_utilization_analytics\"]\\n            print(\"Triggering reset...\")\\n            env.close()\\n            create_new_stage()\\n        print(\"[INFO]: DONE! Feel free to CTRL + C Me \")\\n        print(f\"[INFO]: If you\\'ve made it this far, you can likely simulate {cur_num_cams} {camera_name_prefix}\")\\n        print(\"Keep in mind, this is without any training running on the GPU.\")\\n        print(\"Set lower utilization thresholds to account for training.\")\\n\\n        if not args_cli.autotune:\\n            print(\"[WARNING]: GPU Util Statistics only correct while autotuning, ignore above.\")'),\n",
       " Document(metadata={}, page_content='class RobotSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Configuration for a simple scene with a robot.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # articulation\\n    if args_cli.robot == \"h1\":\\n        robot: ArticulationCfg = H1_MINIMAL_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n    elif args_cli.robot == \"g1\":\\n        robot: ArticulationCfg = G1_MINIMAL_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n    elif args_cli.robot == \"anymal_d\":\\n        robot: ArticulationCfg = ANYMAL_D_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n    else:\\n        raise ValueError(f\"Unsupported robot type: {args_cli.robot}.\")'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Extract scene entities\\n    # note: we only do this here for readability.\\n    robot = scene[\"robot\"]\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n\\n    # Start the timer for creating the scene\\n    step_time_begin = time.perf_counter_ns()\\n    num_steps = 2000\\n\\n    # Simulation loop\\n    for count in range(num_steps):\\n        # Reset\\n        if count % 500 == 0:\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = robot.data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            robot.write_root_pose_to_sim(root_state[:, :7])\\n            robot.write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = robot.data.default_joint_pos.clone(), robot.data.default_joint_vel.clone()\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n        # Apply random action\\n        # -- generate random joint efforts\\n        efforts = torch.randn_like(robot.data.joint_pos) * 5.0\\n        # -- apply action to the robot\\n        robot.set_joint_effort_target(efforts)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # Perform step\\n        sim.step()\\n        # Update buffers\\n        scene.update(sim_dt)\\n\\n    # Stop the timer for reset\\n    step_time_end = time.perf_counter_ns()\\n    print(f\"[INFO]: Per step time: {(step_time_end - step_time_begin) / num_steps / 1e6:.2f} ms\")'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(device=\"cuda:0\")\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 0.0, 4.0], [0.0, 0.0, 2.0])\\n\\n    # Start the timer for creating the scene\\n    setup_time_begin = time.perf_counter_ns()\\n    # Design scene\\n    scene_cfg = RobotSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Stop the timer for creating the scene\\n    setup_time_end = time.perf_counter_ns()\\n    print(f\"[INFO]: Scene creation time: {(setup_time_end - setup_time_begin) / 1e6:.2f} ms\")\\n\\n    # Start the timer for reset\\n    reset_time_begin = time.perf_counter_ns()\\n    # Play the simulator\\n    sim.reset()\\n    # Stop the timer for reset\\n    reset_time_end = time.perf_counter_ns()\\n    print(f\"[INFO]: Sim start time: {(reset_time_end - reset_time_begin) / 1e6:.2f} ms\")\\n\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: dict):\\n    \"\"\"Benchmark without RL in the loop.\"\"\"\\n\\n    # override configurations with non-hydra CLI arguments\\n    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\\n    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\\n\\n    # process distributed\\n    world_size = 1\\n    world_rank = 0\\n    if args_cli.distributed:\\n        env_cfg.sim.device = f\"cuda:{app_launcher.local_rank}\"\\n        world_size = int(os.getenv(\"WORLD_SIZE\", 1))\\n        world_rank = app_launcher.global_rank\\n\\n    task_startup_time_begin = time.perf_counter_ns()\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n    # wrap for video recording\\n    if args_cli.video:\\n        log_root_path = os.path.abs(f\"benchmark/{args_cli.task}\")\\n        log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_root_path, log_dir, \"videos\"),\\n            \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    task_startup_time_end = time.perf_counter_ns()\\n\\n    env.reset()\\n\\n    benchmark.set_phase(\"sim_runtime\")\\n\\n    # counter for number of frames to run for\\n    num_frames = 0\\n    # log frame times\\n    step_times = []\\n    while simulation_app.is_running():\\n        while num_frames < args_cli.num_frames:\\n            # get upper and lower bounds of action space, sample actions randomly on this interval\\n            action_high = 1\\n            action_low = -1\\n            actions = (action_high - action_low) * torch.rand(\\n                env.unwrapped.num_envs, env.unwrapped.single_action_space.shape[0], device=env.unwrapped.device\\n            ) - action_high\\n\\n            # env stepping\\n            env_step_time_begin = time.perf_counter_ns()\\n            _ = env.step(actions)\\n            end_step_time_end = time.perf_counter_ns()\\n            step_times.append(end_step_time_end - env_step_time_begin)\\n\\n            num_frames += 1\\n\\n        # terminate\\n        break\\n\\n    if world_rank == 0:\\n        benchmark.store_measurements()\\n\\n        # compute stats\\n        step_times = np.array(step_times) / 1e6  # ns to ms\\n        fps = 1.0 / (step_times / 1000)\\n        effective_fps = fps * env.unwrapped.num_envs * world_size\\n\\n        # prepare step timing dict\\n        environment_step_times = {\\n            \"Environment step times\": step_times.tolist(),\\n            \"Environment step FPS\": fps.tolist(),\\n            \"Environment step effective FPS\": effective_fps.tolist(),\\n        }\\n\\n        log_app_start_time(benchmark, (app_start_time_end - app_start_time_begin) / 1e6)\\n        log_python_imports_time(benchmark, (imports_time_end - imports_time_begin) / 1e6)\\n        log_task_start_time(benchmark, (task_startup_time_end - task_startup_time_begin) / 1e6)\\n        log_scene_creation_time(benchmark, Timer.get_timer_info(\"scene_creation\") * 1000)\\n        log_simulation_start_time(benchmark, Timer.get_timer_info(\"simulation_start\") * 1000)\\n        log_total_start_time(benchmark, (task_startup_time_end - app_start_time_begin) / 1e6)\\n        log_runtime_step_times(benchmark, environment_step_times, compute_stats=True)\\n\\n        benchmark.stop()\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: dict):\\n    \"\"\"Train with RL-Games agent.\"\"\"\\n\\n    # override configurations with non-hydra CLI arguments\\n    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\\n    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\\n\\n    # randomly sample a seed if seed = -1\\n    if args_cli.seed == -1:\\n        args_cli.seed = random.randint(0, 10000)\\n    agent_cfg[\"params\"][\"seed\"] = args_cli.seed if args_cli.seed is not None else agent_cfg[\"params\"][\"seed\"]\\n\\n    # process distributed\\n    world_rank = 0\\n    if args_cli.distributed:\\n        env_cfg.sim.device = f\"cuda:{app_launcher.local_rank}\"\\n        agent_cfg[\"params\"][\"config\"][\"device\"] = f\"cuda:{app_launcher.local_rank}\"\\n        world_rank = app_launcher.global_rank\\n\\n    # specify directory for logging experiments\\n    log_root_path = os.path.join(\"logs\", \"rl_games\", agent_cfg[\"params\"][\"config\"][\"name\"])\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\\n    # specify directory for logging runs\\n    log_dir = agent_cfg[\"params\"][\"config\"].get(\"full_experiment_name\", datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\\n    # set directory into agent config\\n    # logging directory path: <train_dir>/<full_experiment_name>\\n    agent_cfg[\"params\"][\"config\"][\"train_dir\"] = log_root_path\\n    agent_cfg[\"params\"][\"config\"][\"full_experiment_name\"] = log_dir\\n\\n    # multi-gpu training config\\n    if args_cli.distributed:\\n        agent_cfg[\"params\"][\"seed\"] += app_launcher.global_rank\\n        agent_cfg[\"params\"][\"config\"][\"device\"] = f\"cuda:{app_launcher.local_rank}\"\\n        agent_cfg[\"params\"][\"config\"][\"device_name\"] = f\"cuda:{app_launcher.local_rank}\"\\n        agent_cfg[\"params\"][\"config\"][\"multi_gpu\"] = True\\n        # update env config device\\n        env_cfg.sim.device = f\"cuda:{app_launcher.local_rank}\"\\n\\n    # max iterations\\n    if args_cli.max_iterations:\\n        agent_cfg[\"params\"][\"config\"][\"max_epochs\"] = args_cli.max_iterations\\n\\n    # dump the configuration into log-directory\\n    dump_yaml(os.path.join(log_root_path, log_dir, \"params\", \"env.yaml\"), env_cfg)\\n    dump_yaml(os.path.join(log_root_path, log_dir, \"params\", \"agent.yaml\"), agent_cfg)\\n    dump_pickle(os.path.join(log_root_path, log_dir, \"params\", \"env.pkl\"), env_cfg)\\n    dump_pickle(os.path.join(log_root_path, log_dir, \"params\", \"agent.pkl\"), agent_cfg)\\n\\n    # read configurations about the agent-training\\n    rl_device = agent_cfg[\"params\"][\"config\"][\"device\"]\\n    clip_obs = agent_cfg[\"params\"][\"env\"].get(\"clip_observations\", math.inf)\\n    clip_actions = agent_cfg[\"params\"][\"env\"].get(\"clip_actions\", math.inf)\\n\\n    task_startup_time_begin = time.perf_counter_ns()\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_root_path, log_dir, \"videos\"),\\n            \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for rl-games\\n    env = RlGamesVecEnvWrapper(env, rl_device, clip_obs, clip_actions)\\n\\n    task_startup_time_end = time.perf_counter_ns()\\n\\n    # register the environment to rl-games registry\\n    # note: in agents configuration: environment name must be \"rlgpu\"\\n    vecenv.register(\\n        \"IsaacRlgWrapper\", lambda config_name, num_actors, **kwargs: RlGamesGpuEnv(config_name, num_actors, **kwargs)\\n    )\\n    env_configurations.register(\"rlgpu\", {\"vecenv_type\": \"IsaacRlgWrapper\", \"env_creator\": lambda **kwargs: env})\\n\\n    # set number of actors into agent config\\n    agent_cfg[\"params\"][\"config\"][\"num_actors\"] = env.unwrapped.num_envs\\n    # create runner from rl-games\\n    runner = Runner(IsaacAlgoObserver())\\n    runner.load(agent_cfg)\\n\\n    # set seed of the env\\n    env.seed(agent_cfg[\"params\"][\"seed\"])\\n    # reset the agent and env\\n    runner.reset()\\n\\n    benchmark.set_phase(\"sim_runtime\")\\n\\n    # train the agent\\n    runner.run({\"train\": True, \"play\": False, \"sigma\": None})\\n\\n    if world_rank == 0:\\n        benchmark.store_measurements()\\n\\n        # parse tensorboard file stats\\n        tensorboard_log_dir = os.path.join(log_root_path, log_dir, \"summaries\")\\n        log_data = parse_tf_logs(tensorboard_log_dir)\\n\\n        # prepare RL timing dict\\n        rl_training_times = {\\n            \"Environment only step time\": log_data[\"performance/step_time\"],\\n            \"Environment + Inference step time\": log_data[\"performance/step_inference_time\"],\\n            \"Environment + Inference + Policy update time\": log_data[\"performance/rl_update_time\"],\\n            \"Environment only FPS\": log_data[\"performance/step_fps\"],\\n            \"Environment + Inference FPS\": log_data[\"performance/step_inference_fps\"],\\n            \"Environment + Inference + Policy update FPS\": log_data[\"performance/step_inference_rl_update_fps\"],\\n        }\\n\\n        # log additional metrics to benchmark services\\n        log_app_start_time(benchmark, (app_start_time_end - app_start_time_begin) / 1e6)\\n        log_python_imports_time(benchmark, (imports_time_end - imports_time_begin) / 1e6)\\n        log_task_start_time(benchmark, (task_startup_time_end - task_startup_time_begin) / 1e6)\\n        log_scene_creation_time(benchmark, Timer.get_timer_info(\"scene_creation\") * 1000)\\n        log_simulation_start_time(benchmark, Timer.get_timer_info(\"simulation_start\") * 1000)\\n        log_total_start_time(benchmark, (task_startup_time_end - app_start_time_begin) / 1e6)\\n        log_runtime_step_times(benchmark, rl_training_times, compute_stats=True)\\n        log_rl_policy_rewards(benchmark, log_data[\"rewards/iter\"])\\n        log_rl_policy_episode_lengths(benchmark, log_data[\"episode_lengths/iter\"])\\n\\n        benchmark.stop()\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\\n    \"\"\"Train with RSL-RL agent.\"\"\"\\n    # parse configuration\\n    benchmark.set_phase(\"loading\", start_recording_frametime=False, start_recording_runtime=True)\\n    # override configurations with non-hydra CLI arguments\\n    agent_cfg = cli_args.update_rsl_rl_cfg(agent_cfg, args_cli)\\n    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\\n    agent_cfg.max_iterations = (\\n        args_cli.max_iterations if args_cli.max_iterations is not None else agent_cfg.max_iterations\\n    )\\n\\n    # set the environment seed\\n    # note: certain randomizations occur in the environment initialization so we set the seed here\\n    env_cfg.seed = agent_cfg.seed\\n    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\\n\\n    # multi-gpu training configuration\\n    world_rank = 0\\n    if args_cli.distributed:\\n        env_cfg.sim.device = f\"cuda:{app_launcher.local_rank}\"\\n        agent_cfg.device = f\"cuda:{app_launcher.local_rank}\"\\n\\n        # set seed to have diversity in different threads\\n        seed = agent_cfg.seed + app_launcher.local_rank\\n        env_cfg.seed = seed\\n        agent_cfg.seed = seed\\n        world_rank = app_launcher.global_rank\\n\\n    # specify directory for logging experiments\\n    log_root_path = os.path.join(\"logs\", \"rsl_rl\", agent_cfg.experiment_name)\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\\n    # specify directory for logging runs: {time-stamp}_{run_name}\\n    log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\n    if agent_cfg.run_name:\\n        log_dir += f\"_{agent_cfg.run_name}\"\\n    log_dir = os.path.join(log_root_path, log_dir)\\n\\n    # max iterations for training\\n    if args_cli.max_iterations:\\n        agent_cfg.max_iterations = args_cli.max_iterations\\n\\n    task_startup_time_begin = time.perf_counter_ns()\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_dir, \"videos\"),\\n            \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n    # wrap around environment for rsl-rl\\n    env = RslRlVecEnvWrapper(env)\\n\\n    task_startup_time_end = time.perf_counter_ns()\\n\\n    # create runner from rsl-rl\\n    runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)\\n    # write git state to logs\\n    runner.add_git_repo_to_log(__file__)\\n    # save resume path before creating a new log_dir\\n    if agent_cfg.resume:\\n        # get path to previous checkpoint\\n        resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)\\n        print(f\"[INFO]: Loading model checkpoint from: {resume_path}\")\\n        # load previously trained model\\n        runner.load(resume_path)\\n\\n    # set seed of the environment\\n    env.seed(agent_cfg.seed)\\n\\n    # dump the configuration into log-directory\\n    dump_yaml(os.path.join(log_dir, \"params\", \"env.yaml\"), env_cfg)\\n    dump_yaml(os.path.join(log_dir, \"params\", \"agent.yaml\"), agent_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"env.pkl\"), env_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"agent.pkl\"), agent_cfg)\\n\\n    benchmark.set_phase(\"sim_runtime\")\\n\\n    # run training\\n    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)\\n\\n    if world_rank == 0:\\n        benchmark.store_measurements()\\n\\n        # parse tensorboard file stats\\n        log_data = parse_tf_logs(log_dir)\\n\\n        # prepare RL timing dict\\n        collection_fps = (\\n            1 / (np.array(log_data[\"Perf/collection time\"])) * env.unwrapped.num_envs * agent_cfg.num_steps_per_env\\n        )\\n        rl_training_times = {\\n            \"Collection Time\": (np.array(log_data[\"Perf/collection time\"]) / 1000).tolist(),\\n            \"Learning Time\": (np.array(log_data[\"Perf/learning_time\"]) / 1000).tolist(),\\n            \"Collection FPS\": collection_fps.tolist(),\\n            \"Total FPS\": log_data[\"Perf/total_fps\"],\\n        }\\n\\n        # log additional metrics to benchmark services\\n        log_app_start_time(benchmark, (app_start_time_end - app_start_time_begin) / 1e6)\\n        log_python_imports_time(benchmark, (imports_time_end - imports_time_begin) / 1e6)\\n        log_task_start_time(benchmark, (task_startup_time_end - task_startup_time_begin) / 1e6)\\n        log_scene_creation_time(benchmark, Timer.get_timer_info(\"scene_creation\") * 1000)\\n        log_simulation_start_time(benchmark, Timer.get_timer_info(\"simulation_start\") * 1000)\\n        log_total_start_time(benchmark, (task_startup_time_end - app_start_time_begin) / 1e6)\\n        log_runtime_step_times(benchmark, rl_training_times, compute_stats=True)\\n        log_rl_policy_rewards(benchmark, log_data[\"Train/mean_reward\"])\\n        log_rl_policy_episode_lengths(benchmark, log_data[\"Train/mean_episode_length\"])\\n\\n        benchmark.stop()\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def parse_tf_logs(log_dir: str):\\n    \"\"\"Search for the latest tfevents file in log_dir folder and returns\\n    the tensorboard logs in a dictionary.\\n\\n    Args:\\n        log_dir: directory used to search for tfevents files\\n    \"\"\"\\n\\n    # search log directory for latest log file\\n    list_of_files = glob.glob(f\"{log_dir}/events*\")  # * means all if need specific format then *.csv\\n    latest_file = max(list_of_files, key=os.path.getctime)\\n\\n    log_data = {}\\n    ea = event_accumulator.EventAccumulator(latest_file)\\n    ea.Reload()\\n    tags = ea.Tags()[\"scalars\"]\\n    for tag in tags:\\n        log_data[tag] = []\\n        for event in ea.Scalars(tag):\\n            log_data[tag].append(event.value)\\n\\n    return log_data'),\n",
       " Document(metadata={}, page_content='def log_min_max_mean_stats(benchmark: BaseIsaacBenchmark, values: dict):\\n    for k, v in values.items():\\n        measurement = SingleMeasurement(name=f\"Min {k}\", value=min(v), unit=\"ms\")\\n        benchmark.store_custom_measurement(\"runtime\", measurement)\\n        measurement = SingleMeasurement(name=f\"Max {k}\", value=max(v), unit=\"ms\")\\n        benchmark.store_custom_measurement(\"runtime\", measurement)\\n        measurement = SingleMeasurement(name=f\"Mean {k}\", value=sum(v) / len(v), unit=\"ms\")\\n        benchmark.store_custom_measurement(\"runtime\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_app_start_time(benchmark: BaseIsaacBenchmark, value: float):\\n    measurement = SingleMeasurement(name=\"App Launch Time\", value=value, unit=\"ms\")\\n    benchmark.store_custom_measurement(\"startup\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_python_imports_time(benchmark: BaseIsaacBenchmark, value: float):\\n    measurement = SingleMeasurement(name=\"Python Imports Time\", value=value, unit=\"ms\")\\n    benchmark.store_custom_measurement(\"startup\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_task_start_time(benchmark: BaseIsaacBenchmark, value: float):\\n    measurement = SingleMeasurement(name=\"Task Creation and Start Time\", value=value, unit=\"ms\")\\n    benchmark.store_custom_measurement(\"startup\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_scene_creation_time(benchmark: BaseIsaacBenchmark, value: float):\\n    measurement = SingleMeasurement(name=\"Scene Creation Time\", value=value, unit=\"ms\")\\n    benchmark.store_custom_measurement(\"startup\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_simulation_start_time(benchmark: BaseIsaacBenchmark, value: float):\\n    measurement = SingleMeasurement(name=\"Simulation Start Time\", value=value, unit=\"ms\")\\n    benchmark.store_custom_measurement(\"startup\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_total_start_time(benchmark: BaseIsaacBenchmark, value: float):\\n    measurement = SingleMeasurement(name=\"Total Start Time (Launch to Train)\", value=value, unit=\"ms\")\\n    benchmark.store_custom_measurement(\"startup\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_runtime_step_times(benchmark: BaseIsaacBenchmark, value: dict, compute_stats=True):\\n    measurement = DictMeasurement(name=\"Step Frametimes\", value=value)\\n    benchmark.store_custom_measurement(\"runtime\", measurement)\\n    if compute_stats:\\n        log_min_max_mean_stats(benchmark, value)'),\n",
       " Document(metadata={}, page_content='def log_rl_policy_rewards(benchmark: BaseIsaacBenchmark, value: list):\\n    measurement = ListMeasurement(name=\"Rewards\", value=value)\\n    benchmark.store_custom_measurement(\"train\", measurement)\\n    # log max reward\\n    measurement = SingleMeasurement(name=\"Max Rewards\", value=max(value), unit=\"float\")\\n    benchmark.store_custom_measurement(\"train\", measurement)'),\n",
       " Document(metadata={}, page_content='def log_rl_policy_episode_lengths(benchmark: BaseIsaacBenchmark, value: list):\\n    measurement = ListMeasurement(name=\"Episode Lengths\", value=value)\\n    benchmark.store_custom_measurement(\"train\", measurement)\\n    # log max episode length\\n    measurement = SingleMeasurement(name=\"Max Episode Lengths\", value=max(value), unit=\"float\")\\n    benchmark.store_custom_measurement(\"train\", measurement)'),\n",
       " Document(metadata={}, page_content='def define_origins(num_origins: int, spacing: float) -> list[list[float]]:\\n    \"\"\"Defines the origins of the the scene.\"\"\"\\n    # create tensor based on number of environments\\n    env_origins = torch.zeros(num_origins, 3)\\n    # create a grid of origins\\n    num_rows = np.floor(np.sqrt(num_origins))\\n    num_cols = np.ceil(num_origins / num_rows)\\n    xx, yy = torch.meshgrid(torch.arange(num_rows), torch.arange(num_cols), indexing=\"xy\")\\n    env_origins[:, 0] = spacing * xx.flatten()[:num_origins] - spacing * (num_rows - 1) / 2\\n    env_origins[:, 1] = spacing * yy.flatten()[:num_origins] - spacing * (num_cols - 1) / 2\\n    env_origins[:, 2] = 0.0\\n    # return the origins\\n    return env_origins.tolist()'),\n",
       " Document(metadata={}, page_content='def design_scene() -> tuple[dict, list[list[float]]]:\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create separate groups called \"Origin1\", \"Origin2\", \"Origin3\"\\n    # Each group will have a mount and a robot on top of it\\n    origins = define_origins(num_origins=6, spacing=2.0)\\n\\n    # Origin 1 with Franka Panda\\n    prim_utils.create_prim(\"/World/Origin1\", \"Xform\", translation=origins[0])\\n    # -- Table\\n    cfg = sim_utils.UsdFileCfg(usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/SeattleLabTable/table_instanceable.usd\")\\n    cfg.func(\"/World/Origin1/Table\", cfg, translation=(0.55, 0.0, 1.05))\\n    # -- Robot\\n    franka_arm_cfg = FRANKA_PANDA_CFG.replace(prim_path=\"/World/Origin1/Robot\")\\n    franka_arm_cfg.init_state.pos = (0.0, 0.0, 1.05)\\n    franka_panda = Articulation(cfg=franka_arm_cfg)\\n\\n    # Origin 2 with UR10\\n    prim_utils.create_prim(\"/World/Origin2\", \"Xform\", translation=origins[1])\\n    # -- Table\\n    cfg = sim_utils.UsdFileCfg(\\n        usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/Stand/stand_instanceable.usd\", scale=(2.0, 2.0, 2.0)\\n    )\\n    cfg.func(\"/World/Origin2/Table\", cfg, translation=(0.0, 0.0, 1.03))\\n    # -- Robot\\n    ur10_cfg = UR10_CFG.replace(prim_path=\"/World/Origin2/Robot\")\\n    ur10_cfg.init_state.pos = (0.0, 0.0, 1.03)\\n    ur10 = Articulation(cfg=ur10_cfg)\\n\\n    # Origin 3 with Kinova JACO2 (7-Dof) arm\\n    prim_utils.create_prim(\"/World/Origin3\", \"Xform\", translation=origins[2])\\n    # -- Table\\n    cfg = sim_utils.UsdFileCfg(usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/ThorlabsTable/table_instanceable.usd\")\\n    cfg.func(\"/World/Origin3/Table\", cfg, translation=(0.0, 0.0, 0.8))\\n    # -- Robot\\n    kinova_arm_cfg = KINOVA_JACO2_N7S300_CFG.replace(prim_path=\"/World/Origin3/Robot\")\\n    kinova_arm_cfg.init_state.pos = (0.0, 0.0, 0.8)\\n    kinova_j2n7s300 = Articulation(cfg=kinova_arm_cfg)\\n\\n    # Origin 4 with Kinova JACO2 (6-Dof) arm\\n    prim_utils.create_prim(\"/World/Origin4\", \"Xform\", translation=origins[3])\\n    # -- Table\\n    cfg = sim_utils.UsdFileCfg(usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/ThorlabsTable/table_instanceable.usd\")\\n    cfg.func(\"/World/Origin4/Table\", cfg, translation=(0.0, 0.0, 0.8))\\n    # -- Robot\\n    kinova_arm_cfg = KINOVA_JACO2_N6S300_CFG.replace(prim_path=\"/World/Origin4/Robot\")\\n    kinova_arm_cfg.init_state.pos = (0.0, 0.0, 0.8)\\n    kinova_j2n6s300 = Articulation(cfg=kinova_arm_cfg)\\n\\n    # Origin 5 with Sawyer\\n    prim_utils.create_prim(\"/World/Origin5\", \"Xform\", translation=origins[4])\\n    # -- Table\\n    cfg = sim_utils.UsdFileCfg(usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/SeattleLabTable/table_instanceable.usd\")\\n    cfg.func(\"/World/Origin5/Table\", cfg, translation=(0.55, 0.0, 1.05))\\n    # -- Robot\\n    kinova_arm_cfg = KINOVA_GEN3_N7_CFG.replace(prim_path=\"/World/Origin5/Robot\")\\n    kinova_arm_cfg.init_state.pos = (0.0, 0.0, 1.05)\\n    kinova_gen3n7 = Articulation(cfg=kinova_arm_cfg)\\n\\n    # Origin 6 with Kinova Gen3 (7-Dof) arm\\n    prim_utils.create_prim(\"/World/Origin6\", \"Xform\", translation=origins[5])\\n    # -- Table\\n    cfg = sim_utils.UsdFileCfg(\\n        usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/Stand/stand_instanceable.usd\", scale=(2.0, 2.0, 2.0)\\n    )\\n    cfg.func(\"/World/Origin6/Table\", cfg, translation=(0.0, 0.0, 1.03))\\n    # -- Robot\\n    sawyer_arm_cfg = SAWYER_CFG.replace(prim_path=\"/World/Origin6/Robot\")\\n    sawyer_arm_cfg.init_state.pos = (0.0, 0.0, 1.03)\\n    sawyer = Articulation(cfg=sawyer_arm_cfg)\\n\\n    # return the scene information\\n    scene_entities = {\\n        \"franka_panda\": franka_panda,\\n        \"ur10\": ur10,\\n        \"kinova_j2n7s300\": kinova_j2n7s300,\\n        \"kinova_j2n6s300\": kinova_j2n6s300,\\n        \"kinova_gen3n7\": kinova_gen3n7,\\n        \"sawyer\": sawyer,\\n    }\\n    return scene_entities, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, Articulation], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 200 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n            # reset the scene entities\\n            for index, robot in enumerate(entities.values()):\\n                # root state\\n                root_state = robot.data.default_root_state.clone()\\n                root_state[:, :3] += origins[index]\\n                robot.write_root_pose_to_sim(root_state[:, :7])\\n                robot.write_root_velocity_to_sim(root_state[:, 7:])\\n                # set joint positions\\n                joint_pos, joint_vel = robot.data.default_joint_pos.clone(), robot.data.default_joint_vel.clone()\\n                robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n                # clear internal buffers\\n                robot.reset()\\n            print(\"[INFO]: Resetting robots state...\")\\n        # apply random actions to the robots\\n        for robot in entities.values():\\n            # generate random joint positions\\n            joint_pos_target = robot.data.default_joint_pos + torch.randn_like(robot.data.joint_pos) * 0.1\\n            joint_pos_target = joint_pos_target.clamp_(\\n                robot.data.soft_joint_pos_limits[..., 0], robot.data.soft_joint_pos_limits[..., 1]\\n            )\\n            # apply action to the robot\\n            robot.set_joint_position_target(joint_pos_target)\\n            # write data to sim\\n            robot.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        for robot in entities.values():\\n            robot.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([3.5, 0.0, 3.2], [0.0, 0.0, 0.5])\\n    # design scene\\n    scene_entities, scene_origins = design_scene()\\n    scene_origins = torch.tensor(scene_origins, device=sim.device)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='def design_scene(sim: sim_utils.SimulationContext) -> tuple[list, torch.Tensor]:\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Define origins\\n    origins = torch.tensor([\\n        [0.0, -1.0, 0.0],\\n        [0.0, 0.0, 0.0],\\n        [0.0, 1.0, 0.0],\\n    ]).to(device=sim.device)\\n\\n    # Robots\\n    cassie = Articulation(CASSIE_CFG.replace(prim_path=\"/World/Cassie\"))\\n    h1 = Articulation(H1_CFG.replace(prim_path=\"/World/H1\"))\\n    g1 = Articulation(G1_CFG.replace(prim_path=\"/World/G1\"))\\n    robots = [cassie, h1, g1]\\n\\n    return robots, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, robots: list[Articulation], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 200 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n            for index, robot in enumerate(robots):\\n                # reset dof state\\n                joint_pos, joint_vel = robot.data.default_joint_pos, robot.data.default_joint_vel\\n                robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n                root_state = robot.data.default_root_state.clone()\\n                root_state[:, :3] += origins[index]\\n                robot.write_root_pose_to_sim(root_state[:, :7])\\n                robot.write_root_velocity_to_sim(root_state[:, 7:])\\n                robot.reset()\\n            # reset command\\n            print(\">>>>>>>> Reset!\")\\n        # apply action to the robot\\n        for robot in robots:\\n            robot.set_joint_position_target(robot.data.default_joint_pos.clone())\\n            robot.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        for robot in robots:\\n            robot.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.0, 0.0, 2.25], target=[0.0, 0.0, 1.0])\\n\\n    # design scene\\n    robots, origins = design_scene(sim)\\n\\n    # Play the simulator\\n    sim.reset()\\n\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Run the simulator\\n    run_simulator(sim, robots, origins)'),\n",
       " Document(metadata={}, page_content='def define_origins(num_origins: int, spacing: float) -> list[list[float]]:\\n    \"\"\"Defines the origins of the the scene.\"\"\"\\n    # create tensor based on number of environments\\n    env_origins = torch.zeros(num_origins, 3)\\n    # create a grid of origins\\n    num_cols = np.floor(np.sqrt(num_origins))\\n    num_rows = np.ceil(num_origins / num_cols)\\n    xx, yy = torch.meshgrid(torch.arange(num_rows), torch.arange(num_cols), indexing=\"xy\")\\n    env_origins[:, 0] = spacing * xx.flatten()[:num_origins] - spacing * (num_rows - 1) / 2\\n    env_origins[:, 1] = spacing * yy.flatten()[:num_origins] - spacing * (num_cols - 1) / 2\\n    env_origins[:, 2] = torch.rand(num_origins) + 1.0\\n    # return the origins\\n    return env_origins.tolist()'),\n",
       " Document(metadata={}, page_content='def design_scene() -> tuple[dict, list[list[float]]]:\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg_ground = sim_utils.GroundPlaneCfg()\\n    cfg_ground.func(\"/World/defaultGroundPlane\", cfg_ground)\\n\\n    # spawn distant light\\n    cfg_light = sim_utils.DomeLightCfg(\\n        intensity=3000.0,\\n        color=(0.75, 0.75, 0.75),\\n    )\\n    cfg_light.func(\"/World/light\", cfg_light)\\n\\n    # spawn a red cone\\n    cfg_sphere = sim_utils.MeshSphereCfg(\\n        radius=0.25,\\n        deformable_props=sim_utils.DeformableBodyPropertiesCfg(rest_offset=0.0),\\n        visual_material=sim_utils.PreviewSurfaceCfg(),\\n        physics_material=sim_utils.DeformableBodyMaterialCfg(),\\n    )\\n    cfg_cuboid = sim_utils.MeshCuboidCfg(\\n        size=(0.2, 0.2, 0.2),\\n        deformable_props=sim_utils.DeformableBodyPropertiesCfg(rest_offset=0.0),\\n        visual_material=sim_utils.PreviewSurfaceCfg(),\\n        physics_material=sim_utils.DeformableBodyMaterialCfg(),\\n    )\\n    cfg_cylinder = sim_utils.MeshCylinderCfg(\\n        radius=0.15,\\n        height=0.5,\\n        deformable_props=sim_utils.DeformableBodyPropertiesCfg(rest_offset=0.0),\\n        visual_material=sim_utils.PreviewSurfaceCfg(),\\n        physics_material=sim_utils.DeformableBodyMaterialCfg(),\\n    )\\n    cfg_capsule = sim_utils.MeshCapsuleCfg(\\n        radius=0.15,\\n        height=0.5,\\n        deformable_props=sim_utils.DeformableBodyPropertiesCfg(rest_offset=0.0),\\n        visual_material=sim_utils.PreviewSurfaceCfg(),\\n        physics_material=sim_utils.DeformableBodyMaterialCfg(),\\n    )\\n    cfg_cone = sim_utils.MeshConeCfg(\\n        radius=0.15,\\n        height=0.5,\\n        deformable_props=sim_utils.DeformableBodyPropertiesCfg(rest_offset=0.0),\\n        visual_material=sim_utils.PreviewSurfaceCfg(),\\n        physics_material=sim_utils.DeformableBodyMaterialCfg(),\\n    )\\n    # create a dictionary of all the objects to be spawned\\n    objects_cfg = {\\n        \"sphere\": cfg_sphere,\\n        \"cuboid\": cfg_cuboid,\\n        \"cylinder\": cfg_cylinder,\\n        \"capsule\": cfg_capsule,\\n        \"cone\": cfg_cone,\\n    }\\n\\n    # Create separate groups of deformable objects\\n    origins = define_origins(num_origins=64, spacing=0.6)\\n    print(\"[INFO]: Spawning objects...\")\\n    # Iterate over all the origins and randomly spawn objects\\n    for idx, origin in tqdm.tqdm(enumerate(origins), total=len(origins)):\\n        # randomly select an object to spawn\\n        obj_name = random.choice(list(objects_cfg.keys()))\\n        obj_cfg = objects_cfg[obj_name]\\n        # randomize the young modulus (somewhere between a Silicone 30 and Silicone 70)\\n        obj_cfg.physics_material.youngs_modulus = random.uniform(0.7e6, 3.3e6)\\n        # randomize the poisson\\'s ratio\\n        obj_cfg.physics_material.poissons_ratio = random.uniform(0.25, 0.5)\\n        # randomize the color\\n        obj_cfg.visual_material.diffuse_color = (random.random(), random.random(), random.random())\\n        # spawn the object\\n        obj_cfg.func(f\"/World/Origin/Object{idx:02d}\", obj_cfg, translation=origin)\\n\\n    # create a view for all the deformables\\n    # note: since we manually spawned random deformable meshes above, we don\\'t need to\\n    #   specify the spawn configuration for the deformable object\\n    cfg = DeformableObjectCfg(\\n        prim_path=\"/World/Origin/Object.*\",\\n        spawn=None,\\n        init_state=DeformableObjectCfg.InitialStateCfg(),\\n    )\\n    deformable_object = DeformableObject(cfg=cfg)\\n\\n    # return the scene information\\n    scene_entities = {\"deformable_object\": deformable_object}\\n    return scene_entities, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, DeformableObject], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 400 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n            # reset deformable object state\\n            for _, deform_body in enumerate(entities.values()):\\n                # root state\\n                nodal_state = deform_body.data.default_nodal_state_w.clone()\\n                deform_body.write_nodal_state_to_sim(nodal_state)\\n                # reset the internal state\\n                deform_body.reset()\\n            print(\"[INFO]: Resetting deformable object state...\")\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        for deform_body in entities.values():\\n            deform_body.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([4.0, 4.0, 3.0], [0.5, 0.5, 0.0])\\n\\n    # Design scene by adding assets to it\\n    scene_entities, scene_origins = design_scene()\\n    scene_origins = torch.tensor(scene_origins, device=sim.device)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='class H1RoughDemo:\\n    \"\"\"This class provides an interactive demo for the H1 rough terrain environment.\\n    It loads a pre-trained checkpoint for the Isaac-Velocity-Rough-H1-v0 task, trained with RSL RL\\n    and defines a set of keyboard commands for directing motion of selected robots.\\n\\n    A robot can be selected from the scene through a mouse click. Once selected, the following\\n    keyboard controls can be used to control the robot:\\n\\n    * UP: go forward\\n    * LEFT: turn left\\n    * RIGHT: turn right\\n    * DOWN: stop\\n    * C: switch between third-person and perspective views\\n    * ESC: exit current third-person view\"\"\"\\n\\n    def __init__(self):\\n        \"\"\"Initializes environment config designed for the interactive model and sets up the environment,\\n        loads pre-trained checkpoints, and registers keyboard events.\"\"\"\\n        agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(TASK, args_cli)\\n        # load the trained jit policy\\n        checkpoint = get_published_pretrained_checkpoint(RL_LIBRARY, TASK)\\n        # create envionrment\\n        env_cfg = H1RoughEnvCfg_PLAY()\\n        env_cfg.scene.num_envs = 25\\n        env_cfg.episode_length_s = 1000000\\n        env_cfg.curriculum = None\\n        env_cfg.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)\\n        env_cfg.commands.base_velocity.ranges.heading = (-1.0, 1.0)\\n        # wrap around environment for rsl-rl\\n        self.env = RslRlVecEnvWrapper(ManagerBasedRLEnv(cfg=env_cfg))\\n        self.device = self.env.unwrapped.device\\n        # load previously trained model\\n        ppo_runner = OnPolicyRunner(self.env, agent_cfg.to_dict(), log_dir=None, device=self.device)\\n        ppo_runner.load(checkpoint)\\n        # obtain the trained policy for inference\\n        self.policy = ppo_runner.get_inference_policy(device=self.device)\\n\\n        self.create_camera()\\n        self.commands = torch.zeros(env_cfg.scene.num_envs, 4, device=self.device)\\n        self.commands[:, 0:3] = self.env.unwrapped.command_manager.get_command(\"base_velocity\")\\n        self.set_up_keyboard()\\n        self._prim_selection = omni.usd.get_context().get_selection()\\n        self._selected_id = None\\n        self._previous_selected_id = None\\n        self._camera_local_transform = torch.tensor([-2.5, 0.0, 0.8], device=self.device)\\n\\n    def create_camera(self):\\n        \"\"\"Creates a camera to be used for third-person view.\"\"\"\\n        stage = omni.usd.get_context().get_stage()\\n        self.viewport = get_viewport_from_window_name(\"Viewport\")\\n        # Create camera\\n        self.camera_path = \"/World/Camera\"\\n        self.perspective_path = \"/OmniverseKit_Persp\"\\n        camera_prim = stage.DefinePrim(self.camera_path, \"Camera\")\\n        camera_prim.GetAttribute(\"focalLength\").Set(8.5)\\n        coi_prop = camera_prim.GetProperty(\"omni:kit:centerOfInterest\")\\n        if not coi_prop or not coi_prop.IsValid():\\n            camera_prim.CreateAttribute(\\n                \"omni:kit:centerOfInterest\", Sdf.ValueTypeNames.Vector3d, True, Sdf.VariabilityUniform\\n            ).Set(Gf.Vec3d(0, 0, -10))\\n        self.viewport.set_active_camera(self.perspective_path)\\n\\n    def set_up_keyboard(self):\\n        \"\"\"Sets up interface for keyboard input and registers the desired keys for control.\"\"\"\\n        self._input = carb.input.acquire_input_interface()\\n        self._keyboard = omni.appwindow.get_default_app_window().get_keyboard()\\n        self._sub_keyboard = self._input.subscribe_to_keyboard_events(self._keyboard, self._on_keyboard_event)\\n        T = 1\\n        R = 0.5\\n        self._key_to_control = {\\n            \"UP\": torch.tensor([T, 0.0, 0.0, 0.0], device=self.device),\\n            \"DOWN\": torch.tensor([0.0, 0.0, 0.0, 0.0], device=self.device),\\n            \"LEFT\": torch.tensor([T, 0.0, 0.0, -R], device=self.device),\\n            \"RIGHT\": torch.tensor([T, 0.0, 0.0, R], device=self.device),\\n            \"ZEROS\": torch.tensor([0.0, 0.0, 0.0, 0.0], device=self.device),\\n        }\\n\\n    def _on_keyboard_event(self, event):\\n        \"\"\"Checks for a keyboard event and assign the corresponding command control depending on key pressed.\"\"\"\\n        if event.type == carb.input.KeyboardEventType.KEY_PRESS:\\n            # Arrow keys map to pre-defined command vectors to control navigation of robot\\n            if event.input.name in self._key_to_control:\\n                if self._selected_id:\\n                    self.commands[self._selected_id] = self._key_to_control[event.input.name]\\n            # Escape key exits out of the current selected robot view\\n            elif event.input.name == \"ESCAPE\":\\n                self._prim_selection.clear_selected_prim_paths()\\n            # C key swaps between third-person and perspective views\\n            elif event.input.name == \"C\":\\n                if self._selected_id is not None:\\n                    if self.viewport.get_active_camera() == self.camera_path:\\n                        self.viewport.set_active_camera(self.perspective_path)\\n                    else:\\n                        self.viewport.set_active_camera(self.camera_path)\\n        # On key release, the robot stops moving\\n        elif event.type == carb.input.KeyboardEventType.KEY_RELEASE:\\n            if self._selected_id:\\n                self.commands[self._selected_id] = self._key_to_control[\"ZEROS\"]\\n\\n    def update_selected_object(self):\\n        \"\"\"Determines which robot is currently selected and whether it is a valid H1 robot.\\n        For valid robots, we enter the third-person view for that robot.\\n        When a new robot is selected, we reset the command of the previously selected\\n        to continue random commands.\"\"\"\\n\\n        self._previous_selected_id = self._selected_id\\n        selected_prim_paths = self._prim_selection.get_selected_prim_paths()\\n        if len(selected_prim_paths) == 0:\\n            self._selected_id = None\\n            self.viewport.set_active_camera(self.perspective_path)\\n        elif len(selected_prim_paths) > 1:\\n            print(\"Multiple prims are selected. Please only select one!\")\\n        else:\\n            prim_splitted_path = selected_prim_paths[0].split(\"/\")\\n            # a valid robot was selected, update the camera to go into third-person view\\n            if len(prim_splitted_path) >= 4 and prim_splitted_path[3][0:4] == \"env_\":\\n                self._selected_id = int(prim_splitted_path[3][4:])\\n                if self._previous_selected_id != self._selected_id:\\n                    self.viewport.set_active_camera(self.camera_path)\\n                self._update_camera()\\n            else:\\n                print(\"The selected prim was not a H1 robot\")\\n\\n        # Reset commands for previously selected robot if a new one is selected\\n        if self._previous_selected_id is not None and self._previous_selected_id != self._selected_id:\\n            self.env.unwrapped.command_manager.reset([self._previous_selected_id])\\n            self.commands[:, 0:3] = self.env.unwrapped.command_manager.get_command(\"base_velocity\")\\n\\n    def _update_camera(self):\\n        \"\"\"Updates the per-frame transform of the third-person view camera to follow\\n        the selected robot\\'s torso transform.\"\"\"\\n\\n        base_pos = self.env.unwrapped.scene[\"robot\"].data.root_pos_w[self._selected_id, :]  # - env.scene.env_origins\\n        base_quat = self.env.unwrapped.scene[\"robot\"].data.root_quat_w[self._selected_id, :]\\n\\n        camera_pos = quat_apply(base_quat, self._camera_local_transform) + base_pos\\n\\n        camera_state = ViewportCameraState(self.camera_path, self.viewport)\\n        eye = Gf.Vec3d(camera_pos[0].item(), camera_pos[1].item(), camera_pos[2].item())\\n        target = Gf.Vec3d(base_pos[0].item(), base_pos[1].item(), base_pos[2].item() + 0.6)\\n        camera_state.set_position_world(eye, True)\\n        camera_state.set_target_world(target, True)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    demo_h1 = H1RoughDemo()\\n    obs, _ = demo_h1.env.reset()\\n    while simulation_app.is_running():\\n        # check for selected robots\\n        demo_h1.update_selected_object()\\n        with torch.inference_mode():\\n            action = demo_h1.policy(obs)\\n            obs, _, _, _ = demo_h1.env.step(action)\\n            # overwrite command based on keyboard input\\n            obs[:, 9:13] = demo_h1.commands'),\n",
       " Document(metadata={}, page_content='def define_origins(num_origins: int, spacing: float) -> list[list[float]]:\\n    \"\"\"Defines the origins of the the scene.\"\"\"\\n    # create tensor based on number of environments\\n    env_origins = torch.zeros(num_origins, 3)\\n    # create a grid of origins\\n    num_cols = np.floor(np.sqrt(num_origins))\\n    num_rows = np.ceil(num_origins / num_cols)\\n    xx, yy = torch.meshgrid(torch.arange(num_rows), torch.arange(num_cols), indexing=\"xy\")\\n    env_origins[:, 0] = spacing * xx.flatten()[:num_origins] - spacing * (num_rows - 1) / 2\\n    env_origins[:, 1] = spacing * yy.flatten()[:num_origins] - spacing * (num_cols - 1) / 2\\n    env_origins[:, 2] = 0.0\\n    # return the origins\\n    return env_origins.tolist()'),\n",
       " Document(metadata={}, page_content='def design_scene() -> tuple[dict, list[list[float]]]:\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create separate groups called \"Origin1\", \"Origin2\", \"Origin3\"\\n    # Each group will have a mount and a robot on top of it\\n    origins = define_origins(num_origins=2, spacing=0.5)\\n\\n    # Origin 1 with Allegro Hand\\n    prim_utils.create_prim(\"/World/Origin1\", \"Xform\", translation=origins[0])\\n    # -- Robot\\n    allegro = Articulation(ALLEGRO_HAND_CFG.replace(prim_path=\"/World/Origin1/Robot\"))\\n\\n    # Origin 2 with Shadow Hand\\n    prim_utils.create_prim(\"/World/Origin2\", \"Xform\", translation=origins[1])\\n    # -- Robot\\n    shadow_hand = Articulation(SHADOW_HAND_CFG.replace(prim_path=\"/World/Origin2/Robot\"))\\n\\n    # return the scene information\\n    scene_entities = {\\n        \"allegro\": allegro,\\n        \"shadow_hand\": shadow_hand,\\n    }\\n    return scene_entities, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, Articulation], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n    # Start with hand open\\n    grasp_mode = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 1000 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n            # reset robots\\n            for index, robot in enumerate(entities.values()):\\n                # root state\\n                root_state = robot.data.default_root_state.clone()\\n                root_state[:, :3] += origins[index]\\n                robot.write_root_pose_to_sim(root_state[:, :7])\\n                robot.write_root_velocity_to_sim(root_state[:, 7:])\\n                # joint state\\n                joint_pos, joint_vel = robot.data.default_joint_pos.clone(), robot.data.default_joint_vel.clone()\\n                robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n                # reset the internal state\\n                robot.reset()\\n            print(\"[INFO]: Resetting robots state...\")\\n        # toggle grasp mode\\n        if count % 100 == 0:\\n            grasp_mode = 1 - grasp_mode\\n        # apply default actions to the hands robots\\n        for robot in entities.values():\\n            # generate joint positions\\n            joint_pos_target = robot.data.soft_joint_pos_limits[..., grasp_mode]\\n            # apply action to the robot\\n            robot.set_joint_position_target(joint_pos_target)\\n            # write data to sim\\n            robot.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        for robot in entities.values():\\n            robot.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[0.0, -0.5, 1.5], target=[0.0, -0.2, 0.5])\\n    # design scene\\n    scene_entities, scene_origins = design_scene()\\n    scene_origins = torch.tensor(scene_origins, device=sim.device)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='def define_markers() -> VisualizationMarkers:\\n    \"\"\"Define markers with various different shapes.\"\"\"\\n    marker_cfg = VisualizationMarkersCfg(\\n        prim_path=\"/Visuals/myMarkers\",\\n        markers={\\n            \"frame\": sim_utils.UsdFileCfg(\\n                usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/UIElements/frame_prim.usd\",\\n                scale=(0.5, 0.5, 0.5),\\n            ),\\n            \"arrow_x\": sim_utils.UsdFileCfg(\\n                usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/UIElements/arrow_x.usd\",\\n                scale=(1.0, 0.5, 0.5),\\n                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 1.0)),\\n            ),\\n            \"cube\": sim_utils.CuboidCfg(\\n                size=(1.0, 1.0, 1.0),\\n                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0)),\\n            ),\\n            \"sphere\": sim_utils.SphereCfg(\\n                radius=0.5,\\n                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0)),\\n            ),\\n            \"cylinder\": sim_utils.CylinderCfg(\\n                radius=0.5,\\n                height=1.0,\\n                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 0.0, 1.0)),\\n            ),\\n            \"cone\": sim_utils.ConeCfg(\\n                radius=0.5,\\n                height=1.0,\\n                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 1.0, 0.0)),\\n            ),\\n            \"mesh\": sim_utils.UsdFileCfg(\\n                usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Blocks/DexCube/dex_cube_instanceable.usd\",\\n                scale=(10.0, 10.0, 10.0),\\n            ),\\n            \"mesh_recolored\": sim_utils.UsdFileCfg(\\n                usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Blocks/DexCube/dex_cube_instanceable.usd\",\\n                scale=(10.0, 10.0, 10.0),\\n                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.25, 0.0)),\\n            ),\\n            \"robot_mesh\": sim_utils.UsdFileCfg(\\n                usd_path=f\"{ISAACLAB_NUCLEUS_DIR}/Robots/ANYbotics/ANYmal-C/anymal_c.usd\",\\n                scale=(2.0, 2.0, 2.0),\\n                visual_material=sim_utils.GlassMdlCfg(glass_color=(0.0, 0.1, 0.0)),\\n            ),\\n        },\\n    )\\n    return VisualizationMarkers(marker_cfg)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([0.0, 18.0, 12.0], [0.0, 3.0, 0.0])\\n\\n    # Spawn things into stage\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # create markers\\n    my_visualizer = define_markers()\\n\\n    # define a grid of positions where the markers should be placed\\n    num_markers_per_type = 5\\n    grid_spacing = 2.0\\n    # Calculate the half-width and half-height\\n    half_width = (num_markers_per_type - 1) / 2.0\\n    half_height = (my_visualizer.num_prototypes - 1) / 2.0\\n    # Create the x and y ranges centered around the origin\\n    x_range = torch.arange(-half_width * grid_spacing, (half_width + 1) * grid_spacing, grid_spacing)\\n    y_range = torch.arange(-half_height * grid_spacing, (half_height + 1) * grid_spacing, grid_spacing)\\n    # Create the grid\\n    x_grid, y_grid = torch.meshgrid(x_range, y_range, indexing=\"ij\")\\n    x_grid = x_grid.reshape(-1)\\n    y_grid = y_grid.reshape(-1)\\n    z_grid = torch.zeros_like(x_grid)\\n    # marker locations\\n    marker_locations = torch.stack([x_grid, y_grid, z_grid], dim=1)\\n    marker_indices = torch.arange(my_visualizer.num_prototypes).repeat(num_markers_per_type)\\n\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Yaw angle\\n    yaw = torch.zeros_like(marker_locations[:, 0])\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # rotate the markers around the z-axis for visualization\\n        marker_orientations = quat_from_angle_axis(yaw, torch.tensor([0.0, 0.0, 1.0]))\\n        # visualize\\n        my_visualizer.visualize(marker_locations, marker_orientations, marker_indices=marker_indices)\\n        # roll corresponding indices to show how marker prototype can be changed\\n        if yaw[0].item() % (0.5 * torch.pi) < 0.01:\\n            marker_indices = torch.roll(marker_indices, 1)\\n        # perform step\\n        sim.step()\\n        # increment yaw\\n        yaw += 0.01'),\n",
       " Document(metadata={}, page_content='def randomize_shape_color(prim_path_expr: str):\\n    \"\"\"Randomize the color of the geometry.\"\"\"\\n    # acquire stage\\n    stage = omni.usd.get_context().get_stage()\\n    # resolve prim paths for spawning and cloning\\n    prim_paths = sim_utils.find_matching_prim_paths(prim_path_expr)\\n    # manually clone prims if the source prim path is a regex expression\\n    with Sdf.ChangeBlock():\\n        for prim_path in prim_paths:\\n            # spawn single instance\\n            prim_spec = Sdf.CreatePrimInLayer(stage.GetRootLayer(), prim_path)\\n\\n            # DO YOUR OWN OTHER KIND OF RANDOMIZATION HERE!\\n            # Note: Just need to acquire the right attribute about the property you want to set\\n            # Here is an example on setting color randomly\\n            color_spec = prim_spec.GetAttributeAtPath(prim_path + \"/geometry/material/Shader.inputs:diffuseColor\")\\n            color_spec.default = Gf.Vec3f(random.random(), random.random(), random.random())'),\n",
       " Document(metadata={}, page_content='class MultiObjectSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Configuration for a multi-object scene.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # rigid object\\n    object: RigidObjectCfg = RigidObjectCfg(\\n        prim_path=\"/World/envs/env_.*/Object\",\\n        spawn=sim_utils.MultiAssetSpawnerCfg(\\n            assets_cfg=[\\n                sim_utils.ConeCfg(\\n                    radius=0.3,\\n                    height=0.6,\\n                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0), metallic=0.2),\\n                ),\\n                sim_utils.CuboidCfg(\\n                    size=(0.3, 0.3, 0.3),\\n                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0), metallic=0.2),\\n                ),\\n                sim_utils.SphereCfg(\\n                    radius=0.3,\\n                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 0.0, 1.0), metallic=0.2),\\n                ),\\n            ],\\n            random_choice=True,\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(\\n                solver_position_iteration_count=4, solver_velocity_iteration_count=0\\n            ),\\n            mass_props=sim_utils.MassPropertiesCfg(mass=1.0),\\n            collision_props=sim_utils.CollisionPropertiesCfg(),\\n        ),\\n        init_state=RigidObjectCfg.InitialStateCfg(pos=(0.0, 0.0, 2.0)),\\n    )\\n\\n    # object collection\\n    object_collection: RigidObjectCollectionCfg = RigidObjectCollectionCfg(\\n        rigid_objects={\\n            \"object_A\": RigidObjectCfg(\\n                prim_path=\"/World/envs/env_.*/Object_A\",\\n                spawn=sim_utils.SphereCfg(\\n                    radius=0.1,\\n                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0), metallic=0.2),\\n                    rigid_props=sim_utils.RigidBodyPropertiesCfg(\\n                        solver_position_iteration_count=4, solver_velocity_iteration_count=0\\n                    ),\\n                    mass_props=sim_utils.MassPropertiesCfg(mass=1.0),\\n                    collision_props=sim_utils.CollisionPropertiesCfg(),\\n                ),\\n                init_state=RigidObjectCfg.InitialStateCfg(pos=(0.0, -0.5, 2.0)),\\n            ),\\n            \"object_B\": RigidObjectCfg(\\n                prim_path=\"/World/envs/env_.*/Object_B\",\\n                spawn=sim_utils.CuboidCfg(\\n                    size=(0.1, 0.1, 0.1),\\n                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0), metallic=0.2),\\n                    rigid_props=sim_utils.RigidBodyPropertiesCfg(\\n                        solver_position_iteration_count=4, solver_velocity_iteration_count=0\\n                    ),\\n                    mass_props=sim_utils.MassPropertiesCfg(mass=1.0),\\n                    collision_props=sim_utils.CollisionPropertiesCfg(),\\n                ),\\n                init_state=RigidObjectCfg.InitialStateCfg(pos=(0.0, 0.5, 2.0)),\\n            ),\\n            \"object_C\": RigidObjectCfg(\\n                prim_path=\"/World/envs/env_.*/Object_C\",\\n                spawn=sim_utils.ConeCfg(\\n                    radius=0.1,\\n                    height=0.3,\\n                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0), metallic=0.2),\\n                    rigid_props=sim_utils.RigidBodyPropertiesCfg(\\n                        solver_position_iteration_count=4, solver_velocity_iteration_count=0\\n                    ),\\n                    mass_props=sim_utils.MassPropertiesCfg(mass=1.0),\\n                    collision_props=sim_utils.CollisionPropertiesCfg(),\\n                ),\\n                init_state=RigidObjectCfg.InitialStateCfg(pos=(0.5, 0.0, 2.0)),\\n            ),\\n        }\\n    )\\n\\n    # articulation\\n    robot: ArticulationCfg = ArticulationCfg(\\n        prim_path=\"/World/envs/env_.*/Robot\",\\n        spawn=sim_utils.MultiUsdFileCfg(\\n            usd_path=[\\n                f\"{ISAACLAB_NUCLEUS_DIR}/Robots/ANYbotics/ANYmal-C/anymal_c.usd\",\\n                f\"{ISAACLAB_NUCLEUS_DIR}/Robots/ANYbotics/ANYmal-D/anymal_d.usd\",\\n            ],\\n            random_choice=True,\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(\\n                disable_gravity=False,\\n                retain_accelerations=False,\\n                linear_damping=0.0,\\n                angular_damping=0.0,\\n                max_linear_velocity=1000.0,\\n                max_angular_velocity=1000.0,\\n                max_depenetration_velocity=1.0,\\n            ),\\n            articulation_props=sim_utils.ArticulationRootPropertiesCfg(\\n                enabled_self_collisions=True, solver_position_iteration_count=4, solver_velocity_iteration_count=0\\n            ),\\n            activate_contact_sensors=True,\\n        ),\\n        init_state=ArticulationCfg.InitialStateCfg(\\n            pos=(0.0, 0.0, 0.6),\\n            joint_pos={\\n                \".*HAA\": 0.0,  # all HAA\\n                \".*F_HFE\": 0.4,  # both front HFE\\n                \".*H_HFE\": -0.4,  # both hind HFE\\n                \".*F_KFE\": -0.8,  # both front KFE\\n                \".*H_KFE\": 0.8,  # both hind KFE\\n            },\\n        ),\\n        actuators={\"legs\": ANYDRIVE_3_LSTM_ACTUATOR_CFG},\\n    )'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: SimulationContext, scene: InteractiveScene):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Extract scene entities\\n    # note: we only do this here for readability.\\n    rigid_object: RigidObject = scene[\"object\"]\\n    rigid_object_collection: RigidObjectCollection = scene[\"object_collection\"]\\n    robot: Articulation = scene[\"robot\"]\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    count = 0\\n    # Simulation loop\\n    while simulation_app.is_running():\\n        # Reset\\n        if count % 250 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # object\\n            root_state = rigid_object.data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            rigid_object.write_root_pose_to_sim(root_state[:, :7])\\n            rigid_object.write_root_velocity_to_sim(root_state[:, 7:])\\n            # object collection\\n            object_state = rigid_object_collection.data.default_object_state.clone()\\n            object_state[..., :3] += scene.env_origins.unsqueeze(1)\\n            rigid_object_collection.write_object_link_pose_to_sim(object_state[..., :7])\\n            rigid_object_collection.write_object_com_velocity_to_sim(object_state[..., 7:])\\n            # robot\\n            # -- root state\\n            root_state = robot.data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            robot.write_root_pose_to_sim(root_state[:, :7])\\n            robot.write_root_velocity_to_sim(root_state[:, 7:])\\n            # -- joint state\\n            joint_pos, joint_vel = robot.data.default_joint_pos.clone(), robot.data.default_joint_vel.clone()\\n            robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting scene state...\")\\n\\n        # Apply action to robot\\n        robot.set_joint_position_target(robot.data.default_joint_pos)\\n        # Write data to sim\\n        scene.write_data_to_sim()\\n        # Perform step\\n        sim.step()\\n        # Increment counter\\n        count += 1\\n        # Update buffers\\n        scene.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 0.0, 4.0], [0.0, 0.0, 2.0])\\n\\n    # Design scene\\n    scene_cfg = MultiObjectSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0, replicate_physics=False)\\n    with Timer(\"[INFO] Time to create scene: \"):\\n        scene = InteractiveScene(scene_cfg)\\n\\n    with Timer(\"[INFO] Time to randomize scene: \"):\\n        # DO YOUR OWN OTHER KIND OF RANDOMIZATION HERE!\\n        # Note: Just need to acquire the right attribute about the property you want to set\\n        # Here is an example on setting color randomly\\n        randomize_shape_color(scene_cfg.object.prim_path)\\n\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='def design_scene() -> tuple[dict, torch.Tensor]:\\n    \"\"\"Designs the scene.\"\"\"\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Parse terrain generation\\n    terrain_gen_cfg = ROUGH_TERRAINS_CFG.replace(curriculum=args_cli.use_curriculum, color_scheme=args_cli.color_scheme)\\n\\n    # Add flat patch configuration\\n    # Note: To have separate colors for each sub-terrain type, we set the flat patch sampling configuration name\\n    #   to the sub-terrain name. However, this is not how it should be used in practice. The key name should be\\n    #   the intention of the flat patch. For instance, \"source\" or \"target\" for spawn and command related flat patches.\\n    if args_cli.show_flat_patches:\\n        for sub_terrain_name, sub_terrain_cfg in terrain_gen_cfg.sub_terrains.items():\\n            sub_terrain_cfg.flat_patch_sampling = {\\n                sub_terrain_name: FlatPatchSamplingCfg(num_patches=10, patch_radius=0.5, max_height_diff=0.05)\\n            }\\n\\n    # Handler for terrains importing\\n    terrain_importer_cfg = TerrainImporterCfg(\\n        num_envs=2048,\\n        env_spacing=3.0,\\n        prim_path=\"/World/ground\",\\n        max_init_terrain_level=None,\\n        terrain_type=\"generator\",\\n        terrain_generator=terrain_gen_cfg,\\n        debug_vis=True,\\n    )\\n    # Remove visual material for height and random color schemes to use the default material\\n    if args_cli.color_scheme in [\"height\", \"random\"]:\\n        terrain_importer_cfg.visual_material = None\\n    # Create terrain importer\\n    terrain_importer = TerrainImporter(terrain_importer_cfg)\\n\\n    # Show the flat patches computed\\n    if args_cli.show_flat_patches:\\n        # Configure the flat patches\\n        vis_cfg = VisualizationMarkersCfg(prim_path=\"/Visuals/TerrainFlatPatches\", markers={})\\n        for name in terrain_importer.flat_patches:\\n            vis_cfg.markers[name] = sim_utils.CylinderCfg(\\n                radius=0.5,  # note: manually set to the patch radius for visualization\\n                height=0.1,\\n                visual_material=sim_utils.GlassMdlCfg(glass_color=(random.random(), random.random(), random.random())),\\n            )\\n        flat_patches_visualizer = VisualizationMarkers(vis_cfg)\\n\\n        # Visualize the flat patches\\n        all_patch_locations = []\\n        all_patch_indices = []\\n        for i, patch_locations in enumerate(terrain_importer.flat_patches.values()):\\n            num_patch_locations = patch_locations.view(-1, 3).shape[0]\\n            # store the patch locations and indices\\n            all_patch_locations.append(patch_locations.view(-1, 3))\\n            all_patch_indices += [i] * num_patch_locations\\n        # combine the patch locations and indices\\n        flat_patches_visualizer.visualize(torch.cat(all_patch_locations), marker_indices=all_patch_indices)\\n\\n    # return the scene information\\n    scene_entities = {\"terrain\": terrain_importer}\\n    return scene_entities, terrain_importer.env_origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, AssetBase], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # perform step\\n        sim.step()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[5.0, 5.0, 5.0], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_entities, scene_origins = design_scene()\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[0.5, 0.5, 1.0], target=[0.0, 0.0, 0.5])\\n\\n    # Spawn things into stage\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DistantLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Robots\\n    robot_cfg = CRAZYFLIE_CFG.replace(prim_path=\"/World/Crazyflie\")\\n    robot_cfg.spawn.func(\"/World/Crazyflie\", robot_cfg.spawn, translation=robot_cfg.init_state.pos)\\n\\n    # create handles for the robots\\n    robot = Articulation(robot_cfg)\\n\\n    # Play the simulator\\n    sim.reset()\\n\\n    # Fetch relevant parameters to make the quadcopter hover in place\\n    prop_body_ids = robot.find_bodies(\"m.*_prop\")[0]\\n    robot_mass = robot.root_physx_view.get_masses().sum()\\n    gravity = torch.tensor(sim.cfg.gravity, device=sim.device).norm()\\n\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 2000 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n            # reset dof state\\n            joint_pos, joint_vel = robot.data.default_joint_pos, robot.data.default_joint_vel\\n            robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n            robot.write_root_pose_to_sim(robot.data.default_root_state[:, :7])\\n            robot.write_root_velocity_to_sim(robot.data.default_root_state[:, 7:])\\n            robot.reset()\\n            # reset command\\n            print(\">>>>>>>> Reset!\")\\n        # apply action to the robot (make the robot float in place)\\n        forces = torch.zeros(robot.num_instances, 4, 3, device=sim.device)\\n        torques = torch.zeros_like(forces)\\n        forces[..., 2] = robot_mass * gravity / 4.0\\n        robot.set_external_force_and_torque(forces, torques, body_ids=prop_body_ids)\\n        robot.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        robot.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def define_origins(num_origins: int, spacing: float) -> list[list[float]]:\\n    \"\"\"Defines the origins of the scene.\"\"\"\\n    # create tensor based on number of environments\\n    env_origins = torch.zeros(num_origins, 3)\\n    # create a grid of origins\\n    num_cols = np.floor(np.sqrt(num_origins))\\n    num_rows = np.ceil(num_origins / num_cols)\\n    xx, yy = torch.meshgrid(torch.arange(num_rows), torch.arange(num_cols), indexing=\"xy\")\\n    env_origins[:, 0] = spacing * xx.flatten()[:num_origins] - spacing * (num_rows - 1) / 2\\n    env_origins[:, 1] = spacing * yy.flatten()[:num_origins] - spacing * (num_cols - 1) / 2\\n    env_origins[:, 2] = 0.0\\n    # return the origins\\n    return env_origins.tolist()'),\n",
       " Document(metadata={}, page_content='def design_scene() -> tuple[dict, list[list[float]]]:\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create separate groups called \"Origin1\", \"Origin2\", \"Origin3\"\\n    # Each group will have a mount and a robot on top of it\\n    origins = define_origins(num_origins=7, spacing=1.25)\\n\\n    # Origin 1 with Anymal B\\n    prim_utils.create_prim(\"/World/Origin1\", \"Xform\", translation=origins[0])\\n    # -- Robot\\n    anymal_b = Articulation(ANYMAL_B_CFG.replace(prim_path=\"/World/Origin1/Robot\"))\\n\\n    # Origin 2 with Anymal C\\n    prim_utils.create_prim(\"/World/Origin2\", \"Xform\", translation=origins[1])\\n    # -- Robot\\n    anymal_c = Articulation(ANYMAL_C_CFG.replace(prim_path=\"/World/Origin2/Robot\"))\\n\\n    # Origin 3 with Anymal D\\n    prim_utils.create_prim(\"/World/Origin3\", \"Xform\", translation=origins[2])\\n    # -- Robot\\n    anymal_d = Articulation(ANYMAL_D_CFG.replace(prim_path=\"/World/Origin3/Robot\"))\\n\\n    # Origin 4 with Unitree A1\\n    prim_utils.create_prim(\"/World/Origin4\", \"Xform\", translation=origins[3])\\n    # -- Robot\\n    unitree_a1 = Articulation(UNITREE_A1_CFG.replace(prim_path=\"/World/Origin4/Robot\"))\\n\\n    # Origin 5 with Unitree Go1\\n    prim_utils.create_prim(\"/World/Origin5\", \"Xform\", translation=origins[4])\\n    # -- Robot\\n    unitree_go1 = Articulation(UNITREE_GO1_CFG.replace(prim_path=\"/World/Origin5/Robot\"))\\n\\n    # Origin 6 with Unitree Go2\\n    prim_utils.create_prim(\"/World/Origin6\", \"Xform\", translation=origins[5])\\n    # -- Robot\\n    unitree_go2 = Articulation(UNITREE_GO2_CFG.replace(prim_path=\"/World/Origin6/Robot\"))\\n\\n    # Origin 7 with Boston Dynamics Spot\\n    prim_utils.create_prim(\"/World/Origin7\", \"Xform\", translation=origins[6])\\n    # -- Robot\\n    spot = Articulation(SPOT_CFG.replace(prim_path=\"/World/Origin7/Robot\"))\\n\\n    # return the scene information\\n    scene_entities = {\\n        \"anymal_b\": anymal_b,\\n        \"anymal_c\": anymal_c,\\n        \"anymal_d\": anymal_d,\\n        \"unitree_a1\": unitree_a1,\\n        \"unitree_go1\": unitree_go1,\\n        \"unitree_go2\": unitree_go2,\\n        \"spot\": spot,\\n    }\\n    return scene_entities, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, Articulation], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 200 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n            # reset robots\\n            for index, robot in enumerate(entities.values()):\\n                # root state\\n                root_state = robot.data.default_root_state.clone()\\n                root_state[:, :3] += origins[index]\\n                robot.write_root_pose_to_sim(root_state[:, :7])\\n                robot.write_root_velocity_to_sim(root_state[:, 7:])\\n                # joint state\\n                joint_pos, joint_vel = robot.data.default_joint_pos.clone(), robot.data.default_joint_vel.clone()\\n                robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n                # reset the internal state\\n                robot.reset()\\n            print(\"[INFO]: Resetting robots state...\")\\n        # apply default actions to the quadrupedal robots\\n        for robot in entities.values():\\n            # generate random joint positions\\n            joint_pos_target = robot.data.default_joint_pos + torch.randn_like(robot.data.joint_pos) * 0.1\\n            # apply action to the robot\\n            robot.set_joint_position_target(joint_pos_target)\\n            # write data to sim\\n            robot.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        for robot in entities.values():\\n            robot.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim = sim_utils.SimulationContext(sim_utils.SimulationCfg(dt=0.01))\\n    # Set main camera\\n    sim.set_camera_view(eye=[2.5, 2.5, 2.5], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_entities, scene_origins = design_scene()\\n    scene_origins = torch.tensor(scene_origins, device=sim.device)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='class SensorsSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Design the scene with sensors on the robot.\"\"\"\\n\\n    # ground plane\\n    ground = TerrainImporterCfg(\\n        prim_path=\"/World/ground\",\\n        max_init_terrain_level=None,\\n        terrain_type=\"generator\",\\n        terrain_generator=ROUGH_TERRAINS_CFG.replace(color_scheme=\"random\"),\\n        visual_material=None,\\n        debug_vis=False,\\n    )\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # robot\\n    robot: ArticulationCfg = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    # sensors\\n    camera = CameraCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base/front_cam\",\\n        update_period=0.1,\\n        height=480,\\n        width=640,\\n        data_types=[\"rgb\", \"distance_to_image_plane\"],\\n        spawn=sim_utils.PinholeCameraCfg(\\n            focal_length=24.0, focus_distance=400.0, horizontal_aperture=20.955, clipping_range=(0.1, 1.0e5)\\n        ),\\n        offset=CameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(0.5, -0.5, 0.5, -0.5), convention=\"ros\"),\\n    )\\n    tiled_camera = TiledCameraCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base/front_cam\",\\n        update_period=0.1,\\n        height=480,\\n        width=640,\\n        data_types=[\"rgb\", \"distance_to_image_plane\"],\\n        spawn=None,  # the camera is already spawned in the scene\\n        offset=TiledCameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(0.5, -0.5, 0.5, -0.5), convention=\"ros\"),\\n    )\\n    raycast_camera = RayCasterCameraCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base\",\\n        mesh_prim_paths=[\"/World/ground\"],\\n        update_period=0.1,\\n        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(0.5, -0.5, 0.5, -0.5), convention=\"ros\"),\\n        data_types=[\"distance_to_image_plane\", \"normals\"],\\n        pattern_cfg=patterns.PinholeCameraPatternCfg(\\n            focal_length=24.0,\\n            horizontal_aperture=20.955,\\n            height=480,\\n            width=640,\\n        ),\\n    )'),\n",
       " Document(metadata={}, page_content='def save_images_grid(\\n    images: list[torch.Tensor],\\n    cmap: str | None = None,\\n    nrow: int = 1,\\n    subtitles: list[str] | None = None,\\n    title: str | None = None,\\n    filename: str | None = None,\\n):\\n    \"\"\"Save images in a grid with optional subtitles and title.\\n\\n    Args:\\n        images: A list of images to be plotted. Shape of each image should be (H, W, C).\\n        cmap: Colormap to be used for plotting. Defaults to None, in which case the default colormap is used.\\n        nrows: Number of rows in the grid. Defaults to 1.\\n        subtitles: A list of subtitles for each image. Defaults to None, in which case no subtitles are shown.\\n        title: Title of the grid. Defaults to None, in which case no title is shown.\\n        filename: Path to save the figure. Defaults to None, in which case the figure is not saved.\\n    \"\"\"\\n    # show images in a grid\\n    n_images = len(images)\\n    ncol = int(np.ceil(n_images / nrow))\\n\\n    fig, axes = plt.subplots(nrow, ncol, figsize=(ncol * 2, nrow * 2))\\n    axes = axes.flatten()\\n\\n    # plot images\\n    for idx, (img, ax) in enumerate(zip(images, axes)):\\n        img = img.detach().cpu().numpy()\\n        ax.imshow(img, cmap=cmap)\\n        ax.axis(\"off\")\\n        if subtitles:\\n            ax.set_title(subtitles[idx])\\n    # remove extra axes if any\\n    for ax in axes[n_images:]:\\n        fig.delaxes(ax)\\n    # set title\\n    if title:\\n        plt.suptitle(title)\\n\\n    # adjust layout to fit the title\\n    plt.tight_layout()\\n    # save the figure\\n    if filename:\\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\\n        plt.savefig(filename)\\n    # close the figure\\n    plt.close()'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    # Create output directory to save images\\n    output_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"output\")\\n    os.makedirs(output_dir, exist_ok=True)\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # Reset\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = scene[\"robot\"].data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            scene[\"robot\"].write_root_pose_to_sim(root_state[:, :7])\\n            scene[\"robot\"].write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = (\\n                scene[\"robot\"].data.default_joint_pos.clone(),\\n                scene[\"robot\"].data.default_joint_vel.clone(),\\n            )\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            scene[\"robot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply default actions to the robot\\n        # -- generate actions/commands\\n        targets = scene[\"robot\"].data.default_joint_pos\\n        # -- apply action to the robot\\n        scene[\"robot\"].set_joint_position_target(targets)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        scene.update(sim_dt)\\n\\n        # print information from the sensors\\n        print(\"-------------------------------\")\\n        print(scene[\"camera\"])\\n        print(\"Received shape of rgb   image: \", scene[\"camera\"].data.output[\"rgb\"].shape)\\n        print(\"Received shape of depth image: \", scene[\"camera\"].data.output[\"distance_to_image_plane\"].shape)\\n        print(\"-------------------------------\")\\n        print(scene[\"tiled_camera\"])\\n        print(\"Received shape of rgb   image: \", scene[\"tiled_camera\"].data.output[\"rgb\"].shape)\\n        print(\"Received shape of depth image: \", scene[\"tiled_camera\"].data.output[\"distance_to_image_plane\"].shape)\\n        print(\"-------------------------------\")\\n        print(scene[\"raycast_camera\"])\\n        print(\"Received shape of depth: \", scene[\"raycast_camera\"].data.output[\"distance_to_image_plane\"].shape)\\n        print(\"Received shape of normals: \", scene[\"raycast_camera\"].data.output[\"normals\"].shape)\\n\\n        # save every 10th image (for visualization purposes only)\\n        # note: saving images will slow down the simulation\\n        if count % 10 == 0:\\n            # compare generated RGB images across different cameras\\n            rgb_images = [scene[\"camera\"].data.output[\"rgb\"][0, ..., :3], scene[\"tiled_camera\"].data.output[\"rgb\"][0]]\\n            save_images_grid(\\n                rgb_images,\\n                subtitles=[\"Camera\", \"TiledCamera\"],\\n                title=\"RGB Image: Cam0\",\\n                filename=os.path.join(output_dir, \"rgb\", f\"{count:04d}.jpg\"),\\n            )\\n\\n            # compare generated Depth images across different cameras\\n            depth_images = [\\n                scene[\"camera\"].data.output[\"distance_to_image_plane\"][0],\\n                scene[\"tiled_camera\"].data.output[\"distance_to_image_plane\"][0, ..., 0],\\n                scene[\"raycast_camera\"].data.output[\"distance_to_image_plane\"][0],\\n            ]\\n            save_images_grid(\\n                depth_images,\\n                cmap=\"turbo\",\\n                subtitles=[\"Camera\", \"TiledCamera\", \"RaycasterCamera\"],\\n                title=\"Depth Image: Cam0\",\\n                filename=os.path.join(output_dir, \"distance_to_camera\", f\"{count:04d}.jpg\"),\\n            )\\n\\n            # save all tiled RGB images\\n            tiled_images = scene[\"tiled_camera\"].data.output[\"rgb\"]\\n            save_images_grid(\\n                tiled_images,\\n                subtitles=[f\"Cam{i}\" for i in range(tiled_images.shape[0])],\\n                title=\"Tiled RGB Image\",\\n                filename=os.path.join(output_dir, \"tiled_rgb\", f\"{count:04d}.jpg\"),\\n            )\\n\\n            # save all camera RGB images\\n            cam_images = scene[\"camera\"].data.output[\"rgb\"][..., :3]\\n            save_images_grid(\\n                cam_images,\\n                subtitles=[f\"Cam{i}\" for i in range(cam_images.shape[0])],\\n                title=\"Camera RGB Image\",\\n                filename=os.path.join(output_dir, \"cam_rgb\", f\"{count:04d}.jpg\"),\\n            )'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device, use_fabric=not args_cli.disable_fabric)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_cfg = SensorsSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='class ContactSensorSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Design the scene with sensors on the robot.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # robot\\n    robot = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    # Rigid Object\\n    cube = RigidObjectCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Cube\",\\n        spawn=sim_utils.CuboidCfg(\\n            size=(0.5, 0.5, 0.1),\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(),\\n            mass_props=sim_utils.MassPropertiesCfg(mass=100.0),\\n            collision_props=sim_utils.CollisionPropertiesCfg(),\\n            physics_material=sim_utils.RigidBodyMaterialCfg(static_friction=1.0),\\n            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0), metallic=0.2),\\n        ),\\n        init_state=RigidObjectCfg.InitialStateCfg(pos=(0.5, 0.5, 0.05)),\\n    )\\n\\n    contact_forces_LF = ContactSensorCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/LF_FOOT\",\\n        update_period=0.0,\\n        history_length=6,\\n        debug_vis=True,\\n        filter_prim_paths_expr=[\"{ENV_REGEX_NS}/Cube\"],\\n    )\\n\\n    contact_forces_RF = ContactSensorCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/RF_FOOT\",\\n        update_period=0.0,\\n        history_length=6,\\n        debug_vis=True,\\n        filter_prim_paths_expr=[\"{ENV_REGEX_NS}/Cube\"],\\n    )\\n\\n    contact_forces_H = ContactSensorCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/.*H_FOOT\",\\n        update_period=0.0,\\n        history_length=6,\\n        debug_vis=True,\\n    )'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = scene[\"robot\"].data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            scene[\"robot\"].write_root_pose_to_sim(root_state[:, :7])\\n            scene[\"robot\"].write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = (\\n                scene[\"robot\"].data.default_joint_pos.clone(),\\n                scene[\"robot\"].data.default_joint_vel.clone(),\\n            )\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            scene[\"robot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply default actions to the robot\\n        # -- generate actions/commands\\n        targets = scene[\"robot\"].data.default_joint_pos\\n        # -- apply action to the robot\\n        scene[\"robot\"].set_joint_position_target(targets)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        scene.update(sim_dt)\\n\\n        # print information from the sensors\\n        print(\"-------------------------------\")\\n        print(scene[\"contact_forces_LF\"])\\n        print(\"Received force matrix of: \", scene[\"contact_forces_LF\"].data.force_matrix_w)\\n        print(\"Received contact force of: \", scene[\"contact_forces_LF\"].data.net_forces_w)\\n        print(\"-------------------------------\")\\n        print(scene[\"contact_forces_RF\"])\\n        print(\"Received force matrix of: \", scene[\"contact_forces_RF\"].data.force_matrix_w)\\n        print(\"Received contact force of: \", scene[\"contact_forces_RF\"].data.net_forces_w)\\n        print(\"-------------------------------\")\\n        print(scene[\"contact_forces_H\"])\\n        print(\"Received force matrix of: \", scene[\"contact_forces_H\"].data.force_matrix_w)\\n        print(\"Received contact force of: \", scene[\"contact_forces_H\"].data.net_forces_w)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_cfg = ContactSensorSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='class FrameTransformerSensorSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Design the scene with sensors on the robot.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # robot\\n    robot = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    # Rigid Object\\n    cube = RigidObjectCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Cube\",\\n        spawn=sim_utils.CuboidCfg(\\n            size=(1, 1, 1),\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(),\\n            mass_props=sim_utils.MassPropertiesCfg(mass=100.0),\\n            collision_props=sim_utils.CollisionPropertiesCfg(),\\n            physics_material=sim_utils.RigidBodyMaterialCfg(static_friction=1.0),\\n            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0), metallic=0.2),\\n        ),\\n        init_state=RigidObjectCfg.InitialStateCfg(pos=(5, 0, 0.5)),\\n    )\\n\\n    specific_transforms = FrameTransformerCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base\",\\n        target_frames=[\\n            FrameTransformerCfg.FrameCfg(prim_path=\"{ENV_REGEX_NS}/Robot/LF_FOOT\"),\\n            FrameTransformerCfg.FrameCfg(prim_path=\"{ENV_REGEX_NS}/Robot/RF_FOOT\"),\\n        ],\\n        debug_vis=True,\\n    )\\n\\n    cube_transform = FrameTransformerCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base\",\\n        target_frames=[FrameTransformerCfg.FrameCfg(prim_path=\"{ENV_REGEX_NS}/Cube\")],\\n        debug_vis=False,\\n    )\\n\\n    robot_transforms = FrameTransformerCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base\",\\n        target_frames=[FrameTransformerCfg.FrameCfg(prim_path=\"{ENV_REGEX_NS}/Robot/.*\")],\\n        debug_vis=False,\\n    )'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = scene[\"robot\"].data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            scene[\"robot\"].write_root_pose_to_sim(root_state[:, :7])\\n            scene[\"robot\"].write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = (\\n                scene[\"robot\"].data.default_joint_pos.clone(),\\n                scene[\"robot\"].data.default_joint_vel.clone(),\\n            )\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            scene[\"robot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply default actions to the robot\\n        # -- generate actions/commands\\n        targets = scene[\"robot\"].data.default_joint_pos\\n        # -- apply action to the robot\\n        scene[\"robot\"].set_joint_position_target(targets)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        scene.update(sim_dt)\\n\\n        # print information from the sensors\\n        print(\"-------------------------------\")\\n        print(scene[\"specific_transforms\"])\\n        print(\"relative transforms:\", scene[\"specific_transforms\"].data.target_pos_source)\\n        print(\"relative orientations:\", scene[\"specific_transforms\"].data.target_quat_source)\\n        print(\"-------------------------------\")\\n        print(scene[\"cube_transform\"])\\n        print(\"relative transform:\", scene[\"cube_transform\"].data.target_pos_source)\\n        print(\"-------------------------------\")\\n        print(scene[\"robot_transforms\"])\\n        print(\"relative transforms:\", scene[\"robot_transforms\"].data.target_pos_source)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_cfg = FrameTransformerSensorSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='class ImuSensorSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Design the scene with sensors on the robot.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # robot\\n    robot = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    imu_RF = ImuCfg(prim_path=\"{ENV_REGEX_NS}/Robot/LF_FOOT\", debug_vis=True)\\n\\n    imu_LF = ImuCfg(prim_path=\"{ENV_REGEX_NS}/Robot/RF_FOOT\", gravity_bias=(0, 0, 0), debug_vis=True)'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = scene[\"robot\"].data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            scene[\"robot\"].write_root_link_pose_to_sim(root_state[:, :7])\\n            scene[\"robot\"].write_root_com_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = (\\n                scene[\"robot\"].data.default_joint_pos.clone(),\\n                scene[\"robot\"].data.default_joint_vel.clone(),\\n            )\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            scene[\"robot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply default actions to the robot\\n        # -- generate actions/commands\\n        targets = scene[\"robot\"].data.default_joint_pos\\n        # -- apply action to the robot\\n        scene[\"robot\"].set_joint_position_target(targets)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        scene.update(sim_dt)\\n\\n        # print information from the sensors\\n        print(\"-------------------------------\")\\n        print(scene[\"imu_LF\"])\\n        print(\"Received linear velocity: \", scene[\"imu_LF\"].data.lin_vel_b)\\n        print(\"Received angular velocity: \", scene[\"imu_LF\"].data.ang_vel_b)\\n        print(\"Received linear acceleration: \", scene[\"imu_LF\"].data.lin_acc_b)\\n        print(\"Received angular acceleration: \", scene[\"imu_LF\"].data.ang_acc_b)\\n        print(\"-------------------------------\")\\n        print(scene[\"imu_RF\"])\\n        print(\"Received linear velocity: \", scene[\"imu_RF\"].data.lin_vel_b)\\n        print(\"Received angular velocity: \", scene[\"imu_RF\"].data.ang_vel_b)\\n        print(\"Received linear acceleration: \", scene[\"imu_RF\"].data.lin_acc_b)\\n        print(\"Received angular acceleration: \", scene[\"imu_RF\"].data.ang_acc_b)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_cfg = ImuSensorSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='class RaycasterSensorSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Design the scene with sensors on the robot.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(\\n        prim_path=\"/World/Ground\",\\n        spawn=sim_utils.UsdFileCfg(\\n            usd_path=f\"{ISAAC_NUCLEUS_DIR}/Environments/Terrains/rough_plane.usd\",\\n            scale=(1, 1, 1),\\n        ),\\n    )\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # robot\\n    robot = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    ray_caster = RayCasterCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base/lidar_cage\",\\n        update_period=1 / 60,\\n        offset=RayCasterCfg.OffsetCfg(pos=(0, 0, 0.5)),\\n        mesh_prim_paths=[\"/World/Ground\"],\\n        attach_yaw_only=True,\\n        pattern_cfg=patterns.LidarPatternCfg(\\n            channels=100, vertical_fov_range=[-90, 90], horizontal_fov_range=[-90, 90], horizontal_res=1.0\\n        ),\\n        debug_vis=not args_cli.headless,\\n    )'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    triggered = True\\n    countdown = 42\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = scene[\"robot\"].data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            scene[\"robot\"].write_root_pose_to_sim(root_state[:, :7])\\n            scene[\"robot\"].write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = (\\n                scene[\"robot\"].data.default_joint_pos.clone(),\\n                scene[\"robot\"].data.default_joint_vel.clone(),\\n            )\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            scene[\"robot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply default actions to the robot\\n        # -- generate actions/commands\\n        targets = scene[\"robot\"].data.default_joint_pos\\n        # -- apply action to the robot\\n        scene[\"robot\"].set_joint_position_target(targets)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        scene.update(sim_dt)\\n\\n        # print information from the sensors\\n        print(\"-------------------------------\")\\n        print(scene[\"ray_caster\"])\\n        print(\"Ray cast hit results: \", scene[\"ray_caster\"].data.ray_hits_w)\\n\\n        if not triggered:\\n            if countdown > 0:\\n                countdown -= 1\\n                continue\\n            data = scene[\"ray_caster\"].data.ray_hits_w.cpu().numpy()\\n            np.save(\"cast_data.npy\", data)\\n            triggered = True\\n        else:\\n            continue'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_cfg = RaycasterSensorSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Print all environments registered in `isaaclab_tasks` extension.\"\"\"\\n    # print all the available environments\\n    table = PrettyTable([\"S. No.\", \"Task Name\", \"Entry Point\", \"Config\"])\\n    table.title = \"Available Environments in Isaac Lab\"\\n    # set alignment of table columns\\n    table.align[\"Task Name\"] = \"l\"\\n    table.align[\"Entry Point\"] = \"l\"\\n    table.align[\"Config\"] = \"l\"\\n\\n    # count of environments\\n    index = 0\\n    # acquire all Isaac environments names\\n    for task_spec in gym.registry.values():\\n        if \"Isaac\" in task_spec.id:\\n            # add details to table\\n            table.add_row([index + 1, task_spec.id, task_spec.entry_point, task_spec.kwargs[\"env_cfg_entry_point\"]])\\n            # increment count\\n            index += 1\\n\\n    print(table)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Random actions agent with Isaac Lab environment.\"\"\"\\n    # create environment configuration\\n    env_cfg = parse_env_cfg(\\n        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric\\n    )\\n    # create environment\\n    env = gym.make(args_cli.task, cfg=env_cfg)\\n\\n    # print info (this is vectorized environment)\\n    print(f\"[INFO]: Gym observation space: {env.observation_space}\")\\n    print(f\"[INFO]: Gym action space: {env.action_space}\")\\n    # reset environment\\n    env.reset()\\n    # simulate environment\\n    while simulation_app.is_running():\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # sample actions from -1 to 1\\n            actions = 2 * torch.rand(env.action_space.shape, device=env.unwrapped.device) - 1\\n            # apply actions\\n            env.step(actions)\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Zero actions agent with Isaac Lab environment.\"\"\"\\n    # parse configuration\\n    env_cfg = parse_env_cfg(\\n        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric\\n    )\\n    # create environment\\n    env = gym.make(args_cli.task, cfg=env_cfg)\\n\\n    # print info (this is vectorized environment)\\n    print(f\"[INFO]: Gym observation space: {env.observation_space}\")\\n    print(f\"[INFO]: Gym action space: {env.action_space}\")\\n    # reset environment\\n    env.reset()\\n    # simulate environment\\n    while simulation_app.is_running():\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # compute zero actions\\n            actions = torch.zeros(env.action_space.shape, device=env.unwrapped.device)\\n            # apply actions\\n            env.step(actions)\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='class GripperState:\\n    \"\"\"States for the gripper.\"\"\"\\n\\n    OPEN = wp.constant(1.0)\\n    CLOSE = wp.constant(-1.0)'),\n",
       " Document(metadata={}, page_content='class PickSmState:\\n    \"\"\"States for the pick state machine.\"\"\"\\n\\n    REST = wp.constant(0)\\n    APPROACH_ABOVE_OBJECT = wp.constant(1)\\n    APPROACH_OBJECT = wp.constant(2)\\n    GRASP_OBJECT = wp.constant(3)\\n    LIFT_OBJECT = wp.constant(4)'),\n",
       " Document(metadata={}, page_content='class PickSmWaitTime:\\n    \"\"\"Additional wait times (in s) for states for before switching.\"\"\"\\n\\n    REST = wp.constant(0.2)\\n    APPROACH_ABOVE_OBJECT = wp.constant(0.5)\\n    APPROACH_OBJECT = wp.constant(0.6)\\n    GRASP_OBJECT = wp.constant(0.3)\\n    LIFT_OBJECT = wp.constant(1.0)'),\n",
       " Document(metadata={}, page_content='def distance_below_threshold(current_pos: wp.vec3, desired_pos: wp.vec3, threshold: float) -> bool:\\n    return wp.length(current_pos - desired_pos) < threshold'),\n",
       " Document(metadata={}, page_content='def infer_state_machine(\\n    dt: wp.array(dtype=float),\\n    sm_state: wp.array(dtype=int),\\n    sm_wait_time: wp.array(dtype=float),\\n    ee_pose: wp.array(dtype=wp.transform),\\n    object_pose: wp.array(dtype=wp.transform),\\n    des_object_pose: wp.array(dtype=wp.transform),\\n    des_ee_pose: wp.array(dtype=wp.transform),\\n    gripper_state: wp.array(dtype=float),\\n    offset: wp.array(dtype=wp.transform),\\n    position_threshold: float,\\n):\\n    # retrieve thread id\\n    tid = wp.tid()\\n    # retrieve state machine state\\n    state = sm_state[tid]\\n    # decide next state\\n    if state == PickSmState.REST:\\n        des_ee_pose[tid] = ee_pose[tid]\\n        gripper_state[tid] = GripperState.OPEN\\n        # wait for a while\\n        if sm_wait_time[tid] >= PickSmWaitTime.REST:\\n            # move to next state and reset wait time\\n            sm_state[tid] = PickSmState.APPROACH_ABOVE_OBJECT\\n            sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.APPROACH_ABOVE_OBJECT:\\n        des_ee_pose[tid] = wp.transform_multiply(offset[tid], object_pose[tid])\\n        gripper_state[tid] = GripperState.OPEN\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            # wait for a while\\n            if sm_wait_time[tid] >= PickSmWaitTime.APPROACH_OBJECT:\\n                # move to next state and reset wait time\\n                sm_state[tid] = PickSmState.APPROACH_OBJECT\\n                sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.APPROACH_OBJECT:\\n        des_ee_pose[tid] = object_pose[tid]\\n        gripper_state[tid] = GripperState.OPEN\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            if sm_wait_time[tid] >= PickSmWaitTime.APPROACH_OBJECT:\\n                # move to next state and reset wait time\\n                sm_state[tid] = PickSmState.GRASP_OBJECT\\n                sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.GRASP_OBJECT:\\n        des_ee_pose[tid] = object_pose[tid]\\n        gripper_state[tid] = GripperState.CLOSE\\n        # wait for a while\\n        if sm_wait_time[tid] >= PickSmWaitTime.GRASP_OBJECT:\\n            # move to next state and reset wait time\\n            sm_state[tid] = PickSmState.LIFT_OBJECT\\n            sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.LIFT_OBJECT:\\n        des_ee_pose[tid] = des_object_pose[tid]\\n        gripper_state[tid] = GripperState.CLOSE\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            # wait for a while\\n            if sm_wait_time[tid] >= PickSmWaitTime.LIFT_OBJECT:\\n                # move to next state and reset wait time\\n                sm_state[tid] = PickSmState.LIFT_OBJECT\\n                sm_wait_time[tid] = 0.0\\n    # increment wait time\\n    sm_wait_time[tid] = sm_wait_time[tid] + dt[tid]'),\n",
       " Document(metadata={}, page_content='class PickAndLiftSm:\\n    \"\"\"A simple state machine in a robot\\'s task space to pick and lift an object.\\n\\n    The state machine is implemented as a warp kernel. It takes in the current state of\\n    the robot\\'s end-effector and the object, and outputs the desired state of the robot\\'s\\n    end-effector and the gripper. The state machine is implemented as a finite state\\n    machine with the following states:\\n\\n    1. REST: The robot is at rest.\\n    2. APPROACH_ABOVE_OBJECT: The robot moves above the object.\\n    3. APPROACH_OBJECT: The robot moves to the object.\\n    4. GRASP_OBJECT: The robot grasps the object.\\n    5. LIFT_OBJECT: The robot lifts the object to the desired pose. This is the final state.\\n    \"\"\"\\n\\n    def __init__(self, dt: float, num_envs: int, device: torch.device | str = \"cpu\", position_threshold=0.01):\\n        \"\"\"Initialize the state machine.\\n\\n        Args:\\n            dt: The environment time step.\\n            num_envs: The number of environments to simulate.\\n            device: The device to run the state machine on.\\n        \"\"\"\\n        # save parameters\\n        self.dt = float(dt)\\n        self.num_envs = num_envs\\n        self.device = device\\n        self.position_threshold = position_threshold\\n        # initialize state machine\\n        self.sm_dt = torch.full((self.num_envs,), self.dt, device=self.device)\\n        self.sm_state = torch.full((self.num_envs,), 0, dtype=torch.int32, device=self.device)\\n        self.sm_wait_time = torch.zeros((self.num_envs,), device=self.device)\\n\\n        # desired state\\n        self.des_ee_pose = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.des_gripper_state = torch.full((self.num_envs,), 0.0, device=self.device)\\n\\n        # approach above object offset\\n        self.offset = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.offset[:, 2] = 0.1\\n        self.offset[:, -1] = 1.0  # warp expects quaternion as (x, y, z, w)\\n\\n        # convert to warp\\n        self.sm_dt_wp = wp.from_torch(self.sm_dt, wp.float32)\\n        self.sm_state_wp = wp.from_torch(self.sm_state, wp.int32)\\n        self.sm_wait_time_wp = wp.from_torch(self.sm_wait_time, wp.float32)\\n        self.des_ee_pose_wp = wp.from_torch(self.des_ee_pose, wp.transform)\\n        self.des_gripper_state_wp = wp.from_torch(self.des_gripper_state, wp.float32)\\n        self.offset_wp = wp.from_torch(self.offset, wp.transform)\\n\\n    def reset_idx(self, env_ids: Sequence[int] = None):\\n        \"\"\"Reset the state machine.\"\"\"\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        self.sm_state[env_ids] = 0\\n        self.sm_wait_time[env_ids] = 0.0\\n\\n    def compute(self, ee_pose: torch.Tensor, object_pose: torch.Tensor, des_object_pose: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Compute the desired state of the robot\\'s end-effector and the gripper.\"\"\"\\n        # convert all transformations from (w, x, y, z) to (x, y, z, w)\\n        ee_pose = ee_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n        object_pose = object_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n        des_object_pose = des_object_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n\\n        # convert to warp\\n        ee_pose_wp = wp.from_torch(ee_pose.contiguous(), wp.transform)\\n        object_pose_wp = wp.from_torch(object_pose.contiguous(), wp.transform)\\n        des_object_pose_wp = wp.from_torch(des_object_pose.contiguous(), wp.transform)\\n\\n        # run state machine\\n        wp.launch(\\n            kernel=infer_state_machine,\\n            dim=self.num_envs,\\n            inputs=[\\n                self.sm_dt_wp,\\n                self.sm_state_wp,\\n                self.sm_wait_time_wp,\\n                ee_pose_wp,\\n                object_pose_wp,\\n                des_object_pose_wp,\\n                self.des_ee_pose_wp,\\n                self.des_gripper_state_wp,\\n                self.offset_wp,\\n                self.position_threshold,\\n            ],\\n            device=self.device,\\n        )\\n\\n        # convert transformations back to (w, x, y, z)\\n        des_ee_pose = self.des_ee_pose[:, [0, 1, 2, 6, 3, 4, 5]]\\n        # convert to torch\\n        return torch.cat([des_ee_pose, self.des_gripper_state.unsqueeze(-1)], dim=-1)'),\n",
       " Document(metadata={}, page_content='def main():\\n    # parse configuration\\n    env_cfg: LiftEnvCfg = parse_env_cfg(\\n        \"Isaac-Lift-Cube-Franka-IK-Abs-v0\",\\n        device=args_cli.device,\\n        num_envs=args_cli.num_envs,\\n        use_fabric=not args_cli.disable_fabric,\\n    )\\n    # create environment\\n    env = gym.make(\"Isaac-Lift-Cube-Franka-IK-Abs-v0\", cfg=env_cfg)\\n    # reset environment at start\\n    env.reset()\\n\\n    # create action buffers (position + quaternion)\\n    actions = torch.zeros(env.unwrapped.action_space.shape, device=env.unwrapped.device)\\n    actions[:, 3] = 1.0\\n    # desired object orientation (we only do position control of object)\\n    desired_orientation = torch.zeros((env.unwrapped.num_envs, 4), device=env.unwrapped.device)\\n    desired_orientation[:, 1] = 1.0\\n    # create state machine\\n    pick_sm = PickAndLiftSm(\\n        env_cfg.sim.dt * env_cfg.decimation, env.unwrapped.num_envs, env.unwrapped.device, position_threshold=0.01\\n    )\\n\\n    while simulation_app.is_running():\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # step environment\\n            dones = env.step(actions)[-2]\\n\\n            # observations\\n            # -- end-effector frame\\n            ee_frame_sensor = env.unwrapped.scene[\"ee_frame\"]\\n            tcp_rest_position = ee_frame_sensor.data.target_pos_w[..., 0, :].clone() - env.unwrapped.scene.env_origins\\n            tcp_rest_orientation = ee_frame_sensor.data.target_quat_w[..., 0, :].clone()\\n            # -- object frame\\n            object_data: RigidObjectData = env.unwrapped.scene[\"object\"].data\\n            object_position = object_data.root_pos_w - env.unwrapped.scene.env_origins\\n            # -- target object frame\\n            desired_position = env.unwrapped.command_manager.get_command(\"object_pose\")[..., :3]\\n\\n            # advance state machine\\n            actions = pick_sm.compute(\\n                torch.cat([tcp_rest_position, tcp_rest_orientation], dim=-1),\\n                torch.cat([object_position, desired_orientation], dim=-1),\\n                torch.cat([desired_position, desired_orientation], dim=-1),\\n            )\\n\\n            # reset state machine\\n            if dones.any():\\n                pick_sm.reset_idx(dones.nonzero(as_tuple=False).squeeze(-1))\\n\\n    # close the environment\\n    env.close()'),\n",
       " Document(metadata={}, page_content='class GripperState:\\n    \"\"\"States for the gripper.\"\"\"\\n\\n    OPEN = wp.constant(1.0)\\n    CLOSE = wp.constant(-1.0)'),\n",
       " Document(metadata={}, page_content='class PickSmState:\\n    \"\"\"States for the pick state machine.\"\"\"\\n\\n    REST = wp.constant(0)\\n    APPROACH_ABOVE_OBJECT = wp.constant(1)\\n    APPROACH_OBJECT = wp.constant(2)\\n    GRASP_OBJECT = wp.constant(3)\\n    LIFT_OBJECT = wp.constant(4)\\n    OPEN_GRIPPER = wp.constant(5)'),\n",
       " Document(metadata={}, page_content='class PickSmWaitTime:\\n    \"\"\"Additional wait times (in s) for states for before switching.\"\"\"\\n\\n    REST = wp.constant(0.2)\\n    APPROACH_ABOVE_OBJECT = wp.constant(0.5)\\n    APPROACH_OBJECT = wp.constant(0.6)\\n    GRASP_OBJECT = wp.constant(0.6)\\n    LIFT_OBJECT = wp.constant(1.0)\\n    OPEN_GRIPPER = wp.constant(0.0)'),\n",
       " Document(metadata={}, page_content='def distance_below_threshold(current_pos: wp.vec3, desired_pos: wp.vec3, threshold: float) -> bool:\\n    return wp.length(current_pos - desired_pos) < threshold'),\n",
       " Document(metadata={}, page_content='def infer_state_machine(\\n    dt: wp.array(dtype=float),\\n    sm_state: wp.array(dtype=int),\\n    sm_wait_time: wp.array(dtype=float),\\n    ee_pose: wp.array(dtype=wp.transform),\\n    object_pose: wp.array(dtype=wp.transform),\\n    des_object_pose: wp.array(dtype=wp.transform),\\n    des_ee_pose: wp.array(dtype=wp.transform),\\n    gripper_state: wp.array(dtype=float),\\n    offset: wp.array(dtype=wp.transform),\\n    position_threshold: float,\\n):\\n    # retrieve thread id\\n    tid = wp.tid()\\n    # retrieve state machine state\\n    state = sm_state[tid]\\n    # decide next state\\n    if state == PickSmState.REST:\\n        des_ee_pose[tid] = ee_pose[tid]\\n        gripper_state[tid] = GripperState.OPEN\\n        # wait for a while\\n        if sm_wait_time[tid] >= PickSmWaitTime.REST:\\n            # move to next state and reset wait time\\n            sm_state[tid] = PickSmState.APPROACH_ABOVE_OBJECT\\n            sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.APPROACH_ABOVE_OBJECT:\\n        des_ee_pose[tid] = wp.transform_multiply(offset[tid], object_pose[tid])\\n        gripper_state[tid] = GripperState.OPEN\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            # wait for a while\\n            if sm_wait_time[tid] >= PickSmWaitTime.APPROACH_OBJECT:\\n                # move to next state and reset wait time\\n                sm_state[tid] = PickSmState.APPROACH_OBJECT\\n                sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.APPROACH_OBJECT:\\n        des_ee_pose[tid] = object_pose[tid]\\n        gripper_state[tid] = GripperState.OPEN\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            # wait for a while\\n            if sm_wait_time[tid] >= PickSmWaitTime.APPROACH_OBJECT:\\n                # move to next state and reset wait time\\n                sm_state[tid] = PickSmState.GRASP_OBJECT\\n                sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.GRASP_OBJECT:\\n        des_ee_pose[tid] = object_pose[tid]\\n        gripper_state[tid] = GripperState.CLOSE\\n        # wait for a while\\n        if sm_wait_time[tid] >= PickSmWaitTime.GRASP_OBJECT:\\n            # move to next state and reset wait time\\n            sm_state[tid] = PickSmState.LIFT_OBJECT\\n            sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.LIFT_OBJECT:\\n        des_ee_pose[tid] = des_object_pose[tid]\\n        gripper_state[tid] = GripperState.CLOSE\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            # wait for a while\\n            if sm_wait_time[tid] >= PickSmWaitTime.LIFT_OBJECT:\\n                # move to next state and reset wait time\\n                sm_state[tid] = PickSmState.OPEN_GRIPPER\\n                sm_wait_time[tid] = 0.0\\n    elif state == PickSmState.OPEN_GRIPPER:\\n        # des_ee_pose[tid] = object_pose[tid]\\n        gripper_state[tid] = GripperState.OPEN\\n        # wait for a while\\n        if sm_wait_time[tid] >= PickSmWaitTime.OPEN_GRIPPER:\\n            # move to next state and reset wait time\\n            sm_state[tid] = PickSmState.OPEN_GRIPPER\\n            sm_wait_time[tid] = 0.0\\n    # increment wait time\\n    sm_wait_time[tid] = sm_wait_time[tid] + dt[tid]'),\n",
       " Document(metadata={}, page_content='class PickAndLiftSm:\\n    \"\"\"A simple state machine in a robot\\'s task space to pick and lift an object.\\n\\n    The state machine is implemented as a warp kernel. It takes in the current state of\\n    the robot\\'s end-effector and the object, and outputs the desired state of the robot\\'s\\n    end-effector and the gripper. The state machine is implemented as a finite state\\n    machine with the following states:\\n\\n    1. REST: The robot is at rest.\\n    2. APPROACH_ABOVE_OBJECT: The robot moves above the object.\\n    3. APPROACH_OBJECT: The robot moves to the object.\\n    4. GRASP_OBJECT: The robot grasps the object.\\n    5. LIFT_OBJECT: The robot lifts the object to the desired pose. This is the final state.\\n    \"\"\"\\n\\n    def __init__(self, dt: float, num_envs: int, device: torch.device | str = \"cpu\", position_threshold=0.01):\\n        \"\"\"Initialize the state machine.\\n\\n        Args:\\n            dt: The environment time step.\\n            num_envs: The number of environments to simulate.\\n            device: The device to run the state machine on.\\n        \"\"\"\\n        # save parameters\\n        self.dt = float(dt)\\n        self.num_envs = num_envs\\n        self.device = device\\n        self.position_threshold = position_threshold\\n        # initialize state machine\\n        self.sm_dt = torch.full((self.num_envs,), self.dt, device=self.device)\\n        self.sm_state = torch.full((self.num_envs,), 0, dtype=torch.int32, device=self.device)\\n        self.sm_wait_time = torch.zeros((self.num_envs,), device=self.device)\\n\\n        # desired state\\n        self.des_ee_pose = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.des_gripper_state = torch.full((self.num_envs,), 0.0, device=self.device)\\n\\n        # approach above object offset\\n        self.offset = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.offset[:, 2] = 0.2\\n        self.offset[:, -1] = 1.0  # warp expects quaternion as (x, y, z, w)\\n\\n        # convert to warp\\n        self.sm_dt_wp = wp.from_torch(self.sm_dt, wp.float32)\\n        self.sm_state_wp = wp.from_torch(self.sm_state, wp.int32)\\n        self.sm_wait_time_wp = wp.from_torch(self.sm_wait_time, wp.float32)\\n        self.des_ee_pose_wp = wp.from_torch(self.des_ee_pose, wp.transform)\\n        self.des_gripper_state_wp = wp.from_torch(self.des_gripper_state, wp.float32)\\n        self.offset_wp = wp.from_torch(self.offset, wp.transform)\\n\\n    def reset_idx(self, env_ids: Sequence[int] = None):\\n        \"\"\"Reset the state machine.\"\"\"\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        self.sm_state[env_ids] = 0\\n        self.sm_wait_time[env_ids] = 0.0\\n\\n    def compute(self, ee_pose: torch.Tensor, object_pose: torch.Tensor, des_object_pose: torch.Tensor):\\n        \"\"\"Compute the desired state of the robot\\'s end-effector and the gripper.\"\"\"\\n        # convert all transformations from (w, x, y, z) to (x, y, z, w)\\n        ee_pose = ee_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n        object_pose = object_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n        des_object_pose = des_object_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n\\n        # convert to warp\\n        ee_pose_wp = wp.from_torch(ee_pose.contiguous(), wp.transform)\\n        object_pose_wp = wp.from_torch(object_pose.contiguous(), wp.transform)\\n        des_object_pose_wp = wp.from_torch(des_object_pose.contiguous(), wp.transform)\\n\\n        # run state machine\\n        wp.launch(\\n            kernel=infer_state_machine,\\n            dim=self.num_envs,\\n            inputs=[\\n                self.sm_dt_wp,\\n                self.sm_state_wp,\\n                self.sm_wait_time_wp,\\n                ee_pose_wp,\\n                object_pose_wp,\\n                des_object_pose_wp,\\n                self.des_ee_pose_wp,\\n                self.des_gripper_state_wp,\\n                self.offset_wp,\\n                self.position_threshold,\\n            ],\\n            device=self.device,\\n        )\\n\\n        # convert transformations back to (w, x, y, z)\\n        des_ee_pose = self.des_ee_pose[:, [0, 1, 2, 6, 3, 4, 5]]\\n        # convert to torch\\n        return torch.cat([des_ee_pose, self.des_gripper_state.unsqueeze(-1)], dim=-1)'),\n",
       " Document(metadata={}, page_content='def main():\\n    # parse configuration\\n    env_cfg: LiftEnvCfg = parse_env_cfg(\\n        \"Isaac-Lift-Teddy-Bear-Franka-IK-Abs-v0\",\\n        device=args_cli.device,\\n        num_envs=args_cli.num_envs,\\n    )\\n\\n    env_cfg.viewer.eye = (2.1, 1.0, 1.3)\\n\\n    # create environment\\n    env = gym.make(\"Isaac-Lift-Teddy-Bear-Franka-IK-Abs-v0\", cfg=env_cfg)\\n    # reset environment at start\\n    env.reset()\\n\\n    # create action buffers (position + quaternion)\\n    actions = torch.zeros(env.unwrapped.action_space.shape, device=env.unwrapped.device)\\n    actions[:, 3] = 1.0\\n    # desired rotation after grasping\\n    desired_orientation = torch.zeros((env.unwrapped.num_envs, 4), device=env.unwrapped.device)\\n    desired_orientation[:, 1] = 1.0\\n\\n    object_grasp_orientation = torch.zeros((env.unwrapped.num_envs, 4), device=env.unwrapped.device)\\n    # z-axis pointing down and 45 degrees rotation\\n    object_grasp_orientation[:, 1] = 0.9238795\\n    object_grasp_orientation[:, 2] = -0.3826834\\n    object_local_grasp_position = torch.tensor([0.02, -0.08, 0.0], device=env.unwrapped.device)\\n\\n    # create state machine\\n    pick_sm = PickAndLiftSm(env_cfg.sim.dt * env_cfg.decimation, env.unwrapped.num_envs, env.unwrapped.device)\\n\\n    while simulation_app.is_running():\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # step environment\\n            dones = env.step(actions)[-2]\\n\\n            # observations\\n            # -- end-effector frame\\n            ee_frame_sensor = env.unwrapped.scene[\"ee_frame\"]\\n            tcp_rest_position = ee_frame_sensor.data.target_pos_w[..., 0, :].clone() - env.unwrapped.scene.env_origins\\n            tcp_rest_orientation = ee_frame_sensor.data.target_quat_w[..., 0, :].clone()\\n            # -- object frame\\n            object_data: RigidObjectData = env.unwrapped.scene[\"object\"].data\\n            object_position = object_data.root_pos_w - env.unwrapped.scene.env_origins\\n            object_position += object_local_grasp_position\\n\\n            # -- target object frame\\n            desired_position = env.unwrapped.command_manager.get_command(\"object_pose\")[..., :3]\\n\\n            # advance state machine\\n            actions = pick_sm.compute(\\n                torch.cat([tcp_rest_position, tcp_rest_orientation], dim=-1),\\n                torch.cat([object_position, object_grasp_orientation], dim=-1),\\n                torch.cat([desired_position, desired_orientation], dim=-1),\\n            )\\n\\n            # reset state machine\\n            if dones.any():\\n                pick_sm.reset_idx(dones.nonzero(as_tuple=False).squeeze(-1))\\n\\n    # close the environment\\n    env.close()'),\n",
       " Document(metadata={}, page_content='class GripperState:\\n    \"\"\"States for the gripper.\"\"\"\\n\\n    OPEN = wp.constant(1.0)\\n    CLOSE = wp.constant(-1.0)'),\n",
       " Document(metadata={}, page_content='class OpenDrawerSmState:\\n    \"\"\"States for the cabinet drawer opening state machine.\"\"\"\\n\\n    REST = wp.constant(0)\\n    APPROACH_INFRONT_HANDLE = wp.constant(1)\\n    APPROACH_HANDLE = wp.constant(2)\\n    GRASP_HANDLE = wp.constant(3)\\n    OPEN_DRAWER = wp.constant(4)\\n    RELEASE_HANDLE = wp.constant(5)'),\n",
       " Document(metadata={}, page_content='class OpenDrawerSmWaitTime:\\n    \"\"\"Additional wait times (in s) for states for before switching.\"\"\"\\n\\n    REST = wp.constant(0.5)\\n    APPROACH_INFRONT_HANDLE = wp.constant(1.25)\\n    APPROACH_HANDLE = wp.constant(1.0)\\n    GRASP_HANDLE = wp.constant(1.0)\\n    OPEN_DRAWER = wp.constant(3.0)\\n    RELEASE_HANDLE = wp.constant(0.2)'),\n",
       " Document(metadata={}, page_content='def distance_below_threshold(current_pos: wp.vec3, desired_pos: wp.vec3, threshold: float) -> bool:\\n    return wp.length(current_pos - desired_pos) < threshold'),\n",
       " Document(metadata={}, page_content='def infer_state_machine(\\n    dt: wp.array(dtype=float),\\n    sm_state: wp.array(dtype=int),\\n    sm_wait_time: wp.array(dtype=float),\\n    ee_pose: wp.array(dtype=wp.transform),\\n    handle_pose: wp.array(dtype=wp.transform),\\n    des_ee_pose: wp.array(dtype=wp.transform),\\n    gripper_state: wp.array(dtype=float),\\n    handle_approach_offset: wp.array(dtype=wp.transform),\\n    handle_grasp_offset: wp.array(dtype=wp.transform),\\n    drawer_opening_rate: wp.array(dtype=wp.transform),\\n    position_threshold: float,\\n):\\n    # retrieve thread id\\n    tid = wp.tid()\\n    # retrieve state machine state\\n    state = sm_state[tid]\\n    # decide next state\\n    if state == OpenDrawerSmState.REST:\\n        des_ee_pose[tid] = ee_pose[tid]\\n        gripper_state[tid] = GripperState.OPEN\\n        # wait for a while\\n        if sm_wait_time[tid] >= OpenDrawerSmWaitTime.REST:\\n            # move to next state and reset wait time\\n            sm_state[tid] = OpenDrawerSmState.APPROACH_INFRONT_HANDLE\\n            sm_wait_time[tid] = 0.0\\n    elif state == OpenDrawerSmState.APPROACH_INFRONT_HANDLE:\\n        des_ee_pose[tid] = wp.transform_multiply(handle_approach_offset[tid], handle_pose[tid])\\n        gripper_state[tid] = GripperState.OPEN\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            # wait for a while\\n            if sm_wait_time[tid] >= OpenDrawerSmWaitTime.APPROACH_INFRONT_HANDLE:\\n                # move to next state and reset wait time\\n                sm_state[tid] = OpenDrawerSmState.APPROACH_HANDLE\\n                sm_wait_time[tid] = 0.0\\n    elif state == OpenDrawerSmState.APPROACH_HANDLE:\\n        des_ee_pose[tid] = handle_pose[tid]\\n        gripper_state[tid] = GripperState.OPEN\\n        if distance_below_threshold(\\n            wp.transform_get_translation(ee_pose[tid]),\\n            wp.transform_get_translation(des_ee_pose[tid]),\\n            position_threshold,\\n        ):\\n            # wait for a while\\n            if sm_wait_time[tid] >= OpenDrawerSmWaitTime.APPROACH_HANDLE:\\n                # move to next state and reset wait time\\n                sm_state[tid] = OpenDrawerSmState.GRASP_HANDLE\\n                sm_wait_time[tid] = 0.0\\n    elif state == OpenDrawerSmState.GRASP_HANDLE:\\n        des_ee_pose[tid] = wp.transform_multiply(handle_grasp_offset[tid], handle_pose[tid])\\n        gripper_state[tid] = GripperState.CLOSE\\n        # wait for a while\\n        if sm_wait_time[tid] >= OpenDrawerSmWaitTime.GRASP_HANDLE:\\n            # move to next state and reset wait time\\n            sm_state[tid] = OpenDrawerSmState.OPEN_DRAWER\\n            sm_wait_time[tid] = 0.0\\n    elif state == OpenDrawerSmState.OPEN_DRAWER:\\n        des_ee_pose[tid] = wp.transform_multiply(drawer_opening_rate[tid], handle_pose[tid])\\n        gripper_state[tid] = GripperState.CLOSE\\n        # wait for a while\\n        if sm_wait_time[tid] >= OpenDrawerSmWaitTime.OPEN_DRAWER:\\n            # move to next state and reset wait time\\n            sm_state[tid] = OpenDrawerSmState.RELEASE_HANDLE\\n            sm_wait_time[tid] = 0.0\\n    elif state == OpenDrawerSmState.RELEASE_HANDLE:\\n        des_ee_pose[tid] = ee_pose[tid]\\n        gripper_state[tid] = GripperState.CLOSE\\n        # wait for a while\\n        if sm_wait_time[tid] >= OpenDrawerSmWaitTime.RELEASE_HANDLE:\\n            # move to next state and reset wait time\\n            sm_state[tid] = OpenDrawerSmState.RELEASE_HANDLE\\n            sm_wait_time[tid] = 0.0\\n    # increment wait time\\n    sm_wait_time[tid] = sm_wait_time[tid] + dt[tid]'),\n",
       " Document(metadata={}, page_content='class OpenDrawerSm:\\n    \"\"\"A simple state machine in a robot\\'s task space to open a drawer in the cabinet.\\n\\n    The state machine is implemented as a warp kernel. It takes in the current state of\\n    the robot\\'s end-effector and the object, and outputs the desired state of the robot\\'s\\n    end-effector and the gripper. The state machine is implemented as a finite state\\n    machine with the following states:\\n\\n    1. REST: The robot is at rest.\\n    2. APPROACH_HANDLE: The robot moves towards the handle of the drawer.\\n    3. GRASP_HANDLE: The robot grasps the handle of the drawer.\\n    4. OPEN_DRAWER: The robot opens the drawer.\\n    5. RELEASE_HANDLE: The robot releases the handle of the drawer. This is the final state.\\n    \"\"\"\\n\\n    def __init__(self, dt: float, num_envs: int, device: torch.device | str = \"cpu\", position_threshold=0.01):\\n        \"\"\"Initialize the state machine.\\n\\n        Args:\\n            dt: The environment time step.\\n            num_envs: The number of environments to simulate.\\n            device: The device to run the state machine on.\\n        \"\"\"\\n        # save parameters\\n        self.dt = float(dt)\\n        self.num_envs = num_envs\\n        self.device = device\\n        self.position_threshold = position_threshold\\n        # initialize state machine\\n        self.sm_dt = torch.full((self.num_envs,), self.dt, device=self.device)\\n        self.sm_state = torch.full((self.num_envs,), 0, dtype=torch.int32, device=self.device)\\n        self.sm_wait_time = torch.zeros((self.num_envs,), device=self.device)\\n\\n        # desired state\\n        self.des_ee_pose = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.des_gripper_state = torch.full((self.num_envs,), 0.0, device=self.device)\\n\\n        # approach infront of the handle\\n        self.handle_approach_offset = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.handle_approach_offset[:, 0] = -0.1\\n        self.handle_approach_offset[:, -1] = 1.0  # warp expects quaternion as (x, y, z, w)\\n\\n        # handle grasp offset\\n        self.handle_grasp_offset = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.handle_grasp_offset[:, 0] = 0.025\\n        self.handle_grasp_offset[:, -1] = 1.0  # warp expects quaternion as (x, y, z, w)\\n\\n        # drawer opening rate\\n        self.drawer_opening_rate = torch.zeros((self.num_envs, 7), device=self.device)\\n        self.drawer_opening_rate[:, 0] = -0.015\\n        self.drawer_opening_rate[:, -1] = 1.0  # warp expects quaternion as (x, y, z, w)\\n\\n        # convert to warp\\n        self.sm_dt_wp = wp.from_torch(self.sm_dt, wp.float32)\\n        self.sm_state_wp = wp.from_torch(self.sm_state, wp.int32)\\n        self.sm_wait_time_wp = wp.from_torch(self.sm_wait_time, wp.float32)\\n        self.des_ee_pose_wp = wp.from_torch(self.des_ee_pose, wp.transform)\\n        self.des_gripper_state_wp = wp.from_torch(self.des_gripper_state, wp.float32)\\n        self.handle_approach_offset_wp = wp.from_torch(self.handle_approach_offset, wp.transform)\\n        self.handle_grasp_offset_wp = wp.from_torch(self.handle_grasp_offset, wp.transform)\\n        self.drawer_opening_rate_wp = wp.from_torch(self.drawer_opening_rate, wp.transform)\\n\\n    def reset_idx(self, env_ids: Sequence[int] | None = None):\\n        \"\"\"Reset the state machine.\"\"\"\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset state machine\\n        self.sm_state[env_ids] = 0\\n        self.sm_wait_time[env_ids] = 0.0\\n\\n    def compute(self, ee_pose: torch.Tensor, handle_pose: torch.Tensor):\\n        \"\"\"Compute the desired state of the robot\\'s end-effector and the gripper.\"\"\"\\n        # convert all transformations from (w, x, y, z) to (x, y, z, w)\\n        ee_pose = ee_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n        handle_pose = handle_pose[:, [0, 1, 2, 4, 5, 6, 3]]\\n        # convert to warp\\n        ee_pose_wp = wp.from_torch(ee_pose.contiguous(), wp.transform)\\n        handle_pose_wp = wp.from_torch(handle_pose.contiguous(), wp.transform)\\n\\n        # run state machine\\n        wp.launch(\\n            kernel=infer_state_machine,\\n            dim=self.num_envs,\\n            inputs=[\\n                self.sm_dt_wp,\\n                self.sm_state_wp,\\n                self.sm_wait_time_wp,\\n                ee_pose_wp,\\n                handle_pose_wp,\\n                self.des_ee_pose_wp,\\n                self.des_gripper_state_wp,\\n                self.handle_approach_offset_wp,\\n                self.handle_grasp_offset_wp,\\n                self.drawer_opening_rate_wp,\\n                self.position_threshold,\\n            ],\\n            device=self.device,\\n        )\\n\\n        # convert transformations back to (w, x, y, z)\\n        des_ee_pose = self.des_ee_pose[:, [0, 1, 2, 6, 3, 4, 5]]\\n        # convert to torch\\n        return torch.cat([des_ee_pose, self.des_gripper_state.unsqueeze(-1)], dim=-1)'),\n",
       " Document(metadata={}, page_content='def main():\\n    # parse configuration\\n    env_cfg: CabinetEnvCfg = parse_env_cfg(\\n        \"Isaac-Open-Drawer-Franka-IK-Abs-v0\",\\n        device=args_cli.device,\\n        num_envs=args_cli.num_envs,\\n        use_fabric=not args_cli.disable_fabric,\\n    )\\n    # create environment\\n    env = gym.make(\"Isaac-Open-Drawer-Franka-IK-Abs-v0\", cfg=env_cfg)\\n    # reset environment at start\\n    env.reset()\\n\\n    # create action buffers (position + quaternion)\\n    actions = torch.zeros(env.unwrapped.action_space.shape, device=env.unwrapped.device)\\n    actions[:, 3] = 1.0\\n    # desired object orientation (we only do position control of object)\\n    desired_orientation = torch.zeros((env.unwrapped.num_envs, 4), device=env.unwrapped.device)\\n    desired_orientation[:, 1] = 1.0\\n    # create state machine\\n    open_sm = OpenDrawerSm(env_cfg.sim.dt * env_cfg.decimation, env.unwrapped.num_envs, env.unwrapped.device)\\n\\n    while simulation_app.is_running():\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # step environment\\n            dones = env.step(actions)[-2]\\n\\n            # observations\\n            # -- end-effector frame\\n            ee_frame_tf: FrameTransformer = env.unwrapped.scene[\"ee_frame\"]\\n            tcp_rest_position = ee_frame_tf.data.target_pos_w[..., 0, :].clone() - env.unwrapped.scene.env_origins\\n            tcp_rest_orientation = ee_frame_tf.data.target_quat_w[..., 0, :].clone()\\n            # -- handle frame\\n            cabinet_frame_tf: FrameTransformer = env.unwrapped.scene[\"cabinet_frame\"]\\n            cabinet_position = cabinet_frame_tf.data.target_pos_w[..., 0, :].clone() - env.unwrapped.scene.env_origins\\n            cabinet_orientation = cabinet_frame_tf.data.target_quat_w[..., 0, :].clone()\\n\\n            # advance state machine\\n            actions = open_sm.compute(\\n                torch.cat([tcp_rest_position, tcp_rest_orientation], dim=-1),\\n                torch.cat([cabinet_position, cabinet_orientation], dim=-1),\\n            )\\n\\n            # reset state machine\\n            if dones.any():\\n                open_sm.reset_idx(dones.nonzero(as_tuple=False).squeeze(-1))\\n\\n    # close the environment\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def pre_process_actions(\\n    teleop_data: tuple[np.ndarray, bool] | list[tuple[np.ndarray, np.ndarray, np.ndarray]], num_envs: int, device: str\\n) -> torch.Tensor:\\n    \"\"\"Convert teleop data to the format expected by the environment action space.\\n\\n    Args:\\n        teleop_data: Data from the teleoperation device.\\n        num_envs: Number of environments.\\n        device: Device to create tensors on.\\n\\n    Returns:\\n        Processed actions as a tensor.\\n    \"\"\"\\n    # compute actions based on environment\\n    if \"Reach\" in args_cli.task:\\n        delta_pose, gripper_command = teleop_data\\n        # convert to torch\\n        delta_pose = torch.tensor(delta_pose, dtype=torch.float, device=device).repeat(num_envs, 1)\\n        # note: reach is the only one that uses a different action space\\n        # compute actions\\n        return delta_pose\\n    elif \"PickPlace-GR1T2\" in args_cli.task:\\n        (left_wrist_pose, right_wrist_pose, hand_joints) = teleop_data[0]\\n        # Reconstruct actions_arms tensor with converted positions and rotations\\n        actions = torch.tensor(\\n            np.concatenate([\\n                left_wrist_pose,  # left ee pose\\n                right_wrist_pose,  # right ee pose\\n                hand_joints,  # hand joint angles\\n            ]),\\n            device=device,\\n            dtype=torch.float32,\\n        ).unsqueeze(0)\\n        # Concatenate arm poses and hand joint angles\\n        return actions\\n    else:\\n        # resolve gripper command\\n        delta_pose, gripper_command = teleop_data\\n        # convert to torch\\n        delta_pose = torch.tensor(delta_pose, dtype=torch.float, device=device).repeat(num_envs, 1)\\n        gripper_vel = torch.zeros((delta_pose.shape[0], 1), dtype=torch.float, device=device)\\n        gripper_vel[:] = -1 if gripper_command else 1\\n        # compute actions\\n        return torch.concat([delta_pose, gripper_vel], dim=1)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Running keyboard teleoperation with Isaac Lab manipulation environment.\"\"\"\\n    # parse configuration\\n    env_cfg = parse_env_cfg(args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs)\\n    env_cfg.env_name = args_cli.task\\n    # modify configuration\\n    env_cfg.terminations.time_out = None\\n    if \"Lift\" in args_cli.task:\\n        # set the resampling time range to large number to avoid resampling\\n        env_cfg.commands.object_pose.resampling_time_range = (1.0e9, 1.0e9)\\n        # add termination condition for reaching the goal otherwise the environment won\\'t reset\\n        env_cfg.terminations.object_reached_goal = DoneTerm(func=mdp.object_reached_goal)\\n    # create environment\\n    env = gym.make(args_cli.task, cfg=env_cfg).unwrapped\\n    # check environment name (for reach , we don\\'t allow the gripper)\\n    if \"Reach\" in args_cli.task:\\n        omni.log.warn(\\n            f\"The environment \\'{args_cli.task}\\' does not support gripper control. The device command will be ignored.\"\\n        )\\n\\n    # Flags for controlling teleoperation flow\\n    should_reset_recording_instance = False\\n    teleoperation_active = True\\n\\n    # Callback handlers\\n    def reset_recording_instance():\\n        \"\"\"Reset the environment to its initial state.\\n\\n        This callback is triggered when the user presses the reset key (typically \\'R\\').\\n        It\\'s useful when:\\n        - The robot gets into an undesirable configuration\\n        - The user wants to start over with the task\\n        - Objects in the scene need to be reset to their initial positions\\n\\n        The environment will be reset on the next simulation step.\\n        \"\"\"\\n        nonlocal should_reset_recording_instance\\n        should_reset_recording_instance = True\\n\\n    def start_teleoperation():\\n        \"\"\"Activate teleoperation control of the robot.\\n\\n        This callback enables active control of the robot through the input device.\\n        It\\'s typically triggered by a specific gesture or button press and is used when:\\n        - Beginning a new teleoperation session\\n        - Resuming control after temporarily pausing\\n        - Switching from observation mode to control mode\\n\\n        While active, all commands from the device will be applied to the robot.\\n        \"\"\"\\n        nonlocal teleoperation_active\\n        teleoperation_active = True\\n\\n    def stop_teleoperation():\\n        \"\"\"Deactivate teleoperation control of the robot.\\n\\n        This callback temporarily suspends control of the robot through the input device.\\n        It\\'s typically triggered by a specific gesture or button press and is used when:\\n        - Taking a break from controlling the robot\\n        - Repositioning the input device without moving the robot\\n        - Pausing to observe the scene without interference\\n\\n        While inactive, the simulation continues to render but device commands are ignored.\\n        \"\"\"\\n        nonlocal teleoperation_active\\n        teleoperation_active = False\\n\\n    # create controller\\n    if args_cli.teleop_device.lower() == \"keyboard\":\\n        teleop_interface = Se3Keyboard(\\n            pos_sensitivity=0.05 * args_cli.sensitivity, rot_sensitivity=0.05 * args_cli.sensitivity\\n        )\\n    elif args_cli.teleop_device.lower() == \"spacemouse\":\\n        teleop_interface = Se3SpaceMouse(\\n            pos_sensitivity=0.05 * args_cli.sensitivity, rot_sensitivity=0.05 * args_cli.sensitivity\\n        )\\n    elif args_cli.teleop_device.lower() == \"gamepad\":\\n        teleop_interface = Se3Gamepad(\\n            pos_sensitivity=0.1 * args_cli.sensitivity, rot_sensitivity=0.1 * args_cli.sensitivity\\n        )\\n    elif \"dualhandtracking_abs\" in args_cli.teleop_device.lower() and \"GR1T2\" in args_cli.task:\\n        # Create GR1T2 retargeter with desired configuration\\n        gr1t2_retargeter = GR1T2Retargeter(\\n            enable_visualization=True,\\n            num_open_xr_hand_joints=2 * (int(OpenXRSpec.HandJointEXT.XR_HAND_JOINT_LITTLE_TIP_EXT) + 1),\\n            device=env.unwrapped.device,\\n            hand_joint_names=env.scene[\"robot\"].data.joint_names[-22:],\\n        )\\n\\n        # Create hand tracking device with retargeter\\n        teleop_interface = OpenXRDevice(\\n            env_cfg.xr,\\n            retargeters=[gr1t2_retargeter],\\n        )\\n        teleop_interface.add_callback(\"RESET\", reset_recording_instance)\\n        teleop_interface.add_callback(\"START\", start_teleoperation)\\n        teleop_interface.add_callback(\"STOP\", stop_teleoperation)\\n\\n        # Hand tracking needs explicit start gesture to activate\\n        teleoperation_active = False\\n\\n    elif \"handtracking\" in args_cli.teleop_device.lower():\\n        # Create EE retargeter with desired configuration\\n        if \"_abs\" in args_cli.teleop_device.lower():\\n            retargeter_device = Se3AbsRetargeter(\\n                bound_hand=OpenXRDevice.TrackingTarget.HAND_RIGHT, zero_out_xy_rotation=True\\n            )\\n        else:\\n            retargeter_device = Se3RelRetargeter(\\n                bound_hand=OpenXRDevice.TrackingTarget.HAND_RIGHT, zero_out_xy_rotation=True\\n            )\\n\\n        grip_retargeter = GripperRetargeter(bound_hand=OpenXRDevice.TrackingTarget.HAND_RIGHT)\\n\\n        # Create hand tracking device with retargeter (in a list)\\n        teleop_interface = OpenXRDevice(\\n            env_cfg.xr,\\n            retargeters=[retargeter_device, grip_retargeter],\\n        )\\n        teleop_interface.add_callback(\"RESET\", reset_recording_instance)\\n        teleop_interface.add_callback(\"START\", start_teleoperation)\\n        teleop_interface.add_callback(\"STOP\", stop_teleoperation)\\n\\n        # Hand tracking needs explicit start gesture to activate\\n        teleoperation_active = False\\n    else:\\n        raise ValueError(\\n            f\"Invalid device interface \\'{args_cli.teleop_device}\\'. Supported: \\'keyboard\\', \\'spacemouse\\', \\'gamepad\\',\"\\n            \" \\'handtracking\\', \\'handtracking_abs\\'.\"\\n        )\\n\\n    # add teleoperation key for env reset (for all devices)\\n    teleop_interface.add_callback(\"R\", reset_recording_instance)\\n    print(teleop_interface)\\n\\n    # reset environment\\n    env.reset()\\n    teleop_interface.reset()\\n\\n    # simulate environment\\n    while simulation_app.is_running():\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # get device command\\n            teleop_data = teleop_interface.advance()\\n\\n            # Only apply teleop commands when active\\n            if teleoperation_active:\\n                # compute actions based on environment\\n                actions = pre_process_actions(teleop_data, env.num_envs, env.device)\\n                # apply actions\\n                env.step(actions)\\n            else:\\n                env.sim.render()\\n\\n            if should_reset_recording_instance:\\n                env.reset()\\n                should_reset_recording_instance = False\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def play_cb():\\n    global is_paused\\n    is_paused = False'),\n",
       " Document(metadata={}, page_content='def pause_cb():\\n    global is_paused\\n    is_paused = True'),\n",
       " Document(metadata={}, page_content='def skip_episode_cb():\\n    global skip_episode\\n    skip_episode = True'),\n",
       " Document(metadata={}, page_content='def mark_subtask_cb():\\n    global current_action_index, marked_subtask_action_indices\\n    marked_subtask_action_indices.append(current_action_index)\\n    print(f\"Marked a subtask signal at action index: {current_action_index}\")'),\n",
       " Document(metadata={}, page_content='class PreStepDatagenInfoRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the datagen info data in each step.\"\"\"\\n\\n    def record_pre_step(self):\\n        eef_pose_dict = {}\\n        for eef_name in self._env.cfg.subtask_configs.keys():\\n            eef_pose_dict[eef_name] = self._env.get_robot_eef_pose(eef_name=eef_name)\\n\\n        datagen_info = {\\n            \"object_pose\": self._env.get_object_poses(),\\n            \"eef_pose\": eef_pose_dict,\\n            \"target_eef_pose\": self._env.action_to_target_eef_pose(self._env.action_manager.action),\\n        }\\n        return \"obs/datagen_info\", datagen_info'),\n",
       " Document(metadata={}, page_content='class PreStepDatagenInfoRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the datagen info recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = PreStepDatagenInfoRecorder'),\n",
       " Document(metadata={}, page_content='class PreStepSubtaskTermsObservationsRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the subtask completion observations in each step.\"\"\"\\n\\n    def record_pre_step(self):\\n        return \"obs/datagen_info/subtask_term_signals\", self._env.get_subtask_term_signals()'),\n",
       " Document(metadata={}, page_content='class PreStepSubtaskTermsObservationsRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the step subtask terms observation recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = PreStepSubtaskTermsObservationsRecorder'),\n",
       " Document(metadata={}, page_content='class MimicRecorderManagerCfg(ActionStateRecorderManagerCfg):\\n    \"\"\"Mimic specific recorder terms.\"\"\"\\n\\n    record_pre_step_datagen_info = PreStepDatagenInfoRecorderCfg()\\n    record_pre_step_subtask_term_signals = PreStepSubtaskTermsObservationsRecorderCfg()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Add Isaac Lab Mimic annotations to the given demo dataset file.\"\"\"\\n    global is_paused, current_action_index, marked_subtask_action_indices\\n\\n    # Load input dataset to be annotated\\n    if not os.path.exists(args_cli.input_file):\\n        raise FileNotFoundError(f\"The input dataset file {args_cli.input_file} does not exist.\")\\n    dataset_file_handler = HDF5DatasetFileHandler()\\n    dataset_file_handler.open(args_cli.input_file)\\n    env_name = dataset_file_handler.get_env_name()\\n    episode_count = dataset_file_handler.get_num_episodes()\\n\\n    if episode_count == 0:\\n        print(\"No episodes found in the dataset.\")\\n        exit()\\n\\n    # get output directory path and file name (without extension) from cli arguments\\n    output_dir = os.path.dirname(args_cli.output_file)\\n    output_file_name = os.path.splitext(os.path.basename(args_cli.output_file))[0]\\n    # create output directory if it does not exist\\n    if not os.path.exists(output_dir):\\n        os.makedirs(output_dir)\\n\\n    if args_cli.task is not None:\\n        env_name = args_cli.task\\n    if env_name is None:\\n        raise ValueError(\"Task/env name was not specified nor found in the dataset.\")\\n\\n    env_cfg = parse_env_cfg(env_name, device=args_cli.device, num_envs=1)\\n\\n    env_cfg.env_name = args_cli.task\\n\\n    # extract success checking function to invoke manually\\n    success_term = None\\n    if hasattr(env_cfg.terminations, \"success\"):\\n        success_term = env_cfg.terminations.success\\n        env_cfg.terminations.success = None\\n    else:\\n        raise NotImplementedError(\"No success termination term was found in the environment.\")\\n\\n    # Disable all termination terms\\n    env_cfg.terminations = None\\n\\n    # Set up recorder terms for mimic annotations\\n    env_cfg.recorders: MimicRecorderManagerCfg = MimicRecorderManagerCfg()\\n    if not args_cli.auto:\\n        # disable subtask term signals recorder term if in manual mode\\n        env_cfg.recorders.record_pre_step_subtask_term_signals = None\\n\\n    env_cfg.recorders.dataset_export_dir_path = output_dir\\n    env_cfg.recorders.dataset_filename = output_file_name\\n\\n    # create environment from loaded config\\n    env: ManagerBasedRLMimicEnv = gym.make(args_cli.task, cfg=env_cfg).unwrapped\\n\\n    if not isinstance(env, ManagerBasedRLMimicEnv):\\n        raise ValueError(\"The environment should be derived from ManagerBasedRLMimicEnv\")\\n\\n    if args_cli.auto:\\n        # check if the mimic API env.get_subtask_term_signals() is implemented\\n        if env.get_subtask_term_signals.__func__ is ManagerBasedRLMimicEnv.get_subtask_term_signals:\\n            raise NotImplementedError(\\n                \"The environment does not implement the get_subtask_term_signals method required \"\\n                \"to run automatic annotations.\"\\n            )\\n    else:\\n        # get subtask termination signal names for each eef from the environment configs\\n        subtask_term_signal_names = {}\\n        for eef_name, eef_subtask_configs in env.cfg.subtask_configs.items():\\n            subtask_term_signal_names[eef_name] = [\\n                subtask_config.subtask_term_signal for subtask_config in eef_subtask_configs\\n            ]\\n            # no need to annotate the last subtask term signal, so remove it from the list\\n            subtask_term_signal_names[eef_name].pop()\\n\\n    # reset environment\\n    env.reset()\\n\\n    # Only enables inputs if this script is NOT headless mode\\n    if not args_cli.headless and not os.environ.get(\"HEADLESS\", 0):\\n        keyboard_interface = Se3Keyboard(pos_sensitivity=0.1, rot_sensitivity=0.1)\\n        keyboard_interface.add_callback(\"N\", play_cb)\\n        keyboard_interface.add_callback(\"B\", pause_cb)\\n        keyboard_interface.add_callback(\"Q\", skip_episode_cb)\\n        if not args_cli.auto:\\n            keyboard_interface.add_callback(\"S\", mark_subtask_cb)\\n        keyboard_interface.reset()\\n\\n    # simulate environment -- run everything in inference mode\\n    exported_episode_count = 0\\n    processed_episode_count = 0\\n    with contextlib.suppress(KeyboardInterrupt) and torch.inference_mode():\\n        while simulation_app.is_running() and not simulation_app.is_exiting():\\n            # Iterate over the episodes in the loaded dataset file\\n            for episode_index, episode_name in enumerate(dataset_file_handler.get_episode_names()):\\n                processed_episode_count += 1\\n                print(f\"\\\\nAnnotating episode #{episode_index} ({episode_name})\")\\n                episode = dataset_file_handler.load_episode(episode_name, env.device)\\n\\n                is_episode_annotated_successfully = False\\n                if args_cli.auto:\\n                    is_episode_annotated_successfully = annotate_episode_in_auto_mode(env, episode, success_term)\\n                else:\\n                    is_episode_annotated_successfully = annotate_episode_in_manual_mode(\\n                        env, episode, success_term, subtask_term_signal_names\\n                    )\\n\\n                if is_episode_annotated_successfully and not skip_episode:\\n                    # set success to the recorded episode data and export to file\\n                    env.recorder_manager.set_success_to_episodes(\\n                        None, torch.tensor([[True]], dtype=torch.bool, device=env.device)\\n                    )\\n                    env.recorder_manager.export_episodes()\\n                    exported_episode_count += 1\\n                    print(\"\\\\tExported the annotated episode.\")\\n                else:\\n                    print(\"\\\\tSkipped exporting the episode due to incomplete subtask annotations.\")\\n            break\\n\\n    print(\\n        f\"\\\\nExported {exported_episode_count} (out of {processed_episode_count}) annotated\"\\n        f\" episode{\\'s\\' if exported_episode_count > 1 else \\'\\'}.\"\\n    )\\n    print(\"Exiting the app.\")\\n\\n    # Close environment after annotation is complete\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def replay_episode(\\n    env: ManagerBasedRLMimicEnv,\\n    episode: EpisodeData,\\n    success_term: TerminationTermCfg | None = None,\\n) -> bool:\\n    \"\"\"Replays an episode in the environment.\\n\\n    This function replays the given recorded episode in the environment. It can optionally check if the task\\n    was successfully completed using a success termination condition input.\\n\\n    Args:\\n        env: The environment to replay the episode in.\\n        episode: The recorded episode data to replay.\\n        success_term: Optional termination term to check for task success.\\n\\n    Returns:\\n        True if the episode was successfully replayed and the success condition was met (if provided),\\n        False otherwise.\\n    \"\"\"\\n    global current_action_index, skip_episode, is_paused\\n    # read initial state and actions from the loaded episode\\n    initial_state = episode.data[\"initial_state\"]\\n    actions = episode.data[\"actions\"]\\n    env.sim.reset()\\n    env.recorder_manager.reset()\\n    env.reset_to(initial_state, None, is_relative=True)\\n    first_action = True\\n    for action_index, action in enumerate(actions):\\n        current_action_index = action_index\\n        if first_action:\\n            first_action = False\\n        else:\\n            while is_paused or skip_episode:\\n                env.sim.render()\\n                if skip_episode:\\n                    return False\\n                continue\\n        action_tensor = torch.Tensor(action).reshape([1, action.shape[0]])\\n        env.step(torch.Tensor(action_tensor))\\n    if success_term is not None:\\n        if not bool(success_term.func(env, **success_term.params)[0]):\\n            return False\\n    return True'),\n",
       " Document(metadata={}, page_content='def annotate_episode_in_auto_mode(\\n    env: ManagerBasedRLMimicEnv,\\n    episode: EpisodeData,\\n    success_term: TerminationTermCfg | None = None,\\n) -> bool:\\n    \"\"\"Annotates an episode in automatic mode.\\n\\n    This function replays the given episode in the environment and checks if the task was successfully completed.\\n    If the task was not completed, it will print a message and return False. Otherwise, it will check if all the\\n    subtask term signals are annotated and return True if they are, False otherwise.\\n\\n    Args:\\n        env: The environment to replay the episode in.\\n        episode: The recorded episode data to replay.\\n        success_term: Optional termination term to check for task success.\\n\\n    Returns:\\n        True if the episode was successfully annotated, False otherwise.\\n    \"\"\"\\n    global skip_episode\\n    skip_episode = False\\n    is_episode_annotated_successfully = replay_episode(env, episode, success_term)\\n    if skip_episode:\\n        print(\"\\\\tSkipping the episode.\")\\n        return False\\n    if not is_episode_annotated_successfully:\\n        print(\"\\\\tThe final task was not completed.\")\\n    else:\\n        # check if all the subtask term signals are annotated\\n        annotated_episode = env.recorder_manager.get_episode(0)\\n        subtask_term_signal_dict = annotated_episode.data[\"obs\"][\"datagen_info\"][\"subtask_term_signals\"]\\n        for signal_name, signal_flags in subtask_term_signal_dict.items():\\n            if not torch.any(signal_flags):\\n                is_episode_annotated_successfully = False\\n                print(f\\'\\\\tDid not detect completion for the subtask \"{signal_name}\".\\')\\n    return is_episode_annotated_successfully'),\n",
       " Document(metadata={}, page_content='def annotate_episode_in_manual_mode(\\n    env: ManagerBasedRLMimicEnv,\\n    episode: EpisodeData,\\n    success_term: TerminationTermCfg | None = None,\\n    subtask_term_signal_names: dict[str, list[str]] = {},\\n) -> bool:\\n    \"\"\"Annotates an episode in manual mode.\\n\\n    This function replays the given episode in the environment and allows for manual marking of subtask term signals.\\n    It iterates over each eef and prompts the user to mark the subtask term signals for that eef.\\n\\n    Args:\\n        env: The environment to replay the episode in.\\n        episode: The recorded episode data to replay.\\n        success_term: Optional termination term to check for task success.\\n        subtask_term_signal_names: Dictionary mapping eef names to lists of subtask term signal names.\\n\\n    Returns:\\n        True if the episode was successfully annotated, False otherwise.\\n    \"\"\"\\n    global is_paused, marked_subtask_action_indices, skip_episode\\n    # iterate over the eefs for marking subtask term signals\\n    subtask_term_signal_action_indices = {}\\n    for eef_name, eef_subtask_term_signal_names in subtask_term_signal_names.items():\\n        # skip if no subtask annotation is needed for this eef\\n        if len(eef_subtask_term_signal_names) == 0:\\n            continue\\n\\n        while True:\\n            is_paused = True\\n            skip_episode = False\\n            print(f\\'\\\\tPlaying the episode for subtask annotations for eef \"{eef_name}\".\\')\\n            print(\"\\\\tSubtask signals to annotate:\")\\n            print(f\"\\\\t\\\\t- Termination:\\\\t{eef_subtask_term_signal_names}\")\\n\\n            print(\\'\\\\n\\\\tPress \"N\" to begin.\\')\\n            print(\\'\\\\tPress \"B\" to pause.\\')\\n            print(\\'\\\\tPress \"S\" to annotate subtask signals.\\')\\n            print(\\'\\\\tPress \"Q\" to skip the episode.\\\\n\\')\\n            marked_subtask_action_indices = []\\n            task_success_result = replay_episode(env, episode, success_term)\\n            if skip_episode:\\n                print(\"\\\\tSkipping the episode.\")\\n                return False\\n\\n            print(f\"\\\\tSubtasks marked at action indices: {marked_subtask_action_indices}\")\\n            expected_subtask_signal_count = len(eef_subtask_term_signal_names)\\n            if task_success_result and expected_subtask_signal_count == len(marked_subtask_action_indices):\\n                print(f\\'\\\\tAll {expected_subtask_signal_count} subtask signals for eef \"{eef_name}\" were annotated.\\')\\n                for marked_signal_index in range(expected_subtask_signal_count):\\n                    # collect subtask term signal action indices\\n                    subtask_term_signal_action_indices[eef_subtask_term_signal_names[marked_signal_index]] = (\\n                        marked_subtask_action_indices[marked_signal_index]\\n                    )\\n                break\\n\\n            if not task_success_result:\\n                print(\"\\\\tThe final task was not completed.\")\\n                return False\\n\\n            if expected_subtask_signal_count != len(marked_subtask_action_indices):\\n                print(\\n                    f\"\\\\tOnly {len(marked_subtask_action_indices)} out of\"\\n                    f\\' {expected_subtask_signal_count} subtask signals for eef \"{eef_name}\" were\\'\\n                    \" annotated.\"\\n                )\\n\\n            print(f\\'\\\\tThe episode will be replayed again for re-marking subtask signals for the eef \"{eef_name}\".\\\\n\\')\\n\\n    annotated_episode = env.recorder_manager.get_episode(0)\\n    for (\\n        subtask_term_signal_name,\\n        subtask_term_signal_action_index,\\n    ) in subtask_term_signal_action_indices.items():\\n        # subtask termination signal is false until subtask is complete, and true afterwards\\n        subtask_signals = torch.ones(len(episode.data[\"actions\"]), dtype=torch.bool)\\n        subtask_signals[:subtask_term_signal_action_index] = False\\n        annotated_episode.add(f\"obs/datagen_info/subtask_term_signals/{subtask_term_signal_name}\", subtask_signals)\\n    return True'),\n",
       " Document(metadata={}, page_content='class PreStepDatagenInfoRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the datagen info data in each step.\"\"\"\\n\\n    def record_pre_step(self):\\n        eef_pose_dict = {}\\n        for eef_name in self._env.cfg.subtask_configs.keys():\\n            eef_pose_dict[eef_name] = self._env.get_robot_eef_pose(eef_name)\\n\\n        datagen_info = {\\n            \"object_pose\": self._env.get_object_poses(),\\n            \"eef_pose\": eef_pose_dict,\\n            \"target_eef_pose\": self._env.action_to_target_eef_pose(self._env.action_manager.action),\\n        }\\n        return \"obs/datagen_info\", datagen_info'),\n",
       " Document(metadata={}, page_content='class PreStepDatagenInfoRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the datagen info recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = PreStepDatagenInfoRecorder'),\n",
       " Document(metadata={}, page_content='class PreStepSubtaskTermsObservationsRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the subtask completion observations in each step.\"\"\"\\n\\n    def record_pre_step(self):\\n        return \"obs/datagen_info/subtask_term_signals\", self._env.get_subtask_term_signals()'),\n",
       " Document(metadata={}, page_content='class PreStepSubtaskTermsObservationsRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the step subtask terms observation recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = PreStepSubtaskTermsObservationsRecorder'),\n",
       " Document(metadata={}, page_content='class MimicRecorderManagerCfg(ActionStateRecorderManagerCfg):\\n    \"\"\"Mimic specific recorder terms.\"\"\"\\n\\n    record_pre_step_datagen_info = PreStepDatagenInfoRecorderCfg()\\n    record_pre_step_subtask_term_signals = PreStepSubtaskTermsObservationsRecorderCfg()'),\n",
       " Document(metadata={}, page_content='class RateLimiter:\\n    \"\"\"Convenience class for enforcing rates in loops.\"\"\"\\n\\n    def __init__(self, hz):\\n        \"\"\"\\n        Args:\\n            hz (int): frequency to enforce\\n        \"\"\"\\n        self.hz = hz\\n        self.last_time = time.time()\\n        self.sleep_duration = 1.0 / hz\\n        self.render_period = min(0.033, self.sleep_duration)\\n\\n    def sleep(self, env):\\n        \"\"\"Attempt to sleep at the specified rate in hz.\"\"\"\\n        next_wakeup_time = self.last_time + self.sleep_duration\\n        while time.time() < next_wakeup_time:\\n            time.sleep(self.render_period)\\n            env.unwrapped.sim.render()\\n\\n        self.last_time = self.last_time + self.sleep_duration\\n\\n        # detect time jumping forwards (e.g. loop is too slow)\\n        if self.last_time < time.time():\\n            while self.last_time < time.time():\\n                self.last_time += self.sleep_duration'),\n",
       " Document(metadata={}, page_content='def pre_process_actions(delta_pose: torch.Tensor, gripper_command: bool) -> torch.Tensor:\\n    \"\"\"Pre-process actions for the environment.\"\"\"\\n    # compute actions based on environment\\n    if \"Reach\" in args_cli.task:\\n        # note: reach is the only one that uses a different action space\\n        # compute actions\\n        return delta_pose\\n    else:\\n        # resolve gripper command\\n        gripper_vel = torch.zeros((delta_pose.shape[0], 1), dtype=torch.float, device=delta_pose.device)\\n        gripper_vel[:] = -1 if gripper_command else 1\\n        # compute actions\\n        return torch.concat([delta_pose, gripper_vel], dim=1)'),\n",
       " Document(metadata={}, page_content='async def run_teleop_robot(\\n    env, env_id, env_action_queue, shared_datagen_info_pool, success_term, exported_dataset_path, teleop_interface=None\\n):\\n    \"\"\"Run teleop robot.\"\"\"\\n    global num_recorded\\n    should_reset_teleop_instance = False\\n    # create controller if needed\\n    if teleop_interface is None:\\n        if args_cli.teleop_device.lower() == \"keyboard\":\\n            teleop_interface = Se3Keyboard(pos_sensitivity=0.2, rot_sensitivity=0.5)\\n        elif args_cli.teleop_device.lower() == \"spacemouse\":\\n            teleop_interface = Se3SpaceMouse(pos_sensitivity=0.2, rot_sensitivity=0.5)\\n        else:\\n            raise ValueError(\\n                f\"Invalid device interface \\'{args_cli.teleop_device}\\'. Supported: \\'keyboard\\', \\'spacemouse\\'.\"\\n            )\\n\\n    # add teleoperation key for reset current recording instance\\n    def reset_teleop_instance():\\n        nonlocal should_reset_teleop_instance\\n        should_reset_teleop_instance = True\\n\\n    teleop_interface.add_callback(\"R\", reset_teleop_instance)\\n\\n    teleop_interface.reset()\\n    print(teleop_interface)\\n\\n    recorded_episode_dataset_file_handler = HDF5DatasetFileHandler()\\n    recorded_episode_dataset_file_handler.create(exported_dataset_path, env_name=env.unwrapped.cfg.env_name)\\n\\n    env_id_tensor = torch.tensor([env_id], dtype=torch.int64, device=env.device)\\n    success_step_count = 0\\n    num_recorded = 0\\n    while True:\\n        if should_reset_teleop_instance:\\n            env.unwrapped.recorder_manager.reset(env_id_tensor)\\n            env.unwrapped.reset(env_ids=env_id_tensor)\\n            should_reset_teleop_instance = False\\n            success_step_count = 0\\n\\n        # get keyboard command\\n        delta_pose, gripper_command = teleop_interface.advance()\\n        # convert to torch\\n        delta_pose = torch.tensor(delta_pose, dtype=torch.float, device=env.device).repeat(1, 1)\\n        # compute actions based on environment\\n        teleop_action = pre_process_actions(delta_pose, gripper_command)\\n\\n        await env_action_queue.put((env_id, teleop_action))\\n        await env_action_queue.join()\\n\\n        if success_term is not None:\\n            if bool(success_term.func(env, **success_term.params)[env_id]):\\n                success_step_count += 1\\n                if success_step_count >= args_cli.num_success_steps:\\n                    env.recorder_manager.set_success_to_episodes(\\n                        env_id_tensor, torch.tensor([[True]], dtype=torch.bool, device=env.device)\\n                    )\\n                    teleop_episode = env.unwrapped.recorder_manager.get_episode(env_id)\\n                    await shared_datagen_info_pool.add_episode(teleop_episode)\\n\\n                    recorded_episode_dataset_file_handler.write_episode(teleop_episode)\\n                    recorded_episode_dataset_file_handler.flush()\\n                    env.recorder_manager.reset(env_id_tensor)\\n                    num_recorded += 1\\n                    should_reset_teleop_instance = True\\n            else:\\n                success_step_count = 0'),\n",
       " Document(metadata={}, page_content='async def run_data_generator(\\n    env, env_id, env_action_queue, shared_datagen_info_pool, success_term, pause_subtask=False, export_demo=True\\n):\\n    \"\"\"Run data generator.\"\"\"\\n    global num_success, num_failures, num_attempts\\n    data_generator = DataGenerator(env=env.unwrapped, src_demo_datagen_info_pool=shared_datagen_info_pool)\\n    idle_action = torch.zeros(env.unwrapped.action_space.shape)[0]\\n    while True:\\n        while data_generator.src_demo_datagen_info_pool.num_datagen_infos < 1:\\n            await env_action_queue.put((env_id, idle_action))\\n            await env_action_queue.join()\\n\\n        results = await data_generator.generate(\\n            env_id=env_id,\\n            success_term=success_term,\\n            env_action_queue=env_action_queue,\\n            select_src_per_subtask=env.unwrapped.cfg.datagen_config.generation_select_src_per_subtask,\\n            transform_first_robot_pose=env.unwrapped.cfg.datagen_config.generation_transform_first_robot_pose,\\n            interpolate_from_last_target_pose=env.unwrapped.cfg.datagen_config.generation_interpolate_from_last_target_pose,\\n            pause_subtask=pause_subtask,\\n            export_demo=export_demo,\\n        )\\n        if bool(results[\"success\"]):\\n            num_success += 1\\n        else:\\n            num_failures += 1\\n        num_attempts += 1'),\n",
       " Document(metadata={}, page_content='def env_loop(env, env_action_queue, shared_datagen_info_pool, asyncio_event_loop):\\n    \"\"\"Main loop for the environment.\"\"\"\\n    global num_recorded, num_success, num_failures, num_attempts\\n    prev_num_attempts = 0\\n    prev_num_recorded = 0\\n\\n    rate_limiter = None\\n    if args_cli.step_hz > 0:\\n        rate_limiter = RateLimiter(args_cli.step_hz)\\n\\n    # simulate environment -- run everything in inference mode\\n    is_first_print = True\\n    with contextlib.suppress(KeyboardInterrupt) and torch.inference_mode():\\n        while True:\\n\\n            actions = torch.zeros(env.unwrapped.action_space.shape)\\n\\n            # get actions from all the data generators\\n            for i in range(env.unwrapped.num_envs):\\n                # an async-blocking call to get an action from a data generator\\n                env_id, action = asyncio_event_loop.run_until_complete(env_action_queue.get())\\n                actions[env_id] = action\\n\\n            # perform action on environment\\n            env.step(actions)\\n\\n            # mark done so the data generators can continue with the step results\\n            for i in range(env.unwrapped.num_envs):\\n                env_action_queue.task_done()\\n\\n            if prev_num_attempts != num_attempts or prev_num_recorded != num_recorded:\\n                prev_num_attempts = num_attempts\\n                prev_num_recorded = num_recorded\\n                generated_sucess_rate = 100 * num_success / num_attempts if num_attempts > 0 else 0.0\\n                if is_first_print:\\n                    is_first_print = False\\n                else:\\n                    print(\"\\\\r\", \"\\\\033[F\" * 5, end=\"\")\\n                print(\"\")\\n                print(\"*\" * 50, \"\\\\033[K\")\\n                print(f\"{num_recorded} teleoperated demos recorded\\\\033[K\")\\n                print(\\n                    f\"{num_success}/{num_attempts} ({generated_sucess_rate:.1f}%) successful demos generated by\"\\n                    \" mimic\\\\033[K\"\\n                )\\n                print(\"*\" * 50, \"\\\\033[K\")\\n\\n                if args_cli.num_demos > 0 and num_recorded >= args_cli.num_demos:\\n                    print(f\"All {args_cli.num_demos} demonstrations recorded. Exiting the app.\")\\n                    break\\n\\n            # check that simulation is stopped or not\\n            if env.unwrapped.sim.is_stopped():\\n                break\\n\\n            if rate_limiter:\\n                rate_limiter.sleep(env.unwrapped)\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main():\\n    num_envs = args_cli.num_envs\\n\\n    # create output directory for recorded episodes if it does not exist\\n    recorded_output_dir = os.path.dirname(args_cli.output_file)\\n    if not os.path.exists(recorded_output_dir):\\n        os.makedirs(recorded_output_dir)\\n\\n    # check if the given input dataset file exists\\n    if args_cli.input_file and not os.path.exists(args_cli.input_file):\\n        raise FileNotFoundError(f\"The dataset file {args_cli.input_file} does not exist.\")\\n\\n    # get the environment name\\n    if args_cli.task is not None:\\n        env_name = args_cli.task\\n    elif args_cli.input_file:\\n        # if the environment name is not specified, try to get it from the dataset file\\n        dataset_file_handler = HDF5DatasetFileHandler()\\n        dataset_file_handler.open(args_cli.input_file)\\n        env_name = dataset_file_handler.get_env_name()\\n    else:\\n        raise ValueError(\"Task/env name was not specified nor found in the dataset.\")\\n\\n    # parse configuration\\n    env_cfg = parse_env_cfg(env_name, device=args_cli.device, num_envs=num_envs)\\n    env_cfg.env_name = env_name\\n\\n    # extract success checking function to invoke manually\\n    success_term = None\\n    if hasattr(env_cfg.terminations, \"success\"):\\n        success_term = env_cfg.terminations.success\\n        env_cfg.terminations.success = None\\n    else:\\n        raise NotImplementedError(\"No success termination term was found in the environment.\")\\n\\n    # data generator is in charge of resetting the environment\\n    env_cfg.terminations = None\\n\\n    env_cfg.observations.policy.concatenate_terms = False\\n\\n    env_cfg.recorders = MimicRecorderManagerCfg()\\n\\n    env_cfg.recorders.dataset_export_mode = DatasetExportMode.EXPORT_NONE\\n    if args_cli.generated_output_file:\\n        # create output directory for generated episodes if it does not exist\\n        generated_output_dir = os.path.dirname(args_cli.generated_output_file)\\n        if not os.path.exists(generated_output_dir):\\n            os.makedirs(generated_output_dir)\\n        generated_output_file_name = os.path.splitext(os.path.basename(args_cli.generated_output_file))[0]\\n        env_cfg.recorders.dataset_export_dir_path = generated_output_dir\\n        env_cfg.recorders.dataset_filename = generated_output_file_name\\n        env_cfg.recorders.dataset_export_mode = DatasetExportMode.EXPORT_SUCCEEDED_ONLY\\n\\n    # create environment\\n    env = gym.make(env_name, cfg=env_cfg)\\n\\n    if not isinstance(env.unwrapped, ManagerBasedRLMimicEnv):\\n        raise ValueError(\"The environment should be derived from ManagerBasedRLMimicEnv\")\\n\\n    # check if the mimic API env.unwrapped.get_subtask_term_signals() is implemented\\n    if env.unwrapped.get_subtask_term_signals.__func__ is ManagerBasedRLMimicEnv.get_subtask_term_signals:\\n        raise NotImplementedError(\\n            \"The environment does not implement the get_subtask_term_signals method required to run this script.\"\\n        )\\n\\n    # set seed for generation\\n    random.seed(env.unwrapped.cfg.datagen_config.seed)\\n    np.random.seed(env.unwrapped.cfg.datagen_config.seed)\\n    torch.manual_seed(env.unwrapped.cfg.datagen_config.seed)\\n\\n    # reset before starting\\n    env.reset()\\n\\n    # Set up asyncio stuff\\n    asyncio_event_loop = asyncio.get_event_loop()\\n    env_action_queue = asyncio.Queue()\\n\\n    shared_datagen_info_pool_lock = asyncio.Lock()\\n    shared_datagen_info_pool = DataGenInfoPool(\\n        env.unwrapped, env.unwrapped.cfg, env.unwrapped.device, asyncio_lock=shared_datagen_info_pool_lock\\n    )\\n    if args_cli.input_file:\\n        shared_datagen_info_pool.load_from_dataset_file(args_cli.input_file)\\n        print(f\"Loaded {shared_datagen_info_pool.num_datagen_infos} to datagen info pool\")\\n\\n    # make data generator object\\n    data_generator_asyncio_tasks = []\\n    for i in range(num_envs):\\n        if args_cli.teleop_env_index is not None and i == args_cli.teleop_env_index:\\n            data_generator_asyncio_tasks.append(\\n                asyncio_event_loop.create_task(\\n                    run_teleop_robot(\\n                        env, i, env_action_queue, shared_datagen_info_pool, success_term, args_cli.output_file\\n                    )\\n                )\\n            )\\n            continue\\n        data_generator_asyncio_tasks.append(\\n            asyncio_event_loop.create_task(\\n                run_data_generator(\\n                    env,\\n                    i,\\n                    env_action_queue,\\n                    shared_datagen_info_pool,\\n                    success_term,\\n                    export_demo=bool(args_cli.generated_output_file),\\n                )\\n            )\\n        )\\n\\n    try:\\n        asyncio.ensure_future(asyncio.gather(*data_generator_asyncio_tasks))\\n    except asyncio.CancelledError:\\n        print(\"Tasks were cancelled.\")\\n\\n    env_loop(env, env_action_queue, shared_datagen_info_pool, asyncio_event_loop)'),\n",
       " Document(metadata={}, page_content='def main():\\n    num_envs = args_cli.num_envs\\n\\n    # Setup output paths and get env name\\n    output_dir, output_file_name = setup_output_paths(args_cli.output_file)\\n    env_name = args_cli.task or get_env_name_from_dataset(args_cli.input_file)\\n\\n    # Configure environment\\n    env_cfg, success_term = setup_env_config(\\n        env_name=env_name,\\n        output_dir=output_dir,\\n        output_file_name=output_file_name,\\n        num_envs=num_envs,\\n        device=args_cli.device,\\n        generation_num_trials=args_cli.generation_num_trials,\\n    )\\n\\n    # create environment\\n    env = gym.make(env_name, cfg=env_cfg).unwrapped\\n\\n    if not isinstance(env, ManagerBasedRLMimicEnv):\\n        raise ValueError(\"The environment should be derived from ManagerBasedRLMimicEnv\")\\n\\n    # check if the mimic API from this environment contains decprecated signatures\\n    if \"action_noise_dict\" not in inspect.signature(env.target_eef_pose_to_action).parameters:\\n        omni.log.warn(\\n            f\\'The \"noise\" parameter in the \"{env_name}\" environment\\\\\\'s mimic API \"target_eef_pose_to_action\", \\'\\n            \"is deprecated. Please update the API to take action_noise_dict instead.\"\\n        )\\n\\n    # set seed for generation\\n    random.seed(env.cfg.datagen_config.seed)\\n    np.random.seed(env.cfg.datagen_config.seed)\\n    torch.manual_seed(env.cfg.datagen_config.seed)\\n\\n    # reset before starting\\n    env.reset()\\n\\n    # Setup and run async data generation\\n    async_components = setup_async_generation(\\n        env=env,\\n        num_envs=args_cli.num_envs,\\n        input_file=args_cli.input_file,\\n        success_term=success_term,\\n        pause_subtask=args_cli.pause_subtask,\\n    )\\n\\n    try:\\n        asyncio.ensure_future(asyncio.gather(*async_components[\"tasks\"]))\\n        env_loop(\\n            env,\\n            async_components[\"reset_queue\"],\\n            async_components[\"action_queue\"],\\n            async_components[\"info_pool\"],\\n            async_components[\"event_loop\"],\\n        )\\n    except asyncio.CancelledError:\\n        print(\"Tasks were cancelled.\")'),\n",
       " Document(metadata={}, page_content='def rollout(policy, env, success_term, horizon, device):\\n    \"\"\"Perform a single rollout of the policy in the environment.\\n\\n    Args:\\n        policy: The robomimicpolicy to play.\\n        env: The environment to play in.\\n        horizon: The step horizon of each rollout.\\n        device: The device to run the policy on.\\n\\n    Returns:\\n        terminated: Whether the rollout terminated.\\n        traj: The trajectory of the rollout.\\n    \"\"\"\\n    policy.start_episode()\\n    obs_dict, _ = env.reset()\\n    traj = dict(actions=[], obs=[], next_obs=[])\\n\\n    for i in range(horizon):\\n        # Prepare observations\\n        obs = copy.deepcopy(obs_dict[\"policy\"])\\n        for ob in obs:\\n            obs[ob] = torch.squeeze(obs[ob])\\n\\n        # Check if environment image observations\\n        if hasattr(env.cfg, \"image_obs_list\"):\\n            # Process image observations for robomimic inference\\n            for image_name in env.cfg.image_obs_list:\\n                if image_name in obs_dict[\"policy\"].keys():\\n                    # Convert from chw uint8 to hwc normalized float\\n                    image = torch.squeeze(obs_dict[\"policy\"][image_name])\\n                    image = image.permute(2, 0, 1).clone().float()\\n                    image = image / 255.0\\n                    image = image.clip(0.0, 1.0)\\n                    obs[image_name] = image\\n\\n        traj[\"obs\"].append(obs)\\n\\n        # Compute actions\\n        actions = policy(obs)\\n\\n        # Unnormalize actions\\n        if args_cli.norm_factor_min is not None and args_cli.norm_factor_max is not None:\\n            actions = (\\n                (actions + 1) * (args_cli.norm_factor_max - args_cli.norm_factor_min)\\n            ) / 2 + args_cli.norm_factor_min\\n\\n        actions = torch.from_numpy(actions).to(device=device).view(1, env.action_space.shape[1])\\n\\n        # Apply actions\\n        obs_dict, _, terminated, truncated, _ = env.step(actions)\\n        obs = obs_dict[\"policy\"]\\n\\n        # Record trajectory\\n        traj[\"actions\"].append(actions.tolist())\\n        traj[\"next_obs\"].append(obs)\\n\\n        # Check if rollout was successful\\n        if bool(success_term.func(env, **success_term.params)[0]):\\n            return True, traj\\n        elif terminated or truncated:\\n            return False, traj\\n\\n    return False, traj'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Run a trained policy from robomimic with Isaac Lab environment.\"\"\"\\n    # parse configuration\\n    env_cfg = parse_env_cfg(args_cli.task, device=args_cli.device, num_envs=1, use_fabric=not args_cli.disable_fabric)\\n\\n    # Set observations to dictionary mode for Robomimic\\n    env_cfg.observations.policy.concatenate_terms = False\\n\\n    # Set termination conditions\\n    env_cfg.terminations.time_out = None\\n\\n    # Disable recorder\\n    env_cfg.recorders = None\\n\\n    # Extract success checking function\\n    success_term = env_cfg.terminations.success\\n    env_cfg.terminations.success = None\\n\\n    # Create environment\\n    env = gym.make(args_cli.task, cfg=env_cfg).unwrapped\\n\\n    # Set seed\\n    torch.manual_seed(args_cli.seed)\\n    env.seed(args_cli.seed)\\n\\n    # Acquire device\\n    device = TorchUtils.get_torch_device(try_to_use_cuda=True)\\n\\n    # Load policy\\n    policy, _ = FileUtils.policy_from_checkpoint(ckpt_path=args_cli.checkpoint, device=device, verbose=True)\\n\\n    # Run policy\\n    results = []\\n    for trial in range(args_cli.num_rollouts):\\n        print(f\"[INFO] Starting trial {trial}\")\\n        terminated, traj = rollout(policy, env, success_term, args_cli.horizon, device)\\n        results.append(terminated)\\n        print(f\"[INFO] Trial {trial}: {terminated}\\\\n\")\\n\\n    print(f\"\\\\nSuccessful trials: {results.count(True)}, out of {len(results)} trials\")\\n    print(f\"Success rate: {results.count(True) / len(results)}\")\\n    print(f\"Trial Results: {results}\\\\n\")\\n\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def normalize_hdf5_actions(config: Config, log_dir: str) -> str:\\n    \"\"\"Normalizes actions in hdf5 dataset to [-1, 1] range.\\n\\n    Args:\\n        config: The configuration object containing dataset path.\\n        log_dir: Directory to save normalization parameters.\\n\\n    Returns:\\n        Path to the normalized dataset.\\n    \"\"\"\\n    base, ext = os.path.splitext(config.train.data)\\n    normalized_path = base + \"_normalized\" + ext\\n\\n    # Copy the original dataset\\n    print(f\"Creating normalized dataset at {normalized_path}\")\\n    shutil.copyfile(config.train.data, normalized_path)\\n\\n    # Open the new dataset and normalize the actions\\n    with h5py.File(normalized_path, \"r+\") as f:\\n        dataset_paths = [f\"/data/demo_{str(i)}/actions\" for i in range(len(f[\"data\"].keys()))]\\n\\n        # Compute the min and max of the dataset\\n        dataset = np.array(f[dataset_paths[0]]).flatten()\\n        for i, path in enumerate(dataset_paths):\\n            if i != 0:\\n                data = np.array(f[path]).flatten()\\n                dataset = np.append(dataset, data)\\n\\n        max = np.max(dataset)\\n        min = np.min(dataset)\\n\\n        # Normalize the actions\\n        for i, path in enumerate(dataset_paths):\\n            data = np.array(f[path])\\n            normalized_data = 2 * ((data - min) / (max - min)) - 1  # Scale to [-1, 1] range\\n            del f[path]\\n            f[path] = normalized_data\\n\\n        # Save the min and max values to log directory\\n        with open(os.path.join(log_dir, \"normalization_params.txt\"), \"w\") as f:\\n            f.write(f\"min: {min}\\\\n\")\\n            f.write(f\"max: {max}\\\\n\")\\n\\n    return normalized_path'),\n",
       " Document(metadata={}, page_content='def train(config: Config, device: str, log_dir: str, ckpt_dir: str, video_dir: str):\\n    \"\"\"Train a model using the algorithm specified in config.\\n\\n    Args:\\n        config: Configuration object.\\n        device: PyTorch device to use for training.\\n        log_dir: Directory to save logs.\\n        ckpt_dir: Directory to save checkpoints.\\n        video_dir: Directory to save videos.\\n    \"\"\"\\n    # first set seeds\\n    np.random.seed(config.train.seed)\\n    torch.manual_seed(config.train.seed)\\n\\n    print(\"\\\\n============= New Training Run with Config =============\")\\n    print(config)\\n    print(\"\")\\n\\n    print(f\">>> Saving logs into directory: {log_dir}\")\\n    print(f\">>> Saving checkpoints into directory: {ckpt_dir}\")\\n    print(f\">>> Saving videos into directory: {video_dir}\")\\n\\n    if config.experiment.logging.terminal_output_to_txt:\\n        # log stdout and stderr to a text file\\n        logger = PrintLogger(os.path.join(log_dir, \"log.txt\"))\\n        sys.stdout = logger\\n        sys.stderr = logger\\n\\n    # read config to set up metadata for observation modalities (e.g. detecting rgb observations)\\n    ObsUtils.initialize_obs_utils_with_config(config)\\n\\n    # make sure the dataset exists\\n    dataset_path = os.path.expanduser(config.train.data)\\n    if not os.path.exists(dataset_path):\\n        raise FileNotFoundError(f\"Dataset at provided path {dataset_path} not found!\")\\n\\n    # load basic metadata from training file\\n    print(\"\\\\n============= Loaded Environment Metadata =============\")\\n    env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=config.train.data)\\n    shape_meta = FileUtils.get_shape_metadata_from_dataset(\\n        dataset_path=config.train.data, all_obs_keys=config.all_obs_keys, verbose=True\\n    )\\n\\n    if config.experiment.env is not None:\\n        env_meta[\"env_name\"] = config.experiment.env\\n        print(\"=\" * 30 + \"\\\\n\" + \"Replacing Env to {}\\\\n\".format(env_meta[\"env_name\"]) + \"=\" * 30)\\n\\n    # create environment\\n    envs = OrderedDict()\\n    if config.experiment.rollout.enabled:\\n        # create environments for validation runs\\n        env_names = [env_meta[\"env_name\"]]\\n\\n        if config.experiment.additional_envs is not None:\\n            for name in config.experiment.additional_envs:\\n                env_names.append(name)\\n\\n        for env_name in env_names:\\n            env = EnvUtils.create_env_from_metadata(\\n                env_meta=env_meta,\\n                env_name=env_name,\\n                render=False,\\n                render_offscreen=config.experiment.render_video,\\n                use_image_obs=shape_meta[\"use_images\"],\\n            )\\n            envs[env.name] = env\\n            print(envs[env.name])\\n\\n    print(\"\")\\n\\n    # setup for a new training run\\n    data_logger = DataLogger(log_dir, config=config, log_tb=config.experiment.logging.log_tb)\\n    model = algo_factory(\\n        algo_name=config.algo_name,\\n        config=config,\\n        obs_key_shapes=shape_meta[\"all_shapes\"],\\n        ac_dim=shape_meta[\"ac_dim\"],\\n        device=device,\\n    )\\n\\n    # save the config as a json file\\n    with open(os.path.join(log_dir, \"..\", \"config.json\"), \"w\") as outfile:\\n        json.dump(config, outfile, indent=4)\\n\\n    print(\"\\\\n============= Model Summary =============\")\\n    print(model)  # print model summary\\n    print(\"\")\\n\\n    # load training data\\n    trainset, validset = TrainUtils.load_data_for_training(config, obs_keys=shape_meta[\"all_obs_keys\"])\\n    train_sampler = trainset.get_dataset_sampler()\\n    print(\"\\\\n============= Training Dataset =============\")\\n    print(trainset)\\n    print(\"\")\\n\\n    # maybe retrieve statistics for normalizing observations\\n    obs_normalization_stats = None\\n    if config.train.hdf5_normalize_obs:\\n        obs_normalization_stats = trainset.get_obs_normalization_stats()\\n\\n    # initialize data loaders\\n    train_loader = DataLoader(\\n        dataset=trainset,\\n        sampler=train_sampler,\\n        batch_size=config.train.batch_size,\\n        shuffle=(train_sampler is None),\\n        num_workers=config.train.num_data_workers,\\n        drop_last=True,\\n    )\\n\\n    if config.experiment.validate:\\n        # cap num workers for validation dataset at 1\\n        num_workers = min(config.train.num_data_workers, 1)\\n        valid_sampler = validset.get_dataset_sampler()\\n        valid_loader = DataLoader(\\n            dataset=validset,\\n            sampler=valid_sampler,\\n            batch_size=config.train.batch_size,\\n            shuffle=(valid_sampler is None),\\n            num_workers=num_workers,\\n            drop_last=True,\\n        )\\n    else:\\n        valid_loader = None\\n\\n    # main training loop\\n    best_valid_loss = None\\n    last_ckpt_time = time.time()\\n\\n    # number of learning steps per epoch (defaults to a full dataset pass)\\n    train_num_steps = config.experiment.epoch_every_n_steps\\n    valid_num_steps = config.experiment.validation_epoch_every_n_steps\\n\\n    for epoch in range(1, config.train.num_epochs + 1):  # epoch numbers start at 1\\n        step_log = TrainUtils.run_epoch(model=model, data_loader=train_loader, epoch=epoch, num_steps=train_num_steps)\\n        model.on_epoch_end(epoch)\\n\\n        # setup checkpoint path\\n        epoch_ckpt_name = f\"model_epoch_{epoch}\"\\n\\n        # check for recurring checkpoint saving conditions\\n        should_save_ckpt = False\\n        if config.experiment.save.enabled:\\n            time_check = (config.experiment.save.every_n_seconds is not None) and (\\n                time.time() - last_ckpt_time > config.experiment.save.every_n_seconds\\n            )\\n            epoch_check = (\\n                (config.experiment.save.every_n_epochs is not None)\\n                and (epoch > 0)\\n                and (epoch % config.experiment.save.every_n_epochs == 0)\\n            )\\n            epoch_list_check = epoch in config.experiment.save.epochs\\n            should_save_ckpt = time_check or epoch_check or epoch_list_check\\n        ckpt_reason = None\\n        if should_save_ckpt:\\n            last_ckpt_time = time.time()\\n            ckpt_reason = \"time\"\\n\\n        print(f\"Train Epoch {epoch}\")\\n        print(json.dumps(step_log, sort_keys=True, indent=4))\\n        for k, v in step_log.items():\\n            if k.startswith(\"Time_\"):\\n                data_logger.record(f\"Timing_Stats/Train_{k[5:]}\", v, epoch)\\n            else:\\n                data_logger.record(f\"Train/{k}\", v, epoch)\\n\\n        # Evaluate the model on validation set\\n        if config.experiment.validate:\\n            with torch.no_grad():\\n                step_log = TrainUtils.run_epoch(\\n                    model=model, data_loader=valid_loader, epoch=epoch, validate=True, num_steps=valid_num_steps\\n                )\\n            for k, v in step_log.items():\\n                if k.startswith(\"Time_\"):\\n                    data_logger.record(f\"Timing_Stats/Valid_{k[5:]}\", v, epoch)\\n                else:\\n                    data_logger.record(f\"Valid/{k}\", v, epoch)\\n\\n            print(f\"Validation Epoch {epoch}\")\\n            print(json.dumps(step_log, sort_keys=True, indent=4))\\n\\n            # save checkpoint if achieve new best validation loss\\n            valid_check = \"Loss\" in step_log\\n            if valid_check and (best_valid_loss is None or (step_log[\"Loss\"] <= best_valid_loss)):\\n                best_valid_loss = step_log[\"Loss\"]\\n                if config.experiment.save.enabled and config.experiment.save.on_best_validation:\\n                    epoch_ckpt_name += f\"_best_validation_{best_valid_loss}\"\\n                    should_save_ckpt = True\\n                    ckpt_reason = \"valid\" if ckpt_reason is None else ckpt_reason\\n\\n        # Save model checkpoints based on conditions (success rate, validation loss, etc)\\n        if should_save_ckpt:\\n            TrainUtils.save_model(\\n                model=model,\\n                config=config,\\n                env_meta=env_meta,\\n                shape_meta=shape_meta,\\n                ckpt_path=os.path.join(ckpt_dir, epoch_ckpt_name + \".pth\"),\\n                obs_normalization_stats=obs_normalization_stats,\\n            )\\n\\n        # Finally, log memory usage in MB\\n        process = psutil.Process(os.getpid())\\n        mem_usage = int(process.memory_info().rss / 1000000)\\n        data_logger.record(\"System/RAM Usage (MB)\", mem_usage, epoch)\\n        print(f\"\\\\nEpoch {epoch} Memory Usage: {mem_usage} MB\\\\n\")\\n\\n    # terminate logging\\n    data_logger.close()'),\n",
       " Document(metadata={}, page_content='def main(args: argparse.Namespace):\\n    \"\"\"Train a model on a task using a specified algorithm.\\n\\n    Args:\\n        args: Command line arguments.\\n    \"\"\"\\n    # load config\\n    if args.task is not None:\\n        # obtain the configuration entry point\\n        cfg_entry_point_key = f\"robomimic_{args.algo}_cfg_entry_point\"\\n\\n        print(f\"Loading configuration for task: {args.task}\")\\n        print(gym.envs.registry.keys())\\n        print(\" \")\\n        cfg_entry_point_file = gym.spec(args.task).kwargs.pop(cfg_entry_point_key)\\n        # check if entry point exists\\n        if cfg_entry_point_file is None:\\n            raise ValueError(\\n                f\"Could not find configuration for the environment: \\'{args.task}\\'.\"\\n                f\" Please check that the gym registry has the entry point: \\'{cfg_entry_point_key}\\'.\"\\n            )\\n\\n        with open(cfg_entry_point_file) as f:\\n            ext_cfg = json.load(f)\\n            config = config_factory(ext_cfg[\"algo_name\"])\\n        # update config with external json - this will throw errors if\\n        # the external config has keys not present in the base algo config\\n        with config.values_unlocked():\\n            config.update(ext_cfg)\\n    else:\\n        raise ValueError(\"Please provide a task name through CLI arguments.\")\\n\\n    if args.dataset is not None:\\n        config.train.data = args.dataset\\n\\n    if args.name is not None:\\n        config.experiment.name = args.name\\n\\n    # change location of experiment directory\\n    config.train.output_dir = os.path.abspath(os.path.join(\"./logs\", args.log_dir, args.task))\\n\\n    log_dir, ckpt_dir, video_dir = TrainUtils.get_exp_dir(config)\\n\\n    if args.normalize_training_actions:\\n        config.train.data = normalize_hdf5_actions(config, log_dir)\\n\\n    # get torch device\\n    device = TorchUtils.get_torch_device(try_to_use_cuda=config.train.cuda)\\n\\n    config.lock()\\n\\n    # catch error during training and print it\\n    res_str = \"finished run successfully!\"\\n    try:\\n        train(config, device, log_dir, ckpt_dir, video_dir)\\n    except Exception as e:\\n        res_str = f\"run failed with error:\\\\n{e}\\\\n\\\\n{traceback.format_exc()}\"\\n    print(res_str)'),\n",
       " Document(metadata={}, page_content='def get_namespace() -> str:\\n    \"\"\"Get the current Kubernetes namespace from the context, fallback to default if not set\"\"\"\\n    try:\\n        namespace = (\\n            subprocess.check_output([\"kubectl\", \"config\", \"view\", \"--minify\", \"--output\", \"jsonpath={..namespace}\"])\\n            .decode()\\n            .strip()\\n        )\\n        if not namespace:\\n            namespace = \"default\"\\n    except subprocess.CalledProcessError:\\n        namespace = \"default\"\\n    return namespace'),\n",
       " Document(metadata={}, page_content='def get_pods(namespace: str = \"default\") -> list[tuple]:\\n    \"\"\"Get a list of all of the pods in the namespace\"\"\"\\n    cmd = [\"kubectl\", \"get\", \"pods\", \"-n\", namespace, \"--no-headers\"]\\n    output = subprocess.check_output(cmd).decode()\\n    pods = []\\n    for line in output.strip().split(\"\\\\n\"):\\n        fields = line.split()\\n        pod_name = fields[0]\\n        status = fields[2]\\n        pods.append((pod_name, status))\\n    return pods'),\n",
       " Document(metadata={}, page_content='def get_clusters(pods: list, cluster_name_prefix: str) -> set:\\n    \"\"\"\\n    Get unique cluster name(s). Works for one or more clusters, based off of the number of head nodes.\\n    Excludes MLflow deployments.\\n    \"\"\"\\n    clusters = set()\\n    for pod_name, _ in pods:\\n        # Skip MLflow pods\\n        if \"-mlflow\" in pod_name:\\n            continue\\n\\n        match = re.match(r\"(\" + re.escape(cluster_name_prefix) + r\"[-\\\\w]+)\", pod_name)\\n        if match:\\n            # Get base name without head/worker suffix (skip workers)\\n            if \"head\" in pod_name:\\n                base_name = match.group(1).split(\"-head\")[0]\\n                clusters.add(base_name)\\n    return sorted(clusters)'),\n",
       " Document(metadata={}, page_content='def get_mlflow_info(namespace: str = None, cluster_prefix: str = \"isaacray\") -> str:\\n    \"\"\"\\n    Get MLflow service information if it exists in the namespace with the given prefix.\\n    Only works for a single cluster instance.\\n    Args:\\n        namespace: Kubernetes namespace\\n        cluster_prefix: Base cluster name (without -head/-worker suffixes)\\n    Returns:\\n        MLflow service URL\\n    \"\"\"\\n    # Strip any -head or -worker suffixes to get base name\\n    if namespace is None:\\n        namespace = get_namespace()\\n    pods = get_pods(namespace=namespace)\\n    clusters = get_clusters(pods=pods, cluster_name_prefix=cluster_prefix)\\n    if len(clusters) > 1:\\n        raise ValueError(\"More than one cluster matches prefix, could not automatically determine mlflow info.\")\\n    mlflow_name = f\"{cluster_prefix}-mlflow\"\\n\\n    cmd = [\"kubectl\", \"get\", \"svc\", mlflow_name, \"-n\", namespace, \"--no-headers\"]\\n    try:\\n        output = subprocess.check_output(cmd).decode()\\n        fields = output.strip().split()\\n\\n        # Get cluster IP\\n        cluster_ip = fields[2]\\n        port = \"5000\"  # Default MLflow port\\n        # This needs to be http to be resolved. HTTPS can\\'t be resolved\\n        # This should be fine as it is on a subnet on the cluster regardless\\n        return f\"http://{cluster_ip}:{port}\"\\n    except subprocess.CalledProcessError as e:\\n        raise ValueError(f\"Could not grok MLflow: {e}\")'),\n",
       " Document(metadata={}, page_content='def check_clusters_running(pods: list, clusters: set) -> bool:\\n    \"\"\"\\n    Check that all of the pods in all provided clusters are running.\\n\\n    Args:\\n        pods (list): A list of tuples where each tuple contains the pod name and its status.\\n        clusters (set): A set of cluster names to check.\\n\\n    Returns:\\n        bool: True if all pods in any of the clusters are running, False otherwise.\\n    \"\"\"\\n    clusters_running = False\\n    for cluster in clusters:\\n        cluster_pods = [p for p in pods if p[0].startswith(cluster)]\\n        total_pods = len(cluster_pods)\\n        running_pods = len([p for p in cluster_pods if p[1] == \"Running\"])\\n        if running_pods == total_pods and running_pods > 0:\\n            clusters_running = True\\n            break\\n    return clusters_running'),\n",
       " Document(metadata={}, page_content='def get_ray_address(head_pod: str, namespace: str = \"default\", ray_head_name: str = \"head\") -> str:\\n    \"\"\"\\n    Given a cluster head pod, check its logs, which should include the ray address which can accept job requests.\\n\\n    Args:\\n        head_pod (str): The name of the head pod.\\n        namespace (str, optional): The Kubernetes namespace. Defaults to \"default\".\\n        ray_head_name (str, optional): The name of the ray head container. Defaults to \"head\".\\n\\n    Returns:\\n        str: The ray address if found, None otherwise.\\n\\n    Raises:\\n        ValueError: If the logs cannot be retrieved or the ray address is not found.\\n    \"\"\"\\n    cmd = [\"kubectl\", \"logs\", head_pod, \"-c\", ray_head_name, \"-n\", namespace]\\n    try:\\n        output = subprocess.check_output(cmd).decode()\\n    except subprocess.CalledProcessError as e:\\n        raise ValueError(\\n            f\"Could not enter head container with cmd {cmd}: {e}Perhaps try a different namespace or ray head name.\"\\n        )\\n    match = re.search(r\"RAY_ADDRESS=\\'([^\\']+)\\'\", output)\\n    if match:\\n        return match.group(1)\\n    else:\\n        return None'),\n",
       " Document(metadata={}, page_content='def process_cluster(cluster_info: dict, ray_head_name: str = \"head\") -> str:\\n    \"\"\"\\n    For each cluster, check that it is running, and get the Ray head address that will accept jobs.\\n\\n    Args:\\n        cluster_info (dict): A dictionary containing cluster information with keys \\'cluster\\', \\'pods\\', and \\'namespace\\'.\\n        ray_head_name (str, optional): The name of the ray head container. Defaults to \"head\".\\n\\n    Returns:\\n        str: A string containing the cluster name and its Ray head address, or an error message if the head pod or Ray address is not found.\\n    \"\"\"\\n    cluster, pods, namespace = cluster_info\\n    head_pod = None\\n    for pod_name, status in pods:\\n        if pod_name.startswith(cluster + \"-head\"):\\n            head_pod = pod_name\\n            break\\n    if not head_pod:\\n        return f\"Error: Could not find head pod for cluster {cluster}\\\\n\"\\n\\n    # Get RAY_ADDRESS and status\\n    ray_address = get_ray_address(head_pod, namespace=namespace, ray_head_name=ray_head_name)\\n    if not ray_address:\\n        return f\"Error: Could not find RAY_ADDRESS for cluster {cluster}\\\\n\"\\n\\n    # Return only cluster and ray address\\n    return f\"name: {cluster} address: {ray_address}\\\\n\"'),\n",
       " Document(metadata={}, page_content='def main():\\n    # Parse command-line arguments\\n    parser = argparse.ArgumentParser(description=\"Process Ray clusters and save their specifications.\")\\n    parser.add_argument(\"--prefix\", default=\"isaacray\", help=\"The prefix for the cluster names.\")\\n    parser.add_argument(\"--output\", default=\"~/.cluster_config\", help=\"The file to save cluster specifications.\")\\n    parser.add_argument(\"--ray_head_name\", default=\"head\", help=\"The metadata name for the ray head container\")\\n    parser.add_argument(\\n        \"--namespace\", help=\"Kubernetes namespace to use. If not provided, will detect from current context.\"\\n    )\\n    args = parser.parse_args()\\n\\n    # Get namespace from args or detect it\\n    current_namespace = args.namespace if args.namespace else get_namespace()\\n    print(f\"Using namespace: {current_namespace}\")\\n\\n    cluster_name_prefix = args.prefix\\n    cluster_spec_file = os.path.expanduser(args.output)\\n\\n    # Get all pods\\n    pods = get_pods(namespace=current_namespace)\\n\\n    # Get clusters\\n    clusters = get_clusters(pods, cluster_name_prefix)\\n    if not clusters:\\n        print(f\"No clusters found with prefix {cluster_name_prefix}\")\\n        return\\n\\n    # Wait for clusters to be running\\n    while True:\\n        pods = get_pods(namespace=current_namespace)\\n        if check_clusters_running(pods, clusters):\\n            break\\n        print(\"Waiting for all clusters to spin up...\")\\n        time.sleep(5)\\n\\n    print(\"Checking for MLflow:\")\\n    # Check MLflow status for each cluster\\n    for cluster in clusters:\\n        try:\\n            mlflow_address = get_mlflow_info(current_namespace, cluster)\\n            print(f\"MLflow address for {cluster}: {mlflow_address}\")\\n        except ValueError as e:\\n            print(f\"ML Flow not located: {e}\")\\n    print()\\n\\n    # Prepare cluster info for parallel processing\\n    cluster_infos = []\\n    for cluster in clusters:\\n        cluster_pods = [p for p in pods if p[0].startswith(cluster)]\\n        cluster_infos.append((cluster, cluster_pods, current_namespace))\\n\\n    # Use ThreadPoolExecutor to process clusters in parallel\\n    results = []\\n    results_lock = threading.Lock()\\n\\n    with ThreadPoolExecutor() as executor:\\n        future_to_cluster = {\\n            executor.submit(process_cluster, info, args.ray_head_name): info[0] for info in cluster_infos\\n        }\\n        for future in as_completed(future_to_cluster):\\n            cluster_name = future_to_cluster[future]\\n            try:\\n                result = future.result()\\n                with results_lock:\\n                    results.append(result)\\n            except Exception as exc:\\n                print(f\"{cluster_name} generated an exception: {exc}\")\\n\\n    # Sort results alphabetically by cluster name\\n    results.sort()\\n\\n    # Write sorted results to the output file (Ray info only)\\n    with open(cluster_spec_file, \"w\") as f:\\n        for result in results:\\n            f.write(result)\\n\\n    print(f\"Cluster spec information saved to {cluster_spec_file}\")\\n    # Display the contents of the config file\\n    with open(cluster_spec_file) as f:\\n        print(f.read())'),\n",
       " Document(metadata={}, page_content='def apply_manifest(args: argparse.Namespace) -> None:\\n    \"\"\"Provided a Jinja templated ray.io/v1alpha1 file,\\n    populate the arguments and create the cluster. Additionally, create\\n    kubernetes containers for resources separated by \\'---\\' from the rest\\n    of the file.\\n\\n    Args:\\n        args: Possible arguments concerning cluster parameters.\\n    \"\"\"\\n    # Load Kubernetes configuration\\n    config.load_kube_config()\\n\\n    # Set up Jinja2 environment for loading templates\\n    templates_dir = RAY_DIR / \"cluster_configs\" / args.cluster_host\\n    file_loader = FileSystemLoader(str(templates_dir))\\n    jinja_env = Environment(loader=file_loader, keep_trailing_newline=True, autoescape=True)\\n\\n    # Define template filename\\n    template_file = \"kuberay.yaml.jinja\"\\n\\n    # Convert args namespace to a dictionary\\n    template_params = vars(args)\\n\\n    # Load and render the template\\n    template = jinja_env.get_template(template_file)\\n    file_contents = template.render(template_params)\\n\\n    # Parse all YAML documents in the rendered template\\n    all_yamls = []\\n    for doc in yaml.safe_load_all(file_contents):\\n        all_yamls.append(doc)\\n\\n    # Convert back to YAML string, preserving multiple documents\\n    cleaned_yaml_string = \"\"\\n    for i, doc in enumerate(all_yamls):\\n        if i > 0:\\n            cleaned_yaml_string += \"\\\\n---\\\\n\"\\n        cleaned_yaml_string += yaml.dump(doc)\\n\\n    # Apply the Kubernetes manifest using kubectl\\n    try:\\n        print(cleaned_yaml_string)\\n        subprocess.run([\"kubectl\", \"apply\", \"-f\", \"-\"], input=cleaned_yaml_string, text=True, check=True)\\n    except subprocess.CalledProcessError as e:\\n        exit(f\"An error occurred while running `kubectl`: {e}\")'),\n",
       " Document(metadata={}, page_content='def parse_args() -> argparse.Namespace:\\n    \"\"\"\\n    Parse command-line arguments for Kubernetes deployment script.\\n\\n    Returns:\\n        argparse.Namespace: Parsed command-line arguments.\\n    \"\"\"\\n    arg_parser = argparse.ArgumentParser(\\n        description=\"Script to apply manifests to create Kubernetes objects for Ray clusters.\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\\n    )\\n\\n    arg_parser.add_argument(\\n        \"--cluster_host\",\\n        type=str,\\n        default=\"google_cloud\",\\n        choices=[\"google_cloud\"],\\n        help=(\\n            \"In the cluster_configs directory, the name of the folder where a tune.yaml.jinja\"\\n            \"file exists defining the KubeRay config. Currently only google_cloud is supported.\"\\n        ),\\n    )\\n\\n    arg_parser.add_argument(\\n        \"--name\",\\n        type=str,\\n        required=False,\\n        default=\"isaacray\",\\n        help=\"Name of the Kubernetes deployment.\",\\n    )\\n\\n    arg_parser.add_argument(\\n        \"--namespace\",\\n        type=str,\\n        required=False,\\n        default=\"default\",\\n        help=\"Kubernetes namespace to deploy the Ray cluster.\",\\n    )\\n\\n    arg_parser.add_argument(\\n        \"--service_acount_name\", type=str, required=False, default=\"default\", help=\"The service account name to use.\"\\n    )\\n\\n    arg_parser.add_argument(\\n        \"--image\",\\n        type=str,\\n        required=True,\\n        help=\"Docker image for the Ray cluster pods.\",\\n    )\\n\\n    arg_parser.add_argument(\\n        \"--worker_accelerator\",\\n        nargs=\"+\",\\n        type=str,\\n        default=[\"nvidia-l4\"],\\n        help=\"GPU accelerator name. Supply more than one for heterogeneous resources.\",\\n    )\\n\\n    arg_parser = util.add_resource_arguments(arg_parser, cluster_create_defaults=True)\\n\\n    arg_parser.add_argument(\\n        \"--num_clusters\",\\n        type=int,\\n        default=1,\\n        help=\"How many Ray Clusters to create.\",\\n    )\\n    arg_parser.add_argument(\\n        \"--num_head_cpu\",\\n        type=float,  # to be able to schedule partial CPU heads\\n        default=8,\\n        help=\"The number of CPUs to give the Ray head.\",\\n    )\\n\\n    arg_parser.add_argument(\"--head_ram_gb\", type=int, default=8, help=\"How many gigs of ram to give the Ray head\")\\n    args = arg_parser.parse_args()\\n    return util.fill_in_missing_resources(args, cluster_creation_flag=True)'),\n",
       " Document(metadata={}, page_content='def main():\\n    args = parse_args()\\n\\n    if \"head\" in args.name:\\n        raise ValueError(\"For compatibility with other scripts, do not include head in the name\")\\n    if args.num_clusters == 1:\\n        apply_manifest(args)\\n    else:\\n        default_name = args.name\\n        for i in range(args.num_clusters):\\n            args.name = default_name + \"-\" + str(i)\\n            apply_manifest(args)'),\n",
       " Document(metadata={}, page_content='def setup_logging(level=logging.INFO):\\n    logging.basicConfig(level=level, format=\"%(asctime)s - %(levelname)s - %(message)s\")'),\n",
       " Document(metadata={}, page_content='def get_existing_runs(download_dir: str) -> set[str]:\\n    \"\"\"Get set of run IDs that have already been downloaded.\"\"\"\\n    existing_runs = set()\\n    tensorboard_dir = os.path.join(download_dir, \"tensorboard\")\\n    if os.path.exists(tensorboard_dir):\\n        for entry in os.listdir(tensorboard_dir):\\n            if entry.startswith(\"run_\"):\\n                existing_runs.add(entry[4:])\\n    return existing_runs'),\n",
       " Document(metadata={}, page_content='def process_run(args):\\n    \"\"\"Convert MLflow run to TensorBoard format.\"\"\"\\n    run_id, download_dir, tracking_uri = args\\n\\n    try:\\n        # Set up MLflow client\\n        mlflow.set_tracking_uri(tracking_uri)\\n        client = MlflowClient()\\n        run = client.get_run(run_id)\\n\\n        # Create TensorBoard writer\\n        tensorboard_log_dir = os.path.join(download_dir, \"tensorboard\", f\"run_{run_id}\")\\n        writer = SummaryWriter(log_dir=tensorboard_log_dir)\\n\\n        # Log parameters\\n        for key, value in run.data.params.items():\\n            writer.add_text(f\"params/{key}\", str(value))\\n\\n        # Log metrics with history\\n        for key in run.data.metrics.keys():\\n            history = client.get_metric_history(run_id, key)\\n            for m in history:\\n                writer.add_scalar(f\"metrics/{key}\", m.value, m.step)\\n\\n        # Log tags\\n        for key, value in run.data.tags.items():\\n            writer.add_text(f\"tags/{key}\", str(value))\\n\\n        writer.close()\\n        return run_id, True\\n    except Exception:\\n        return run_id, False'),\n",
       " Document(metadata={}, page_content='def download_experiment_tensorboard_logs(uri: str, experiment_name: str, download_dir: str) -> None:\\n    \"\"\"Download MLflow experiment logs and convert to TensorBoard format.\"\"\"\\n    logger = logging.getLogger(__name__)\\n\\n    try:\\n        # Set up MLflow\\n        mlflow.set_tracking_uri(uri)\\n        logger.info(f\"Connected to MLflow tracking server at {uri}\")\\n\\n        # Get experiment\\n        experiment = mlflow.get_experiment_by_name(experiment_name)\\n        if experiment is None:\\n            raise ValueError(f\"Experiment \\'{experiment_name}\\' not found at URI \\'{uri}\\'.\")\\n\\n        # Get all runs\\n        runs = mlflow.search_runs([experiment.experiment_id])\\n        logger.info(f\"Found {len(runs)} total runs in experiment \\'{experiment_name}\\'\")\\n\\n        # Check existing runs\\n        existing_runs = get_existing_runs(download_dir)\\n        logger.info(f\"Found {len(existing_runs)} existing runs in {download_dir}\")\\n\\n        # Create directory structure\\n        os.makedirs(os.path.join(download_dir, \"tensorboard\"), exist_ok=True)\\n\\n        # Process new runs\\n        new_run_ids = [run.run_id for _, run in runs.iterrows() if run.run_id not in existing_runs]\\n\\n        if not new_run_ids:\\n            logger.info(\"No new runs to process\")\\n            return\\n\\n        logger.info(f\"Processing {len(new_run_ids)} new runs...\")\\n\\n        # Process runs in parallel\\n        num_processes = min(mp.cpu_count(), len(new_run_ids))\\n        processed = 0\\n\\n        with ProcessPoolExecutor(max_workers=num_processes) as executor:\\n            future_to_run = {\\n                executor.submit(process_run, (run_id, download_dir, uri)): run_id for run_id in new_run_ids\\n            }\\n\\n            for future in as_completed(future_to_run):\\n                run_id = future_to_run[future]\\n                try:\\n                    run_id, success = future.result()\\n                    processed += 1\\n                    if success:\\n                        logger.info(f\"[{processed}/{len(new_run_ids)}] Successfully processed run {run_id}\")\\n                    else:\\n                        logger.error(f\"[{processed}/{len(new_run_ids)}] Failed to process run {run_id}\")\\n                except Exception as e:\\n                    logger.error(f\"Error processing run {run_id}: {e}\")\\n\\n        logger.info(f\"\\\\nAll data saved to {download_dir}/tensorboard\")\\n\\n    except Exception as e:\\n        logger.error(f\"Error during download: {e}\")\\n        raise'),\n",
       " Document(metadata={}, page_content='def main():\\n    parser = argparse.ArgumentParser(description=\"Download MLflow experiment logs for TensorBoard visualization.\")\\n    parser.add_argument(\"--uri\", required=True, help=\"The MLflow tracking URI (e.g., http://localhost:5000)\")\\n    parser.add_argument(\"--experiment-name\", required=True, help=\"Name of the experiment to download\")\\n    parser.add_argument(\"--download-dir\", required=True, help=\"Directory to save TensorBoard logs\")\\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\\n\\n    args = parser.parse_args()\\n    setup_logging(level=logging.DEBUG if args.debug else logging.INFO)\\n\\n    try:\\n        download_experiment_tensorboard_logs(args.uri, args.experiment_name, args.download_dir)\\n        print(\"\\\\nSuccess! To view the logs, run:\")\\n        print(f\"tensorboard --logdir {os.path.join(args.download_dir, \\'tensorboard\\')}\")\\n    except Exception as e:\\n        logging.error(f\"Failed to download experiment logs: {e}\")\\n        sys.exit(1)'),\n",
       " Document(metadata={}, page_content='def read_cluster_spec(fn: str | None = None) -> list[dict]:\\n    if fn is None:\\n        cluster_spec_path = os.path.expanduser(\"~/.cluster_config\")\\n    else:\\n        cluster_spec_path = os.path.expanduser(fn)\\n\\n    if not os.path.exists(cluster_spec_path):\\n        raise FileNotFoundError(f\"Cluster spec file not found at {cluster_spec_path}\")\\n\\n    clusters = []\\n    with open(cluster_spec_path) as f:\\n        for line in f:\\n            parts = line.strip().split(\" \")\\n            http_address = parts[3]\\n            cluster_info = {\"name\": parts[1], \"address\": http_address}\\n            print(f\"[INFO] Setting {cluster_info[\\'name\\']}\")  # with {cluster_info[\\'num_gpu\\']} GPUs.\")\\n            clusters.append(cluster_info)\\n\\n    return clusters'),\n",
       " Document(metadata={}, page_content='def submit_job(cluster: dict, job_command: str) -> None:\\n    \"\"\"\\n    Submits a job to a single cluster, prints the final result and Ray dashboard URL at the end.\\n    \"\"\"\\n    address = cluster[\"address\"]\\n    cluster_name = cluster[\"name\"]\\n    print(f\"[INFO]: Submitting job to cluster \\'{cluster_name}\\' at {address}\")  # with {num_gpus} GPUs.\")\\n    client = job_submission.JobSubmissionClient(address)\\n    runtime_env = {\"working_dir\": CONFIG[\"working_dir\"], \"executable\": CONFIG[\"executable\"]}\\n    print(f\"[INFO]: Checking contents of the directory: {CONFIG[\\'working_dir\\']}\")\\n    try:\\n        dir_contents = os.listdir(CONFIG[\"working_dir\"])\\n        print(f\"[INFO]: Directory contents: {dir_contents}\")\\n    except Exception as e:\\n        print(f\"[INFO]: Failed to list directory contents: {str(e)}\")\\n    entrypoint = f\"{CONFIG[\\'executable\\']} {job_command}\"\\n    print(f\"[INFO]: Attempting entrypoint {entrypoint=} in cluster {cluster}\")\\n    job_id = client.submit_job(entrypoint=entrypoint, runtime_env=runtime_env)\\n    status = client.get_job_status(job_id)\\n    while status in [job_submission.JobStatus.PENDING, job_submission.JobStatus.RUNNING]:\\n        time.sleep(5)\\n        status = client.get_job_status(job_id)\\n\\n    final_logs = client.get_job_logs(job_id)\\n    print(\"----------------------------------------------------\")\\n    print(f\"[INFO]: Cluster {cluster_name} Logs: \\\\n\")\\n    print(final_logs)\\n    print(\"----------------------------------------------------\")'),\n",
       " Document(metadata={}, page_content='def submit_jobs_to_clusters(jobs: list[str], clusters: list[dict]) -> None:\\n    \"\"\"\\n    Submit all jobs to their respective clusters, cycling through clusters if there are more jobs than clusters.\\n    \"\"\"\\n    if not clusters:\\n        raise ValueError(\"No clusters available for job submission.\")\\n\\n    if len(jobs) < len(clusters):\\n        print(\"[INFO]: Less jobs than clusters, some clusters will not receive jobs\")\\n    elif len(jobs) == len(clusters):\\n        print(\"[INFO]: Exactly one job per cluster\")\\n    else:\\n        print(\"[INFO]: More jobs than clusters, jobs submitted as clusters become available.\")\\n    with ThreadPoolExecutor() as executor:\\n        for idx, job_command in enumerate(jobs):\\n            # Cycle through clusters using modulus to wrap around if there are more jobs than clusters\\n            cluster = clusters[idx % len(clusters)]\\n            executor.submit(submit_job, cluster, job_command)'),\n",
       " Document(metadata={}, page_content='class IsaacLabTuneTrainable(tune.Trainable):\\n    \"\"\"The Isaac Lab Ray Tune Trainable.\\n    This class uses the standalone workflows to start jobs, along with the hydra integration.\\n    This class achieves Ray-based logging through reading the tensorboard logs from\\n    the standalone workflows. This depends on a config generated in the format of\\n    :class:`JobCfg`\\n    \"\"\"\\n\\n    def setup(self, config: dict) -> None:\\n        \"\"\"Get the invocation command, return quick for easy scheduling.\"\"\"\\n        self.data = None\\n        self.time_since_last_proc_response = 0.0\\n        self.invoke_cmd = util.get_invocation_command_from_cfg(cfg=config, python_cmd=PYTHON_EXEC, workflow=WORKFLOW)\\n        print(f\"[INFO]: Recovered invocation with {self.invoke_cmd}\")\\n        self.experiment = None\\n\\n    def reset_config(self, new_config: dict):\\n        \"\"\"Allow environments to be re-used by fetching a new invocation command\"\"\"\\n        self.setup(new_config)\\n        return True\\n\\n    def step(self) -> dict:\\n        if self.experiment is None:  # start experiment\\n            # When including this as first step instead of setup, experiments get scheduled faster\\n            # Don\\'t want to block the scheduler while the experiment spins up\\n            print(f\"[INFO]: Invoking experiment as first step with {self.invoke_cmd}...\")\\n            try:\\n                experiment = util.execute_job(\\n                    self.invoke_cmd,\\n                    identifier_string=\"\",\\n                    extract_experiment=True,  # Keep this as True to return a valid dictionary\\n                    persistent_dir=BASE_DIR,\\n                    max_lines_to_search_logs=MAX_LINES_TO_SEARCH_EXPERIMENT_LOGS,\\n                    max_time_to_search_logs=PROCESS_RESPONSE_TIMEOUT,\\n                )\\n            except util.LogExtractionError:\\n                self.data = {\\n                    \"LOG_EXTRACTION_ERROR_STOPPER_FLAG\": True,\\n                    \"done\": True,\\n                }\\n                return self.data\\n            self.experiment = experiment\\n            print(f\"[INFO]: Tuner recovered experiment info {experiment}\")\\n            self.proc = experiment[\"proc\"]\\n            self.experiment_name = experiment[\"experiment_name\"]\\n            self.isaac_logdir = experiment[\"logdir\"]\\n            self.tensorboard_logdir = self.isaac_logdir + \"/\" + self.experiment_name\\n            self.done = False\\n\\n        if self.proc is None:\\n            raise ValueError(\"Could not start trial.\")\\n        proc_status = self.proc.poll()\\n        if proc_status is not None:  # process finished, signal finish\\n            self.data[\"done\"] = True\\n            print(f\"[INFO]: Process finished with {proc_status}, returning...\")\\n        else:  # wait until the logs are ready or fresh\\n            data = util.load_tensorboard_logs(self.tensorboard_logdir)\\n\\n            while data is None:\\n                data = util.load_tensorboard_logs(self.tensorboard_logdir)\\n                proc_status = self.proc.poll()\\n                if proc_status is not None:\\n                    break\\n                sleep(2)  # Lazy report metrics to avoid performance overhead\\n\\n            if self.data is not None:\\n                data_ = {k: v for k, v in data.items() if k != \"done\"}\\n                self_data_ = {k: v for k, v in self.data.items() if k != \"done\"}\\n                unresponsiveness_start_time = time()\\n                while util._dicts_equal(data_, self_data_):\\n                    self.time_since_last_proc_response = time() - unresponsiveness_start_time\\n                    data = util.load_tensorboard_logs(self.tensorboard_logdir)\\n                    data_ = {k: v for k, v in data.items() if k != \"done\"}\\n                    proc_status = self.proc.poll()\\n                    if proc_status is not None:\\n                        break\\n                    if self.time_since_last_proc_response > PROCESS_RESPONSE_TIMEOUT:\\n                        self.time_since_last_proc_response = 0.0\\n                        print(\"[WARNING]: Training workflow process is not responding, terminating...\")\\n                        self.proc.terminate()\\n                        try:\\n                            self.proc.wait(timeout=20)\\n                        except subprocess.TimeoutExpired:\\n                            print(\"[ERROR]: The process did not terminate within timeout duration.\")\\n                            self.proc.kill()\\n                            self.proc.wait()\\n                        self.data = data\\n                        self.data[\"done\"] = True\\n                        return self.data\\n                    sleep(2)  # Lazy report metrics to avoid performance overhead\\n\\n            self.data = data\\n            self.data[\"done\"] = False\\n        return self.data\\n\\n    def default_resource_request(self):\\n        \"\"\"How many resources each trainable uses. Assumes homogeneous resources across gpu nodes,\\n        and that each trainable is meant for one node, where it uses all available resources.\"\"\"\\n        resources = util.get_gpu_node_resources(one_node_only=True)\\n        if NUM_WORKERS_PER_NODE != 1:\\n            print(\"[WARNING]: Splitting node into more than one worker\")\\n        return tune.PlacementGroupFactory(\\n            [{\"CPU\": resources[\"CPU\"] / NUM_WORKERS_PER_NODE, \"GPU\": resources[\"GPU\"] / NUM_WORKERS_PER_NODE}],\\n            strategy=\"STRICT_PACK\",\\n        )'),\n",
       " Document(metadata={}, page_content='class LogExtractionErrorStopper(tune.Stopper):\\n    \"\"\"Stopper that stops all trials if multiple LogExtractionErrors occur.\\n\\n    Args:\\n        max_errors: The maximum number of LogExtractionErrors allowed before terminating the experiment.\\n    \"\"\"\\n\\n    def __init__(self, max_errors: int):\\n        self.max_errors = max_errors\\n        self.error_count = 0\\n\\n    def __call__(self, trial_id, result):\\n        \"\"\"Increments the error count if trial has encountered a LogExtractionError.\\n\\n        It does not stop the trial based on the metrics, always returning False.\\n        \"\"\"\\n        if result.get(\"LOG_EXTRACTION_ERROR_STOPPER_FLAG\", False):\\n            self.error_count += 1\\n            print(\\n                f\"[ERROR]: Encountered LogExtractionError {self.error_count} times. \"\\n                f\"Maximum allowed is {self.max_errors}.\"\\n            )\\n        return False\\n\\n    def stop_all(self):\\n        \"\"\"Returns true if number of LogExtractionErrors exceeds the maximum allowed, terminating the experiment.\"\"\"\\n        if self.error_count > self.max_errors:\\n            print(\"[FATAL]: Encountered LogExtractionError more than allowed, aborting entire tuning run... \")\\n            return True\\n        else:\\n            return False'),\n",
       " Document(metadata={}, page_content='def invoke_tuning_run(cfg: dict, args: argparse.Namespace) -> None:\\n    \"\"\"Invoke an Isaac-Ray tuning run.\\n\\n    Log either to a local directory or to MLFlow.\\n    Args:\\n        cfg: Configuration dictionary extracted from job setup\\n        args: Command-line arguments related to tuning.\\n    \"\"\"\\n    # Allow for early exit\\n    os.environ[\"TUNE_DISABLE_STRICT_METRIC_CHECKING\"] = \"1\"\\n\\n    print(\"[WARNING]: Not saving checkpoints, just running experiment...\")\\n    print(\"[INFO]: Model parameters and metrics will be preserved.\")\\n    print(\"[WARNING]: For homogeneous cluster resources only...\")\\n    # Get available resources\\n    resources = util.get_gpu_node_resources()\\n    print(f\"[INFO]: Available resources {resources}\")\\n\\n    if not ray.is_initialized():\\n        ray.init(\\n            address=args.ray_address,\\n            log_to_driver=True,\\n            num_gpus=len(resources),\\n        )\\n\\n    print(f\"[INFO]: Using config {cfg}\")\\n\\n    # Configure the search algorithm and the repeater\\n    searcher = OptunaSearch(\\n        metric=args.metric,\\n        mode=args.mode,\\n    )\\n    repeat_search = Repeater(searcher, repeat=args.repeat_run_count)\\n\\n    if args.run_mode == \"local\":  # Standard config, to file\\n        run_config = air.RunConfig(\\n            storage_path=\"/tmp/ray\",\\n            name=f\"IsaacRay-{args.cfg_class}-tune\",\\n            verbose=1,\\n            checkpoint_config=air.CheckpointConfig(\\n                checkpoint_frequency=0,  # Disable periodic checkpointing\\n                checkpoint_at_end=False,  # Disable final checkpoint\\n            ),\\n            stop=LogExtractionErrorStopper(max_errors=MAX_LOG_EXTRACTION_ERRORS),\\n        )\\n\\n    elif args.run_mode == \"remote\":  # MLFlow, to MLFlow server\\n        mlflow_callback = MLflowLoggerCallback(\\n            tracking_uri=args.mlflow_uri,\\n            experiment_name=f\"IsaacRay-{args.cfg_class}-tune\",\\n            save_artifact=False,\\n            tags={\"run_mode\": \"remote\", \"cfg_class\": args.cfg_class},\\n        )\\n\\n        run_config = ray.train.RunConfig(\\n            name=\"mlflow\",\\n            storage_path=\"/tmp/ray\",\\n            callbacks=[mlflow_callback],\\n            checkpoint_config=ray.train.CheckpointConfig(checkpoint_frequency=0, checkpoint_at_end=False),\\n            stop=LogExtractionErrorStopper(max_errors=MAX_LOG_EXTRACTION_ERRORS),\\n        )\\n    else:\\n        raise ValueError(\"Unrecognized run mode.\")\\n\\n    # Configure the tuning job\\n    tuner = tune.Tuner(\\n        IsaacLabTuneTrainable,\\n        param_space=cfg,\\n        tune_config=tune.TuneConfig(\\n            metric=args.metric,\\n            mode=args.mode,\\n            search_alg=repeat_search,\\n            num_samples=args.num_samples,\\n            reuse_actors=True,\\n        ),\\n        run_config=run_config,\\n    )\\n\\n    # Execute the tuning\\n    tuner.fit()\\n\\n    # Save results to mounted volume\\n    if args.run_mode == \"local\":\\n        print(\"[DONE!]: Check results with tensorboard dashboard\")\\n    else:\\n        print(\"[DONE!]: Check results with MLFlow dashboard\")'),\n",
       " Document(metadata={}, page_content='class JobCfg:\\n    \"\"\"To be compatible with :meth: invoke_tuning_run and :class:IsaacLabTuneTrainable,\\n    at a minimum, the tune job should inherit from this class.\"\"\"\\n\\n    def __init__(self, cfg: dict):\\n        \"\"\"\\n        Runner args include command line arguments passed to the task.\\n        For example:\\n        cfg[\"runner_args\"][\"headless_singleton\"] = \"--headless\"\\n        cfg[\"runner_args\"][\"enable_cameras_singleton\"] = \"--enable_cameras\"\\n        \"\"\"\\n        assert \"runner_args\" in cfg, \"No runner arguments specified.\"\\n        \"\"\"\\n        Task is the desired task to train on. For example:\\n        cfg[\"runner_args\"][\"--task\"] = tune.choice([\"Isaac-Cartpole-RGB-TheiaTiny-v0\"])\\n        \"\"\"\\n        assert \"--task\" in cfg[\"runner_args\"], \"No task specified.\"\\n        \"\"\"\\n        Hydra args define the hyperparameters varied within the sweep. For example:\\n        cfg[\"hydra_args\"][\"agent.params.network.cnn.activation\"] = tune.choice([\"relu\", \"elu\"])\\n        \"\"\"\\n        assert \"hydra_args\" in cfg, \"No hyperparameters specified.\"\\n        self.cfg = cfg'),\n",
       " Document(metadata={}, page_content='def load_tensorboard_logs(directory: str) -> dict:\\n    \"\"\"From a tensorboard directory, get the latest scalar values. If the logs can\\'t be\\n    found, check the summaries sublevel.\\n\\n    Args:\\n        directory: The directory of the tensorboard logging.\\n\\n    Returns:\\n        The latest available scalar values.\\n    \"\"\"\\n\\n    # replace any non-alnum/underscore/dot with \"_\", then collapse runs of \"_\"\\n    def replace_invalid_chars(t):\\n        t2 = re.sub(r\"[^0-9A-Za-z_./]\", \"_\", t)\\n        t2 = re.sub(r\"_+\", \"_\", t2)\\n        return t2.strip(\"_\")\\n\\n    # Initialize the event accumulator with a size guidance for only the latest entry\\n    def get_latest_scalars(path: str) -> dict:\\n        event_acc = EventAccumulator(path, size_guidance={\"scalars\": 1})\\n        try:\\n            event_acc.Reload()\\n            if event_acc.Tags()[\"scalars\"]:\\n                return {\\n                    replace_invalid_chars(tag): event_acc.Scalars(tag)[-1].value\\n                    for tag in event_acc.Tags()[\"scalars\"]\\n                    if event_acc.Scalars(tag)\\n                }\\n        except (KeyError, OSError, RuntimeError, DirectoryDeletedError):\\n            return {}\\n\\n    scalars = get_latest_scalars(directory)\\n    return scalars or get_latest_scalars(os.path.join(directory, \"summaries\"))'),\n",
       " Document(metadata={}, page_content='def get_invocation_command_from_cfg(\\n    cfg: dict,\\n    python_cmd: str = \"/workspace/isaaclab/isaaclab.sh -p\",\\n    workflow: str = \"scripts/reinforcement_learning/rl_games/train.py\",\\n) -> str:\\n    \"\"\"Generate command with proper Hydra arguments\"\"\"\\n    runner_args = []\\n    hydra_args = []\\n\\n    def process_args(args, target_list, is_hydra=False):\\n        for key, value in args.items():\\n            if not is_hydra:\\n                if key.endswith(\"_singleton\"):\\n                    target_list.append(value)\\n                elif key.startswith(\"--\"):\\n                    target_list.append(f\"{key} {value}\")  # Space instead of = for runner args\\n                else:\\n                    target_list.append(f\"{value}\")\\n            else:\\n                if isinstance(value, list):\\n                    # Check the type of the first item to determine formatting\\n                    if value and isinstance(value[0], dict):\\n                        # Handle list of dictionaries (e.g., CNN convs)\\n                        formatted_items = [f\"{{{\\',\\'.join(f\\'{k}:{v}\\' for k, v in item.items())}}}\" for item in value]\\n                    else:\\n                        # Handle list of primitives (e.g., MLP units)\\n                        formatted_items = [str(x) for x in value]\\n                    target_list.append(f\"\\'{key}=[{\\',\\'.join(formatted_items)}]\\'\")\\n                elif isinstance(value, str) and (\"{\" in value or \"}\" in value):\\n                    target_list.append(f\"\\'{key}={value}\\'\")\\n                else:\\n                    target_list.append(f\"{key}={value}\")\\n\\n    print(f\"[INFO]: Starting workflow {workflow}\")\\n    process_args(cfg[\"runner_args\"], runner_args)\\n    print(f\"[INFO]: Retrieved workflow runner args: {runner_args}\")\\n    process_args(cfg[\"hydra_args\"], hydra_args, is_hydra=True)\\n    print(f\"[INFO]: Retrieved hydra args: {hydra_args}\")\\n\\n    invoke_cmd = f\"{python_cmd} {workflow} \"\\n    invoke_cmd += \" \".join(runner_args) + \" \" + \" \".join(hydra_args)\\n    return invoke_cmd'),\n",
       " Document(metadata={}, page_content='def remote_execute_job(\\n    job_cmd: str, identifier_string: str, test_mode: bool = False, extract_experiment: bool = False\\n) -> str | dict:\\n    \"\"\"This method has an identical signature to :meth:`execute_job`, with the ray remote decorator\"\"\"\\n    return execute_job(\\n        job_cmd=job_cmd, identifier_string=identifier_string, test_mode=test_mode, extract_experiment=extract_experiment\\n    )'),\n",
       " Document(metadata={}, page_content='class LogExtractionError(Exception):\\n    \"\"\"Raised when we cannot extract experiment_name/logdir from the trainer output.\"\"\"\\n\\n    pass'),\n",
       " Document(metadata={}, page_content='def execute_job(\\n    job_cmd: str,\\n    identifier_string: str = \"job 0\",\\n    test_mode: bool = False,\\n    extract_experiment: bool = False,\\n    persistent_dir: str | None = None,\\n    log_all_output: bool = False,\\n    max_lines_to_search_logs: int = 1000,\\n    max_time_to_search_logs: float = 200.0,\\n) -> str | dict:\\n    \"\"\"Issue a job (shell command).\\n\\n    Args:\\n        job_cmd: The shell command to run.\\n        identifier_string: What prefix to add to make logs easier to differentiate\\n            across clusters or jobs. Defaults to \"job 0\".\\n        test_mode: When true, only run \\'nvidia-smi\\'. Defaults to False.\\n        extract_experiment: When true, search for experiment details from a training run. Defaults to False.\\n        persistent_dir: When supplied, change to run the directory in a persistent\\n            directory. Can be used to avoid losing logs in the /tmp directory. Defaults to None.\\n        log_all_output: When true, print all output to the console. Defaults to False.\\n        max_lines_to_search_logs: Maximum number of lines to search for experiment info. Defaults to 1000.\\n        max_time_to_search_logs: Maximum time to wait for experiment info before giving up. Defaults to 200.0 seconds.\\n    Raises:\\n        ValueError: If the job is unable to start, or throws an error. Most likely to happen\\n            due to running out of memory.\\n\\n    Returns:\\n        Relevant information from the job\\n    \"\"\"\\n    start_time = datetime.now().strftime(\"%H:%M:%S.%f\")\\n    result_details = [f\"{identifier_string}: ---------------------------------\\\\n\"]\\n    result_details.append(f\"{identifier_string}:[INFO]: Invocation {job_cmd} \\\\n\")\\n    node_id = ray.get_runtime_context().get_node_id()\\n    result_details.append(f\"{identifier_string}:[INFO]: Ray Node ID: {node_id} \\\\n\")\\n\\n    if test_mode:\\n        import torch\\n\\n        try:\\n            result = subprocess.run(\\n                [\"nvidia-smi\", \"--query-gpu=name,memory.free,serial\", \"--format=csv,noheader,nounits\"],\\n                capture_output=True,\\n                check=True,\\n                text=True,\\n            )\\n            output = result.stdout.strip().split(\"\\\\n\")\\n            for gpu_info in output:\\n                name, memory_free, serial = gpu_info.split(\", \")\\n                result_details.append(\\n                    f\"{identifier_string}[INFO]: Name: {name}|Memory Available: {memory_free} MB|Serial Number\"\\n                    f\" {serial} \\\\n\"\\n                )\\n\\n            # Get GPU count from PyTorch\\n            num_gpus_detected = torch.cuda.device_count()\\n            result_details.append(f\"{identifier_string}[INFO]: Detected GPUs from PyTorch: {num_gpus_detected} \\\\n\")\\n\\n            # Check CUDA_VISIBLE_DEVICES and count the number of visible GPUs\\n            cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\\n            if cuda_visible_devices:\\n                visible_devices_count = len(cuda_visible_devices.split(\",\"))\\n                result_details.append(\\n                    f\"{identifier_string}[INFO]: GPUs visible via CUDA_VISIBLE_DEVICES: {visible_devices_count} \\\\n\"\\n                )\\n            else:\\n                visible_devices_count = len(output)  # All GPUs visible if CUDA_VISIBLE_DEVICES is not set\\n                result_details.append(\\n                    f\"{identifier_string}[INFO]: CUDA_VISIBLE_DEVICES not set; all GPUs visible\"\\n                    f\" ({visible_devices_count}) \\\\n\"\\n                )\\n\\n            # If PyTorch GPU count disagrees with nvidia-smi, reset CUDA_VISIBLE_DEVICES and rerun detection\\n            if num_gpus_detected != len(output):\\n                result_details.append(\\n                    f\"{identifier_string}[WARNING]: PyTorch and nvidia-smi disagree on GPU count! Re-running with all\"\\n                    \" GPUs visible. \\\\n\"\\n                )\\n                result_details.append(f\"{identifier_string}[INFO]: This shows that GPU resources were isolated.\\\\n\")\\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in range(len(output))])\\n                num_gpus_detected_after_reset = torch.cuda.device_count()\\n                result_details.append(\\n                    f\"{identifier_string}[INFO]: After setting CUDA_VISIBLE_DEVICES, PyTorch detects\"\\n                    f\" {num_gpus_detected_after_reset} GPUs \\\\n\"\\n                )\\n\\n        except subprocess.CalledProcessError as e:\\n            print(f\"Error calling nvidia-smi: {e.stderr}\")\\n            result_details.append({\"error\": \"Failed to retrieve GPU information\"})\\n    else:\\n        if persistent_dir:\\n            og_dir = os.getcwd()\\n            os.chdir(persistent_dir)\\n        process = subprocess.Popen(\\n            job_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1\\n        )\\n        process_file_descriptor = process.stdout.fileno()\\n\\n        if persistent_dir:\\n            os.chdir(og_dir)\\n        experiment_name = None\\n        logdir = None\\n        experiment_info_pattern = re.compile(\"Exact experiment name requested from command line: (.+)\")\\n        logdir_pattern = re.compile(r\"\\\\[INFO\\\\] Logging experiment in directory: (.+)$\")\\n        err_pattern = re.compile(\"There was an error (.+)$\")\\n\\n        def stream_reader(stream, identifier_string, result_details):\\n            for line in iter(stream.readline, \"\"):\\n                line = line.strip()\\n                result_details.append(f\"{identifier_string}: {line}\\\\n\")\\n                if log_all_output:\\n                    print(f\"{identifier_string}: {line}\")\\n\\n        # Read stdout until we find exp. info, up to max_lines_to_search_logs lines, max_time_to_search_logs, or EOF.\\n        # Do some careful handling prevent overflowing the pipe reading buffer with error 141\\n        lines_read = 0\\n        search_duration = 0.0\\n        search_start_time = time()\\n        while True:\\n            new_line_ready, _, _ = select.select([process_file_descriptor], [], [], 1.0)  # Wait up to 1s for stdout\\n            if new_line_ready:\\n                line = process.stdout.readline()\\n                if not line:  # EOF\\n                    break\\n\\n                lines_read += 1\\n                line = line.strip()\\n                result_details.append(f\"{identifier_string}: {line} \\\\n\")\\n\\n                if log_all_output:\\n                    print(f\"{identifier_string}: {line}\")\\n\\n                if extract_experiment:\\n                    exp_match = experiment_info_pattern.search(line)\\n                    log_match = logdir_pattern.search(line)\\n                    err_match = err_pattern.search(line)\\n\\n                    if err_match:\\n                        raise ValueError(f\"Encountered an error during trial run. {\\' \\'.join(result_details)}\")\\n\\n                    if exp_match:\\n                        experiment_name = exp_match.group(1)\\n                    if log_match:\\n                        logdir = log_match.group(1)\\n\\n                    if experiment_name and logdir:\\n                        # Start stderr reader after finding experiment info\\n                        stderr_thread = threading.Thread(\\n                            target=stream_reader, args=(process.stderr, identifier_string, result_details)\\n                        )\\n                        stderr_thread.daemon = True\\n                        stderr_thread.start()\\n\\n                        # Start stdout reader to continue reading to flush buffer\\n                        stdout_thread = threading.Thread(\\n                            target=stream_reader, args=(process.stdout, identifier_string, result_details)\\n                        )\\n                        stdout_thread.daemon = True\\n                        stdout_thread.start()\\n\\n                        return {\\n                            \"experiment_name\": experiment_name,\\n                            \"logdir\": logdir,\\n                            \"proc\": process,\\n                            \"result\": \" \".join(result_details),\\n                        }\\n\\n            if extract_experiment:  # if we are looking for experiment info, check for timeouts and line limits\\n                search_duration = time() - search_start_time\\n                if search_duration > max_time_to_search_logs:\\n                    print(f\"[ERROR]: Could not find experiment logs within {max_time_to_search_logs} seconds.\")\\n                    break\\n                if lines_read >= max_lines_to_search_logs:\\n                    print(f\"[ERROR]: Could not find experiment logs within first {max_lines_to_search_logs} lines.\")\\n                    break\\n\\n        # If we reach here, we didn\\'t find experiment info in the output\\n        if extract_experiment and not (experiment_name and logdir):\\n            error_msg = (\\n                \"Could not extract experiment_name/logdir from trainer output \"\\n                f\"(experiment_name={experiment_name!r}, logdir={logdir!r}).\\\\n\"\\n                \"\\\\tMake sure your training script prints the following correctly:\\\\n\"\\n                \"\\\\t\\\\tExact experiment name requested from command line: <name>\\\\n\"\\n                \"\\\\t\\\\t[INFO] Logging experiment in directory: <logdir>\\\\n\\\\n\"\\n            )\\n            print(f\"[ERROR]: {error_msg}\")\\n            raise LogExtractionError(\"Could not extract experiment_name/logdir from training workflow output.\")\\n        process.wait()\\n        now = datetime.now().strftime(\"%H:%M:%S.%f\")\\n        completion_info = f\"\\\\n[INFO]: {identifier_string}: Job Started at {start_time}, completed at {now}\\\\n\"\\n        print(completion_info)\\n        result_details.append(completion_info)\\n        return \" \".join(result_details)'),\n",
       " Document(metadata={}, page_content='def get_gpu_node_resources(\\n    total_resources: bool = False,\\n    one_node_only: bool = False,\\n    include_gb_ram: bool = False,\\n    include_id: bool = False,\\n    ray_address: str = \"auto\",\\n) -> list[dict] | dict:\\n    \"\"\"Get information about available GPU node resources.\\n\\n    Args:\\n        total_resources: When true, return total available resources. Defaults to False.\\n        one_node_only: When true, return resources for a single node. Defaults to False.\\n        include_gb_ram: Set to true to convert MB to GB in result\\n        include_id: Set to true to include node ID\\n        ray_address: The ray address to connect to.\\n\\n    Returns:\\n        Resource information for all nodes, sorted by descending GPU count, then descending CPU\\n        count, then descending RAM capacity, and finally by node ID in ascending order if available,\\n        or simply the resource for a single node if requested.\\n    \"\"\"\\n    if not ray.is_initialized():\\n        ray.init(address=ray_address)\\n\\n    nodes = ray.nodes()\\n    node_resources = []\\n    total_cpus = 0\\n    total_gpus = 0\\n    total_memory = 0  # in bytes\\n\\n    for node in nodes:\\n        if node[\"Alive\"] and \"GPU\" in node[\"Resources\"]:\\n            node_id = node[\"NodeID\"]\\n            resources = node[\"Resources\"]\\n            cpus = resources.get(\"CPU\", 0)\\n            gpus = resources.get(\"GPU\", 0)\\n            memory = resources.get(\"memory\", 0)\\n            node_resources.append({\"CPU\": cpus, \"GPU\": gpus, \"memory\": memory})\\n\\n            if include_id:\\n                node_resources[-1][\"id\"] = node_id\\n            if include_gb_ram:\\n                node_resources[-1][\"ram_gb\"] = memory / 1024**3\\n\\n            total_cpus += cpus\\n            total_gpus += gpus\\n            total_memory += memory\\n    node_resources = sorted(node_resources, key=lambda x: (-x[\"GPU\"], -x[\"CPU\"], -x[\"memory\"], x.get(\"id\", \"\")))\\n\\n    if total_resources:\\n        # Return summed total resources\\n        return {\"CPU\": total_cpus, \"GPU\": total_gpus, \"memory\": total_memory}\\n\\n    if one_node_only and node_resources:\\n        return node_resources[0]\\n\\n    return node_resources'),\n",
       " Document(metadata={}, page_content='def add_resource_arguments(\\n    arg_parser: argparse.ArgumentParser,\\n    defaults: list | None = None,\\n    cluster_create_defaults: bool = False,\\n) -> argparse.ArgumentParser:\\n    \"\"\"Add resource arguments to a cluster; this is shared across both\\n    wrapping resources and launching clusters.\\n\\n    Args:\\n        arg_parser: the argparser to add the arguments to. This argparser is mutated.\\n        defaults: The default values for GPUs, CPUs, RAM, and Num Workers\\n        cluster_create_defaults: Set to true to populate reasonable defaults for creating clusters.\\n    Returns:\\n        The argparser with the standard resource arguments.\\n    \"\"\"\\n    if defaults is None:\\n        if cluster_create_defaults:\\n            defaults = [[1], [8], [16], [1]]\\n        else:\\n            defaults = [None, None, None, [1]]\\n    arg_parser.add_argument(\\n        \"--gpu_per_worker\",\\n        nargs=\"+\",\\n        type=int,\\n        default=defaults[0],\\n        help=\"Number of GPUs per worker node. Supply more than one for heterogeneous resources\",\\n    )\\n    arg_parser.add_argument(\\n        \"--cpu_per_worker\",\\n        nargs=\"+\",\\n        type=int,\\n        default=defaults[1],\\n        help=\"Number of CPUs per worker node. Supply more than one for heterogeneous resources\",\\n    )\\n    arg_parser.add_argument(\\n        \"--ram_gb_per_worker\",\\n        nargs=\"+\",\\n        type=int,\\n        default=defaults[2],\\n        help=\"RAM in GB per worker node. Supply more than one for heterogeneous resources.\",\\n    )\\n    arg_parser.add_argument(\\n        \"--num_workers\",\\n        nargs=\"+\",\\n        type=int,\\n        default=defaults[3],\\n        help=\"Number of desired workers. Supply more than one for heterogeneous resources.\",\\n    )\\n    return arg_parser'),\n",
       " Document(metadata={}, page_content='def fill_in_missing_resources(\\n    args: argparse.Namespace, resources: dict | None = None, cluster_creation_flag: bool = False, policy: callable = max\\n):\\n    \"\"\"Normalize the lengths of resource lists based on the longest list provided.\"\"\"\\n    print(\"[INFO]: Filling in missing command line arguments with best guess...\")\\n    if resources is None:\\n        resources = {\\n            \"gpu_per_worker\": args.gpu_per_worker,\\n            \"cpu_per_worker\": args.cpu_per_worker,\\n            \"ram_gb_per_worker\": args.ram_gb_per_worker,\\n            \"num_workers\": args.num_workers,\\n        }\\n        if cluster_creation_flag:\\n            cluster_creation_resources = {\"worker_accelerator\": args.worker_accelerator}\\n            resources.update(cluster_creation_resources)\\n\\n    # Calculate the maximum length of any list\\n    max_length = max(len(v) for v in resources.values())\\n    print(\"[INFO]: Resource list lengths:\")\\n    for key, value in resources.items():\\n        print(f\"[INFO] {key}: {len(value)} values {value}\")\\n\\n    # Extend each list to match the maximum length using the maximum value in each list\\n    for key, value in resources.items():\\n        potential_value = getattr(args, key)\\n        if potential_value is not None:\\n            max_value = policy(policy(value), policy(potential_value))\\n        else:\\n            max_value = policy(value)\\n        extension_length = max_length - len(value)\\n        if extension_length > 0:  # Only extend if the current list is shorter than max_length\\n            print(f\"\\\\n[WARNING]: Resource \\'{key}\\' needs extension:\")\\n            print(f\"[INFO] Current length: {len(value)}\")\\n            print(f\"[INFO] Target length: {max_length}\")\\n            print(f\"[INFO] Filling in {extension_length} missing values with {max_value}\")\\n            print(f\"[INFO] To avoid auto-filling, provide {extension_length} more {key} value(s)\")\\n            value.extend([max_value] * extension_length)\\n        setattr(args, key, value)\\n        resources[key] = value\\n        print(f\"[INFO] Final {key} values: {getattr(args, key)}\")\\n    print(\"[INFO]: Done filling in command line arguments...\\\\n\\\\n\")\\n    return args'),\n",
       " Document(metadata={}, page_content='def populate_isaac_ray_cfg_args(cfg: dict = {}) -> dict:\\n    \"\"\"Small utility method to create empty fields if needed for a configuration.\"\"\"\\n    if \"runner_args\" not in cfg:\\n        cfg[\"runner_args\"] = {}\\n    if \"hydra_args\" not in cfg:\\n        cfg[\"hydra_args\"] = {}\\n    return cfg'),\n",
       " Document(metadata={}, page_content='def _dicts_equal(d1: dict, d2: dict, tol=1e-9) -> bool:\\n    \"\"\"Check if two dicts are equal; helps ensure only new logs are returned.\"\"\"\\n    if d1.keys() != d2.keys():\\n        return False\\n    for key in d1:\\n        if isinstance(d1[key], float) and isinstance(d2[key], float):\\n            if not isclose(d1[key], d2[key], abs_tol=tol):\\n                return False\\n        elif d1[key] != d2[key]:\\n            return False\\n    return True'),\n",
       " Document(metadata={}, page_content='def wrap_resources_to_jobs(jobs: list[str], args: argparse.Namespace) -> None:\\n    \"\"\"\\n    Provided a list of jobs, dispatch jobs to one worker per available node,\\n    unless otherwise specified by resource constraints.\\n\\n    Args:\\n        jobs: bash commands to execute on a Ray cluster\\n        args: The arguments for resource allocation\\n\\n    \"\"\"\\n    if not ray.is_initialized():\\n        ray.init(address=args.ray_address, log_to_driver=True)\\n    job_results = []\\n    gpu_node_resources = util.get_gpu_node_resources(include_id=True, include_gb_ram=True)\\n\\n    if any([args.gpu_per_worker, args.cpu_per_worker, args.ram_gb_per_worker]) and args.num_workers:\\n        raise ValueError(\"Either specify only num_workers or only granular resources(GPU,CPU,RAM_GB).\")\\n\\n    num_nodes = len(gpu_node_resources)\\n    # Populate arguments\\n    formatted_node_resources = {\\n        \"gpu_per_worker\": [gpu_node_resources[i][\"GPU\"] for i in range(num_nodes)],\\n        \"cpu_per_worker\": [gpu_node_resources[i][\"CPU\"] for i in range(num_nodes)],\\n        \"ram_gb_per_worker\": [gpu_node_resources[i][\"ram_gb\"] for i in range(num_nodes)],\\n        \"num_workers\": args.num_workers,  # By default, 1 worker por node\\n    }\\n    args = util.fill_in_missing_resources(args, resources=formatted_node_resources, policy=min)\\n    print(f\"[INFO]: Number of GPU nodes found: {num_nodes}\")\\n    if args.test:\\n        jobs = [\"nvidia-smi\"] * num_nodes\\n    for i, job in enumerate(jobs):\\n        gpu_node = gpu_node_resources[i % num_nodes]\\n        print(f\"[INFO]: Submitting job {i + 1} of {len(jobs)} with job \\'{job}\\' to node {gpu_node}\")\\n        print(\\n            f\"[INFO]: Resource parameters: GPU: {args.gpu_per_worker[i]}\"\\n            f\" CPU: {args.cpu_per_worker[i]} RAM {args.ram_gb_per_worker[i]}\"\\n        )\\n        print(f\"[INFO] For the node parameters, creating {args.num_workers[i]} workers\")\\n        num_gpus = args.gpu_per_worker[i] / args.num_workers[i]\\n        num_cpus = args.cpu_per_worker[i] / args.num_workers[i]\\n        memory = (args.ram_gb_per_worker[i] * 1024**3) / args.num_workers[i]\\n        print(f\"[INFO]: Requesting {num_gpus=} {num_cpus=} {memory=} id={gpu_node[\\'id\\']}\")\\n        job = util.remote_execute_job.options(\\n            num_gpus=num_gpus,\\n            num_cpus=num_cpus,\\n            memory=memory,\\n            scheduling_strategy=NodeAffinitySchedulingStrategy(gpu_node[\"id\"], soft=False),\\n        ).remote(job, f\"Job {i}\", args.test)\\n        job_results.append(job)\\n\\n    results = ray.get(job_results)\\n    for i, result in enumerate(results):\\n        print(f\"[INFO]: Job {i} result: {result}\")\\n    print(\"[INFO]: All jobs completed.\")'),\n",
       " Document(metadata={}, page_content='class CartpoleRGBNoTuneJobCfg(vision_cfg.CameraJobCfg):\\n    def __init__(self, cfg: dict = {}):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n        cfg[\"runner_args\"][\"--task\"] = tune.choice([\"Isaac-Cartpole-RGB-v0\"])\\n        super().__init__(cfg, vary_env_count=False, vary_cnn=False, vary_mlp=False)'),\n",
       " Document(metadata={}, page_content='class CartpoleRGBCNNOnlyJobCfg(vision_cfg.CameraJobCfg):\\n    def __init__(self, cfg: dict = {}):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n        cfg[\"runner_args\"][\"--task\"] = tune.choice([\"Isaac-Cartpole-RGB-v0\"])\\n        super().__init__(cfg, vary_env_count=False, vary_cnn=True, vary_mlp=False)'),\n",
       " Document(metadata={}, page_content='class CartpoleRGBJobCfg(vision_cfg.CameraJobCfg):\\n    def __init__(self, cfg: dict = {}):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n        cfg[\"runner_args\"][\"--task\"] = tune.choice([\"Isaac-Cartpole-RGB-v0\"])\\n        super().__init__(cfg, vary_env_count=True, vary_cnn=True, vary_mlp=True)'),\n",
       " Document(metadata={}, page_content='class CartpoleResNetJobCfg(vision_cfg.ResNetCameraJob):\\n    def __init__(self, cfg: dict = {}):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n        cfg[\"runner_args\"][\"--task\"] = tune.choice([\"Isaac-Cartpole-RGB-ResNet18-v0\"])\\n        super().__init__(cfg)'),\n",
       " Document(metadata={}, page_content='class CartpoleTheiaJobCfg(vision_cfg.TheiaCameraJob):\\n    def __init__(self, cfg: dict = {}):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n        cfg[\"runner_args\"][\"--task\"] = tune.choice([\"Isaac-Cartpole-RGB-TheiaTiny-v0\"])\\n        super().__init__(cfg)'),\n",
       " Document(metadata={}, page_content='class CameraJobCfg(tuner.JobCfg):\\n    \"\"\"In order to be compatible with :meth: invoke_tuning_run, and\\n    :class:IsaacLabTuneTrainable , configurations should\\n    be in a similar format to this class. This class can vary env count/horizon length,\\n    CNN structure, and MLP structure. Broad possible ranges are set, the specific values\\n    that work can be found via tuning. Tuning results can inform better ranges for a second tuning run.\\n    These ranges were selected for demonstration purposes. Best ranges are run/task specific.\"\"\"\\n\\n    @staticmethod\\n    def _get_batch_size_divisors(batch_size: int, min_size: int = 128) -> list[int]:\\n        \"\"\"Get valid batch divisors to combine with num_envs and horizon length\"\"\"\\n        divisors = [i for i in range(min_size, batch_size + 1) if batch_size % i == 0]\\n        return divisors if divisors else [min_size]\\n\\n    def __init__(self, cfg={}, vary_env_count: bool = False, vary_cnn: bool = False, vary_mlp: bool = False):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n\\n        # Basic configuration\\n        cfg[\"runner_args\"][\"headless_singleton\"] = \"--headless\"\\n        cfg[\"runner_args\"][\"enable_cameras_singleton\"] = \"--enable_cameras\"\\n        cfg[\"hydra_args\"][\"agent.params.config.max_epochs\"] = 200\\n\\n        if vary_env_count:  # Vary the env count, and horizon length, and select a compatible mini-batch size\\n            # Check from 512 to 8196 envs in powers of 2\\n            # check horizon lengths of 8 to 256\\n            # More envs should be better, but different batch sizes can improve gradient estimation\\n            env_counts = [2**x for x in range(9, 13)]\\n            horizon_lengths = [2**x for x in range(3, 8)]\\n\\n            selected_env_count = tune.choice(env_counts)\\n            selected_horizon = tune.choice(horizon_lengths)\\n\\n            cfg[\"runner_args\"][\"--num_envs\"] = selected_env_count\\n            cfg[\"hydra_args\"][\"agent.params.config.horizon_length\"] = selected_horizon\\n\\n            def get_valid_batch_size(config):\\n                num_envs = config[\"runner_args\"][\"--num_envs\"]\\n                horizon_length = config[\"hydra_args\"][\"agent.params.config.horizon_length\"]\\n                total_batch = horizon_length * num_envs\\n                divisors = self._get_batch_size_divisors(total_batch)\\n                return divisors[0]\\n\\n            cfg[\"hydra_args\"][\"agent.params.config.minibatch_size\"] = tune.sample_from(get_valid_batch_size)\\n\\n        if vary_cnn:  # Vary the depth, and size of the layers in the CNN part of the agent\\n            # Also varies kernel size, and stride.\\n            num_layers = tune.randint(2, 3)\\n            cfg[\"hydra_args\"][\"agent.params.network.cnn.type\"] = \"conv2d\"\\n            cfg[\"hydra_args\"][\"agent.params.network.cnn.activation\"] = tune.choice([\"relu\", \"elu\"])\\n            cfg[\"hydra_args\"][\"agent.params.network.cnn.initializer\"] = \"{name:default}\"\\n            cfg[\"hydra_args\"][\"agent.params.network.cnn.regularizer\"] = \"{name:None}\"\\n\\n            def get_cnn_layers(_):\\n                layers = []\\n                size = 64  # Initial input size\\n\\n                for _ in range(num_layers.sample()):\\n                    # Get valid kernel sizes for current size\\n                    valid_kernels = [k for k in [3, 4, 6, 8, 10, 12] if k <= size]\\n                    if not valid_kernels:\\n                        break\\n\\n                    kernel = int(tune.choice([str(k) for k in valid_kernels]).sample())\\n                    stride = int(tune.choice([\"1\", \"2\", \"3\", \"4\"]).sample())\\n                    padding = int(tune.choice([\"0\", \"1\"]).sample())\\n\\n                    # Calculate next size\\n                    next_size = ((size + 2 * padding - kernel) // stride) + 1\\n                    if next_size <= 0:\\n                        break\\n\\n                    layers.append({\\n                        \"filters\": tune.randint(16, 32).sample(),\\n                        \"kernel_size\": str(kernel),\\n                        \"strides\": str(stride),\\n                        \"padding\": str(padding),\\n                    })\\n                    size = next_size\\n\\n                return layers\\n\\n            cfg[\"hydra_args\"][\"agent.params.network.cnn.convs\"] = tune.sample_from(get_cnn_layers)\\n\\n        if vary_mlp:  # Vary the MLP structure; neurons (units) per layer, number of layers,\\n\\n            max_num_layers = 6\\n            max_neurons_per_layer = 128\\n            if \"env.observations.policy.image.params.model_name\" in cfg[\"hydra_args\"]:\\n                # By decreasing MLP size when using pretrained helps prevent out of memory on L4\\n                max_num_layers = 3\\n                max_neurons_per_layer = 32\\n            if \"agent.params.network.cnn.convs\" in cfg[\"hydra_args\"]:\\n                # decrease MLP size to prevent running out of memory on L4\\n                max_num_layers = 2\\n                max_neurons_per_layer = 32\\n\\n            num_layers = tune.randint(1, max_num_layers)\\n\\n            def get_mlp_layers(_):\\n                return [tune.randint(4, max_neurons_per_layer).sample() for _ in range(num_layers.sample())]\\n\\n            cfg[\"hydra_args\"][\"agent.params.network.mlp.units\"] = tune.sample_from(get_mlp_layers)\\n            cfg[\"hydra_args\"][\"agent.params.network.mlp.initializer.name\"] = tune.choice([\"default\"]).sample()\\n            cfg[\"hydra_args\"][\"agent.params.network.mlp.activation\"] = tune.choice(\\n                [\"relu\", \"tanh\", \"sigmoid\", \"elu\"]\\n            ).sample()\\n\\n        super().__init__(cfg)'),\n",
       " Document(metadata={}, page_content='class ResNetCameraJob(CameraJobCfg):\\n    \"\"\"Try different ResNet sizes.\"\"\"\\n\\n    def __init__(self, cfg: dict = {}):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n        cfg[\"hydra_args\"][\"env.observations.policy.image.params.model_name\"] = tune.choice(\\n            [\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\"]\\n        )\\n        super().__init__(cfg, vary_env_count=True, vary_cnn=False, vary_mlp=True)'),\n",
       " Document(metadata={}, page_content='class TheiaCameraJob(CameraJobCfg):\\n    \"\"\"Try different Theia sizes.\"\"\"\\n\\n    def __init__(self, cfg: dict = {}):\\n        cfg = util.populate_isaac_ray_cfg_args(cfg)\\n        cfg[\"hydra_args\"][\"env.observations.policy.image.params.model_name\"] = tune.choice([\\n            \"theia-tiny-patch16-224-cddsv\",\\n            \"theia-tiny-patch16-224-cdiv\",\\n            \"theia-small-patch16-224-cdiv\",\\n            \"theia-base-patch16-224-cdiv\",\\n            \"theia-small-patch16-224-cddsv\",\\n            \"theia-base-patch16-224-cddsv\",\\n        ])\\n        super().__init__(cfg, vary_env_count=True, vary_cnn=False, vary_mlp=True)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Play with RL-Games agent.\"\"\"\\n    # parse env configuration\\n    env_cfg = parse_env_cfg(\\n        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric\\n    )\\n    agent_cfg = load_cfg_from_registry(args_cli.task, \"rl_games_cfg_entry_point\")\\n\\n    # specify directory for logging experiments\\n    log_root_path = os.path.join(\"logs\", \"rl_games\", agent_cfg[\"params\"][\"config\"][\"name\"])\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Loading experiment from directory: {log_root_path}\")\\n    # find checkpoint\\n    if args_cli.use_pretrained_checkpoint:\\n        resume_path = get_published_pretrained_checkpoint(\"rl_games\", args_cli.task)\\n        if not resume_path:\\n            print(\"[INFO] Unfortunately a pre-trained checkpoint is currently unavailable for this task.\")\\n            return\\n    elif args_cli.checkpoint is None:\\n        # specify directory for logging runs\\n        run_dir = agent_cfg[\"params\"][\"config\"].get(\"full_experiment_name\", \".*\")\\n        # specify name of checkpoint\\n        if args_cli.use_last_checkpoint:\\n            checkpoint_file = \".*\"\\n        else:\\n            # this loads the best checkpoint\\n            checkpoint_file = f\"{agent_cfg[\\'params\\'][\\'config\\'][\\'name\\']}.pth\"\\n        # get path to previous checkpoint\\n        resume_path = get_checkpoint_path(log_root_path, run_dir, checkpoint_file, other_dirs=[\"nn\"])\\n    else:\\n        resume_path = retrieve_file_path(args_cli.checkpoint)\\n    log_dir = os.path.dirname(os.path.dirname(resume_path))\\n\\n    # wrap around environment for rl-games\\n    rl_device = agent_cfg[\"params\"][\"config\"][\"device\"]\\n    clip_obs = agent_cfg[\"params\"][\"env\"].get(\"clip_observations\", math.inf)\\n    clip_actions = agent_cfg[\"params\"][\"env\"].get(\"clip_actions\", math.inf)\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv):\\n        env = multi_agent_to_single_agent(env)\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_root_path, log_dir, \"videos\", \"play\"),\\n            \"step_trigger\": lambda step: step == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for rl-games\\n    env = RlGamesVecEnvWrapper(env, rl_device, clip_obs, clip_actions)\\n\\n    # register the environment to rl-games registry\\n    # note: in agents configuration: environment name must be \"rlgpu\"\\n    vecenv.register(\\n        \"IsaacRlgWrapper\", lambda config_name, num_actors, **kwargs: RlGamesGpuEnv(config_name, num_actors, **kwargs)\\n    )\\n    env_configurations.register(\"rlgpu\", {\"vecenv_type\": \"IsaacRlgWrapper\", \"env_creator\": lambda **kwargs: env})\\n\\n    # load previously trained model\\n    agent_cfg[\"params\"][\"load_checkpoint\"] = True\\n    agent_cfg[\"params\"][\"load_path\"] = resume_path\\n    print(f\"[INFO]: Loading model checkpoint from: {agent_cfg[\\'params\\'][\\'load_path\\']}\")\\n\\n    # set number of actors into agent config\\n    agent_cfg[\"params\"][\"config\"][\"num_actors\"] = env.unwrapped.num_envs\\n    # create runner from rl-games\\n    runner = Runner()\\n    runner.load(agent_cfg)\\n    # obtain the agent from the runner\\n    agent: BasePlayer = runner.create_player()\\n    agent.restore(resume_path)\\n    agent.reset()\\n\\n    dt = env.unwrapped.step_dt\\n\\n    # reset environment\\n    obs = env.reset()\\n    if isinstance(obs, dict):\\n        obs = obs[\"obs\"]\\n    timestep = 0\\n    # required: enables the flag for batched observations\\n    _ = agent.get_batch_size(obs, 1)\\n    # initialize RNN states if used\\n    if agent.is_rnn:\\n        agent.init_rnn()\\n    # simulate environment\\n    # note: We simplified the logic in rl-games player.py (:func:`BasePlayer.run()`) function in an\\n    #   attempt to have complete control over environment stepping. However, this removes other\\n    #   operations such as masking that is used for multi-agent learning by RL-Games.\\n    while simulation_app.is_running():\\n        start_time = time.time()\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # convert obs to agent format\\n            obs = agent.obs_to_torch(obs)\\n            # agent stepping\\n            actions = agent.get_action(obs, is_deterministic=agent.is_deterministic)\\n            # env stepping\\n            obs, _, dones, _ = env.step(actions)\\n\\n            # perform operations for terminated episodes\\n            if len(dones) > 0:\\n                # reset rnn state for terminated episodes\\n                if agent.is_rnn and agent.states is not None:\\n                    for s in agent.states:\\n                        s[:, dones, :] = 0.0\\n        if args_cli.video:\\n            timestep += 1\\n            # Exit the play loop after recording one video\\n            if timestep == args_cli.video_length:\\n                break\\n\\n        # time delay for real-time evaluation\\n        sleep_time = dt - (time.time() - start_time)\\n        if args_cli.real_time and sleep_time > 0:\\n            time.sleep(sleep_time)\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: dict):\\n    \"\"\"Train with RL-Games agent.\"\"\"\\n    # override configurations with non-hydra CLI arguments\\n    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\\n    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\\n\\n    # randomly sample a seed if seed = -1\\n    if args_cli.seed == -1:\\n        args_cli.seed = random.randint(0, 10000)\\n\\n    agent_cfg[\"params\"][\"seed\"] = args_cli.seed if args_cli.seed is not None else agent_cfg[\"params\"][\"seed\"]\\n    agent_cfg[\"params\"][\"config\"][\"max_epochs\"] = (\\n        args_cli.max_iterations if args_cli.max_iterations is not None else agent_cfg[\"params\"][\"config\"][\"max_epochs\"]\\n    )\\n    if args_cli.checkpoint is not None:\\n        resume_path = retrieve_file_path(args_cli.checkpoint)\\n        agent_cfg[\"params\"][\"load_checkpoint\"] = True\\n        agent_cfg[\"params\"][\"load_path\"] = resume_path\\n        print(f\"[INFO]: Loading model checkpoint from: {agent_cfg[\\'params\\'][\\'load_path\\']}\")\\n    train_sigma = float(args_cli.sigma) if args_cli.sigma is not None else None\\n\\n    # multi-gpu training config\\n    if args_cli.distributed:\\n        agent_cfg[\"params\"][\"seed\"] += app_launcher.global_rank\\n        agent_cfg[\"params\"][\"config\"][\"device\"] = f\"cuda:{app_launcher.local_rank}\"\\n        agent_cfg[\"params\"][\"config\"][\"device_name\"] = f\"cuda:{app_launcher.local_rank}\"\\n        agent_cfg[\"params\"][\"config\"][\"multi_gpu\"] = True\\n        # update env config device\\n        env_cfg.sim.device = f\"cuda:{app_launcher.local_rank}\"\\n\\n    # set the environment seed (after multi-gpu config for updated rank from agent seed)\\n    # note: certain randomizations occur in the environment initialization so we set the seed here\\n    env_cfg.seed = agent_cfg[\"params\"][\"seed\"]\\n\\n    # specify directory for logging experiments\\n    log_root_path = os.path.join(\"logs\", \"rl_games\", agent_cfg[\"params\"][\"config\"][\"name\"])\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\\n    # specify directory for logging runs\\n    log_dir = agent_cfg[\"params\"][\"config\"].get(\"full_experiment_name\", datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\\n    # set directory into agent config\\n    # logging directory path: <train_dir>/<full_experiment_name>\\n    agent_cfg[\"params\"][\"config\"][\"train_dir\"] = log_root_path\\n    agent_cfg[\"params\"][\"config\"][\"full_experiment_name\"] = log_dir\\n\\n    # dump the configuration into log-directory\\n    dump_yaml(os.path.join(log_root_path, log_dir, \"params\", \"env.yaml\"), env_cfg)\\n    dump_yaml(os.path.join(log_root_path, log_dir, \"params\", \"agent.yaml\"), agent_cfg)\\n    dump_pickle(os.path.join(log_root_path, log_dir, \"params\", \"env.pkl\"), env_cfg)\\n    dump_pickle(os.path.join(log_root_path, log_dir, \"params\", \"agent.pkl\"), agent_cfg)\\n\\n    # read configurations about the agent-training\\n    rl_device = agent_cfg[\"params\"][\"config\"][\"device\"]\\n    clip_obs = agent_cfg[\"params\"][\"env\"].get(\"clip_observations\", math.inf)\\n    clip_actions = agent_cfg[\"params\"][\"env\"].get(\"clip_actions\", math.inf)\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv):\\n        env = multi_agent_to_single_agent(env)\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_root_path, log_dir, \"videos\", \"train\"),\\n            \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for rl-games\\n    env = RlGamesVecEnvWrapper(env, rl_device, clip_obs, clip_actions)\\n\\n    # register the environment to rl-games registry\\n    # note: in agents configuration: environment name must be \"rlgpu\"\\n    vecenv.register(\\n        \"IsaacRlgWrapper\", lambda config_name, num_actors, **kwargs: RlGamesGpuEnv(config_name, num_actors, **kwargs)\\n    )\\n    env_configurations.register(\"rlgpu\", {\"vecenv_type\": \"IsaacRlgWrapper\", \"env_creator\": lambda **kwargs: env})\\n\\n    # set number of actors into agent config\\n    agent_cfg[\"params\"][\"config\"][\"num_actors\"] = env.unwrapped.num_envs\\n    # create runner from rl-games\\n    runner = Runner(IsaacAlgoObserver())\\n    runner.load(agent_cfg)\\n\\n    # reset the agent and env\\n    runner.reset()\\n    # train the agent\\n    if args_cli.checkpoint is not None:\\n        runner.run({\"train\": True, \"play\": False, \"sigma\": train_sigma, \"checkpoint\": resume_path})\\n    else:\\n        runner.run({\"train\": True, \"play\": False, \"sigma\": train_sigma})\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def add_rsl_rl_args(parser: argparse.ArgumentParser):\\n    \"\"\"Add RSL-RL arguments to the parser.\\n\\n    Args:\\n        parser: The parser to add the arguments to.\\n    \"\"\"\\n    # create a new argument group\\n    arg_group = parser.add_argument_group(\"rsl_rl\", description=\"Arguments for RSL-RL agent.\")\\n    # -- experiment arguments\\n    arg_group.add_argument(\\n        \"--experiment_name\", type=str, default=None, help=\"Name of the experiment folder where logs will be stored.\"\\n    )\\n    arg_group.add_argument(\"--run_name\", type=str, default=None, help=\"Run name suffix to the log directory.\")\\n    # -- load arguments\\n    arg_group.add_argument(\"--resume\", action=\"store_true\", default=False, help=\"Whether to resume from a checkpoint.\")\\n    arg_group.add_argument(\"--load_run\", type=str, default=None, help=\"Name of the run folder to resume from.\")\\n    arg_group.add_argument(\"--checkpoint\", type=str, default=None, help=\"Checkpoint file to resume from.\")\\n    # -- logger arguments\\n    arg_group.add_argument(\\n        \"--logger\", type=str, default=None, choices={\"wandb\", \"tensorboard\", \"neptune\"}, help=\"Logger module to use.\"\\n    )\\n    arg_group.add_argument(\\n        \"--log_project_name\", type=str, default=None, help=\"Name of the logging project when using wandb or neptune.\"\\n    )'),\n",
       " Document(metadata={}, page_content='def parse_rsl_rl_cfg(task_name: str, args_cli: argparse.Namespace) -> RslRlOnPolicyRunnerCfg:\\n    \"\"\"Parse configuration for RSL-RL agent based on inputs.\\n\\n    Args:\\n        task_name: The name of the environment.\\n        args_cli: The command line arguments.\\n\\n    Returns:\\n        The parsed configuration for RSL-RL agent based on inputs.\\n    \"\"\"\\n    from isaaclab_tasks.utils.parse_cfg import load_cfg_from_registry\\n\\n    # load the default configuration\\n    rslrl_cfg: RslRlOnPolicyRunnerCfg = load_cfg_from_registry(task_name, \"rsl_rl_cfg_entry_point\")\\n    rslrl_cfg = update_rsl_rl_cfg(rslrl_cfg, args_cli)\\n    return rslrl_cfg'),\n",
       " Document(metadata={}, page_content='def update_rsl_rl_cfg(agent_cfg: RslRlOnPolicyRunnerCfg, args_cli: argparse.Namespace):\\n    \"\"\"Update configuration for RSL-RL agent based on inputs.\\n\\n    Args:\\n        agent_cfg: The configuration for RSL-RL agent.\\n        args_cli: The command line arguments.\\n\\n    Returns:\\n        The updated configuration for RSL-RL agent based on inputs.\\n    \"\"\"\\n    # override the default configuration with CLI arguments\\n    if hasattr(args_cli, \"seed\") and args_cli.seed is not None:\\n        # randomly sample a seed if seed = -1\\n        if args_cli.seed == -1:\\n            args_cli.seed = random.randint(0, 10000)\\n        agent_cfg.seed = args_cli.seed\\n    if args_cli.resume is not None:\\n        agent_cfg.resume = args_cli.resume\\n    if args_cli.load_run is not None:\\n        agent_cfg.load_run = args_cli.load_run\\n    if args_cli.checkpoint is not None:\\n        agent_cfg.load_checkpoint = args_cli.checkpoint\\n    if args_cli.run_name is not None:\\n        agent_cfg.run_name = args_cli.run_name\\n    if args_cli.logger is not None:\\n        agent_cfg.logger = args_cli.logger\\n    # set the project name for wandb and neptune\\n    if agent_cfg.logger in {\"wandb\", \"neptune\"} and args_cli.log_project_name:\\n        agent_cfg.wandb_project = args_cli.log_project_name\\n        agent_cfg.neptune_project = args_cli.log_project_name\\n\\n    return agent_cfg'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Play with RSL-RL agent.\"\"\"\\n    # parse configuration\\n    env_cfg = parse_env_cfg(\\n        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric\\n    )\\n    agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(args_cli.task, args_cli)\\n\\n    # specify directory for logging experiments\\n    log_root_path = os.path.join(\"logs\", \"rsl_rl\", agent_cfg.experiment_name)\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Loading experiment from directory: {log_root_path}\")\\n    if args_cli.use_pretrained_checkpoint:\\n        resume_path = get_published_pretrained_checkpoint(\"rsl_rl\", args_cli.task)\\n        if not resume_path:\\n            print(\"[INFO] Unfortunately a pre-trained checkpoint is currently unavailable for this task.\")\\n            return\\n    elif args_cli.checkpoint:\\n        resume_path = retrieve_file_path(args_cli.checkpoint)\\n    else:\\n        resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)\\n\\n    log_dir = os.path.dirname(resume_path)\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv):\\n        env = multi_agent_to_single_agent(env)\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_dir, \"videos\", \"play\"),\\n            \"step_trigger\": lambda step: step == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for rsl-rl\\n    env = RslRlVecEnvWrapper(env, clip_actions=agent_cfg.clip_actions)\\n\\n    print(f\"[INFO]: Loading model checkpoint from: {resume_path}\")\\n    # load previously trained model\\n    ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)\\n    ppo_runner.load(resume_path)\\n\\n    # obtain the trained policy for inference\\n    policy = ppo_runner.get_inference_policy(device=env.unwrapped.device)\\n\\n    # extract the neural network module\\n    # we do this in a try-except to maintain backwards compatibility.\\n    try:\\n        # version 2.3 onwards\\n        policy_nn = ppo_runner.alg.policy\\n    except AttributeError:\\n        # version 2.2 and below\\n        policy_nn = ppo_runner.alg.actor_critic\\n\\n    # export policy to onnx/jit\\n    export_model_dir = os.path.join(os.path.dirname(resume_path), \"exported\")\\n    export_policy_as_jit(policy_nn, ppo_runner.obs_normalizer, path=export_model_dir, filename=\"policy.pt\")\\n    export_policy_as_onnx(\\n        policy_nn, normalizer=ppo_runner.obs_normalizer, path=export_model_dir, filename=\"policy.onnx\"\\n    )\\n\\n    dt = env.unwrapped.step_dt\\n\\n    # reset environment\\n    obs, _ = env.get_observations()\\n    timestep = 0\\n    # simulate environment\\n    while simulation_app.is_running():\\n        start_time = time.time()\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # agent stepping\\n            actions = policy(obs)\\n            # env stepping\\n            obs, _, _, _ = env.step(actions)\\n        if args_cli.video:\\n            timestep += 1\\n            # Exit the play loop after recording one video\\n            if timestep == args_cli.video_length:\\n                break\\n\\n        # time delay for real-time evaluation\\n        sleep_time = dt - (time.time() - start_time)\\n        if args_cli.real_time and sleep_time > 0:\\n            time.sleep(sleep_time)\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\\n    \"\"\"Train with RSL-RL agent.\"\"\"\\n    # override configurations with non-hydra CLI arguments\\n    agent_cfg = cli_args.update_rsl_rl_cfg(agent_cfg, args_cli)\\n    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\\n    agent_cfg.max_iterations = (\\n        args_cli.max_iterations if args_cli.max_iterations is not None else agent_cfg.max_iterations\\n    )\\n\\n    # set the environment seed\\n    # note: certain randomizations occur in the environment initialization so we set the seed here\\n    env_cfg.seed = agent_cfg.seed\\n    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\\n\\n    # multi-gpu training configuration\\n    if args_cli.distributed:\\n        env_cfg.sim.device = f\"cuda:{app_launcher.local_rank}\"\\n        agent_cfg.device = f\"cuda:{app_launcher.local_rank}\"\\n\\n        # set seed to have diversity in different threads\\n        seed = agent_cfg.seed + app_launcher.local_rank\\n        env_cfg.seed = seed\\n        agent_cfg.seed = seed\\n\\n    # specify directory for logging experiments\\n    log_root_path = os.path.join(\"logs\", \"rsl_rl\", agent_cfg.experiment_name)\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\\n    # specify directory for logging runs: {time-stamp}_{run_name}\\n    log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\n    # The Ray Tune workflow extracts experiment name using the logging line below, hence, do not change it (see PR #2346, comment-2819298849)\\n    print(f\"Exact experiment name requested from command line: {log_dir}\")\\n    if agent_cfg.run_name:\\n        log_dir += f\"_{agent_cfg.run_name}\"\\n    log_dir = os.path.join(log_root_path, log_dir)\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv):\\n        env = multi_agent_to_single_agent(env)\\n\\n    # save resume path before creating a new log_dir\\n    if agent_cfg.resume or agent_cfg.algorithm.class_name == \"Distillation\":\\n        resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_dir, \"videos\", \"train\"),\\n            \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for rsl-rl\\n    env = RslRlVecEnvWrapper(env, clip_actions=agent_cfg.clip_actions)\\n\\n    # create runner from rsl-rl\\n    runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)\\n    # write git state to logs\\n    runner.add_git_repo_to_log(__file__)\\n    # load the checkpoint\\n    if agent_cfg.resume or agent_cfg.algorithm.class_name == \"Distillation\":\\n        print(f\"[INFO]: Loading model checkpoint from: {resume_path}\")\\n        # load previously trained model\\n        runner.load(resume_path)\\n\\n    # dump the configuration into log-directory\\n    dump_yaml(os.path.join(log_dir, \"params\", \"env.yaml\"), env_cfg)\\n    dump_yaml(os.path.join(log_dir, \"params\", \"agent.yaml\"), agent_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"env.pkl\"), env_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"agent.pkl\"), agent_cfg)\\n\\n    # run training\\n    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Play with stable-baselines agent.\"\"\"\\n    # parse configuration\\n    env_cfg = parse_env_cfg(\\n        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric\\n    )\\n    agent_cfg = load_cfg_from_registry(args_cli.task, \"sb3_cfg_entry_point\")\\n\\n    # directory for logging into\\n    log_root_path = os.path.join(\"logs\", \"sb3\", args_cli.task)\\n    log_root_path = os.path.abspath(log_root_path)\\n    # checkpoint and log_dir stuff\\n    if args_cli.use_pretrained_checkpoint:\\n        checkpoint_path = get_published_pretrained_checkpoint(\"sb3\", args_cli.task)\\n        if not checkpoint_path:\\n            print(\"[INFO] Unfortunately a pre-trained checkpoint is currently unavailable for this task.\")\\n            return\\n    elif args_cli.checkpoint is None:\\n        if args_cli.use_last_checkpoint:\\n            checkpoint = \"model_.*.zip\"\\n        else:\\n            checkpoint = \"model.zip\"\\n        checkpoint_path = get_checkpoint_path(log_root_path, \".*\", checkpoint)\\n    else:\\n        checkpoint_path = args_cli.checkpoint\\n    log_dir = os.path.dirname(checkpoint_path)\\n\\n    # post-process agent configuration\\n    agent_cfg = process_sb3_cfg(agent_cfg)\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv):\\n        env = multi_agent_to_single_agent(env)\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_dir, \"videos\", \"play\"),\\n            \"step_trigger\": lambda step: step == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n    # wrap around environment for stable baselines\\n    env = Sb3VecEnvWrapper(env)\\n\\n    # normalize environment (if needed)\\n    if \"normalize_input\" in agent_cfg:\\n        env = VecNormalize(\\n            env,\\n            training=True,\\n            norm_obs=\"normalize_input\" in agent_cfg and agent_cfg.pop(\"normalize_input\"),\\n            norm_reward=\"normalize_value\" in agent_cfg and agent_cfg.pop(\"normalize_value\"),\\n            clip_obs=\"clip_obs\" in agent_cfg and agent_cfg.pop(\"clip_obs\"),\\n            gamma=agent_cfg[\"gamma\"],\\n            clip_reward=np.inf,\\n        )\\n\\n    # create agent from stable baselines\\n    print(f\"Loading checkpoint from: {checkpoint_path}\")\\n    agent = PPO.load(checkpoint_path, env, print_system_info=True)\\n\\n    dt = env.unwrapped.step_dt\\n\\n    # reset environment\\n    obs = env.reset()\\n    timestep = 0\\n    # simulate environment\\n    while simulation_app.is_running():\\n        start_time = time.time()\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # agent stepping\\n            actions, _ = agent.predict(obs, deterministic=True)\\n            # env stepping\\n            obs, _, _, _ = env.step(actions)\\n        if args_cli.video:\\n            timestep += 1\\n            # Exit the play loop after recording one video\\n            if timestep == args_cli.video_length:\\n                break\\n\\n        # time delay for real-time evaluation\\n        sleep_time = dt - (time.time() - start_time)\\n        if args_cli.real_time and sleep_time > 0:\\n            time.sleep(sleep_time)\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: dict):\\n    \"\"\"Train with stable-baselines agent.\"\"\"\\n    # randomly sample a seed if seed = -1\\n    if args_cli.seed == -1:\\n        args_cli.seed = random.randint(0, 10000)\\n\\n    # override configurations with non-hydra CLI arguments\\n    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\\n    agent_cfg[\"seed\"] = args_cli.seed if args_cli.seed is not None else agent_cfg[\"seed\"]\\n    # max iterations for training\\n    if args_cli.max_iterations is not None:\\n        agent_cfg[\"n_timesteps\"] = args_cli.max_iterations * agent_cfg[\"n_steps\"] * env_cfg.scene.num_envs\\n\\n    # set the environment seed\\n    # note: certain randomizations occur in the environment initialization so we set the seed here\\n    env_cfg.seed = agent_cfg[\"seed\"]\\n    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\\n\\n    # directory for logging into\\n    run_info = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\n    log_root_path = os.path.abspath(os.path.join(\"logs\", \"sb3\", args_cli.task))\\n    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\\n    # The Ray Tune workflow extracts experiment name using the logging line below, hence, do not change it (see PR #2346, comment-2819298849)\\n    print(f\"Exact experiment name requested from command line: {run_info}\")\\n    log_dir = os.path.join(log_root_path, run_info)\\n    # dump the configuration into log-directory\\n    dump_yaml(os.path.join(log_dir, \"params\", \"env.yaml\"), env_cfg)\\n    dump_yaml(os.path.join(log_dir, \"params\", \"agent.yaml\"), agent_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"env.pkl\"), env_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"agent.pkl\"), agent_cfg)\\n\\n    # post-process agent configuration\\n    agent_cfg = process_sb3_cfg(agent_cfg)\\n    # read configurations about the agent-training\\n    policy_arch = agent_cfg.pop(\"policy\")\\n    n_timesteps = agent_cfg.pop(\"n_timesteps\")\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv):\\n        env = multi_agent_to_single_agent(env)\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_dir, \"videos\", \"train\"),\\n            \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for stable baselines\\n    env = Sb3VecEnvWrapper(env)\\n\\n    if \"normalize_input\" in agent_cfg:\\n        env = VecNormalize(\\n            env,\\n            training=True,\\n            norm_obs=\"normalize_input\" in agent_cfg and agent_cfg.pop(\"normalize_input\"),\\n            norm_reward=\"normalize_value\" in agent_cfg and agent_cfg.pop(\"normalize_value\"),\\n            clip_obs=\"clip_obs\" in agent_cfg and agent_cfg.pop(\"clip_obs\"),\\n            gamma=agent_cfg[\"gamma\"],\\n            clip_reward=np.inf,\\n        )\\n\\n    # create agent from stable baselines\\n    agent = PPO(policy_arch, env, verbose=1, **agent_cfg)\\n    # configure the logger\\n    new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\\n    agent.set_logger(new_logger)\\n\\n    # callbacks for agent\\n    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=log_dir, name_prefix=\"model\", verbose=2)\\n    # train the agent\\n    agent.learn(total_timesteps=n_timesteps, callback=checkpoint_callback)\\n    # save the final model\\n    agent.save(os.path.join(log_dir, \"model\"))\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Play with skrl agent.\"\"\"\\n    # configure the ML framework into the global skrl variable\\n    if args_cli.ml_framework.startswith(\"jax\"):\\n        skrl.config.jax.backend = \"jax\" if args_cli.ml_framework == \"jax\" else \"numpy\"\\n\\n    # parse configuration\\n    env_cfg = parse_env_cfg(\\n        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric\\n    )\\n    try:\\n        experiment_cfg = load_cfg_from_registry(args_cli.task, f\"skrl_{algorithm}_cfg_entry_point\")\\n    except ValueError:\\n        experiment_cfg = load_cfg_from_registry(args_cli.task, \"skrl_cfg_entry_point\")\\n\\n    # specify directory for logging experiments (load checkpoint)\\n    log_root_path = os.path.join(\"logs\", \"skrl\", experiment_cfg[\"agent\"][\"experiment\"][\"directory\"])\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Loading experiment from directory: {log_root_path}\")\\n    # get checkpoint path\\n    if args_cli.use_pretrained_checkpoint:\\n        resume_path = get_published_pretrained_checkpoint(\"skrl\", args_cli.task)\\n        if not resume_path:\\n            print(\"[INFO] Unfortunately a pre-trained checkpoint is currently unavailable for this task.\")\\n            return\\n    elif args_cli.checkpoint:\\n        resume_path = os.path.abspath(args_cli.checkpoint)\\n    else:\\n        resume_path = get_checkpoint_path(\\n            log_root_path, run_dir=f\".*_{algorithm}_{args_cli.ml_framework}\", other_dirs=[\"checkpoints\"]\\n        )\\n    log_dir = os.path.dirname(os.path.dirname(resume_path))\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv) and algorithm in [\"ppo\"]:\\n        env = multi_agent_to_single_agent(env)\\n\\n    # get environment (step) dt for real-time evaluation\\n    try:\\n        dt = env.step_dt\\n    except AttributeError:\\n        dt = env.unwrapped.step_dt\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_dir, \"videos\", \"play\"),\\n            \"step_trigger\": lambda step: step == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for skrl\\n    env = SkrlVecEnvWrapper(env, ml_framework=args_cli.ml_framework)  # same as: `wrap_env(env, wrapper=\"auto\")`\\n\\n    # configure and instantiate the skrl runner\\n    # https://skrl.readthedocs.io/en/latest/api/utils/runner.html\\n    experiment_cfg[\"trainer\"][\"close_environment_at_exit\"] = False\\n    experiment_cfg[\"agent\"][\"experiment\"][\"write_interval\"] = 0  # don\\'t log to TensorBoard\\n    experiment_cfg[\"agent\"][\"experiment\"][\"checkpoint_interval\"] = 0  # don\\'t generate checkpoints\\n    runner = Runner(env, experiment_cfg)\\n\\n    print(f\"[INFO] Loading model checkpoint from: {resume_path}\")\\n    runner.agent.load(resume_path)\\n    # set agent to evaluation mode\\n    runner.agent.set_running_mode(\"eval\")\\n\\n    # reset environment\\n    obs, _ = env.reset()\\n    timestep = 0\\n    # simulate environment\\n    while simulation_app.is_running():\\n        start_time = time.time()\\n\\n        # run everything in inference mode\\n        with torch.inference_mode():\\n            # agent stepping\\n            outputs = runner.agent.act(obs, timestep=0, timesteps=0)\\n            # - multi-agent (deterministic) actions\\n            if hasattr(env, \"possible_agents\"):\\n                actions = {a: outputs[-1][a].get(\"mean_actions\", outputs[0][a]) for a in env.possible_agents}\\n            # - single-agent (deterministic) actions\\n            else:\\n                actions = outputs[-1].get(\"mean_actions\", outputs[0])\\n            # env stepping\\n            obs, _, _, _, _ = env.step(actions)\\n        if args_cli.video:\\n            timestep += 1\\n            # exit the play loop after recording one video\\n            if timestep == args_cli.video_length:\\n                break\\n\\n        # time delay for real-time evaluation\\n        sleep_time = dt - (time.time() - start_time)\\n        if args_cli.real_time and sleep_time > 0:\\n            time.sleep(sleep_time)\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: dict):\\n    \"\"\"Train with skrl agent.\"\"\"\\n    # override configurations with non-hydra CLI arguments\\n    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\\n    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\\n\\n    # multi-gpu training config\\n    if args_cli.distributed:\\n        env_cfg.sim.device = f\"cuda:{app_launcher.local_rank}\"\\n    # max iterations for training\\n    if args_cli.max_iterations:\\n        agent_cfg[\"trainer\"][\"timesteps\"] = args_cli.max_iterations * agent_cfg[\"agent\"][\"rollouts\"]\\n    agent_cfg[\"trainer\"][\"close_environment_at_exit\"] = False\\n    # configure the ML framework into the global skrl variable\\n    if args_cli.ml_framework.startswith(\"jax\"):\\n        skrl.config.jax.backend = \"jax\" if args_cli.ml_framework == \"jax\" else \"numpy\"\\n\\n    # randomly sample a seed if seed = -1\\n    if args_cli.seed == -1:\\n        args_cli.seed = random.randint(0, 10000)\\n\\n    # set the agent and environment seed from command line\\n    # note: certain randomization occur in the environment initialization so we set the seed here\\n    agent_cfg[\"seed\"] = args_cli.seed if args_cli.seed is not None else agent_cfg[\"seed\"]\\n    env_cfg.seed = agent_cfg[\"seed\"]\\n\\n    # specify directory for logging experiments\\n    log_root_path = os.path.join(\"logs\", \"skrl\", agent_cfg[\"agent\"][\"experiment\"][\"directory\"])\\n    log_root_path = os.path.abspath(log_root_path)\\n    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\\n    # specify directory for logging runs: {time-stamp}_{run_name}\\n    log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + f\"_{algorithm}_{args_cli.ml_framework}\"\\n    # The Ray Tune workflow extracts experiment name using the logging line below, hence, do not change it (see PR #2346, comment-2819298849)\\n    print(f\"Exact experiment name requested from command line: {log_dir}\")\\n    if agent_cfg[\"agent\"][\"experiment\"][\"experiment_name\"]:\\n        log_dir += f\\'_{agent_cfg[\"agent\"][\"experiment\"][\"experiment_name\"]}\\'\\n    # set directory into agent config\\n    agent_cfg[\"agent\"][\"experiment\"][\"directory\"] = log_root_path\\n    agent_cfg[\"agent\"][\"experiment\"][\"experiment_name\"] = log_dir\\n    # update log_dir\\n    log_dir = os.path.join(log_root_path, log_dir)\\n\\n    # dump the configuration into log-directory\\n    dump_yaml(os.path.join(log_dir, \"params\", \"env.yaml\"), env_cfg)\\n    dump_yaml(os.path.join(log_dir, \"params\", \"agent.yaml\"), agent_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"env.pkl\"), env_cfg)\\n    dump_pickle(os.path.join(log_dir, \"params\", \"agent.pkl\"), agent_cfg)\\n\\n    # get checkpoint path (to resume training)\\n    resume_path = retrieve_file_path(args_cli.checkpoint) if args_cli.checkpoint else None\\n\\n    # create isaac environment\\n    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\\n\\n    # convert to single-agent instance if required by the RL algorithm\\n    if isinstance(env.unwrapped, DirectMARLEnv) and algorithm in [\"ppo\"]:\\n        env = multi_agent_to_single_agent(env)\\n\\n    # wrap for video recording\\n    if args_cli.video:\\n        video_kwargs = {\\n            \"video_folder\": os.path.join(log_dir, \"videos\", \"train\"),\\n            \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\\n            \"video_length\": args_cli.video_length,\\n            \"disable_logger\": True,\\n        }\\n        print(\"[INFO] Recording videos during training.\")\\n        print_dict(video_kwargs, nesting=4)\\n        env = gym.wrappers.RecordVideo(env, **video_kwargs)\\n\\n    # wrap around environment for skrl\\n    env = SkrlVecEnvWrapper(env, ml_framework=args_cli.ml_framework)  # same as: `wrap_env(env, wrapper=\"auto\")`\\n\\n    # configure and instantiate the skrl runner\\n    # https://skrl.readthedocs.io/en/latest/api/utils/runner.html\\n    runner = Runner(env, agent_cfg)\\n\\n    # load checkpoint (if specified)\\n    if resume_path:\\n        print(f\"[INFO] Loading model checkpoint from: {resume_path}\")\\n        runner.agent.load(resume_path)\\n\\n    # run training\\n    runner.run()\\n\\n    # close the simulator\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def parse_cli_args():\\n    \"\"\"Parse the input command line arguments.\"\"\"\\n    import argparse\\n\\n    # get the args passed to blender after \"--\", all of which are ignored by\\n    # blender so scripts may receive their own arguments\\n    argv = sys.argv\\n\\n    if \"--\" not in argv:\\n        argv = []  # as if no args are passed\\n    else:\\n        argv = argv[argv.index(\"--\") + 1 :]  # get all args after \"--\"\\n\\n    # When --help or no args are given, print this help\\n    usage_text = (\\n        f\"Run blender in background mode with this script:\\\\n\\\\tblender --background --python {__file__} -- [options]\"\\n    )\\n    parser = argparse.ArgumentParser(description=usage_text)\\n    # Add arguments\\n    parser.add_argument(\"-i\", \"--in_file\", metavar=\"FILE\", type=str, required=True, help=\"Path to input OBJ file.\")\\n    parser.add_argument(\"-o\", \"--out_file\", metavar=\"FILE\", type=str, required=True, help=\"Path to output OBJ file.\")\\n    args = parser.parse_args(argv)\\n    # Check if any arguments provided\\n    if not argv or not args.in_file or not args.out_file:\\n        parser.print_help()\\n        return None\\n    # return arguments\\n    return args'),\n",
       " Document(metadata={}, page_content='def convert_to_obj(in_file: str, out_file: str, save_usd: bool = False):\\n    \"\"\"Convert a mesh file to `.obj` using blender.\\n\\n    Args:\\n        in_file: Input mesh file to process.\\n        out_file: Path to store output obj file.\\n    \"\"\"\\n    # check valid input file\\n    if not os.path.exists(in_file):\\n        raise FileNotFoundError(in_file)\\n    # add ending of file format\\n    if not out_file.endswith(\".obj\"):\\n        out_file += \".obj\"\\n    # create directory if it doesn\\'t exist for destination file\\n    if not os.path.exists(os.path.dirname(out_file)):\\n        os.makedirs(os.path.dirname(out_file), exist_ok=True)\\n    # reset scene to empty\\n    bpy.ops.wm.read_factory_settings(use_empty=True)\\n    # load object into scene\\n    if in_file.endswith(\".dae\"):\\n        bpy.ops.wm.collada_import(filepath=in_file)\\n    elif in_file.endswith(\".stl\") or in_file.endswith(\".STL\"):\\n        bpy.ops.import_mesh.stl(filepath=in_file)\\n    else:\\n        raise ValueError(f\"Input file not in dae/stl format: {in_file}\")\\n    # convert to obj format and store with z up\\n    # TODO: Read the convention from dae file instead of manually fixing it.\\n    # Reference: https://docs.blender.org/api/2.79/bpy.ops.export_scene.html\\n    bpy.ops.export_scene.obj(\\n        filepath=out_file, check_existing=False, axis_forward=\"Y\", axis_up=\"Z\", global_scale=1, path_mode=\"RELATIVE\"\\n    )\\n    # save it as usd as well\\n    if save_usd:\\n        out_file = out_file.replace(\"obj\", \"usd\")\\n        bpy.ops.wm.usd_export(filepath=out_file, check_existing=False)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Spawns the USD asset robot and clones it using Isaac Gym Cloner API.\"\"\"\\n    # check valid file path\\n    if not check_file_path(args_cli.input):\\n        raise ValueError(f\"Invalid file path: {args_cli.input}\")\\n    # Load kit helper\\n    sim = SimulationContext(\\n        stage_units_in_meters=1.0, physics_dt=0.01, rendering_dt=0.01, backend=\"torch\", device=\"cuda:0\"\\n    )\\n    # enable fabric which avoids passing data over to USD structure\\n    # this speeds up the read-write operation of GPU buffers\\n    if sim.get_physics_context().use_gpu_pipeline:\\n        sim.get_physics_context().enable_fabric(True)\\n    # increase GPU buffer dimensions\\n    sim.get_physics_context().set_gpu_found_lost_aggregate_pairs_capacity(2**25)\\n    sim.get_physics_context().set_gpu_total_aggregate_pairs_capacity(2**21)\\n    # enable hydra scene-graph instancing\\n    # this is needed to visualize the scene when fabric is enabled\\n    set_carb_setting(sim._settings, \"/persistent/omnihydra/useSceneGraphInstancing\", True)\\n\\n    # Create interface to clone the scene\\n    cloner = GridCloner(spacing=args_cli.spacing)\\n    cloner.define_base_env(\"/World/envs\")\\n    prim_utils.define_prim(\"/World/envs/env_0\")\\n    # Spawn things into stage\\n    prim_utils.create_prim(\"/World/Light\", \"DistantLight\")\\n\\n    # Everything under the namespace \"/World/envs/env_0\" will be cloned\\n    prim_utils.create_prim(\"/World/envs/env_0/Asset\", \"Xform\", usd_path=os.path.abspath(args_cli.input))\\n    # Clone the scene\\n    num_clones = args_cli.num_clones\\n\\n    # Create a timer to measure the cloning time\\n    with Timer(f\"[#clones: {num_clones}, physics: {args_cli.physics}] Asset: {args_cli.input}\"):\\n        # Clone the scene\\n        with Timer(\">>> Cloning time (cloner.clone)\"):\\n            cloner.define_base_env(\"/World/envs\")\\n            envs_prim_paths = cloner.generate_paths(\"/World/envs/env\", num_paths=num_clones)\\n            _ = cloner.clone(\\n                source_prim_path=\"/World/envs/env_0\", prim_paths=envs_prim_paths, replicate_physics=args_cli.physics\\n            )\\n        # Play the simulator\\n        with Timer(\">>> Setup time (sim.reset)\"):\\n            sim.reset()\\n\\n    # Simulate scene (if not headless)\\n    if not args_cli.headless:\\n        with contextlib.suppress(KeyboardInterrupt):\\n            while sim.is_playing():\\n                # perform step\\n                sim.step()'),\n",
       " Document(metadata={}, page_content='def main():\\n\\n    # Define conversion time given\\n    conversion_type = args_cli.conversion_type.lower()\\n    # Warning if conversion type input is not valid\\n    if conversion_type != \"urdf\" and conversion_type != \"mesh\" and conversion_type != \"both\":\\n        raise Warning(\"Conversion type is not valid, please select either \\'urdf\\', \\'mesh\\', or \\'both\\'.\")\\n\\n    if not os.path.exists(args_cli.input):\\n        print(f\"Error: The directory {args_cli.input} does not exist.\")\\n\\n    # For each file and subsequent sub-directory\\n    for root, dirs, files in os.walk(args_cli.input):\\n        # For each file\\n        for filename in files:\\n            # Check for URDF extensions\\n            if (conversion_type == \"urdf\" or conversion_type == \"both\") and filename.lower().endswith(\".urdf\"):\\n                # URDF converter call\\n                urdf_converter_cfg = UrdfConverterCfg(\\n                    asset_path=f\"{root}/{filename}\",\\n                    usd_dir=f\"{args_cli.output}/{filename[:-5]}\",\\n                    usd_file_name=f\"{filename[:-5]}.usd\",\\n                    fix_base=args_cli.fix_base,\\n                    merge_fixed_joints=args_cli.merge_joints,\\n                    force_usd_conversion=True,\\n                    make_instanceable=args_cli.make_instanceable,\\n                )\\n                # Create Urdf converter and import the file\\n                urdf_converter = UrdfConverter(urdf_converter_cfg)\\n                print(f\"Generated USD file: {urdf_converter.usd_path}\")\\n\\n            elif (conversion_type == \"mesh\" or conversion_type == \"both\") and (\\n                filename.lower().endswith(\".fbx\")\\n                or filename.lower().endswith(\".obj\")\\n                or filename.lower().endswith(\".dae\")\\n                or filename.lower().endswith(\".stl\")\\n            ):\\n                # Mass properties\\n                if args_cli.mass is not None:\\n                    mass_props = schemas_cfg.MassPropertiesCfg(mass=args_cli.mass)\\n                    rigid_props = schemas_cfg.RigidBodyPropertiesCfg()\\n                else:\\n                    mass_props = None\\n                    rigid_props = None\\n\\n                # Collision properties\\n                collision_props = schemas_cfg.CollisionPropertiesCfg(\\n                    collision_enabled=args_cli.collision_approximation != \"none\"\\n                )\\n                # Mesh converter call\\n                mesh_converter_cfg = MeshConverterCfg(\\n                    mass_props=mass_props,\\n                    rigid_props=rigid_props,\\n                    collision_props=collision_props,\\n                    asset_path=f\"{root}/{filename}\",\\n                    force_usd_conversion=True,\\n                    usd_dir=f\"{args_cli.output}/{filename[:-4]}\",\\n                    usd_file_name=f\"{filename[:-4]}.usd\",\\n                    make_instanceable=args_cli.make_instanceable,\\n                    collision_approximation=args_cli.collision_approximation,\\n                )\\n                # Create mesh converter and import the file\\n                mesh_converter = MeshConverter(mesh_converter_cfg)\\n                print(f\"Generated USD file: {mesh_converter.usd_path}\")'),\n",
       " Document(metadata={}, page_content='def main():\\n    # check valid file path\\n    mesh_path = args_cli.input\\n    if not os.path.isabs(mesh_path):\\n        mesh_path = os.path.abspath(mesh_path)\\n    if not check_file_path(mesh_path):\\n        raise ValueError(f\"Invalid mesh file path: {mesh_path}\")\\n\\n    # create destination path\\n    dest_path = args_cli.output\\n    if not os.path.isabs(dest_path):\\n        dest_path = os.path.abspath(dest_path)\\n\\n    # Mass properties\\n    if args_cli.mass is not None:\\n        mass_props = schemas_cfg.MassPropertiesCfg(mass=args_cli.mass)\\n        rigid_props = schemas_cfg.RigidBodyPropertiesCfg()\\n    else:\\n        mass_props = None\\n        rigid_props = None\\n\\n    # Collision properties\\n    collision_props = schemas_cfg.CollisionPropertiesCfg(collision_enabled=args_cli.collision_approximation != \"none\")\\n\\n    # Create Mesh converter config\\n    mesh_converter_cfg = MeshConverterCfg(\\n        mass_props=mass_props,\\n        rigid_props=rigid_props,\\n        collision_props=collision_props,\\n        asset_path=mesh_path,\\n        force_usd_conversion=True,\\n        usd_dir=os.path.dirname(dest_path),\\n        usd_file_name=os.path.basename(dest_path),\\n        make_instanceable=args_cli.make_instanceable,\\n        collision_approximation=args_cli.collision_approximation,\\n    )\\n\\n    # Print info\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n    print(f\"Input Mesh file: {mesh_path}\")\\n    print(\"Mesh importer config:\")\\n    print_dict(mesh_converter_cfg.to_dict(), nesting=0)\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n\\n    # Create Mesh converter and import the file\\n    mesh_converter = MeshConverter(mesh_converter_cfg)\\n    # print output\\n    print(\"Mesh importer output:\")\\n    print(f\"Generated USD file: {mesh_converter.usd_path}\")\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n\\n    # Determine if there is a GUI to update:\\n    # acquire settings interface\\n    carb_settings_iface = carb.settings.get_settings()\\n    # read flag for whether a local GUI is enabled\\n    local_gui = carb_settings_iface.get(\"/app/window/enabled\")\\n    # read flag for whether livestreaming GUI is enabled\\n    livestream_gui = carb_settings_iface.get(\"/app/livestream/enabled\")\\n\\n    # Simulate scene (if not headless)\\n    if local_gui or livestream_gui:\\n        # Open the stage with USD\\n        stage_utils.open_stage(mesh_converter.usd_path)\\n        # Reinitialize the simulation\\n        app = omni.kit.app.get_app_interface()\\n        # Run simulation\\n        with contextlib.suppress(KeyboardInterrupt):\\n            while app.is_running():\\n                # perform step\\n                app.update()'),\n",
       " Document(metadata={}, page_content='def main():\\n    # check valid file path\\n    mjcf_path = args_cli.input\\n    if not os.path.isabs(mjcf_path):\\n        mjcf_path = os.path.abspath(mjcf_path)\\n    if not check_file_path(mjcf_path):\\n        raise ValueError(f\"Invalid file path: {mjcf_path}\")\\n    # create destination path\\n    dest_path = args_cli.output\\n    if not os.path.isabs(dest_path):\\n        dest_path = os.path.abspath(dest_path)\\n\\n    # create the converter configuration\\n    mjcf_converter_cfg = MjcfConverterCfg(\\n        asset_path=mjcf_path,\\n        usd_dir=os.path.dirname(dest_path),\\n        usd_file_name=os.path.basename(dest_path),\\n        fix_base=args_cli.fix_base,\\n        import_sites=args_cli.import_sites,\\n        force_usd_conversion=True,\\n        make_instanceable=args_cli.make_instanceable,\\n    )\\n\\n    # Print info\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n    print(f\"Input MJCF file: {mjcf_path}\")\\n    print(\"MJCF importer config:\")\\n    print_dict(mjcf_converter_cfg.to_dict(), nesting=0)\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n\\n    # Create mjcf converter and import the file\\n    mjcf_converter = MjcfConverter(mjcf_converter_cfg)\\n    # print output\\n    print(\"MJCF importer output:\")\\n    print(f\"Generated USD file: {mjcf_converter.usd_path}\")\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n\\n    # Determine if there is a GUI to update:\\n    # acquire settings interface\\n    carb_settings_iface = carb.settings.get_settings()\\n    # read flag for whether a local GUI is enabled\\n    local_gui = carb_settings_iface.get(\"/app/window/enabled\")\\n    # read flag for whether livestreaming GUI is enabled\\n    livestream_gui = carb_settings_iface.get(\"/app/livestream/enabled\")\\n\\n    # Simulate scene (if not headless)\\n    if local_gui or livestream_gui:\\n        # Open the stage with USD\\n        stage_utils.open_stage(mjcf_converter.usd_path)\\n        # Reinitialize the simulation\\n        app = omni.kit.app.get_app_interface()\\n        # Run simulation\\n        with contextlib.suppress(KeyboardInterrupt):\\n            while app.is_running():\\n                # perform step\\n                app.update()'),\n",
       " Document(metadata={}, page_content='def main():\\n    # check valid file path\\n    urdf_path = args_cli.input\\n    if not os.path.isabs(urdf_path):\\n        urdf_path = os.path.abspath(urdf_path)\\n    if not check_file_path(urdf_path):\\n        raise ValueError(f\"Invalid file path: {urdf_path}\")\\n    # create destination path\\n    dest_path = args_cli.output\\n    if not os.path.isabs(dest_path):\\n        dest_path = os.path.abspath(dest_path)\\n\\n    # Create Urdf converter config\\n    urdf_converter_cfg = UrdfConverterCfg(\\n        asset_path=urdf_path,\\n        usd_dir=os.path.dirname(dest_path),\\n        usd_file_name=os.path.basename(dest_path),\\n        fix_base=args_cli.fix_base,\\n        merge_fixed_joints=args_cli.merge_joints,\\n        force_usd_conversion=True,\\n        joint_drive=UrdfConverterCfg.JointDriveCfg(\\n            gains=UrdfConverterCfg.JointDriveCfg.PDGainsCfg(\\n                stiffness=args_cli.joint_stiffness,\\n                damping=args_cli.joint_damping,\\n            ),\\n            target_type=args_cli.joint_target_type,\\n        ),\\n    )\\n\\n    # Print info\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n    print(f\"Input URDF file: {urdf_path}\")\\n    print(\"URDF importer config:\")\\n    print_dict(urdf_converter_cfg.to_dict(), nesting=0)\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n\\n    # Create Urdf converter and import the file\\n    urdf_converter = UrdfConverter(urdf_converter_cfg)\\n    # print output\\n    print(\"URDF importer output:\")\\n    print(f\"Generated USD file: {urdf_converter.usd_path}\")\\n    print(\"-\" * 80)\\n    print(\"-\" * 80)\\n\\n    # Determine if there is a GUI to update:\\n    # acquire settings interface\\n    carb_settings_iface = carb.settings.get_settings()\\n    # read flag for whether a local GUI is enabled\\n    local_gui = carb_settings_iface.get(\"/app/window/enabled\")\\n    # read flag for whether livestreaming GUI is enabled\\n    livestream_gui = carb_settings_iface.get(\"/app/livestream/enabled\")\\n\\n    # Simulate scene (if not headless)\\n    if local_gui or livestream_gui:\\n        # Open the stage with USD\\n        stage_utils.open_stage(urdf_converter.usd_path)\\n        # Reinitialize the simulation\\n        app = omni.kit.app.get_app_interface()\\n        # Run simulation\\n        with contextlib.suppress(KeyboardInterrupt):\\n            while app.is_running():\\n                # perform step\\n                app.update()'),\n",
       " Document(metadata={}, page_content='def merge_datasets():\\n    for filepath in args_cli.input_files:\\n        if not os.path.exists(filepath):\\n            raise FileNotFoundError(f\"The dataset file {filepath} does not exist.\")\\n\\n    with h5py.File(args_cli.output_file, \"w\") as output:\\n        episode_idx = 0\\n        copy_attributes = True\\n\\n        for filepath in args_cli.input_files:\\n\\n            with h5py.File(filepath, \"r\") as input:\\n                for episode, data in input[\"data\"].items():\\n                    input.copy(f\"data/{episode}\", output, f\"data/demo_{episode_idx}\")\\n                    episode_idx += 1\\n\\n                if copy_attributes:\\n                    output[\"data\"].attrs[\"env_args\"] = input[\"data\"].attrs[\"env_args\"]\\n                    copy_attributes = False\\n\\n    print(f\"Merged dataset saved to {args_cli.output_file}\")'),\n",
       " Document(metadata={}, page_content='def train_job(workflow, task_name, headless=False, force=False, num_envs=None):\\n    \"\"\"\\n    This trains a task using the workflow\\'s train.py script, overriding the experiment name to ensure unique\\n    log directories.  By default it will return if an experiment has already been run.\\n\\n    Args:\\n        workflow: The workflow.\\n        task_name: The task name.\\n        headless: Should the training run without the UI.\\n        force: Run training even if previous experiments have been run.\\n        num_envs: How many simultaneous environments to simulate, overriding the config.\\n    \"\"\"\\n\\n    log_root_path = get_log_root_path(workflow, task_name)\\n\\n    # We already ran this\\n    if not force and os.path.exists(log_root_path) and len(os.listdir(log_root_path)) > 0:\\n        print(f\"Skipping training of {workflow}:{task_name}, already has been run\")\\n        return\\n\\n    print(f\"Training {workflow}:{task_name}\")\\n\\n    # Construct our command\\n    cmd = [\\n        sys.executable,\\n        WORKFLOW_TRAINER[workflow],\\n        \"--task\",\\n        task_name,\\n        \"--enable_cameras\",\\n    ]\\n\\n    # Changes the directory name for logging\\n    if WORKFLOW_EXPERIMENT_NAME_VARIABLE[workflow]:\\n        cmd.append(f\"{WORKFLOW_EXPERIMENT_NAME_VARIABLE[workflow]}={task_name}\")\\n\\n    if headless:\\n        cmd.append(\"--headless\")\\n    if num_envs:\\n        cmd.extend([\"--num_envs\", str(num_envs)])\\n\\n    print(\"Running : \" + \" \".join(cmd))\\n\\n    subprocess.run(cmd)'),\n",
       " Document(metadata={}, page_content='def review_pretrained_checkpoint(workflow, task_name, force_review=False, num_envs=None):\\n    \"\"\"\\n    This initiates a review of the pretrained checkpoint.  The play.py script for the workflow is run, and the user\\n    inspects the results.  When done they close the simulator and will be prompted for their review.\\n\\n    Args:\\n        workflow: The workflow.\\n        task_name: The task name.\\n        force_review: Performs the review even if a review already exists.\\n        num_envs: How many simultaneous environments to simulate, overriding the config.\\n    \"\"\"\\n\\n    # This workflow task pair hasn\\'t been trained\\n    if not has_pretrained_checkpoint_job_run(workflow, task_name):\\n        print(f\"Skipping review of {workflow}:{task_name}, hasn\\'t been trained yet\")\\n        return\\n\\n    # Couldn\\'t find the checkpoint\\n    if not has_pretrained_checkpoint_job_finished(workflow, task_name):\\n        print(f\"Training not complete for {workflow}:{task_name}\")\\n        return\\n\\n    review = get_pretrained_checkpoint_review(workflow, task_name)\\n\\n    if not force_review and review and review[\"reviewed\"]:\\n        print(f\"Review already complete for {workflow}:{task_name}\")\\n        return\\n\\n    print(f\"Reviewing {workflow}:{task_name}\")\\n\\n    # Construct our command\\n    cmd = [\\n        sys.executable,\\n        WORKFLOW_PLAYER[workflow],\\n        \"--task\",\\n        task_name,\\n        \"--checkpoint\",\\n        get_pretrained_checkpoint_path(workflow, task_name),\\n        \"--enable_cameras\",\\n    ]\\n\\n    if num_envs:\\n        cmd.extend([\"--num_envs\", str(num_envs)])\\n\\n    print(\"Running : \" + \" \".join(cmd))\\n\\n    subprocess.run(cmd)\\n\\n    # Give user a chance to leave the old review\\n    if force_review and review and review[\"reviewed\"]:\\n        result = review[\"result\"]\\n        notes = review.get(\"notes\")\\n        print(f\"A review already exists for {workflow}:{task_name}, it was marked as \\'{result}\\'.\")\\n        print(f\"  Notes: {notes}\")\\n        answer = input(\"Would you like to replace it?  Please answer yes or no (y/n) [n]: \").strip().lower()\\n        if answer != \"y\":\\n            return\\n\\n    # Get the verdict from the user\\n    print(f\"Do you accept this checkpoint for {workflow}:{task_name}?\")\\n\\n    answer = input(\"Please answer yes, no or undetermined (y/n/u) [u]: \").strip().lower()\\n    if answer not in {\"y\", \"n\", \"u\"}:\\n        answer = \"u\"\\n    answer_map = {\\n        \"y\": \"accepted\",\\n        \"n\": \"rejected\",\\n        \"u\": \"undetermined\",\\n    }\\n\\n    # Create the review dict\\n    review = {\\n        \"reviewed\": True,\\n        \"result\": answer_map[answer],\\n    }\\n\\n    # Maybe add some notes\\n    notes = input(\"Please add notes or hit enter: \").strip().lower()\\n    if notes:\\n        review[\"notes\"] = notes\\n\\n    # Save the review JSON file\\n    path = get_pretrained_checkpoint_review_path(workflow, task_name)\\n    if not path:\\n        raise Exception(\"This shouldn\\'t be possible, something went very wrong.\")\\n\\n    with open(path, \"w\") as f:\\n        json.dump(review, f, indent=4)'),\n",
       " Document(metadata={}, page_content='def publish_pretrained_checkpoint(workflow, task_name, force_publish=False):\\n    \"\"\"\\n    This publishes the pretrained checkpoint to Nucleus using the asset path in the\\n    /persistent/isaaclab/asset_root/pretrained_checkpoints Carb variable.\\n\\n    Args:\\n        workflow: The workflow.\\n        task_name: The task name.\\n        force_publish: Publish without review.\\n    \"\"\"\\n\\n    # This workflow task pair hasn\\'t been trained\\n    if not has_pretrained_checkpoint_job_run(workflow, task_name):\\n        print(f\"Skipping publishing of {workflow}:{task_name}, hasn\\'t been trained yet\")\\n        return\\n\\n    # Couldn\\'t find the checkpoint\\n    if not has_pretrained_checkpoint_job_finished(workflow, task_name):\\n        print(f\"Training not complete for {workflow}:{task_name}\")\\n        return\\n\\n    # Get local pretrained checkpoint path\\n    local_path = get_pretrained_checkpoint_path(workflow, task_name)\\n    if not local_path:\\n        raise Exception(\"This shouldn\\'t be possible, something went very wrong.\")\\n\\n    # Not forcing, need to check review results\\n    if not force_publish:\\n\\n        # Grab the review if it exists\\n        review = get_pretrained_checkpoint_review(workflow, task_name)\\n\\n        if not review or not review[\"reviewed\"]:\\n            print(f\"Skipping publishing of {workflow}:{task_name}, hasn\\'t been reviewed yet\")\\n            return\\n\\n        result = review[\"result\"]\\n        if result != \"accepted\":\\n            print(f\\'Skipping publishing of {workflow}:{task_name}, review result was \"{result}\"\\')\\n            return\\n\\n    print(f\"Publishing {workflow}:{task_name}\")\\n\\n    # Copy the file\\n    publish_path = get_pretrained_checkpoint_publish_path(workflow, task_name)\\n    omni.client.copy_file(local_path, publish_path, CopyBehavior.OVERWRITE)'),\n",
       " Document(metadata={}, page_content='def get_job_summary_row(workflow, task_name):\\n    \"\"\"Returns a single row summary of the job\"\"\"\\n\\n    has_run = has_pretrained_checkpoint_job_run(workflow, task_name)\\n    has_finished = has_pretrained_checkpoint_job_finished(workflow, task_name)\\n    review = get_pretrained_checkpoint_review(workflow, task_name)\\n\\n    if review:\\n        result = review.get(\"result\", \"undetermined\")\\n        notes = review.get(\"notes\", \"\")\\n    else:\\n        result = \"\"\\n        notes = \"\"\\n\\n    return [workflow, task_name, has_run, has_finished, result, notes]'),\n",
       " Document(metadata={}, page_content='def main():\\n\\n    # Figure out what workflows and tasks we\\'ll be using\\n    if args.all:\\n        jobs = [\"*:*\"]\\n    else:\\n        jobs = args.jobs\\n\\n    if args.list:\\n        print()\\n        print(\"# Workflow, Task, Ran, Finished, Review, Notes\")\\n\\n    summary_rows = []\\n\\n    # Could be implemented more efficiently, but the performance gain would be inconsequential\\n    for workflow in WORKFLOWS:\\n        for task_spec in sorted(gym.registry.values(), key=lambda t: t.id):\\n            job_id = f\"{workflow}:{task_spec.id}\"\\n\\n            # We\\'ve excluded this job\\n            if any(fnmatch.fnmatch(job_id, e) for e in args.exclude):\\n                continue\\n\\n            # None of our jobs match this pair\\n            if not np.any(np.array([fnmatch.fnmatch(job_id, job) for job in jobs])):\\n                continue\\n\\n            # No config for this workflow\\n            if workflow + \"_cfg_entry_point\" not in task_spec.kwargs:\\n                continue\\n\\n            if args.list:\\n                summary_rows.append(get_job_summary_row(workflow, task_spec.id))\\n                continue\\n\\n            # Training reviewing and publishing\\n            if args.train:\\n                train_job(workflow, task_spec.id, args.headless, args.force, args.num_envs)\\n\\n            if args.review:\\n                review_pretrained_checkpoint(workflow, task_spec.id, args.force_review, args.num_envs)\\n\\n            if args.publish_checkpoint:\\n                publish_pretrained_checkpoint(workflow, task_spec.id, args.force_publish)\\n\\n    if args.list:\\n        writer = csv.writer(sys.stdout, quotechar=\\'\"\\', quoting=csv.QUOTE_MINIMAL)\\n        writer.writerows(summary_rows)'),\n",
       " Document(metadata={}, page_content='def parse_cli_args():\\n    \"\"\"Parse the input command line arguments.\"\"\"\\n    # add argparse arguments\\n    parser = argparse.ArgumentParser(\"Utility to convert all mesh files to `.obj` in given folders.\")\\n    parser.add_argument(\"input_dir\", type=str, help=\"The input directory from which to load meshes.\")\\n    parser.add_argument(\\n        \"-o\",\\n        \"--output_dir\",\\n        type=str,\\n        default=None,\\n        help=\"The output directory to save converted meshes into. Default is same as input directory.\",\\n    )\\n    args_cli = parser.parse_args()\\n    # resolve output directory\\n    if args_cli.output_dir is None:\\n        args_cli.output_dir = args_cli.input_dir\\n    # return arguments\\n    return args_cli'),\n",
       " Document(metadata={}, page_content='def run_blender_convert2obj(in_file: str, out_file: str):\\n    \"\"\"Calls the python script using `subprocess` to perform processing of mesh file.\\n\\n    Args:\\n        in_file: Input mesh file.\\n        out_file: Output obj file.\\n    \"\"\"\\n    # resolve for python file\\n    tools_dirname = os.path.dirname(os.path.abspath(__file__))\\n    script_file = os.path.join(tools_dirname, \"blender_obj.py\")\\n    # complete command\\n    command_exe = f\"{BLENDER_EXE_PATH} --background --python {script_file} -- -i {in_file} -o {out_file}\"\\n    # break command into list\\n    command_exe_list = command_exe.split(\" \")\\n    # run command\\n    subprocess.run(command_exe_list)'),\n",
       " Document(metadata={}, page_content='def convert_meshes(source_folders: list[str], destination_folders: list[str]):\\n    \"\"\"Processes all mesh files of supported format into OBJ file using blender.\\n\\n    Args:\\n        source_folders: List of directories to search for meshes.\\n        destination_folders: List of directories to dump converted files.\\n    \"\"\"\\n    # create folder for corresponding destination\\n    for folder in destination_folders:\\n        os.makedirs(folder, exist_ok=True)\\n    # iterate over each folder\\n    for in_folder, out_folder in zip(source_folders, destination_folders):\\n        # extract all dae files in the directory\\n        mesh_filenames = [f for f in os.listdir(in_folder) if f.endswith(\"dae\")]\\n        mesh_filenames += [f for f in os.listdir(in_folder) if f.endswith(\"stl\")]\\n        mesh_filenames += [f for f in os.listdir(in_folder) if f.endswith(\"STL\")]\\n        # print status\\n        print(f\"Found {len(mesh_filenames)} files to process in directory: {in_folder}\")\\n        # iterate over each OBJ file\\n        for mesh_file in mesh_filenames:\\n            # extract meshname\\n            mesh_name = os.path.splitext(mesh_file)[0]\\n            # complete path of input and output files\\n            in_file_path = os.path.join(in_folder, mesh_file)\\n            out_file_path = os.path.join(out_folder, mesh_name + \".obj\")\\n            # perform blender processing\\n            print(\"Processing: \", in_file_path)\\n            run_blender_convert2obj(in_file_path, out_file_path)'),\n",
       " Document(metadata={}, page_content='class RateLimiter:\\n    \"\"\"Convenience class for enforcing rates in loops.\"\"\"\\n\\n    def __init__(self, hz: int):\\n        \"\"\"Initialize a RateLimiter with specified frequency.\\n\\n        Args:\\n            hz: Frequency to enforce in Hertz.\\n        \"\"\"\\n        self.hz = hz\\n        self.last_time = time.time()\\n        self.sleep_duration = 1.0 / hz\\n        self.render_period = min(0.033, self.sleep_duration)\\n\\n    def sleep(self, env: gym.Env):\\n        \"\"\"Attempt to sleep at the specified rate in hz.\\n\\n        Args:\\n            env: Environment to render during sleep periods.\\n        \"\"\"\\n        next_wakeup_time = self.last_time + self.sleep_duration\\n        while time.time() < next_wakeup_time:\\n            time.sleep(self.render_period)\\n            env.sim.render()\\n\\n        self.last_time = self.last_time + self.sleep_duration\\n\\n        # detect time jumping forwards (e.g. loop is too slow)\\n        if self.last_time < time.time():\\n            while self.last_time < time.time():\\n                self.last_time += self.sleep_duration'),\n",
       " Document(metadata={}, page_content='def pre_process_actions(\\n    teleop_data: tuple[np.ndarray, bool] | list[tuple[np.ndarray, np.ndarray, np.ndarray]], num_envs: int, device: str\\n) -> torch.Tensor:\\n    \"\"\"Convert teleop data to the format expected by the environment action space.\\n\\n    Args:\\n        teleop_data: Data from the teleoperation device.\\n        num_envs: Number of environments.\\n        device: Device to create tensors on.\\n\\n    Returns:\\n        Processed actions as a tensor.\\n    \"\"\"\\n    # compute actions based on environment\\n    if \"Reach\" in args_cli.task:\\n        delta_pose, gripper_command = teleop_data\\n        # convert to torch\\n        delta_pose = torch.tensor(delta_pose, dtype=torch.float, device=device).repeat(num_envs, 1)\\n        # note: reach is the only one that uses a different action space\\n        # compute actions\\n        return delta_pose\\n    elif \"PickPlace-GR1T2\" in args_cli.task:\\n        (left_wrist_pose, right_wrist_pose, hand_joints) = teleop_data[0]\\n        # Reconstruct actions_arms tensor with converted positions and rotations\\n        actions = torch.tensor(\\n            np.concatenate([\\n                left_wrist_pose,  # left ee pose\\n                right_wrist_pose,  # right ee pose\\n                hand_joints,  # hand joint angles\\n            ]),\\n            device=device,\\n            dtype=torch.float32,\\n        ).unsqueeze(0)\\n        # Concatenate arm poses and hand joint angles\\n        return actions\\n    else:\\n        # resolve gripper command\\n        delta_pose, gripper_command = teleop_data\\n        # convert to torch\\n        delta_pose = torch.tensor(delta_pose, dtype=torch.float, device=device).repeat(num_envs, 1)\\n        gripper_vel = torch.zeros((delta_pose.shape[0], 1), dtype=torch.float, device=device)\\n        gripper_vel[:] = -1 if gripper_command else 1\\n        # compute actions\\n        return torch.concat([delta_pose, gripper_vel], dim=1)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Collect demonstrations from the environment using teleop interfaces.\"\"\"\\n\\n    # if handtracking is selected, rate limiting is achieved via OpenXR\\n    if \"handtracking\" in args_cli.teleop_device.lower():\\n        rate_limiter = None\\n    else:\\n        rate_limiter = RateLimiter(args_cli.step_hz)\\n\\n    # get directory path and file name (without extension) from cli arguments\\n    output_dir = os.path.dirname(args_cli.dataset_file)\\n    output_file_name = os.path.splitext(os.path.basename(args_cli.dataset_file))[0]\\n\\n    # create directory if it does not exist\\n    if not os.path.exists(output_dir):\\n        os.makedirs(output_dir)\\n\\n    # parse configuration\\n    env_cfg = parse_env_cfg(args_cli.task, device=args_cli.device, num_envs=1)\\n    env_cfg.env_name = args_cli.task\\n\\n    # extract success checking function to invoke in the main loop\\n    success_term = None\\n    if hasattr(env_cfg.terminations, \"success\"):\\n        success_term = env_cfg.terminations.success\\n        env_cfg.terminations.success = None\\n    else:\\n        omni.log.warn(\\n            \"No success termination term was found in the environment.\"\\n            \" Will not be able to mark recorded demos as successful.\"\\n        )\\n\\n    # modify configuration such that the environment runs indefinitely until\\n    # the goal is reached or other termination conditions are met\\n    env_cfg.terminations.time_out = None\\n\\n    env_cfg.observations.policy.concatenate_terms = False\\n\\n    env_cfg.recorders: ActionStateRecorderManagerCfg = ActionStateRecorderManagerCfg()\\n    env_cfg.recorders.dataset_export_dir_path = output_dir\\n    env_cfg.recorders.dataset_filename = output_file_name\\n    env_cfg.recorders.dataset_export_mode = DatasetExportMode.EXPORT_SUCCEEDED_ONLY\\n\\n    # create environment\\n    env = gym.make(args_cli.task, cfg=env_cfg).unwrapped\\n\\n    # Flags for controlling the demonstration recording process\\n    should_reset_recording_instance = False\\n    running_recording_instance = True\\n\\n    def reset_recording_instance():\\n        \"\"\"Reset the current recording instance.\\n\\n        This function is triggered when the user indicates the current demo attempt\\n        has failed and should be discarded. When called, it marks the environment\\n        for reset, which will start a fresh recording instance. This is useful when:\\n        - The robot gets into an unrecoverable state\\n        - The user makes a mistake during demonstration\\n        - The objects in the scene need to be reset to their initial positions\\n        \"\"\"\\n        nonlocal should_reset_recording_instance\\n        should_reset_recording_instance = True\\n\\n    def start_recording_instance():\\n        \"\"\"Start or resume recording the current demonstration.\\n\\n        This function enables active recording of robot actions. It\\'s used when:\\n        - Beginning a new demonstration after positioning the robot\\n        - Resuming recording after temporarily stopping to reposition\\n        - Continuing demonstration after pausing to adjust approach or strategy\\n\\n        The user can toggle between stop/start to reposition the robot without\\n        recording those transitional movements in the final demonstration.\\n        \"\"\"\\n        nonlocal running_recording_instance\\n        running_recording_instance = True\\n\\n    def stop_recording_instance():\\n        \"\"\"Temporarily stop recording the current demonstration.\\n\\n        This function pauses the active recording of robot actions, allowing the user to:\\n        - Reposition the robot or hand tracking device without recording those movements\\n        - Take a break without terminating the entire demonstration\\n        - Adjust their approach before continuing with the task\\n\\n        The environment will continue rendering but won\\'t record actions or advance\\n        the simulation until recording is resumed with start_recording_instance().\\n        \"\"\"\\n        nonlocal running_recording_instance\\n        running_recording_instance = False\\n\\n    def create_teleop_device(device_name: str, env: gym.Env):\\n        \"\"\"Create and configure teleoperation device for robot control.\\n\\n        Args:\\n            device_name: Control device to use. Options include:\\n                - \"keyboard\": Use keyboard keys for simple discrete movements\\n                - \"spacemouse\": Use 3D mouse for precise 6-DOF control\\n                - \"handtracking\": Use VR hand tracking for intuitive manipulation\\n                - \"handtracking_abs\": Use VR hand tracking for intuitive manipulation with absolute EE pose\\n\\n        Returns:\\n            DeviceBase: Configured teleoperation device ready for robot control\\n        \"\"\"\\n        device_name = device_name.lower()\\n        nonlocal running_recording_instance\\n        if device_name == \"keyboard\":\\n            return Se3Keyboard(pos_sensitivity=0.2, rot_sensitivity=0.5)\\n        elif device_name == \"spacemouse\":\\n            return Se3SpaceMouse(pos_sensitivity=0.2, rot_sensitivity=0.5)\\n        elif \"dualhandtracking_abs\" in device_name and \"GR1T2\" in env.cfg.env_name:\\n            # Create GR1T2 retargeter with desired configuration\\n            gr1t2_retargeter = GR1T2Retargeter(\\n                enable_visualization=True,\\n                num_open_xr_hand_joints=2 * (int(OpenXRSpec.HandJointEXT.XR_HAND_JOINT_LITTLE_TIP_EXT) + 1),\\n                device=env.unwrapped.device,\\n                hand_joint_names=env.scene[\"robot\"].data.joint_names[-22:],\\n            )\\n\\n            # Create hand tracking device with retargeter\\n            device = OpenXRDevice(\\n                env_cfg.xr,\\n                retargeters=[gr1t2_retargeter],\\n            )\\n            device.add_callback(\"RESET\", reset_recording_instance)\\n            device.add_callback(\"START\", start_recording_instance)\\n            device.add_callback(\"STOP\", stop_recording_instance)\\n\\n            running_recording_instance = False\\n            return device\\n        elif \"handtracking\" in device_name:\\n            # Create Franka retargeter with desired configuration\\n            if \"_abs\" in device_name:\\n                retargeter_device = Se3AbsRetargeter(\\n                    bound_hand=OpenXRDevice.TrackingTarget.HAND_RIGHT, zero_out_xy_rotation=True\\n                )\\n            else:\\n                retargeter_device = Se3RelRetargeter(\\n                    bound_hand=OpenXRDevice.TrackingTarget.HAND_RIGHT, zero_out_xy_rotation=True\\n                )\\n\\n            grip_retargeter = GripperRetargeter(bound_hand=OpenXRDevice.TrackingTarget.HAND_RIGHT)\\n\\n            # Create hand tracking device with retargeter (in a list)\\n            device = OpenXRDevice(\\n                env_cfg.xr,\\n                retargeters=[retargeter_device, grip_retargeter],\\n            )\\n            device.add_callback(\"RESET\", reset_recording_instance)\\n            device.add_callback(\"START\", start_recording_instance)\\n            device.add_callback(\"STOP\", stop_recording_instance)\\n\\n            running_recording_instance = False\\n            return device\\n        else:\\n            raise ValueError(\\n                f\"Invalid device interface \\'{device_name}\\'. Supported: \\'keyboard\\', \\'spacemouse\\', \\'handtracking\\',\"\\n                \" \\'handtracking_abs\\', \\'dualhandtracking_abs\\'.\"\\n            )\\n\\n    teleop_interface = create_teleop_device(args_cli.teleop_device, env)\\n    teleop_interface.add_callback(\"R\", reset_recording_instance)\\n    print(teleop_interface)\\n\\n    # reset before starting\\n    env.sim.reset()\\n    env.reset()\\n    teleop_interface.reset()\\n\\n    # simulate environment -- run everything in inference mode\\n    current_recorded_demo_count = 0\\n    success_step_count = 0\\n\\n    label_text = f\"Recorded {current_recorded_demo_count} successful demonstrations.\"\\n\\n    instruction_display = InstructionDisplay(args_cli.teleop_device)\\n    if args_cli.teleop_device.lower() != \"handtracking\":\\n        window = EmptyWindow(env, \"Instruction\")\\n        with window.ui_window_elements[\"main_vstack\"]:\\n            demo_label = ui.Label(label_text)\\n            subtask_label = ui.Label(\"\")\\n            instruction_display.set_labels(subtask_label, demo_label)\\n\\n    subtasks = {}\\n\\n    with contextlib.suppress(KeyboardInterrupt) and torch.inference_mode():\\n        while simulation_app.is_running():\\n            # get data from teleop device\\n            teleop_data = teleop_interface.advance()\\n\\n            # perform action on environment\\n            if running_recording_instance:\\n                # compute actions based on environment\\n                actions = pre_process_actions(teleop_data, env.num_envs, env.device)\\n                obv = env.step(actions)\\n                if subtasks is not None:\\n                    if subtasks == {}:\\n                        subtasks = obv[0].get(\"subtask_terms\")\\n                    elif subtasks:\\n                        show_subtask_instructions(instruction_display, subtasks, obv, env.cfg)\\n            else:\\n                env.sim.render()\\n\\n            if success_term is not None:\\n                if bool(success_term.func(env, **success_term.params)[0]):\\n                    success_step_count += 1\\n                    if success_step_count >= args_cli.num_success_steps:\\n                        env.recorder_manager.record_pre_reset([0], force_export_or_skip=False)\\n                        env.recorder_manager.set_success_to_episodes(\\n                            [0], torch.tensor([[True]], dtype=torch.bool, device=env.device)\\n                        )\\n                        env.recorder_manager.export_episodes([0])\\n                        should_reset_recording_instance = True\\n                else:\\n                    success_step_count = 0\\n\\n            # print out the current demo count if it has changed\\n            if env.recorder_manager.exported_successful_episode_count > current_recorded_demo_count:\\n                current_recorded_demo_count = env.recorder_manager.exported_successful_episode_count\\n                label_text = f\"Recorded {current_recorded_demo_count} successful demonstrations.\"\\n                print(label_text)\\n\\n            if should_reset_recording_instance:\\n                env.sim.reset()\\n                env.recorder_manager.reset()\\n                env.reset()\\n                should_reset_recording_instance = False\\n                success_step_count = 0\\n                instruction_display.show_demo(label_text)\\n\\n            if args_cli.num_demos > 0 and env.recorder_manager.exported_successful_episode_count >= args_cli.num_demos:\\n                print(f\"All {args_cli.num_demos} demonstrations recorded. Exiting the app.\")\\n                break\\n\\n            # check that simulation is stopped or not\\n            if env.sim.is_stopped():\\n                break\\n\\n            if rate_limiter:\\n                rate_limiter.sleep(env)\\n\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def play_cb():\\n    global is_paused\\n    is_paused = False'),\n",
       " Document(metadata={}, page_content='def pause_cb():\\n    global is_paused\\n    is_paused = True'),\n",
       " Document(metadata={}, page_content='def compare_states(state_from_dataset, runtime_state, runtime_env_index) -> (bool, str):\\n    \"\"\"Compare states from dataset and runtime.\\n\\n    Args:\\n        state_from_dataset: State from dataset.\\n        runtime_state: State from runtime.\\n        runtime_env_index: Index of the environment in the runtime states to be compared.\\n\\n    Returns:\\n        bool: True if states match, False otherwise.\\n        str: Log message if states don\\'t match.\\n    \"\"\"\\n    states_matched = True\\n    output_log = \"\"\\n    for asset_type in [\"articulation\", \"rigid_object\"]:\\n        for asset_name in runtime_state[asset_type].keys():\\n            for state_name in runtime_state[asset_type][asset_name].keys():\\n                runtime_asset_state = runtime_state[asset_type][asset_name][state_name][runtime_env_index]\\n                dataset_asset_state = state_from_dataset[asset_type][asset_name][state_name]\\n                if len(dataset_asset_state) != len(runtime_asset_state):\\n                    raise ValueError(f\"State shape of {state_name} for asset {asset_name} don\\'t match\")\\n                for i in range(len(dataset_asset_state)):\\n                    if abs(dataset_asset_state[i] - runtime_asset_state[i]) > 0.01:\\n                        states_matched = False\\n                        output_log += f\\'\\\\tState [\"{asset_type}\"][\"{asset_name}\"][\"{state_name}\"][{i}] don\\\\\\'t match\\\\r\\\\n\\'\\n                        output_log += f\"\\\\t  Dataset:\\\\t{dataset_asset_state[i]}\\\\r\\\\n\"\\n                        output_log += f\"\\\\t  Runtime: \\\\t{runtime_asset_state[i]}\\\\r\\\\n\"\\n    return states_matched, output_log'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Replay episodes loaded from a file.\"\"\"\\n    global is_paused\\n\\n    # Load dataset\\n    if not os.path.exists(args_cli.dataset_file):\\n        raise FileNotFoundError(f\"The dataset file {args_cli.dataset_file} does not exist.\")\\n    dataset_file_handler = HDF5DatasetFileHandler()\\n    dataset_file_handler.open(args_cli.dataset_file)\\n    env_name = dataset_file_handler.get_env_name()\\n    episode_count = dataset_file_handler.get_num_episodes()\\n\\n    if episode_count == 0:\\n        print(\"No episodes found in the dataset.\")\\n        exit()\\n\\n    episode_indices_to_replay = args_cli.select_episodes\\n    if len(episode_indices_to_replay) == 0:\\n        episode_indices_to_replay = list(range(episode_count))\\n\\n    if args_cli.task is not None:\\n        env_name = args_cli.task\\n    if env_name is None:\\n        raise ValueError(\"Task/env name was not specified nor found in the dataset.\")\\n\\n    num_envs = args_cli.num_envs\\n\\n    env_cfg = parse_env_cfg(env_name, device=args_cli.device, num_envs=num_envs)\\n\\n    # Disable all recorders and terminations\\n    env_cfg.recorders = {}\\n    env_cfg.terminations = {}\\n\\n    # create environment from loaded config\\n    env = gym.make(env_name, cfg=env_cfg).unwrapped\\n\\n    teleop_interface = Se3Keyboard(pos_sensitivity=0.1, rot_sensitivity=0.1)\\n    teleop_interface.add_callback(\"N\", play_cb)\\n    teleop_interface.add_callback(\"B\", pause_cb)\\n    print(\\'Press \"B\" to pause and \"N\" to resume the replayed actions.\\')\\n\\n    # Determine if state validation should be conducted\\n    state_validation_enabled = False\\n    if args_cli.validate_states and num_envs == 1:\\n        state_validation_enabled = True\\n    elif args_cli.validate_states and num_envs > 1:\\n        print(\"Warning: State validation is only supported with a single environment. Skipping state validation.\")\\n\\n    # Get idle action (idle actions are applied to envs without next action)\\n    if hasattr(env_cfg, \"idle_action\"):\\n        idle_action = env_cfg.idle_action.repeat(num_envs, 1)\\n    else:\\n        idle_action = torch.zeros(env.action_space.shape)\\n\\n    # reset before starting\\n    env.reset()\\n    teleop_interface.reset()\\n\\n    # simulate environment -- run everything in inference mode\\n    episode_names = list(dataset_file_handler.get_episode_names())\\n    replayed_episode_count = 0\\n    with contextlib.suppress(KeyboardInterrupt) and torch.inference_mode():\\n        while simulation_app.is_running() and not simulation_app.is_exiting():\\n            env_episode_data_map = {index: EpisodeData() for index in range(num_envs)}\\n            first_loop = True\\n            has_next_action = True\\n            while has_next_action:\\n                # initialize actions with idle action so those without next action will not move\\n                actions = idle_action\\n                has_next_action = False\\n                for env_id in range(num_envs):\\n                    env_next_action = env_episode_data_map[env_id].get_next_action()\\n                    if env_next_action is None:\\n                        next_episode_index = None\\n                        while episode_indices_to_replay:\\n                            next_episode_index = episode_indices_to_replay.pop(0)\\n                            if next_episode_index < episode_count:\\n                                break\\n                            next_episode_index = None\\n\\n                        if next_episode_index is not None:\\n                            replayed_episode_count += 1\\n                            print(f\"{replayed_episode_count :4}: Loading #{next_episode_index} episode to env_{env_id}\")\\n                            episode_data = dataset_file_handler.load_episode(\\n                                episode_names[next_episode_index], env.device\\n                            )\\n                            env_episode_data_map[env_id] = episode_data\\n                            # Set initial state for the new episode\\n                            initial_state = episode_data.get_initial_state()\\n                            env.reset_to(initial_state, torch.tensor([env_id], device=env.device), is_relative=True)\\n                            # Get the first action for the new episode\\n                            env_next_action = env_episode_data_map[env_id].get_next_action()\\n                            has_next_action = True\\n                        else:\\n                            continue\\n                    else:\\n                        has_next_action = True\\n                    actions[env_id] = env_next_action\\n                if first_loop:\\n                    first_loop = False\\n                else:\\n                    while is_paused:\\n                        env.sim.render()\\n                        continue\\n                env.step(actions)\\n\\n                if state_validation_enabled:\\n                    state_from_dataset = env_episode_data_map[0].get_next_state()\\n                    if state_from_dataset is not None:\\n                        print(\\n                            f\"Validating states at action-index: {env_episode_data_map[0].next_state_index - 1 :4}\",\\n                            end=\"\",\\n                        )\\n                        current_runtime_state = env.scene.get_state(is_relative=True)\\n                        states_matched, comparison_log = compare_states(state_from_dataset, current_runtime_state, 0)\\n                        if states_matched:\\n                            print(\"\\\\t- matched.\")\\n                        else:\\n                            print(\"\\\\t- mismatched.\")\\n                            print(comparison_log)\\n            break\\n    # Close environment after replay in complete\\n    plural_trailing_s = \"s\" if replayed_episode_count > 1 else \"\"\\n    print(f\"Finished replaying {replayed_episode_count} episode{plural_trailing_s}.\")\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = SimulationCfg(dt=0.01)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 2.5, 2.5], [0.0, 0.0, 0.0])\\n\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # perform step\\n        sim.step()'),\n",
       " Document(metadata={}, page_content='def design_scene():\\n    \"\"\"Designs the scene by spawning ground plane, light, objects and meshes from usd files.\"\"\"\\n    # Ground-plane\\n    cfg_ground = sim_utils.GroundPlaneCfg()\\n    cfg_ground.func(\"/World/defaultGroundPlane\", cfg_ground)\\n\\n    # spawn distant light\\n    cfg_light_distant = sim_utils.DistantLightCfg(\\n        intensity=3000.0,\\n        color=(0.75, 0.75, 0.75),\\n    )\\n    cfg_light_distant.func(\"/World/lightDistant\", cfg_light_distant, translation=(1, 0, 10))\\n\\n    # spawn a cuboid\\n    cfg_cuboid = sim_utils.CuboidCfg(\\n        size=[args_cli.size] * 3,\\n        visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 1.0, 1.0)),\\n    )\\n    # Spawn cuboid, altering translation on the z-axis to scale to its size\\n    cfg_cuboid.func(\"/World/Object\", cfg_cuboid, translation=(0.0, 0.0, args_cli.size / 2))'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.0, 0.0, 2.5], [-0.5, 0.0, 0.5])\\n\\n    # Design scene by adding assets to it\\n    design_scene()\\n\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # perform step\\n        sim.step()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Specify that the logs must be in logs/docker_tutorial\\n    log_dir_path = os.path.join(\"logs\")\\n    if not os.path.isdir(log_dir_path):\\n        os.mkdir(log_dir_path)\\n    # In the container, the absolute path will be\\n    # /workspace/isaaclab/logs/docker_tutorial, because\\n    # all python execution is done through /workspace/isaaclab/isaaclab.sh\\n    # and the calling process\\' path will be /workspace/isaaclab\\n    log_dir_path = os.path.abspath(os.path.join(log_dir_path, \"docker_tutorial\"))\\n    if not os.path.isdir(log_dir_path):\\n        os.mkdir(log_dir_path)\\n    print(f\"[INFO] Logging experiment to directory: {log_dir_path}\")\\n\\n    # Initialize the simulation context\\n    sim_cfg = SimulationCfg(dt=0.01)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 2.5, 2.5], [0.0, 0.0, 0.0])\\n\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Prepare to count sim_time\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n\\n    # Open logging file\\n    with open(os.path.join(log_dir_path, \"log.txt\"), \"w\") as log_file:\\n        # Simulate physics\\n        while simulation_app.is_running():\\n            log_file.write(f\"{sim_time}\" + \"\\\\n\")\\n            # perform step\\n            sim.step()\\n            sim_time += sim_dt'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # rendering modes include performance, balanced, and quality\\n    rendering_mode = \"performance\"\\n\\n    # carb setting dictionary can include any rtx carb setting which will overwrite the native preset setting\\n    carb_settings = {\"rtx.reflections.enabled\": True}\\n\\n    # Initialize render config\\n    render_cfg = sim_utils.RenderCfg(\\n        rendering_mode=rendering_mode,\\n        carb_settings=carb_settings,\\n    )\\n\\n    # Initialize the simulation context with render coofig\\n    sim_cfg = sim_utils.SimulationCfg(render=render_cfg)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n\\n    # Pose camera in the hospital lobby area\\n    sim.set_camera_view([-11, -0.5, 2], [0, 0, 0.5])\\n\\n    # Load hospital scene\\n    hospital_usd_path = f\"{ISAAC_NUCLEUS_DIR}/Environments/Hospital/hospital.usd\"\\n    cfg = sim_utils.UsdFileCfg(usd_path=hospital_usd_path)\\n    cfg.func(\"/Scene\", cfg)\\n\\n    # Play the simulator\\n    sim.reset()\\n\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Run simulation and view scene\\n    while simulation_app.is_running():\\n        sim.step()'),\n",
       " Document(metadata={}, page_content='def design_scene():\\n    \"\"\"Designs the scene by spawning ground plane, light, objects and meshes from usd files.\"\"\"\\n    # Ground-plane\\n    cfg_ground = sim_utils.GroundPlaneCfg()\\n    cfg_ground.func(\"/World/defaultGroundPlane\", cfg_ground)\\n\\n    # spawn distant light\\n    cfg_light_distant = sim_utils.DistantLightCfg(\\n        intensity=3000.0,\\n        color=(0.75, 0.75, 0.75),\\n    )\\n    cfg_light_distant.func(\"/World/lightDistant\", cfg_light_distant, translation=(1, 0, 10))\\n\\n    # create a new xform prim for all objects to be spawned under\\n    prim_utils.create_prim(\"/World/Objects\", \"Xform\")\\n    # spawn a red cone\\n    cfg_cone = sim_utils.ConeCfg(\\n        radius=0.15,\\n        height=0.5,\\n        visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0)),\\n    )\\n    cfg_cone.func(\"/World/Objects/Cone1\", cfg_cone, translation=(-1.0, 1.0, 1.0))\\n    cfg_cone.func(\"/World/Objects/Cone2\", cfg_cone, translation=(-1.0, -1.0, 1.0))\\n\\n    # spawn a green cone with colliders and rigid body\\n    cfg_cone_rigid = sim_utils.ConeCfg(\\n        radius=0.15,\\n        height=0.5,\\n        rigid_props=sim_utils.RigidBodyPropertiesCfg(),\\n        mass_props=sim_utils.MassPropertiesCfg(mass=1.0),\\n        collision_props=sim_utils.CollisionPropertiesCfg(),\\n        visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0)),\\n    )\\n    cfg_cone_rigid.func(\\n        \"/World/Objects/ConeRigid\", cfg_cone_rigid, translation=(-0.2, 0.0, 2.0), orientation=(0.5, 0.0, 0.5, 0.0)\\n    )\\n\\n    # spawn a blue cuboid with deformable body\\n    cfg_cuboid_deformable = sim_utils.MeshCuboidCfg(\\n        size=(0.2, 0.5, 0.2),\\n        deformable_props=sim_utils.DeformableBodyPropertiesCfg(),\\n        visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 0.0, 1.0)),\\n        physics_material=sim_utils.DeformableBodyMaterialCfg(),\\n    )\\n    cfg_cuboid_deformable.func(\"/World/Objects/CuboidDeformable\", cfg_cuboid_deformable, translation=(0.15, 0.0, 2.0))\\n\\n    # spawn a usd file of a table into the scene\\n    cfg = sim_utils.UsdFileCfg(usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/SeattleLabTable/table_instanceable.usd\")\\n    cfg.func(\"/World/Objects/Table\", cfg, translation=(0.0, 0.0, 1.05))'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.0, 0.0, 2.5], [-0.5, 0.0, 0.5])\\n\\n    # Design scene by adding assets to it\\n    design_scene()\\n\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # perform step\\n        sim.step()'),\n",
       " Document(metadata={}, page_content='class NewRobotsSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Designs the scene.\"\"\"\\n\\n    # Ground-plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # robot\\n    Jetbot = JETBOT_CONFIG.replace(prim_path=\"{ENV_REGEX_NS}/Jetbot\")\\n    Dofbot = DOFBOT_CONFIG.replace(prim_path=\"{ENV_REGEX_NS}/Dofbot\")'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 500 == 0:\\n            # reset counters\\n            count = 0\\n            # reset the scene entities to their initial positions offset by the environment origins\\n            root_jetbot_state = scene[\"Jetbot\"].data.default_root_state.clone()\\n            root_jetbot_state[:, :3] += scene.env_origins\\n            root_dofbot_state = scene[\"Dofbot\"].data.default_root_state.clone()\\n            root_dofbot_state[:, :3] += scene.env_origins\\n\\n            # copy the default root state to the sim for the jetbot\\'s orientation and velocity\\n            scene[\"Jetbot\"].write_root_pose_to_sim(root_jetbot_state[:, :7])\\n            scene[\"Jetbot\"].write_root_velocity_to_sim(root_jetbot_state[:, 7:])\\n            scene[\"Dofbot\"].write_root_pose_to_sim(root_dofbot_state[:, :7])\\n            scene[\"Dofbot\"].write_root_velocity_to_sim(root_dofbot_state[:, 7:])\\n\\n            # copy the default joint states to the sim\\n            joint_pos, joint_vel = (\\n                scene[\"Jetbot\"].data.default_joint_pos.clone(),\\n                scene[\"Jetbot\"].data.default_joint_vel.clone(),\\n            )\\n            scene[\"Jetbot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            joint_pos, joint_vel = (\\n                scene[\"Dofbot\"].data.default_joint_pos.clone(),\\n                scene[\"Dofbot\"].data.default_joint_vel.clone(),\\n            )\\n            scene[\"Dofbot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting Jetbot and Dofbot state...\")\\n\\n        # drive around\\n        if count % 100 < 75:\\n            # Drive straight by setting equal wheel velocities\\n            action = torch.Tensor([[10.0, 10.0]])\\n        else:\\n            # Turn by applying different velocities\\n            action = torch.Tensor([[5.0, -5.0]])\\n\\n        scene[\"Jetbot\"].set_joint_velocity_target(action)\\n\\n        # wave\\n        wave_action = scene[\"Dofbot\"].data.default_joint_pos\\n        wave_action[:, 0:4] = 0.25 * np.sin(2 * np.pi * 0.5 * sim_time)\\n        scene[\"Dofbot\"].set_joint_position_target(wave_action)\\n\\n        scene.write_data_to_sim()\\n        sim.step()\\n        sim_time += sim_dt\\n        count += 1\\n        scene.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n\\n    sim.set_camera_view([3.5, 0.0, 3.2], [0.0, 0.0, 0.5])\\n    # design scene\\n    scene_cfg = NewRobotsSceneCfg(args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='def design_scene() -> tuple[dict, list[list[float]]]:\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create separate groups called \"Origin1\", \"Origin2\"\\n    # Each group will have a robot in it\\n    origins = [[0.0, 0.0, 0.0], [-1.0, 0.0, 0.0]]\\n    # Origin 1\\n    prim_utils.create_prim(\"/World/Origin1\", \"Xform\", translation=origins[0])\\n    # Origin 2\\n    prim_utils.create_prim(\"/World/Origin2\", \"Xform\", translation=origins[1])\\n\\n    # Articulation\\n    cartpole_cfg = CARTPOLE_CFG.copy()\\n    cartpole_cfg.prim_path = \"/World/Origin.*/Robot\"\\n    cartpole = Articulation(cfg=cartpole_cfg)\\n\\n    # return the scene information\\n    scene_entities = {\"cartpole\": cartpole}\\n    return scene_entities, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, Articulation], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Extract scene entities\\n    # note: we only do this here for readability. In general, it is better to access the entities directly from\\n    #   the dictionary. This dictionary is replaced by the InteractiveScene class in the next tutorial.\\n    robot = entities[\"cartpole\"]\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    count = 0\\n    # Simulation loop\\n    while simulation_app.is_running():\\n        # Reset\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = robot.data.default_root_state.clone()\\n            root_state[:, :3] += origins\\n            robot.write_root_pose_to_sim(root_state[:, :7])\\n            robot.write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = robot.data.default_joint_pos.clone(), robot.data.default_joint_vel.clone()\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            robot.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply random action\\n        # -- generate random joint efforts\\n        efforts = torch.randn_like(robot.data.joint_pos) * 5.0\\n        # -- apply action to the robot\\n        robot.set_joint_effort_target(efforts)\\n        # -- write data to sim\\n        robot.write_data_to_sim()\\n        # Perform step\\n        sim.step()\\n        # Increment counter\\n        count += 1\\n        # Update buffers\\n        robot.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 0.0, 4.0], [0.0, 0.0, 2.0])\\n    # Design scene\\n    scene_entities, scene_origins = design_scene()\\n    scene_origins = torch.tensor(scene_origins, device=sim.device)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='def design_scene():\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.8, 0.8, 0.8))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create separate groups called \"Origin1\", \"Origin2\", \"Origin3\"\\n    # Each group will have a robot in it\\n    origins = [[0.25, 0.25, 0.0], [-0.25, 0.25, 0.0], [0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]]\\n    for i, origin in enumerate(origins):\\n        prim_utils.create_prim(f\"/World/Origin{i}\", \"Xform\", translation=origin)\\n\\n    # Deformable Object\\n    cfg = DeformableObjectCfg(\\n        prim_path=\"/World/Origin.*/Cube\",\\n        spawn=sim_utils.MeshCuboidCfg(\\n            size=(0.2, 0.2, 0.2),\\n            deformable_props=sim_utils.DeformableBodyPropertiesCfg(rest_offset=0.0, contact_offset=0.001),\\n            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.5, 0.1, 0.0)),\\n            physics_material=sim_utils.DeformableBodyMaterialCfg(poissons_ratio=0.4, youngs_modulus=1e5),\\n        ),\\n        init_state=DeformableObjectCfg.InitialStateCfg(pos=(0.0, 0.0, 1.0)),\\n        debug_vis=True,\\n    )\\n    cube_object = DeformableObject(cfg=cfg)\\n\\n    # return the scene information\\n    scene_entities = {\"cube_object\": cube_object}\\n    return scene_entities, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, DeformableObject], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Extract scene entities\\n    # note: we only do this here for readability. In general, it is better to access the entities directly from\\n    #   the dictionary. This dictionary is replaced by the InteractiveScene class in the next tutorial.\\n    cube_object = entities[\"cube_object\"]\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    # Nodal kinematic targets of the deformable bodies\\n    nodal_kinematic_target = cube_object.data.nodal_kinematic_target.clone()\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 250 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n\\n            # reset the nodal state of the object\\n            nodal_state = cube_object.data.default_nodal_state_w.clone()\\n            # apply random pose to the object\\n            pos_w = torch.rand(cube_object.num_instances, 3, device=sim.device) * 0.1 + origins\\n            quat_w = math_utils.random_orientation(cube_object.num_instances, device=sim.device)\\n            nodal_state[..., :3] = cube_object.transform_nodal_pos(nodal_state[..., :3], pos_w, quat_w)\\n\\n            # write nodal state to simulation\\n            cube_object.write_nodal_state_to_sim(nodal_state)\\n\\n            # Write the nodal state to the kinematic target and free all vertices\\n            nodal_kinematic_target[..., :3] = nodal_state[..., :3]\\n            nodal_kinematic_target[..., 3] = 1.0\\n            cube_object.write_nodal_kinematic_target_to_sim(nodal_kinematic_target)\\n\\n            # reset buffers\\n            cube_object.reset()\\n\\n            print(\"----------------------------------------\")\\n            print(\"[INFO]: Resetting object state...\")\\n\\n        # update the kinematic target for cubes at index 0 and 3\\n        # we slightly move the cube in the z-direction by picking the vertex at index 0\\n        nodal_kinematic_target[[0, 3], 0, 2] += 0.001\\n        # set vertex at index 0 to be kinematically constrained\\n        # 0: constrained, 1: free\\n        nodal_kinematic_target[[0, 3], 0, 3] = 0.0\\n        # write kinematic target to simulation\\n        cube_object.write_nodal_kinematic_target_to_sim(nodal_kinematic_target)\\n\\n        # write internal data to simulation\\n        cube_object.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        cube_object.update(sim_dt)\\n        # print the root position\\n        if count % 50 == 0:\\n            print(f\"Root position (in world): {cube_object.data.root_pos_w[:, :3]}\")'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.0, 0.0, 1.0], target=[0.0, 0.0, 0.5])\\n    # Design scene\\n    scene_entities, scene_origins = design_scene()\\n    scene_origins = torch.tensor(scene_origins, device=sim.device)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='def design_scene():\\n    \"\"\"Designs the scene.\"\"\"\\n    # Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # Lights\\n    cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.8, 0.8, 0.8))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create separate groups called \"Origin1\", \"Origin2\", \"Origin3\"\\n    # Each group will have a robot in it\\n    origins = [[0.25, 0.25, 0.0], [-0.25, 0.25, 0.0], [0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]]\\n    for i, origin in enumerate(origins):\\n        prim_utils.create_prim(f\"/World/Origin{i}\", \"Xform\", translation=origin)\\n\\n    # Rigid Object\\n    cone_cfg = RigidObjectCfg(\\n        prim_path=\"/World/Origin.*/Cone\",\\n        spawn=sim_utils.ConeCfg(\\n            radius=0.1,\\n            height=0.2,\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(),\\n            mass_props=sim_utils.MassPropertiesCfg(mass=1.0),\\n            collision_props=sim_utils.CollisionPropertiesCfg(),\\n            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0), metallic=0.2),\\n        ),\\n        init_state=RigidObjectCfg.InitialStateCfg(),\\n    )\\n    cone_object = RigidObject(cfg=cone_cfg)\\n\\n    # return the scene information\\n    scene_entities = {\"cone\": cone_object}\\n    return scene_entities, origins'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, RigidObject], origins: torch.Tensor):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Extract scene entities\\n    # note: we only do this here for readability. In general, it is better to access the entities directly from\\n    #   the dictionary. This dictionary is replaced by the InteractiveScene class in the next tutorial.\\n    cone_object = entities[\"cone\"]\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 250 == 0:\\n            # reset counters\\n            sim_time = 0.0\\n            count = 0\\n            # reset root state\\n            root_state = cone_object.data.default_root_state.clone()\\n            # sample a random position on a cylinder around the origins\\n            root_state[:, :3] += origins\\n            root_state[:, :3] += math_utils.sample_cylinder(\\n                radius=0.1, h_range=(0.25, 0.5), size=cone_object.num_instances, device=cone_object.device\\n            )\\n            # write root state to simulation\\n            cone_object.write_root_pose_to_sim(root_state[:, :7])\\n            cone_object.write_root_velocity_to_sim(root_state[:, 7:])\\n            # reset buffers\\n            cone_object.reset()\\n            print(\"----------------------------------------\")\\n            print(\"[INFO]: Resetting object state...\")\\n        # apply sim data\\n        cone_object.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        cone_object.update(sim_dt)\\n        # print the root position\\n        if count % 50 == 0:\\n            print(f\"Root position (in world): {cone_object.data.root_state_w[:, :3]}\")'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[1.5, 0.0, 1.0], target=[0.0, 0.0, 0.0])\\n    # Design scene\\n    scene_entities, scene_origins = design_scene()\\n    scene_origins = torch.tensor(scene_origins, device=sim.device)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities, scene_origins)'),\n",
       " Document(metadata={}, page_content='class CartpoleSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Configuration for a cart-pole scene.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # articulation\\n    cartpole: ArticulationCfg = CARTPOLE_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Extract scene entities\\n    # note: we only do this here for readability.\\n    robot = scene[\"cartpole\"]\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    count = 0\\n    # Simulation loop\\n    while simulation_app.is_running():\\n        # Reset\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = robot.data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            robot.write_root_pose_to_sim(root_state[:, :7])\\n            robot.write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = robot.data.default_joint_pos.clone(), robot.data.default_joint_vel.clone()\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply random action\\n        # -- generate random joint efforts\\n        efforts = torch.randn_like(robot.data.joint_pos) * 5.0\\n        # -- apply action to the robot\\n        robot.set_joint_effort_target(efforts)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # Perform step\\n        sim.step()\\n        # Increment counter\\n        count += 1\\n        # Update buffers\\n        scene.update(sim_dt)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 0.0, 4.0], [0.0, 0.0, 2.0])\\n    # Design scene\\n    scene_cfg = CartpoleSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='class ActionsCfg:\\n    \"\"\"Action specifications for the environment.\"\"\"\\n\\n    joint_efforts = mdp.JointEffortActionCfg(asset_name=\"robot\", joint_names=[\"slider_to_cart\"], scale=5.0)'),\n",
       " Document(metadata={}, page_content='class ObservationsCfg:\\n    \"\"\"Observation specifications for the environment.\"\"\"\\n\\n    @configclass\\n    class PolicyCfg(ObsGroup):\\n        \"\"\"Observations for policy group.\"\"\"\\n\\n        # observation terms (order preserved)\\n        joint_pos_rel = ObsTerm(func=mdp.joint_pos_rel)\\n        joint_vel_rel = ObsTerm(func=mdp.joint_vel_rel)\\n\\n        def __post_init__(self) -> None:\\n            self.enable_corruption = False\\n            self.concatenate_terms = True\\n\\n    # observation groups\\n    policy: PolicyCfg = PolicyCfg()'),\n",
       " Document(metadata={}, page_content='class EventCfg:\\n    \"\"\"Configuration for events.\"\"\"\\n\\n    # on startup\\n    add_pole_mass = EventTerm(\\n        func=mdp.randomize_rigid_body_mass,\\n        mode=\"startup\",\\n        params={\\n            \"asset_cfg\": SceneEntityCfg(\"robot\", body_names=[\"pole\"]),\\n            \"mass_distribution_params\": (0.1, 0.5),\\n            \"operation\": \"add\",\\n        },\\n    )\\n\\n    # on reset\\n    reset_cart_position = EventTerm(\\n        func=mdp.reset_joints_by_offset,\\n        mode=\"reset\",\\n        params={\\n            \"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"]),\\n            \"position_range\": (-1.0, 1.0),\\n            \"velocity_range\": (-0.1, 0.1),\\n        },\\n    )\\n\\n    reset_pole_position = EventTerm(\\n        func=mdp.reset_joints_by_offset,\\n        mode=\"reset\",\\n        params={\\n            \"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]),\\n            \"position_range\": (-0.125 * math.pi, 0.125 * math.pi),\\n            \"velocity_range\": (-0.01 * math.pi, 0.01 * math.pi),\\n        },\\n    )'),\n",
       " Document(metadata={}, page_content='class CartpoleEnvCfg(ManagerBasedEnvCfg):\\n    \"\"\"Configuration for the cartpole environment.\"\"\"\\n\\n    # Scene settings\\n    scene = CartpoleSceneCfg(num_envs=1024, env_spacing=2.5)\\n    # Basic settings\\n    observations = ObservationsCfg()\\n    actions = ActionsCfg()\\n    events = EventCfg()\\n\\n    def __post_init__(self):\\n        \"\"\"Post initialization.\"\"\"\\n        # viewer settings\\n        self.viewer.eye = [4.5, 0.0, 6.0]\\n        self.viewer.lookat = [0.0, 0.0, 2.0]\\n        # step settings\\n        self.decimation = 4  # env step every 4 sim steps: 200Hz / 4 = 50Hz\\n        # simulation settings\\n        self.sim.dt = 0.005'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # parse the arguments\\n    env_cfg = CartpoleEnvCfg()\\n    env_cfg.scene.num_envs = args_cli.num_envs\\n    env_cfg.sim.device = args_cli.device\\n    # setup base environment\\n    env = ManagerBasedEnv(cfg=env_cfg)\\n\\n    # simulate physics\\n    count = 0\\n    while simulation_app.is_running():\\n        with torch.inference_mode():\\n            # reset\\n            if count % 300 == 0:\\n                count = 0\\n                env.reset()\\n                print(\"-\" * 80)\\n                print(\"[INFO]: Resetting environment...\")\\n            # sample random actions\\n            joint_efforts = torch.randn_like(env.action_manager.action)\\n            # step the environment\\n            obs, _ = env.step(joint_efforts)\\n            # print current orientation of pole\\n            print(\"[Env 0]: Pole joint: \", obs[\"policy\"][0][1].item())\\n            # update counter\\n            count += 1\\n\\n    # close the environment\\n    env.close()'),\n",
       " Document(metadata={}, page_content='class CubeActionTerm(ActionTerm):\\n    \"\"\"Simple action term that implements a PD controller to track a target position.\\n\\n    The action term is applied to the cube asset. It involves two steps:\\n\\n    1. **Process the raw actions**: Typically, this includes any transformations of the raw actions\\n       that are required to map them to the desired space. This is called once per environment step.\\n    2. **Apply the processed actions**: This step applies the processed actions to the asset.\\n       It is called once per simulation step.\\n\\n    In this case, the action term simply applies the raw actions to the cube asset. The raw actions\\n    are the desired target positions of the cube in the environment frame. The pre-processing step\\n    simply copies the raw actions to the processed actions as no additional processing is required.\\n    The processed actions are then applied to the cube asset by implementing a PD controller to\\n    track the target position.\\n    \"\"\"\\n\\n    _asset: RigidObject\\n    \"\"\"The articulation asset on which the action term is applied.\"\"\"\\n\\n    def __init__(self, cfg: CubeActionTermCfg, env: ManagerBasedEnv):\\n        # call super constructor\\n        super().__init__(cfg, env)\\n        # create buffers\\n        self._raw_actions = torch.zeros(env.num_envs, 3, device=self.device)\\n        self._processed_actions = torch.zeros(env.num_envs, 3, device=self.device)\\n        self._vel_command = torch.zeros(self.num_envs, 6, device=self.device)\\n        # gains of controller\\n        self.p_gain = cfg.p_gain\\n        self.d_gain = cfg.d_gain\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        return self._raw_actions.shape[1]\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        return self._processed_actions\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        # store the raw actions\\n        self._raw_actions[:] = actions\\n        # no-processing of actions\\n        self._processed_actions[:] = self._raw_actions[:]\\n\\n    def apply_actions(self):\\n        # implement a PD controller to track the target position\\n        pos_error = self._processed_actions - (self._asset.data.root_pos_w - self._env.scene.env_origins)\\n        vel_error = -self._asset.data.root_lin_vel_w\\n        # set velocity targets\\n        self._vel_command[:, :3] = self.p_gain * pos_error + self.d_gain * vel_error\\n        self._asset.write_root_velocity_to_sim(self._vel_command)'),\n",
       " Document(metadata={}, page_content='class CubeActionTermCfg(ActionTermCfg):\\n    \"\"\"Configuration for the cube action term.\"\"\"\\n\\n    class_type: type = CubeActionTerm\\n    \"\"\"The class corresponding to the action term.\"\"\"\\n\\n    p_gain: float = 5.0\\n    \"\"\"Proportional gain of the PD controller.\"\"\"\\n    d_gain: float = 0.5\\n    \"\"\"Derivative gain of the PD controller.\"\"\"'),\n",
       " Document(metadata={}, page_content='def base_position(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:\\n    \"\"\"Root linear velocity in the asset\\'s root frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.root_pos_w - env.scene.env_origins'),\n",
       " Document(metadata={}, page_content='class MySceneCfg(InteractiveSceneCfg):\\n    \"\"\"Example scene configuration.\\n\\n    The scene comprises of a ground plane, light source and floating cubes (gravity disabled).\\n    \"\"\"\\n\\n    # add terrain\\n    terrain = TerrainImporterCfg(prim_path=\"/World/ground\", terrain_type=\"plane\", debug_vis=False)\\n\\n    # add cube\\n    cube: RigidObjectCfg = RigidObjectCfg(\\n        prim_path=\"{ENV_REGEX_NS}/cube\",\\n        spawn=sim_utils.CuboidCfg(\\n            size=(0.2, 0.2, 0.2),\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(max_depenetration_velocity=1.0, disable_gravity=True),\\n            mass_props=sim_utils.MassPropertiesCfg(mass=1.0),\\n            physics_material=sim_utils.RigidBodyMaterialCfg(),\\n            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.5, 0.0, 0.0)),\\n        ),\\n        init_state=RigidObjectCfg.InitialStateCfg(pos=(0.0, 0.0, 5)),\\n    )\\n\\n    # lights\\n    light = AssetBaseCfg(\\n        prim_path=\"/World/light\",\\n        spawn=sim_utils.DomeLightCfg(color=(0.75, 0.75, 0.75), intensity=2000.0),\\n    )'),\n",
       " Document(metadata={}, page_content='class ActionsCfg:\\n    \"\"\"Action specifications for the MDP.\"\"\"\\n\\n    joint_pos = CubeActionTermCfg(asset_name=\"cube\")'),\n",
       " Document(metadata={}, page_content='class ObservationsCfg:\\n    \"\"\"Observation specifications for the MDP.\"\"\"\\n\\n    @configclass\\n    class PolicyCfg(ObsGroup):\\n        \"\"\"Observations for policy group.\"\"\"\\n\\n        # cube velocity\\n        position = ObsTerm(func=base_position, params={\"asset_cfg\": SceneEntityCfg(\"cube\")})\\n\\n        def __post_init__(self):\\n            self.enable_corruption = True\\n            self.concatenate_terms = True\\n\\n    # observation groups\\n    policy: PolicyCfg = PolicyCfg()'),\n",
       " Document(metadata={}, page_content='class EventCfg:\\n    \"\"\"Configuration for events.\"\"\"\\n\\n    # This event term resets the base position of the cube.\\n    # The mode is set to \\'reset\\', which means that the base position is reset whenever\\n    # the environment instance is reset (because of terminations defined in \\'TerminationCfg\\').\\n    reset_base = EventTerm(\\n        func=mdp.reset_root_state_uniform,\\n        mode=\"reset\",\\n        params={\\n            \"pose_range\": {\"x\": (-0.5, 0.5), \"y\": (-0.5, 0.5), \"yaw\": (-3.14, 3.14)},\\n            \"velocity_range\": {\\n                \"x\": (-0.5, 0.5),\\n                \"y\": (-0.5, 0.5),\\n                \"z\": (-0.5, 0.5),\\n            },\\n            \"asset_cfg\": SceneEntityCfg(\"cube\"),\\n        },\\n    )\\n\\n    # This event term randomizes the scale of the cube.\\n    # The mode is set to \\'prestartup\\', which means that the scale is randomize on the USD stage before the\\n    # simulation starts.\\n    # Note: USD-level randomizations require the flag \\'replicate_physics\\' to be set to False.\\n    randomize_scale = EventTerm(\\n        func=mdp.randomize_rigid_body_scale,\\n        mode=\"prestartup\",\\n        params={\\n            \"scale_range\": {\"x\": (0.5, 1.5), \"y\": (0.5, 1.5), \"z\": (0.5, 1.5)},\\n            \"asset_cfg\": SceneEntityCfg(\"cube\"),\\n        },\\n    )\\n\\n    # This event term randomizes the visual color of the cube.\\n    # Similar to the scale randomization, this is also a USD-level randomization and requires the flag\\n    # \\'replicate_physics\\' to be set to False.\\n    randomize_color = EventTerm(\\n        func=mdp.randomize_visual_color,\\n        mode=\"prestartup\",\\n        params={\\n            \"colors\": {\"r\": (0.0, 1.0), \"g\": (0.0, 1.0), \"b\": (0.0, 1.0)},\\n            \"asset_cfg\": SceneEntityCfg(\"cube\"),\\n            \"mesh_name\": \"geometry/mesh\",\\n            \"event_name\": \"rep_cube_randomize_color\",\\n        },\\n    )'),\n",
       " Document(metadata={}, page_content='class CubeEnvCfg(ManagerBasedEnvCfg):\\n    \"\"\"Configuration for the locomotion velocity-tracking environment.\"\"\"\\n\\n    # Scene settings\\n    # The flag \\'replicate_physics\\' is set to False, which means that the cube is not replicated\\n    # across multiple environments but rather each environment gets its own cube instance.\\n    # This allows modifying the cube\\'s properties independently for each environment.\\n    scene: MySceneCfg = MySceneCfg(num_envs=args_cli.num_envs, env_spacing=2.5, replicate_physics=False)\\n\\n    # Basic settings\\n    observations: ObservationsCfg = ObservationsCfg()\\n    actions: ActionsCfg = ActionsCfg()\\n    events: EventCfg = EventCfg()\\n\\n    def __post_init__(self):\\n        \"\"\"Post initialization.\"\"\"\\n        # general settings\\n        self.decimation = 2\\n        # simulation settings\\n        self.sim.dt = 0.01\\n        self.sim.physics_material = self.scene.terrain.physics_material\\n        self.sim.render_interval = 2  # render interval should be a multiple of decimation\\n        self.sim.device = args_cli.device\\n        # viewer settings\\n        self.viewer.eye = (5.0, 5.0, 5.0)\\n        self.viewer.lookat = (0.0, 0.0, 2.0)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # setup base environment\\n    env = ManagerBasedEnv(cfg=CubeEnvCfg())\\n\\n    # setup target position commands\\n    target_position = torch.rand(env.num_envs, 3, device=env.device) * 2\\n    target_position[:, 2] += 2.0\\n    # offset all targets so that they move to the world origin\\n    target_position -= env.scene.env_origins\\n\\n    # simulate physics\\n    count = 0\\n    obs, _ = env.reset()\\n    while simulation_app.is_running():\\n        with torch.inference_mode():\\n            # reset\\n            if count % 300 == 0:\\n                count = 0\\n                obs, _ = env.reset()\\n                print(\"-\" * 80)\\n                print(\"[INFO]: Resetting environment...\")\\n            # step env\\n            obs, _ = env.step(target_position)\\n            # print mean squared position error between target and current position\\n            error = torch.norm(obs[\"policy\"] - target_position).mean().item()\\n            print(f\"[Step: {count:04d}]: Mean position error: {error:.4f}\")\\n            # update counter\\n            count += 1\\n\\n    # close the environment\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def constant_commands(env: ManagerBasedEnv) -> torch.Tensor:\\n    \"\"\"The generated command from the command generator.\"\"\"\\n    return torch.tensor([[1, 0, 0]], device=env.device).repeat(env.num_envs, 1)'),\n",
       " Document(metadata={}, page_content='class MySceneCfg(InteractiveSceneCfg):\\n    \"\"\"Example scene configuration.\"\"\"\\n\\n    # add terrain\\n    terrain = TerrainImporterCfg(\\n        prim_path=\"/World/ground\",\\n        terrain_type=\"generator\",\\n        terrain_generator=ROUGH_TERRAINS_CFG,\\n        max_init_terrain_level=5,\\n        collision_group=-1,\\n        physics_material=sim_utils.RigidBodyMaterialCfg(\\n            friction_combine_mode=\"multiply\",\\n            restitution_combine_mode=\"multiply\",\\n            static_friction=1.0,\\n            dynamic_friction=1.0,\\n        ),\\n        debug_vis=False,\\n    )\\n\\n    # add robot\\n    robot: ArticulationCfg = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    # sensors\\n    height_scanner = RayCasterCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base\",\\n        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),\\n        attach_yaw_only=True,\\n        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),\\n        debug_vis=True,\\n        mesh_prim_paths=[\"/World/ground\"],\\n    )\\n\\n    # lights\\n    light = AssetBaseCfg(\\n        prim_path=\"/World/light\",\\n        spawn=sim_utils.DistantLightCfg(color=(0.75, 0.75, 0.75), intensity=3000.0),\\n    )'),\n",
       " Document(metadata={}, page_content='class ActionsCfg:\\n    \"\"\"Action specifications for the MDP.\"\"\"\\n\\n    joint_pos = mdp.JointPositionActionCfg(asset_name=\"robot\", joint_names=[\".*\"], scale=0.5, use_default_offset=True)'),\n",
       " Document(metadata={}, page_content='class ObservationsCfg:\\n    \"\"\"Observation specifications for the MDP.\"\"\"\\n\\n    @configclass\\n    class PolicyCfg(ObsGroup):\\n        \"\"\"Observations for policy group.\"\"\"\\n\\n        # observation terms (order preserved)\\n        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))\\n        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))\\n        projected_gravity = ObsTerm(\\n            func=mdp.projected_gravity,\\n            noise=Unoise(n_min=-0.05, n_max=0.05),\\n        )\\n        velocity_commands = ObsTerm(func=constant_commands)\\n        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))\\n        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))\\n        actions = ObsTerm(func=mdp.last_action)\\n        height_scan = ObsTerm(\\n            func=mdp.height_scan,\\n            params={\"sensor_cfg\": SceneEntityCfg(\"height_scanner\")},\\n            noise=Unoise(n_min=-0.1, n_max=0.1),\\n            clip=(-1.0, 1.0),\\n        )\\n\\n        def __post_init__(self):\\n            self.enable_corruption = True\\n            self.concatenate_terms = True\\n\\n    # observation groups\\n    policy: PolicyCfg = PolicyCfg()'),\n",
       " Document(metadata={}, page_content='class EventCfg:\\n    \"\"\"Configuration for events.\"\"\"\\n\\n    reset_scene = EventTerm(func=mdp.reset_scene_to_default, mode=\"reset\")'),\n",
       " Document(metadata={}, page_content='class QuadrupedEnvCfg(ManagerBasedEnvCfg):\\n    \"\"\"Configuration for the locomotion velocity-tracking environment.\"\"\"\\n\\n    # Scene settings\\n    scene: MySceneCfg = MySceneCfg(num_envs=args_cli.num_envs, env_spacing=2.5)\\n    # Basic settings\\n    observations: ObservationsCfg = ObservationsCfg()\\n    actions: ActionsCfg = ActionsCfg()\\n    events: EventCfg = EventCfg()\\n\\n    def __post_init__(self):\\n        \"\"\"Post initialization.\"\"\"\\n        # general settings\\n        self.decimation = 4  # env decimation -> 50 Hz control\\n        # simulation settings\\n        self.sim.dt = 0.005  # simulation timestep -> 200 Hz physics\\n        self.sim.physics_material = self.scene.terrain.physics_material\\n        self.sim.device = args_cli.device\\n        # update sensor update periods\\n        # we tick all the sensors based on the smallest update period (physics update period)\\n        if self.scene.height_scanner is not None:\\n            self.scene.height_scanner.update_period = self.decimation * self.sim.dt'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # setup base environment\\n    env_cfg = QuadrupedEnvCfg()\\n    env = ManagerBasedEnv(cfg=env_cfg)\\n\\n    # load level policy\\n    policy_path = ISAACLAB_NUCLEUS_DIR + \"/Policies/ANYmal-C/HeightScan/policy.pt\"\\n    # check if policy file exists\\n    if not check_file_path(policy_path):\\n        raise FileNotFoundError(f\"Policy file \\'{policy_path}\\' does not exist.\")\\n    file_bytes = read_file(policy_path)\\n    # jit load the policy\\n    policy = torch.jit.load(file_bytes).to(env.device).eval()\\n\\n    # simulate physics\\n    count = 0\\n    obs, _ = env.reset()\\n    while simulation_app.is_running():\\n        with torch.inference_mode():\\n            # reset\\n            if count % 1000 == 0:\\n                obs, _ = env.reset()\\n                count = 0\\n                print(\"-\" * 80)\\n                print(\"[INFO]: Resetting environment...\")\\n            # infer action\\n            action = policy(obs[\"policy\"])\\n            # step env\\n            obs, _ = env.step(action)\\n            # update counter\\n            count += 1\\n\\n    # close the environment\\n    env.close()'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # load the trained jit policy\\n    policy_path = os.path.abspath(args_cli.checkpoint)\\n    file_content = omni.client.read_file(policy_path)[2]\\n    file = io.BytesIO(memoryview(file_content).tobytes())\\n    policy = torch.jit.load(file, map_location=args_cli.device)\\n\\n    # setup environment\\n    env_cfg = H1RoughEnvCfg_PLAY()\\n    env_cfg.scene.num_envs = 1\\n    env_cfg.curriculum = None\\n    env_cfg.scene.terrain = TerrainImporterCfg(\\n        prim_path=\"/World/ground\",\\n        terrain_type=\"usd\",\\n        usd_path=f\"{ISAAC_NUCLEUS_DIR}/Environments/Simple_Warehouse/warehouse.usd\",\\n    )\\n    env_cfg.sim.device = args_cli.device\\n    if args_cli.device == \"cpu\":\\n        env_cfg.sim.use_fabric = False\\n\\n    # create environment\\n    env = ManagerBasedRLEnv(cfg=env_cfg)\\n\\n    # run inference with the policy\\n    obs, _ = env.reset()\\n    with torch.inference_mode():\\n        while simulation_app.is_running():\\n            action = policy(obs[\"policy\"])\\n            obs, _, _, _, _ = env.step(action)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # create environment configuration\\n    env_cfg = CartpoleEnvCfg()\\n    env_cfg.scene.num_envs = args_cli.num_envs\\n    env_cfg.sim.device = args_cli.device\\n    # setup RL environment\\n    env = ManagerBasedRLEnv(cfg=env_cfg)\\n\\n    # simulate physics\\n    count = 0\\n    while simulation_app.is_running():\\n        with torch.inference_mode():\\n            # reset\\n            if count % 300 == 0:\\n                count = 0\\n                env.reset()\\n                print(\"-\" * 80)\\n                print(\"[INFO]: Resetting environment...\")\\n            # sample random actions\\n            joint_efforts = torch.randn_like(env.action_manager.action)\\n            # step the environment\\n            obs, rew, terminated, truncated, info = env.step(joint_efforts)\\n            # print current orientation of pole\\n            print(\"[Env 0]: Pole joint: \", obs[\"policy\"][0][1].item())\\n            # update counter\\n            count += 1\\n\\n    # close the environment\\n    env.close()'),\n",
       " Document(metadata={}, page_content='class SensorsSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Design the scene with sensors on the robot.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(prim_path=\"/World/defaultGroundPlane\", spawn=sim_utils.GroundPlaneCfg())\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # robot\\n    robot: ArticulationCfg = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    # sensors\\n    camera = CameraCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base/front_cam\",\\n        update_period=0.1,\\n        height=480,\\n        width=640,\\n        data_types=[\"rgb\", \"distance_to_image_plane\"],\\n        spawn=sim_utils.PinholeCameraCfg(\\n            focal_length=24.0, focus_distance=400.0, horizontal_aperture=20.955, clipping_range=(0.1, 1.0e5)\\n        ),\\n        offset=CameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(0.5, -0.5, 0.5, -0.5), convention=\"ros\"),\\n    )\\n    height_scanner = RayCasterCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/base\",\\n        update_period=0.02,\\n        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),\\n        attach_yaw_only=True,\\n        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),\\n        debug_vis=True,\\n        mesh_prim_paths=[\"/World/defaultGroundPlane\"],\\n    )\\n    contact_forces = ContactSensorCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Robot/.*_FOOT\", update_period=0.0, history_length=6, debug_vis=True\\n    )'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # Reset\\n        if count % 500 == 0:\\n            # reset counter\\n            count = 0\\n            # reset the scene entities\\n            # root state\\n            # we offset the root state by the origin since the states are written in simulation world frame\\n            # if this is not done, then the robots will be spawned at the (0, 0, 0) of the simulation world\\n            root_state = scene[\"robot\"].data.default_root_state.clone()\\n            root_state[:, :3] += scene.env_origins\\n            scene[\"robot\"].write_root_pose_to_sim(root_state[:, :7])\\n            scene[\"robot\"].write_root_velocity_to_sim(root_state[:, 7:])\\n            # set joint positions with some noise\\n            joint_pos, joint_vel = (\\n                scene[\"robot\"].data.default_joint_pos.clone(),\\n                scene[\"robot\"].data.default_joint_vel.clone(),\\n            )\\n            joint_pos += torch.rand_like(joint_pos) * 0.1\\n            scene[\"robot\"].write_joint_state_to_sim(joint_pos, joint_vel)\\n            # clear internal buffers\\n            scene.reset()\\n            print(\"[INFO]: Resetting robot state...\")\\n        # Apply default actions to the robot\\n        # -- generate actions/commands\\n        targets = scene[\"robot\"].data.default_joint_pos\\n        # -- apply action to the robot\\n        scene[\"robot\"].set_joint_position_target(targets)\\n        # -- write data to sim\\n        scene.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # update buffers\\n        scene.update(sim_dt)\\n\\n        # print information from the sensors\\n        print(\"-------------------------------\")\\n        print(scene[\"camera\"])\\n        print(\"Received shape of rgb   image: \", scene[\"camera\"].data.output[\"rgb\"].shape)\\n        print(\"Received shape of depth image: \", scene[\"camera\"].data.output[\"distance_to_image_plane\"].shape)\\n        print(\"-------------------------------\")\\n        print(scene[\"height_scanner\"])\\n        print(\"Received max height value: \", torch.max(scene[\"height_scanner\"].data.ray_hits_w[..., -1]).item())\\n        print(\"-------------------------------\")\\n        print(scene[\"contact_forces\"])\\n        print(\"Received max contact force of: \", torch.max(scene[\"contact_forces\"].data.net_forces_w).item())'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n\\n    # Initialize the simulation context\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\\n    # design scene\\n    scene_cfg = SensorsSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='def define_sensor() -> FrameTransformer:\\n    \"\"\"Defines the FrameTransformer sensor to add to the scene.\"\"\"\\n    # define offset\\n    rot_offset = math_utils.quat_from_euler_xyz(torch.zeros(1), torch.zeros(1), torch.tensor(-math.pi / 2))\\n    pos_offset = math_utils.quat_apply(rot_offset, torch.tensor([0.08795, 0.01305, -0.33797]))\\n\\n    # Example using .* to get full body + LF_FOOT\\n    frame_transformer_cfg = FrameTransformerCfg(\\n        prim_path=\"/World/Robot/base\",\\n        target_frames=[\\n            FrameTransformerCfg.FrameCfg(prim_path=\"/World/Robot/.*\"),\\n            FrameTransformerCfg.FrameCfg(\\n                prim_path=\"/World/Robot/LF_SHANK\",\\n                name=\"LF_FOOT_USER\",\\n                offset=OffsetCfg(pos=tuple(pos_offset.tolist()), rot=tuple(rot_offset[0].tolist())),\\n            ),\\n        ],\\n        debug_vis=False,\\n    )\\n    frame_transformer = FrameTransformer(frame_transformer_cfg)\\n\\n    return frame_transformer'),\n",
       " Document(metadata={}, page_content='def design_scene() -> dict:\\n    \"\"\"Design the scene.\"\"\"\\n    # Populate scene\\n    # -- Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # -- Lights\\n    cfg = sim_utils.DistantLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n    # -- Robot\\n    robot = Articulation(ANYMAL_C_CFG.replace(prim_path=\"/World/Robot\"))\\n    # -- Sensors\\n    frame_transformer = define_sensor()\\n\\n    # return the scene information\\n    scene_entities = {\"robot\": robot, \"frame_transformer\": frame_transformer}\\n    return scene_entities'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene_entities: dict):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    sim_time = 0.0\\n    count = 0\\n\\n    # extract entities for simplified notation\\n    robot: Articulation = scene_entities[\"robot\"]\\n    frame_transformer: FrameTransformer = scene_entities[\"frame_transformer\"]\\n\\n    # We only want one visualization at a time. This visualizer will be used\\n    # to step through each frame so the user can verify that the correct frame\\n    # is being visualized as the frame names are printing to console\\n    if not args_cli.headless:\\n        cfg = FRAME_MARKER_CFG.replace(prim_path=\"/Visuals/FrameVisualizerFromScript\")\\n        cfg.markers[\"frame\"].scale = (0.1, 0.1, 0.1)\\n        transform_visualizer = VisualizationMarkers(cfg)\\n        # debug drawing for lines connecting the frame\\n        draw_interface = omni_debug_draw.acquire_debug_draw_interface()\\n    else:\\n        transform_visualizer = None\\n        draw_interface = None\\n\\n    frame_index = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # perform this loop at policy control freq (50 Hz)\\n        robot.set_joint_position_target(robot.data.default_joint_pos.clone())\\n        robot.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        sim_time += sim_dt\\n        count += 1\\n        # read data from sim\\n        robot.update(sim_dt)\\n        frame_transformer.update(dt=sim_dt)\\n\\n        # Change the frame that we are visualizing to ensure that frame names\\n        # are correctly associated with the frames\\n        if not args_cli.headless:\\n            if count % 50 == 0:\\n                # get frame names\\n                frame_names = frame_transformer.data.target_frame_names\\n                # increment frame index\\n                frame_index += 1\\n                frame_index = frame_index % len(frame_names)\\n                print(f\"Displaying Frame ID {frame_index}: {frame_names[frame_index]}\")\\n\\n            # visualize frame\\n            source_pos = frame_transformer.data.source_pos_w\\n            source_quat = frame_transformer.data.source_quat_w\\n            target_pos = frame_transformer.data.target_pos_w[:, frame_index]\\n            target_quat = frame_transformer.data.target_quat_w[:, frame_index]\\n            # draw the frames\\n            transform_visualizer.visualize(\\n                torch.cat([source_pos, target_pos], dim=0), torch.cat([source_quat, target_quat], dim=0)\\n            )\\n            # draw the line connecting the frames\\n            draw_interface.clear_lines()\\n            # plain color for lines\\n            lines_colors = [[1.0, 1.0, 0.0, 1.0]] * source_pos.shape[0]\\n            line_thicknesses = [5.0] * source_pos.shape[0]\\n            draw_interface.draw_lines(source_pos.tolist(), target_pos.tolist(), lines_colors, line_thicknesses)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.005, device=args_cli.device)\\n    sim = SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view(eye=[2.5, 2.5, 2.5], target=[0.0, 0.0, 0.0])\\n    # Design the scene\\n    scene_entities = design_scene()\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene_entities)'),\n",
       " Document(metadata={}, page_content='def define_sensor() -> RayCaster:\\n    \"\"\"Defines the ray-caster sensor to add to the scene.\"\"\"\\n    # Create a ray-caster sensor\\n    ray_caster_cfg = RayCasterCfg(\\n        prim_path=\"/World/Origin.*/ball\",\\n        mesh_prim_paths=[\"/World/ground\"],\\n        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=(2.0, 2.0)),\\n        attach_yaw_only=True,\\n        debug_vis=not args_cli.headless,\\n    )\\n    ray_caster = RayCaster(cfg=ray_caster_cfg)\\n\\n    return ray_caster'),\n",
       " Document(metadata={}, page_content='def design_scene() -> dict:\\n    \"\"\"Design the scene.\"\"\"\\n    # Populate scene\\n    # -- Rough terrain\\n    cfg = sim_utils.UsdFileCfg(usd_path=f\"{ISAAC_NUCLEUS_DIR}/Environments/Terrains/rough_plane.usd\")\\n    cfg.func(\"/World/ground\", cfg)\\n    # -- Light\\n    cfg = sim_utils.DistantLightCfg(intensity=2000)\\n    cfg.func(\"/World/light\", cfg)\\n\\n    # Create separate groups called \"Origin1\", \"Origin2\", \"Origin3\"\\n    # Each group will have a robot in it\\n    origins = [[0.25, 0.25, 0.0], [-0.25, 0.25, 0.0], [0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]]\\n    for i, origin in enumerate(origins):\\n        prim_utils.create_prim(f\"/World/Origin{i}\", \"Xform\", translation=origin)\\n    # -- Balls\\n    cfg = RigidObjectCfg(\\n        prim_path=\"/World/Origin.*/ball\",\\n        spawn=sim_utils.SphereCfg(\\n            radius=0.25,\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(),\\n            mass_props=sim_utils.MassPropertiesCfg(mass=0.5),\\n            collision_props=sim_utils.CollisionPropertiesCfg(),\\n            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 0.0, 1.0)),\\n        ),\\n    )\\n    balls = RigidObject(cfg)\\n    # -- Sensors\\n    ray_caster = define_sensor()\\n\\n    # return the scene information\\n    scene_entities = {\"balls\": balls, \"ray_caster\": ray_caster}\\n    return scene_entities'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene_entities: dict):\\n    \"\"\"Run the simulator.\"\"\"\\n    # Extract scene_entities for simplified notation\\n    ray_caster: RayCaster = scene_entities[\"ray_caster\"]\\n    balls: RigidObject = scene_entities[\"balls\"]\\n\\n    # define an initial position of the sensor\\n    ball_default_state = balls.data.default_root_state.clone()\\n    ball_default_state[:, :3] = torch.rand_like(ball_default_state[:, :3]) * 10\\n\\n    # Create a counter for resetting the scene\\n    step_count = 0\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # Reset the scene\\n        if step_count % 250 == 0:\\n            # reset the balls\\n            balls.write_root_pose_to_sim(ball_default_state[:, :7])\\n            balls.write_root_velocity_to_sim(ball_default_state[:, 7:])\\n            # reset the sensor\\n            ray_caster.reset()\\n            # reset the counter\\n            step_count = 0\\n        # Step simulation\\n        sim.step()\\n        # Update the ray-caster\\n        with Timer(\\n            f\"Ray-caster update with {4} x {ray_caster.num_rays} rays with max height of\"\\n            f\" {torch.max(ray_caster.data.pos_w).item():.2f}\"\\n        ):\\n            ray_caster.update(dt=sim.get_physics_dt(), force_recompute=True)\\n        # Update counter\\n        step_count += 1'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load simulation context\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([0.0, 15.0, 15.0], [0.0, 0.0, -2.5])\\n    # Design the scene\\n    scene_entities = design_scene()\\n    # Play simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run simulator\\n    run_simulator(sim=sim, scene_entities=scene_entities)'),\n",
       " Document(metadata={}, page_content='def define_sensor() -> RayCasterCamera:\\n    \"\"\"Defines the ray-cast camera sensor to add to the scene.\"\"\"\\n    # Camera base frames\\n    # In contras to the USD camera, we associate the sensor to the prims at these locations.\\n    # This means that parent prim of the sensor is the prim at this location.\\n    prim_utils.create_prim(\"/World/Origin_00/CameraSensor\", \"Xform\")\\n    prim_utils.create_prim(\"/World/Origin_01/CameraSensor\", \"Xform\")\\n\\n    # Setup camera sensor\\n    camera_cfg = RayCasterCameraCfg(\\n        prim_path=\"/World/Origin_.*/CameraSensor\",\\n        mesh_prim_paths=[\"/World/ground\"],\\n        update_period=0.1,\\n        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.0, 0.0, 0.0), rot=(1.0, 0.0, 0.0, 0.0)),\\n        data_types=[\"distance_to_image_plane\", \"normals\", \"distance_to_camera\"],\\n        debug_vis=True,\\n        pattern_cfg=patterns.PinholeCameraPatternCfg(\\n            focal_length=24.0,\\n            horizontal_aperture=20.955,\\n            height=480,\\n            width=640,\\n        ),\\n    )\\n    # Create camera\\n    camera = RayCasterCamera(cfg=camera_cfg)\\n\\n    return camera'),\n",
       " Document(metadata={}, page_content='def design_scene():\\n    # Populate scene\\n    # -- Rough terrain\\n    cfg = sim_utils.UsdFileCfg(usd_path=f\"{ISAAC_NUCLEUS_DIR}/Environments/Terrains/rough_plane.usd\")\\n    cfg.func(\"/World/ground\", cfg)\\n    # -- Lights\\n    cfg = sim_utils.DistantLightCfg(intensity=600.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n    # -- Sensors\\n    camera = define_sensor()\\n\\n    # return the scene information\\n    scene_entities = {\"camera\": camera}\\n    return scene_entities'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene_entities: dict):\\n    \"\"\"Run the simulator.\"\"\"\\n    # extract entities for simplified notation\\n    camera: RayCasterCamera = scene_entities[\"camera\"]\\n\\n    # Create replicator writer\\n    output_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"output\", \"ray_caster_camera\")\\n    rep_writer = rep.BasicWriter(output_dir=output_dir, frame_padding=3)\\n\\n    # Set pose: There are two ways to set the pose of the camera.\\n    # -- Option-1: Set pose using view\\n    eyes = torch.tensor([[2.5, 2.5, 2.5], [-2.5, -2.5, 2.5]], device=sim.device)\\n    targets = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], device=sim.device)\\n    camera.set_world_poses_from_view(eyes, targets)\\n    # -- Option-2: Set pose using ROS\\n    # position = torch.tensor([[2.5, 2.5, 2.5]], device=sim.device)\\n    # orientation = torch.tensor([[-0.17591989, 0.33985114, 0.82047325, -0.42470819]], device=sim.device)\\n    # camera.set_world_poses(position, orientation, indices=[0], convention=\"ros\")\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # Step simulation\\n        sim.step()\\n        # Update camera data\\n        camera.update(dt=sim.get_physics_dt())\\n\\n        # Print camera info\\n        print(camera)\\n        print(\"Received shape of depth image: \", camera.data.output[\"distance_to_image_plane\"].shape)\\n        print(\"-------------------------------\")\\n\\n        # Extract camera data\\n        if args_cli.save:\\n            # Extract camera data\\n            camera_index = 0\\n            # note: BasicWriter only supports saving data in numpy format, so we need to convert the data to numpy.\\n            single_cam_data = convert_dict_to_backend(\\n                {k: v[camera_index] for k, v in camera.data.output.items()}, backend=\"numpy\"\\n            )\\n            # Extract the other information\\n            single_cam_info = camera.data.info[camera_index]\\n\\n            # Pack data back into replicator format to save them using its writer\\n            rep_output = {\"annotators\": {}}\\n            for key, data, info in zip(single_cam_data.keys(), single_cam_data.values(), single_cam_info.values()):\\n                if info is not None:\\n                    rep_output[\"annotators\"][key] = {\"render_product\": {\"data\": data, **info}}\\n                else:\\n                    rep_output[\"annotators\"][key] = {\"render_product\": {\"data\": data}}\\n            # Save images\\n            rep_output[\"trigger_outputs\"] = {\"on_time\": camera.frame[camera_index]}\\n            rep_writer.write(rep_output)\\n\\n            # Pointcloud in world frame\\n            points_3d_cam = unproject_depth(\\n                camera.data.output[\"distance_to_image_plane\"], camera.data.intrinsic_matrices\\n            )\\n\\n            # Check methods are valid\\n            im_height, im_width = camera.image_shape\\n            # -- project points to (u, v, d)\\n            reproj_points = project_points(points_3d_cam, camera.data.intrinsic_matrices)\\n            reproj_depths = reproj_points[..., -1].view(-1, im_width, im_height).transpose_(1, 2)\\n            sim_depths = camera.data.output[\"distance_to_image_plane\"].squeeze(-1)\\n            torch.testing.assert_close(reproj_depths, sim_depths)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim = sim_utils.SimulationContext()\\n    # Set main camera\\n    sim.set_camera_view([2.5, 2.5, 3.5], [0.0, 0.0, 0.0])\\n    # design the scene\\n    scene_entities = design_scene()\\n    # Play simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run simulator\\n    run_simulator(sim=sim, scene_entities=scene_entities)'),\n",
       " Document(metadata={}, page_content='def define_sensor() -> Camera:\\n    \"\"\"Defines the camera sensor to add to the scene.\"\"\"\\n    # Setup camera sensor\\n    # In contrast to the ray-cast camera, we spawn the prim at these locations.\\n    # This means the camera sensor will be attached to these prims.\\n    prim_utils.create_prim(\"/World/Origin_00\", \"Xform\")\\n    prim_utils.create_prim(\"/World/Origin_01\", \"Xform\")\\n    camera_cfg = CameraCfg(\\n        prim_path=\"/World/Origin_.*/CameraSensor\",\\n        update_period=0,\\n        height=480,\\n        width=640,\\n        data_types=[\\n            \"rgb\",\\n            \"distance_to_image_plane\",\\n            \"normals\",\\n            \"semantic_segmentation\",\\n            \"instance_segmentation_fast\",\\n            \"instance_id_segmentation_fast\",\\n        ],\\n        colorize_semantic_segmentation=True,\\n        colorize_instance_id_segmentation=True,\\n        colorize_instance_segmentation=True,\\n        spawn=sim_utils.PinholeCameraCfg(\\n            focal_length=24.0, focus_distance=400.0, horizontal_aperture=20.955, clipping_range=(0.1, 1.0e5)\\n        ),\\n    )\\n    # Create camera\\n    camera = Camera(cfg=camera_cfg)\\n\\n    return camera'),\n",
       " Document(metadata={}, page_content='def design_scene() -> dict:\\n    \"\"\"Design the scene.\"\"\"\\n    # Populate scene\\n    # -- Ground-plane\\n    cfg = sim_utils.GroundPlaneCfg()\\n    cfg.func(\"/World/defaultGroundPlane\", cfg)\\n    # -- Lights\\n    cfg = sim_utils.DistantLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    cfg.func(\"/World/Light\", cfg)\\n\\n    # Create a dictionary for the scene entities\\n    scene_entities = {}\\n\\n    # Xform to hold objects\\n    prim_utils.create_prim(\"/World/Objects\", \"Xform\")\\n    # Random objects\\n    for i in range(8):\\n        # sample random position\\n        position = np.random.rand(3) - np.asarray([0.05, 0.05, -1.0])\\n        position *= np.asarray([1.5, 1.5, 0.5])\\n        # sample random color\\n        color = (random.random(), random.random(), random.random())\\n        # choose random prim type\\n        prim_type = random.choice([\"Cube\", \"Cone\", \"Cylinder\"])\\n        common_properties = {\\n            \"rigid_props\": sim_utils.RigidBodyPropertiesCfg(),\\n            \"mass_props\": sim_utils.MassPropertiesCfg(mass=5.0),\\n            \"collision_props\": sim_utils.CollisionPropertiesCfg(),\\n            \"visual_material\": sim_utils.PreviewSurfaceCfg(diffuse_color=color, metallic=0.5),\\n            \"semantic_tags\": [(\"class\", prim_type)],\\n        }\\n        if prim_type == \"Cube\":\\n            shape_cfg = sim_utils.CuboidCfg(size=(0.25, 0.25, 0.25), **common_properties)\\n        elif prim_type == \"Cone\":\\n            shape_cfg = sim_utils.ConeCfg(radius=0.1, height=0.25, **common_properties)\\n        elif prim_type == \"Cylinder\":\\n            shape_cfg = sim_utils.CylinderCfg(radius=0.25, height=0.25, **common_properties)\\n        # Rigid Object\\n        obj_cfg = RigidObjectCfg(\\n            prim_path=f\"/World/Objects/Obj_{i:02d}\",\\n            spawn=shape_cfg,\\n            init_state=RigidObjectCfg.InitialStateCfg(pos=position),\\n        )\\n        scene_entities[f\"rigid_object{i}\"] = RigidObject(cfg=obj_cfg)\\n\\n    # Sensors\\n    camera = define_sensor()\\n\\n    # return the scene information\\n    scene_entities[\"camera\"] = camera\\n    return scene_entities'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene_entities: dict):\\n    \"\"\"Run the simulator.\"\"\"\\n    # extract entities for simplified notation\\n    camera: Camera = scene_entities[\"camera\"]\\n\\n    # Create replicator writer\\n    output_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"output\", \"camera\")\\n    rep_writer = rep.BasicWriter(\\n        output_dir=output_dir,\\n        frame_padding=0,\\n        colorize_instance_id_segmentation=camera.cfg.colorize_instance_id_segmentation,\\n        colorize_instance_segmentation=camera.cfg.colorize_instance_segmentation,\\n        colorize_semantic_segmentation=camera.cfg.colorize_semantic_segmentation,\\n    )\\n\\n    # Camera positions, targets, orientations\\n    camera_positions = torch.tensor([[2.5, 2.5, 2.5], [-2.5, -2.5, 2.5]], device=sim.device)\\n    camera_targets = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], device=sim.device)\\n    # These orientations are in ROS-convention, and will position the cameras to view the origin\\n    camera_orientations = torch.tensor(  # noqa: F841\\n        [[-0.1759, 0.3399, 0.8205, -0.4247], [-0.4247, 0.8205, -0.3399, 0.1759]], device=sim.device\\n    )\\n\\n    # Set pose: There are two ways to set the pose of the camera.\\n    # -- Option-1: Set pose using view\\n    camera.set_world_poses_from_view(camera_positions, camera_targets)\\n    # -- Option-2: Set pose using ROS\\n    # camera.set_world_poses(camera_positions, camera_orientations, convention=\"ros\")\\n\\n    # Index of the camera to use for visualization and saving\\n    camera_index = args_cli.camera_id\\n\\n    # Create the markers for the --draw option outside of is_running() loop\\n    if sim.has_gui() and args_cli.draw:\\n        cfg = RAY_CASTER_MARKER_CFG.replace(prim_path=\"/Visuals/CameraPointCloud\")\\n        cfg.markers[\"hit\"].radius = 0.002\\n        pc_markers = VisualizationMarkers(cfg)\\n\\n    # Simulate physics\\n    while simulation_app.is_running():\\n        # Step simulation\\n        sim.step()\\n        # Update camera data\\n        camera.update(dt=sim.get_physics_dt())\\n\\n        # Print camera info\\n        print(camera)\\n        if \"rgb\" in camera.data.output.keys():\\n            print(\"Received shape of rgb image        : \", camera.data.output[\"rgb\"].shape)\\n        if \"distance_to_image_plane\" in camera.data.output.keys():\\n            print(\"Received shape of depth image      : \", camera.data.output[\"distance_to_image_plane\"].shape)\\n        if \"normals\" in camera.data.output.keys():\\n            print(\"Received shape of normals          : \", camera.data.output[\"normals\"].shape)\\n        if \"semantic_segmentation\" in camera.data.output.keys():\\n            print(\"Received shape of semantic segm.   : \", camera.data.output[\"semantic_segmentation\"].shape)\\n        if \"instance_segmentation_fast\" in camera.data.output.keys():\\n            print(\"Received shape of instance segm.   : \", camera.data.output[\"instance_segmentation_fast\"].shape)\\n        if \"instance_id_segmentation_fast\" in camera.data.output.keys():\\n            print(\"Received shape of instance id segm.: \", camera.data.output[\"instance_id_segmentation_fast\"].shape)\\n        print(\"-------------------------------\")\\n\\n        # Extract camera data\\n        if args_cli.save:\\n            # Save images from camera at camera_index\\n            # note: BasicWriter only supports saving data in numpy format, so we need to convert the data to numpy.\\n            single_cam_data = convert_dict_to_backend(\\n                {k: v[camera_index] for k, v in camera.data.output.items()}, backend=\"numpy\"\\n            )\\n\\n            # Extract the other information\\n            single_cam_info = camera.data.info[camera_index]\\n\\n            # Pack data back into replicator format to save them using its writer\\n            rep_output = {\"annotators\": {}}\\n            for key, data, info in zip(single_cam_data.keys(), single_cam_data.values(), single_cam_info.values()):\\n                if info is not None:\\n                    rep_output[\"annotators\"][key] = {\"render_product\": {\"data\": data, **info}}\\n                else:\\n                    rep_output[\"annotators\"][key] = {\"render_product\": {\"data\": data}}\\n            # Save images\\n            # Note: We need to provide On-time data for Replicator to save the images.\\n            rep_output[\"trigger_outputs\"] = {\"on_time\": camera.frame[camera_index]}\\n            rep_writer.write(rep_output)\\n\\n        # Draw pointcloud if there is a GUI and --draw has been passed\\n        if sim.has_gui() and args_cli.draw and \"distance_to_image_plane\" in camera.data.output.keys():\\n            # Derive pointcloud from camera at camera_index\\n            pointcloud = create_pointcloud_from_depth(\\n                intrinsic_matrix=camera.data.intrinsic_matrices[camera_index],\\n                depth=camera.data.output[\"distance_to_image_plane\"][camera_index],\\n                position=camera.data.pos_w[camera_index],\\n                orientation=camera.data.quat_w_ros[camera_index],\\n                device=sim.device,\\n            )\\n\\n            # In the first few steps, things are still being instanced and Camera.data\\n            # can be empty. If we attempt to visualize an empty pointcloud it will crash\\n            # the sim, so we check that the pointcloud is not empty.\\n            if pointcloud.size()[0] > 0:\\n                pc_markers.visualize(translations=pointcloud)'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load simulation context\\n    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 2.5, 2.5], [0.0, 0.0, 0.0])\\n    # design the scene\\n    scene_entities = design_scene()\\n    # Play simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run simulator\\n    run_simulator(sim, scene_entities)'),\n",
       " Document(metadata={}, page_content='class TableTopSceneCfg(InteractiveSceneCfg):\\n    \"\"\"Configuration for a cart-pole scene.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(\\n        prim_path=\"/World/defaultGroundPlane\",\\n        spawn=sim_utils.GroundPlaneCfg(),\\n        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, -1.05)),\\n    )\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # mount\\n    table = AssetBaseCfg(\\n        prim_path=\"{ENV_REGEX_NS}/Table\",\\n        spawn=sim_utils.UsdFileCfg(\\n            usd_path=f\"{ISAAC_NUCLEUS_DIR}/Props/Mounts/Stand/stand_instanceable.usd\", scale=(2.0, 2.0, 2.0)\\n        ),\\n    )\\n\\n    # articulation\\n    if args_cli.robot == \"franka_panda\":\\n        robot = FRANKA_PANDA_HIGH_PD_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n    elif args_cli.robot == \"ur10\":\\n        robot = UR10_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n    else:\\n        raise ValueError(f\"Robot {args_cli.robot} is not supported. Valid: franka_panda, ur10\")'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Runs the simulation loop.\"\"\"\\n    # Extract scene entities\\n    # note: we only do this here for readability.\\n    robot = scene[\"robot\"]\\n\\n    # Create controller\\n    diff_ik_cfg = DifferentialIKControllerCfg(command_type=\"pose\", use_relative_mode=False, ik_method=\"dls\")\\n    diff_ik_controller = DifferentialIKController(diff_ik_cfg, num_envs=scene.num_envs, device=sim.device)\\n\\n    # Markers\\n    frame_marker_cfg = FRAME_MARKER_CFG.copy()\\n    frame_marker_cfg.markers[\"frame\"].scale = (0.1, 0.1, 0.1)\\n    ee_marker = VisualizationMarkers(frame_marker_cfg.replace(prim_path=\"/Visuals/ee_current\"))\\n    goal_marker = VisualizationMarkers(frame_marker_cfg.replace(prim_path=\"/Visuals/ee_goal\"))\\n\\n    # Define goals for the arm\\n    ee_goals = [\\n        [0.5, 0.5, 0.7, 0.707, 0, 0.707, 0],\\n        [0.5, -0.4, 0.6, 0.707, 0.707, 0.0, 0.0],\\n        [0.5, 0, 0.5, 0.0, 1.0, 0.0, 0.0],\\n    ]\\n    ee_goals = torch.tensor(ee_goals, device=sim.device)\\n    # Track the given command\\n    current_goal_idx = 0\\n    # Create buffers to store actions\\n    ik_commands = torch.zeros(scene.num_envs, diff_ik_controller.action_dim, device=robot.device)\\n    ik_commands[:] = ee_goals[current_goal_idx]\\n\\n    # Specify robot-specific parameters\\n    if args_cli.robot == \"franka_panda\":\\n        robot_entity_cfg = SceneEntityCfg(\"robot\", joint_names=[\"panda_joint.*\"], body_names=[\"panda_hand\"])\\n    elif args_cli.robot == \"ur10\":\\n        robot_entity_cfg = SceneEntityCfg(\"robot\", joint_names=[\".*\"], body_names=[\"ee_link\"])\\n    else:\\n        raise ValueError(f\"Robot {args_cli.robot} is not supported. Valid: franka_panda, ur10\")\\n    # Resolving the scene entities\\n    robot_entity_cfg.resolve(scene)\\n    # Obtain the frame index of the end-effector\\n    # For a fixed base robot, the frame index is one less than the body index. This is because\\n    # the root body is not included in the returned Jacobians.\\n    if robot.is_fixed_base:\\n        ee_jacobi_idx = robot_entity_cfg.body_ids[0] - 1\\n    else:\\n        ee_jacobi_idx = robot_entity_cfg.body_ids[0]\\n\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n    count = 0\\n    # Simulation loop\\n    while simulation_app.is_running():\\n        # reset\\n        if count % 150 == 0:\\n            # reset time\\n            count = 0\\n            # reset joint state\\n            joint_pos = robot.data.default_joint_pos.clone()\\n            joint_vel = robot.data.default_joint_vel.clone()\\n            robot.write_joint_state_to_sim(joint_pos, joint_vel)\\n            robot.reset()\\n            # reset actions\\n            ik_commands[:] = ee_goals[current_goal_idx]\\n            joint_pos_des = joint_pos[:, robot_entity_cfg.joint_ids].clone()\\n            # reset controller\\n            diff_ik_controller.reset()\\n            diff_ik_controller.set_command(ik_commands)\\n            # change goal\\n            current_goal_idx = (current_goal_idx + 1) % len(ee_goals)\\n        else:\\n            # obtain quantities from simulation\\n            jacobian = robot.root_physx_view.get_jacobians()[:, ee_jacobi_idx, :, robot_entity_cfg.joint_ids]\\n            ee_pose_w = robot.data.body_state_w[:, robot_entity_cfg.body_ids[0], 0:7]\\n            root_pose_w = robot.data.root_state_w[:, 0:7]\\n            joint_pos = robot.data.joint_pos[:, robot_entity_cfg.joint_ids]\\n            # compute frame in root frame\\n            ee_pos_b, ee_quat_b = subtract_frame_transforms(\\n                root_pose_w[:, 0:3], root_pose_w[:, 3:7], ee_pose_w[:, 0:3], ee_pose_w[:, 3:7]\\n            )\\n            # compute the joint commands\\n            joint_pos_des = diff_ik_controller.compute(ee_pos_b, ee_quat_b, jacobian, joint_pos)\\n\\n        # apply actions\\n        robot.set_joint_position_target(joint_pos_des, joint_ids=robot_entity_cfg.joint_ids)\\n        scene.write_data_to_sim()\\n        # perform step\\n        sim.step()\\n        # update sim-time\\n        count += 1\\n        # update buffers\\n        scene.update(sim_dt)\\n\\n        # obtain quantities from simulation\\n        ee_pose_w = robot.data.body_state_w[:, robot_entity_cfg.body_ids[0], 0:7]\\n        # update marker positions\\n        ee_marker.visualize(ee_pose_w[:, 0:3], ee_pose_w[:, 3:7])\\n        goal_marker.visualize(ik_commands[:, 0:3] + scene.env_origins, ik_commands[:, 3:7])'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 2.5, 2.5], [0.0, 0.0, 0.0])\\n    # Design scene\\n    scene_cfg = TableTopSceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='class SceneCfg(InteractiveSceneCfg):\\n    \"\"\"Configuration for a simple scene with a tilted wall.\"\"\"\\n\\n    # ground plane\\n    ground = AssetBaseCfg(\\n        prim_path=\"/World/defaultGroundPlane\",\\n        spawn=sim_utils.GroundPlaneCfg(),\\n    )\\n\\n    # lights\\n    dome_light = AssetBaseCfg(\\n        prim_path=\"/World/Light\", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\\n    )\\n\\n    # Tilted wall\\n    tilted_wall = AssetBaseCfg(\\n        prim_path=\"{ENV_REGEX_NS}/TiltedWall\",\\n        spawn=sim_utils.CuboidCfg(\\n            size=(2.0, 1.5, 0.01),\\n            collision_props=sim_utils.CollisionPropertiesCfg(),\\n            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0), opacity=0.1),\\n            rigid_props=sim_utils.RigidBodyPropertiesCfg(kinematic_enabled=True),\\n            activate_contact_sensors=True,\\n        ),\\n        init_state=AssetBaseCfg.InitialStateCfg(\\n            pos=(0.6 + 0.085, 0.0, 0.3), rot=(0.9238795325, 0.0, -0.3826834324, 0.0)\\n        ),\\n    )\\n\\n    contact_forces = ContactSensorCfg(\\n        prim_path=\"/World/envs/env_.*/TiltedWall\",\\n        update_period=0.0,\\n        history_length=2,\\n        debug_vis=False,\\n    )\\n\\n    robot = FRANKA_PANDA_HIGH_PD_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n    robot.actuators[\"panda_shoulder\"].stiffness = 0.0\\n    robot.actuators[\"panda_shoulder\"].damping = 0.0\\n    robot.actuators[\"panda_forearm\"].stiffness = 0.0\\n    robot.actuators[\"panda_forearm\"].damping = 0.0\\n    robot.spawn.rigid_props.disable_gravity = True'),\n",
       " Document(metadata={}, page_content='def run_simulator(sim: sim_utils.SimulationContext, scene: InteractiveScene):\\n    \"\"\"Runs the simulation loop.\\n\\n    Args:\\n        sim: (SimulationContext) Simulation context.\\n        scene: (InteractiveScene) Interactive scene.\\n    \"\"\"\\n\\n    # Extract scene entities for readability.\\n    robot = scene[\"robot\"]\\n    contact_forces = scene[\"contact_forces\"]\\n\\n    # Obtain indices for the end-effector and arm joints\\n    ee_frame_name = \"panda_leftfinger\"\\n    arm_joint_names = [\"panda_joint.*\"]\\n    ee_frame_idx = robot.find_bodies(ee_frame_name)[0][0]\\n    arm_joint_ids = robot.find_joints(arm_joint_names)[0]\\n\\n    # Create the OSC\\n    osc_cfg = OperationalSpaceControllerCfg(\\n        target_types=[\"pose_abs\", \"wrench_abs\"],\\n        impedance_mode=\"variable_kp\",\\n        inertial_dynamics_decoupling=True,\\n        partial_inertial_dynamics_decoupling=False,\\n        gravity_compensation=False,\\n        motion_damping_ratio_task=1.0,\\n        contact_wrench_stiffness_task=[0.0, 0.0, 0.1, 0.0, 0.0, 0.0],\\n        motion_control_axes_task=[1, 1, 0, 1, 1, 1],\\n        contact_wrench_control_axes_task=[0, 0, 1, 0, 0, 0],\\n        nullspace_control=\"position\",\\n    )\\n    osc = OperationalSpaceController(osc_cfg, num_envs=scene.num_envs, device=sim.device)\\n\\n    # Markers\\n    frame_marker_cfg = FRAME_MARKER_CFG.copy()\\n    frame_marker_cfg.markers[\"frame\"].scale = (0.1, 0.1, 0.1)\\n    ee_marker = VisualizationMarkers(frame_marker_cfg.replace(prim_path=\"/Visuals/ee_current\"))\\n    goal_marker = VisualizationMarkers(frame_marker_cfg.replace(prim_path=\"/Visuals/ee_goal\"))\\n\\n    # Define targets for the arm\\n    ee_goal_pose_set_tilted_b = torch.tensor(\\n        [\\n            [0.6, 0.15, 0.3, 0.0, 0.92387953, 0.0, 0.38268343],\\n            [0.6, -0.3, 0.3, 0.0, 0.92387953, 0.0, 0.38268343],\\n            [0.8, 0.0, 0.5, 0.0, 0.92387953, 0.0, 0.38268343],\\n        ],\\n        device=sim.device,\\n    )\\n    ee_goal_wrench_set_tilted_task = torch.tensor(\\n        [\\n            [0.0, 0.0, 10.0, 0.0, 0.0, 0.0],\\n            [0.0, 0.0, 10.0, 0.0, 0.0, 0.0],\\n            [0.0, 0.0, 10.0, 0.0, 0.0, 0.0],\\n        ],\\n        device=sim.device,\\n    )\\n    kp_set_task = torch.tensor(\\n        [\\n            [360.0, 360.0, 360.0, 360.0, 360.0, 360.0],\\n            [420.0, 420.0, 420.0, 420.0, 420.0, 420.0],\\n            [320.0, 320.0, 320.0, 320.0, 320.0, 320.0],\\n        ],\\n        device=sim.device,\\n    )\\n    ee_target_set = torch.cat([ee_goal_pose_set_tilted_b, ee_goal_wrench_set_tilted_task, kp_set_task], dim=-1)\\n\\n    # Define simulation stepping\\n    sim_dt = sim.get_physics_dt()\\n\\n    # Update existing buffers\\n    # Note: We need to update buffers before the first step for the controller.\\n    robot.update(dt=sim_dt)\\n\\n    # Get the center of the robot soft joint limits\\n    joint_centers = torch.mean(robot.data.soft_joint_pos_limits[:, arm_joint_ids, :], dim=-1)\\n\\n    # get the updated states\\n    (\\n        jacobian_b,\\n        mass_matrix,\\n        gravity,\\n        ee_pose_b,\\n        ee_vel_b,\\n        root_pose_w,\\n        ee_pose_w,\\n        ee_force_b,\\n        joint_pos,\\n        joint_vel,\\n    ) = update_states(sim, scene, robot, ee_frame_idx, arm_joint_ids, contact_forces)\\n\\n    # Track the given target command\\n    current_goal_idx = 0  # Current goal index for the arm\\n    command = torch.zeros(\\n        scene.num_envs, osc.action_dim, device=sim.device\\n    )  # Generic target command, which can be pose, position, force, etc.\\n    ee_target_pose_b = torch.zeros(scene.num_envs, 7, device=sim.device)  # Target pose in the body frame\\n    ee_target_pose_w = torch.zeros(scene.num_envs, 7, device=sim.device)  # Target pose in the world frame (for marker)\\n\\n    # Set joint efforts to zero\\n    zero_joint_efforts = torch.zeros(scene.num_envs, robot.num_joints, device=sim.device)\\n    joint_efforts = torch.zeros(scene.num_envs, len(arm_joint_ids), device=sim.device)\\n\\n    count = 0\\n    # Simulation loop\\n    while simulation_app.is_running():\\n        # reset every 500 steps\\n        if count % 500 == 0:\\n            # reset joint state to default\\n            default_joint_pos = robot.data.default_joint_pos.clone()\\n            default_joint_vel = robot.data.default_joint_vel.clone()\\n            robot.write_joint_state_to_sim(default_joint_pos, default_joint_vel)\\n            robot.set_joint_effort_target(zero_joint_efforts)  # Set zero torques in the initial step\\n            robot.write_data_to_sim()\\n            robot.reset()\\n            # reset contact sensor\\n            contact_forces.reset()\\n            # reset target pose\\n            robot.update(sim_dt)\\n            _, _, _, ee_pose_b, _, _, _, _, _, _ = update_states(\\n                sim, scene, robot, ee_frame_idx, arm_joint_ids, contact_forces\\n            )  # at reset, the jacobians are not updated to the latest state\\n            command, ee_target_pose_b, ee_target_pose_w, current_goal_idx = update_target(\\n                sim, scene, osc, root_pose_w, ee_target_set, current_goal_idx\\n            )\\n            # set the osc command\\n            osc.reset()\\n            command, task_frame_pose_b = convert_to_task_frame(osc, command=command, ee_target_pose_b=ee_target_pose_b)\\n            osc.set_command(command=command, current_ee_pose_b=ee_pose_b, current_task_frame_pose_b=task_frame_pose_b)\\n        else:\\n            # get the updated states\\n            (\\n                jacobian_b,\\n                mass_matrix,\\n                gravity,\\n                ee_pose_b,\\n                ee_vel_b,\\n                root_pose_w,\\n                ee_pose_w,\\n                ee_force_b,\\n                joint_pos,\\n                joint_vel,\\n            ) = update_states(sim, scene, robot, ee_frame_idx, arm_joint_ids, contact_forces)\\n            # compute the joint commands\\n            joint_efforts = osc.compute(\\n                jacobian_b=jacobian_b,\\n                current_ee_pose_b=ee_pose_b,\\n                current_ee_vel_b=ee_vel_b,\\n                current_ee_force_b=ee_force_b,\\n                mass_matrix=mass_matrix,\\n                gravity=gravity,\\n                current_joint_pos=joint_pos,\\n                current_joint_vel=joint_vel,\\n                nullspace_joint_pos_target=joint_centers,\\n            )\\n            # apply actions\\n            robot.set_joint_effort_target(joint_efforts, joint_ids=arm_joint_ids)\\n            robot.write_data_to_sim()\\n\\n        # update marker positions\\n        ee_marker.visualize(ee_pose_w[:, 0:3], ee_pose_w[:, 3:7])\\n        goal_marker.visualize(ee_target_pose_w[:, 0:3], ee_target_pose_w[:, 3:7])\\n\\n        # perform step\\n        sim.step(render=True)\\n        # update robot buffers\\n        robot.update(sim_dt)\\n        # update buffers\\n        scene.update(sim_dt)\\n        # update sim-time\\n        count += 1'),\n",
       " Document(metadata={}, page_content='def update_states(\\n    sim: sim_utils.SimulationContext,\\n    scene: InteractiveScene,\\n    robot: Articulation,\\n    ee_frame_idx: int,\\n    arm_joint_ids: list[int],\\n    contact_forces,\\n):\\n    \"\"\"Update the robot states.\\n\\n    Args:\\n        sim: (SimulationContext) Simulation context.\\n        scene: (InteractiveScene) Interactive scene.\\n        robot: (Articulation) Robot articulation.\\n        ee_frame_idx: (int) End-effector frame index.\\n        arm_joint_ids: (list[int]) Arm joint indices.\\n        contact_forces: (ContactSensor) Contact sensor.\\n\\n    Returns:\\n        jacobian_b (torch.tensor): Jacobian in the body frame.\\n        mass_matrix (torch.tensor): Mass matrix.\\n        gravity (torch.tensor): Gravity vector.\\n        ee_pose_b (torch.tensor): End-effector pose in the body frame.\\n        ee_vel_b (torch.tensor): End-effector velocity in the body frame.\\n        root_pose_w (torch.tensor): Root pose in the world frame.\\n        ee_pose_w (torch.tensor): End-effector pose in the world frame.\\n        ee_force_b (torch.tensor): End-effector force in the body frame.\\n        joint_pos (torch.tensor): The joint positions.\\n        joint_vel (torch.tensor): The joint velocities.\\n\\n    Raises:\\n        ValueError: Undefined target_type.\\n    \"\"\"\\n    # obtain dynamics related quantities from simulation\\n    ee_jacobi_idx = ee_frame_idx - 1\\n    jacobian_w = robot.root_physx_view.get_jacobians()[:, ee_jacobi_idx, :, arm_joint_ids]\\n    mass_matrix = robot.root_physx_view.get_generalized_mass_matrices()[:, arm_joint_ids, :][:, :, arm_joint_ids]\\n    gravity = robot.root_physx_view.get_gravity_compensation_forces()[:, arm_joint_ids]\\n    # Convert the Jacobian from world to root frame\\n    jacobian_b = jacobian_w.clone()\\n    root_rot_matrix = matrix_from_quat(quat_inv(robot.data.root_quat_w))\\n    jacobian_b[:, :3, :] = torch.bmm(root_rot_matrix, jacobian_b[:, :3, :])\\n    jacobian_b[:, 3:, :] = torch.bmm(root_rot_matrix, jacobian_b[:, 3:, :])\\n\\n    # Compute current pose of the end-effector\\n    root_pos_w = robot.data.root_pos_w\\n    root_quat_w = robot.data.root_quat_w\\n    ee_pos_w = robot.data.body_pos_w[:, ee_frame_idx]\\n    ee_quat_w = robot.data.body_quat_w[:, ee_frame_idx]\\n    ee_pos_b, ee_quat_b = subtract_frame_transforms(root_pos_w, root_quat_w, ee_pos_w, ee_quat_w)\\n    root_pose_w = torch.cat([root_pos_w, root_quat_w], dim=-1)\\n    ee_pose_w = torch.cat([ee_pos_w, ee_quat_w], dim=-1)\\n    ee_pose_b = torch.cat([ee_pos_b, ee_quat_b], dim=-1)\\n\\n    # Compute the current velocity of the end-effector\\n    ee_vel_w = robot.data.body_vel_w[:, ee_frame_idx, :]  # Extract end-effector velocity in the world frame\\n    root_vel_w = robot.data.root_vel_w  # Extract root velocity in the world frame\\n    relative_vel_w = ee_vel_w - root_vel_w  # Compute the relative velocity in the world frame\\n    ee_lin_vel_b = quat_apply_inverse(robot.data.root_quat_w, relative_vel_w[:, 0:3])  # From world to root frame\\n    ee_ang_vel_b = quat_apply_inverse(robot.data.root_quat_w, relative_vel_w[:, 3:6])\\n    ee_vel_b = torch.cat([ee_lin_vel_b, ee_ang_vel_b], dim=-1)\\n\\n    # Calculate the contact force\\n    ee_force_w = torch.zeros(scene.num_envs, 3, device=sim.device)\\n    sim_dt = sim.get_physics_dt()\\n    contact_forces.update(sim_dt)  # update contact sensor\\n    # Calculate the contact force by averaging over last four time steps (i.e., to smoothen) and\\n    # taking the max of three surfaces as only one should be the contact of interest\\n    ee_force_w, _ = torch.max(torch.mean(contact_forces.data.net_forces_w_history, dim=1), dim=1)\\n\\n    # This is a simplification, only for the sake of testing.\\n    ee_force_b = ee_force_w\\n\\n    # Get joint positions and velocities\\n    joint_pos = robot.data.joint_pos[:, arm_joint_ids]\\n    joint_vel = robot.data.joint_vel[:, arm_joint_ids]\\n\\n    return (\\n        jacobian_b,\\n        mass_matrix,\\n        gravity,\\n        ee_pose_b,\\n        ee_vel_b,\\n        root_pose_w,\\n        ee_pose_w,\\n        ee_force_b,\\n        joint_pos,\\n        joint_vel,\\n    )'),\n",
       " Document(metadata={}, page_content='def update_target(\\n    sim: sim_utils.SimulationContext,\\n    scene: InteractiveScene,\\n    osc: OperationalSpaceController,\\n    root_pose_w: torch.tensor,\\n    ee_target_set: torch.tensor,\\n    current_goal_idx: int,\\n):\\n    \"\"\"Update the targets for the operational space controller.\\n\\n    Args:\\n        sim: (SimulationContext) Simulation context.\\n        scene: (InteractiveScene) Interactive scene.\\n        osc: (OperationalSpaceController) Operational space controller.\\n        root_pose_w: (torch.tensor) Root pose in the world frame.\\n        ee_target_set: (torch.tensor) End-effector target set.\\n        current_goal_idx: (int) Current goal index.\\n\\n    Returns:\\n        command (torch.tensor): Updated target command.\\n        ee_target_pose_b (torch.tensor): Updated target pose in the body frame.\\n        ee_target_pose_w (torch.tensor): Updated target pose in the world frame.\\n        next_goal_idx (int): Next goal index.\\n\\n    Raises:\\n        ValueError: Undefined target_type.\\n    \"\"\"\\n\\n    # update the ee desired command\\n    command = torch.zeros(scene.num_envs, osc.action_dim, device=sim.device)\\n    command[:] = ee_target_set[current_goal_idx]\\n\\n    # update the ee desired pose\\n    ee_target_pose_b = torch.zeros(scene.num_envs, 7, device=sim.device)\\n    for target_type in osc.cfg.target_types:\\n        if target_type == \"pose_abs\":\\n            ee_target_pose_b[:] = command[:, :7]\\n        elif target_type == \"wrench_abs\":\\n            pass  # ee_target_pose_b could stay at the root frame for force control, what matters is ee_target_b\\n        else:\\n            raise ValueError(\"Undefined target_type within update_target().\")\\n\\n    # update the target desired pose in world frame (for marker)\\n    ee_target_pos_w, ee_target_quat_w = combine_frame_transforms(\\n        root_pose_w[:, 0:3], root_pose_w[:, 3:7], ee_target_pose_b[:, 0:3], ee_target_pose_b[:, 3:7]\\n    )\\n    ee_target_pose_w = torch.cat([ee_target_pos_w, ee_target_quat_w], dim=-1)\\n\\n    next_goal_idx = (current_goal_idx + 1) % len(ee_target_set)\\n\\n    return command, ee_target_pose_b, ee_target_pose_w, next_goal_idx'),\n",
       " Document(metadata={}, page_content='def convert_to_task_frame(osc: OperationalSpaceController, command: torch.tensor, ee_target_pose_b: torch.tensor):\\n    \"\"\"Converts the target commands to the task frame.\\n\\n    Args:\\n        osc: OperationalSpaceController object.\\n        command: Command to be converted.\\n        ee_target_pose_b: Target pose in the body frame.\\n\\n    Returns:\\n        command (torch.tensor): Target command in the task frame.\\n        task_frame_pose_b (torch.tensor): Target pose in the task frame.\\n\\n    Raises:\\n        ValueError: Undefined target_type.\\n    \"\"\"\\n    command = command.clone()\\n    task_frame_pose_b = ee_target_pose_b.clone()\\n\\n    cmd_idx = 0\\n    for target_type in osc.cfg.target_types:\\n        if target_type == \"pose_abs\":\\n            command[:, :3], command[:, 3:7] = subtract_frame_transforms(\\n                task_frame_pose_b[:, :3], task_frame_pose_b[:, 3:], command[:, :3], command[:, 3:7]\\n            )\\n            cmd_idx += 7\\n        elif target_type == \"wrench_abs\":\\n            # These are already defined in target frame for ee_goal_wrench_set_tilted_task (since it is\\n            # easier), so not transforming\\n            cmd_idx += 6\\n        else:\\n            raise ValueError(\"Undefined target_type within _convert_to_task_frame().\")\\n\\n    return command, task_frame_pose_b'),\n",
       " Document(metadata={}, page_content='def main():\\n    \"\"\"Main function.\"\"\"\\n    # Load kit helper\\n    sim_cfg = sim_utils.SimulationCfg(dt=0.01, device=args_cli.device)\\n    sim = sim_utils.SimulationContext(sim_cfg)\\n    # Set main camera\\n    sim.set_camera_view([2.5, 2.5, 2.5], [0.0, 0.0, 0.0])\\n    # Design scene\\n    scene_cfg = SceneCfg(num_envs=args_cli.num_envs, env_spacing=2.0)\\n    scene = InteractiveScene(scene_cfg)\\n    # Play the simulator\\n    sim.reset()\\n    # Now we are ready!\\n    print(\"[INFO]: Setup complete...\")\\n    # Run the simulator\\n    run_simulator(sim, scene)'),\n",
       " Document(metadata={}, page_content='[build-system]\\nrequires = [\"setuptools\", \"wheel\", \"toml\"]\\nbuild-backend = \"setuptools.build_meta\"'),\n",
       " Document(metadata={}, page_content='[package]\\n\\n# Note: Semantic Versioning is used: https://semver.org/\\nversion = \"0.40.1\"\\n\\n\\n# Description\\ntitle = \"Isaac Lab framework for Robot Learning\"\\ndescription=\"Extension providing main framework interfaces and abstractions for robot learning.\"\\nreadme  = \"docs/README.md\"\\nrepository = \"https://github.com/isaac-sim/IsaacLab\"\\ncategory = \"robotics\"\\nkeywords = [\"kit\", \"robotics\", \"learning\", \"ai\"]'),\n",
       " Document(metadata={}, page_content='[python.pipapi]\\nrequirements = [\\n    \"numpy\",\\n    \"prettytable==3.3.0\",\\n    \"toml\",\\n    \"hidapi\",\\n    \"gymnasium==0.29.0\",\\n    \"trimesh\"\\n]\\n\\nmodules = [\\n    \"numpy\",\\n    \"prettytable\",\\n    \"toml\",\\n    \"hid\",\\n    \"gymnasium\",\\n    \"trimesh\"\\n]\\n\\nuse_online_index=true\\n\\n[core]\\nreloadable = false\\n\\n[[python.module]]\\nname = \"isaaclab\"'),\n",
       " Document(metadata={}, page_content='# Isaac Lab: Framework'),\n",
       " Document(metadata={}, page_content='Isaac Lab includes its own set of interfaces and wrappers around Isaac Sim classes. One of the main goals behind this\\ndecision is to have a unified description for different systems. While isaac Sim tries to be general for a wider\\nvariety of simulation requires, our goal has been to specialize these for learning requirements. These include\\nfeatures such as augmenting simulators with non-ideal actuator models, managing different observation and reward'),\n",
       " Document(metadata={}, page_content='settings, integrate different sensors, as well as provide interfaces to features that are currently not available in\\nIsaac Sim but are available from the physics side (such as deformable bodies).'),\n",
       " Document(metadata={}, page_content='We recommend the users to try out the demo scripts present in `scripts/demos` that display how different parts\\nof the framework can be integrated together.'),\n",
       " Document(metadata={}, page_content='class ActuatorBase(ABC):\\n    \"\"\"Base class for actuator models over a collection of actuated joints in an articulation.\\n\\n    Actuator models augment the simulated articulation joints with an external drive dynamics model.\\n    The model is used to convert the user-provided joint commands (positions, velocities and efforts)\\n    into the desired joint positions, velocities and efforts that are applied to the simulated articulation.\\n\\n    The base class provides the interface for the actuator models. It is responsible for parsing the\\n    actuator parameters from the configuration and storing them as buffers. It also provides the\\n    interface for resetting the actuator state and computing the desired joint commands for the simulation.\\n\\n    For each actuator model, a corresponding configuration class is provided. The configuration class\\n    is used to parse the actuator parameters from the configuration. It also specifies the joint names\\n    for which the actuator model is applied. These names can be specified as regular expressions, which\\n    are matched against the joint names in the articulation.\\n\\n    To see how the class is used, check the :class:`isaaclab.assets.Articulation` class.\\n    \"\"\"\\n\\n    is_implicit_model: ClassVar[bool] = False\\n    \"\"\"Flag indicating if the actuator is an implicit or explicit actuator model.\\n\\n    If a class inherits from :class:`ImplicitActuator`, then this flag should be set to :obj:`True`.\\n    \"\"\"\\n\\n    computed_effort: torch.Tensor\\n    \"\"\"The computed effort for the actuator group. Shape is (num_envs, num_joints).\"\"\"\\n\\n    applied_effort: torch.Tensor\\n    \"\"\"The applied effort for the actuator group. Shape is (num_envs, num_joints).\\n\\n    This is the effort obtained after clipping the :attr:`computed_effort` based on the\\n    actuator characteristics.\\n    \"\"\"\\n\\n    effort_limit: torch.Tensor\\n    \"\"\"The effort limit for the actuator group. Shape is (num_envs, num_joints).\\n\\n    For implicit actuators, the :attr:`effort_limit` and :attr:`effort_limit_sim` are the same.\\n    \"\"\"\\n\\n    effort_limit_sim: torch.Tensor\\n    \"\"\"The effort limit for the actuator group in the simulation. Shape is (num_envs, num_joints).\\n\\n    For implicit actuators, the :attr:`effort_limit` and :attr:`effort_limit_sim` are the same.\\n    \"\"\"\\n\\n    velocity_limit: torch.Tensor\\n    \"\"\"The velocity limit for the actuator group. Shape is (num_envs, num_joints).\\n\\n    For implicit actuators, the :attr:`velocity_limit` and :attr:`velocity_limit_sim` are the same.\\n    \"\"\"\\n\\n    velocity_limit_sim: torch.Tensor\\n    \"\"\"The velocity limit for the actuator group in the simulation. Shape is (num_envs, num_joints).\\n\\n    For implicit actuators, the :attr:`velocity_limit` and :attr:`velocity_limit_sim` are the same.\\n    \"\"\"\\n\\n    stiffness: torch.Tensor\\n    \"\"\"The stiffness (P gain) of the PD controller. Shape is (num_envs, num_joints).\"\"\"\\n\\n    damping: torch.Tensor\\n    \"\"\"The damping (D gain) of the PD controller. Shape is (num_envs, num_joints).\"\"\"\\n\\n    armature: torch.Tensor\\n    \"\"\"The armature of the actuator joints. Shape is (num_envs, num_joints).\"\"\"\\n\\n    friction: torch.Tensor\\n    \"\"\"The joint friction of the actuator joints. Shape is (num_envs, num_joints).\"\"\"\\n\\n    _DEFAULT_MAX_EFFORT_SIM: ClassVar[float] = 1.0e9\\n    \"\"\"The default maximum effort for the actuator joints in the simulation. Defaults to 1.0e9.\\n\\n    If the :attr:`ActuatorBaseCfg.effort_limit_sim` is not specified and the actuator is an explicit\\n    actuator, then this value is used.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        cfg: ActuatorBaseCfg,\\n        joint_names: list[str],\\n        joint_ids: slice | torch.Tensor,\\n        num_envs: int,\\n        device: str,\\n        stiffness: torch.Tensor | float = 0.0,\\n        damping: torch.Tensor | float = 0.0,\\n        armature: torch.Tensor | float = 0.0,\\n        friction: torch.Tensor | float = 0.0,\\n        effort_limit: torch.Tensor | float = torch.inf,\\n        velocity_limit: torch.Tensor | float = torch.inf,\\n    ):\\n        \"\"\"Initialize the actuator.\\n\\n        The actuator parameters are parsed from the configuration and stored as buffers. If the parameters\\n        are not specified in the configuration, then their values provided in the constructor are used.\\n\\n        .. note::\\n            The values in the constructor are typically obtained through the USD schemas corresponding\\n            to the joints in the actuator model.\\n\\n        Args:\\n            cfg: The configuration of the actuator model.\\n            joint_names: The joint names in the articulation.\\n            joint_ids: The joint indices in the articulation. If :obj:`slice(None)`, then all\\n                the joints in the articulation are part of the group.\\n            num_envs: Number of articulations in the view.\\n            device: Device used for processing.\\n            stiffness: The default joint stiffness (P gain). Defaults to 0.0.\\n                If a tensor, then the shape is (num_envs, num_joints).\\n            damping: The default joint damping (D gain). Defaults to 0.0.\\n                If a tensor, then the shape is (num_envs, num_joints).\\n            armature: The default joint armature. Defaults to 0.0.\\n                If a tensor, then the shape is (num_envs, num_joints).\\n            friction: The default joint friction. Defaults to 0.0.\\n                If a tensor, then the shape is (num_envs, num_joints).\\n            effort_limit: The default effort limit. Defaults to infinity.\\n                If a tensor, then the shape is (num_envs, num_joints).\\n            velocity_limit: The default velocity limit. Defaults to infinity.\\n                If a tensor, then the shape is (num_envs, num_joints).\\n        \"\"\"\\n        # save parameters\\n        self.cfg = cfg\\n        self._num_envs = num_envs\\n        self._device = device\\n        self._joint_names = joint_names\\n        self._joint_indices = joint_ids\\n\\n        # For explicit models, we do not want to enforce the effort limit through the solver\\n        # (unless it is explicitly set)\\n        if not self.is_implicit_model and self.cfg.effort_limit_sim is None:\\n            self.cfg.effort_limit_sim = self._DEFAULT_MAX_EFFORT_SIM\\n\\n        # parse joint stiffness and damping\\n        self.stiffness = self._parse_joint_parameter(self.cfg.stiffness, stiffness)\\n        self.damping = self._parse_joint_parameter(self.cfg.damping, damping)\\n        # parse joint armature and friction\\n        self.armature = self._parse_joint_parameter(self.cfg.armature, armature)\\n        self.friction = self._parse_joint_parameter(self.cfg.friction, friction)\\n        # parse joint limits\\n        # -- velocity\\n        self.velocity_limit_sim = self._parse_joint_parameter(self.cfg.velocity_limit_sim, velocity_limit)\\n        self.velocity_limit = self._parse_joint_parameter(self.cfg.velocity_limit, self.velocity_limit_sim)\\n        # -- effort\\n        self.effort_limit_sim = self._parse_joint_parameter(self.cfg.effort_limit_sim, effort_limit)\\n        self.effort_limit = self._parse_joint_parameter(self.cfg.effort_limit, self.effort_limit_sim)\\n\\n        # create commands buffers for allocation\\n        self.computed_effort = torch.zeros(self._num_envs, self.num_joints, device=self._device)\\n        self.applied_effort = torch.zeros_like(self.computed_effort)\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation of the actuator group.\"\"\"\\n        # resolve joint indices for printing\\n        joint_indices = self.joint_indices\\n        if joint_indices == slice(None):\\n            joint_indices = list(range(self.num_joints))\\n        # resolve model type (implicit or explicit)\\n        model_type = \"implicit\" if self.is_implicit_model else \"explicit\"\\n\\n        return (\\n            f\"<class {self.__class__.__name__}> object:\\\\n\"\\n            f\"\\\\tModel type            : {model_type}\\\\n\"\\n            f\"\\\\tNumber of joints      : {self.num_joints}\\\\n\"\\n            f\"\\\\tJoint names expression: {self.cfg.joint_names_expr}\\\\n\"\\n            f\"\\\\tJoint names           : {self.joint_names}\\\\n\"\\n            f\"\\\\tJoint indices         : {joint_indices}\\\\n\"\\n        )\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_joints(self) -> int:\\n        \"\"\"Number of actuators in the group.\"\"\"\\n        return len(self._joint_names)\\n\\n    @property\\n    def joint_names(self) -> list[str]:\\n        \"\"\"Articulation\\'s joint names that are part of the group.\"\"\"\\n        return self._joint_names\\n\\n    @property\\n    def joint_indices(self) -> slice | torch.Tensor:\\n        \"\"\"Articulation\\'s joint indices that are part of the group.\\n\\n        Note:\\n            If :obj:`slice(None)` is returned, then the group contains all the joints in the articulation.\\n            We do this to avoid unnecessary indexing of the joints for performance reasons.\\n        \"\"\"\\n        return self._joint_indices\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    @abstractmethod\\n    def reset(self, env_ids: Sequence[int]):\\n        \"\"\"Reset the internals within the group.\\n\\n        Args:\\n            env_ids: List of environment IDs to reset.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        \"\"\"Process the actuator group actions and compute the articulation actions.\\n\\n        It computes the articulation actions based on the actuator model type\\n\\n        Args:\\n            control_action: The joint action instance comprising of the desired joint positions, joint velocities\\n                and (feed-forward) joint efforts.\\n            joint_pos: The current joint positions of the joints in the group. Shape is (num_envs, num_joints).\\n            joint_vel: The current joint velocities of the joints in the group. Shape is (num_envs, num_joints).\\n\\n        Returns:\\n            The computed desired joint positions, joint velocities and joint efforts.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _parse_joint_parameter(\\n        self, cfg_value: float | dict[str, float] | None, default_value: float | torch.Tensor | None\\n    ) -> torch.Tensor:\\n        \"\"\"Parse the joint parameter from the configuration.\\n\\n        Args:\\n            cfg_value: The parameter value from the configuration. If None, then use the default value.\\n            default_value: The default value to use if the parameter is None. If it is also None,\\n                then an error is raised.\\n\\n        Returns:\\n            The parsed parameter value.\\n\\n        Raises:\\n            TypeError: If the parameter value is not of the expected type.\\n            TypeError: If the default value is not of the expected type.\\n            ValueError: If the parameter value is None and no default value is provided.\\n            ValueError: If the default value tensor is the wrong shape.\\n        \"\"\"\\n        # create parameter buffer\\n        param = torch.zeros(self._num_envs, self.num_joints, device=self._device)\\n        # parse the parameter\\n        if cfg_value is not None:\\n            if isinstance(cfg_value, (float, int)):\\n                # if float, then use the same value for all joints\\n                param[:] = float(cfg_value)\\n            elif isinstance(cfg_value, dict):\\n                # if dict, then parse the regular expression\\n                indices, _, values = string_utils.resolve_matching_names_values(cfg_value, self.joint_names)\\n                # note: need to specify type to be safe (e.g. values are ints, but we want floats)\\n                param[:, indices] = torch.tensor(values, dtype=torch.float, device=self._device)\\n            else:\\n                raise TypeError(\\n                    f\"Invalid type for parameter value: {type(cfg_value)} for \"\\n                    + f\"actuator on joints {self.joint_names}. Expected float or dict.\"\\n                )\\n        elif default_value is not None:\\n            if isinstance(default_value, (float, int)):\\n                # if float, then use the same value for all joints\\n                param[:] = float(default_value)\\n            elif isinstance(default_value, torch.Tensor):\\n                # if tensor, then use the same tensor for all joints\\n                if default_value.shape == (self._num_envs, self.num_joints):\\n                    param = default_value.float()\\n                else:\\n                    raise ValueError(\\n                        \"Invalid default value tensor shape.\\\\n\"\\n                        f\"Got: {default_value.shape}\\\\n\"\\n                        f\"Expected: {(self._num_envs, self.num_joints)}\"\\n                    )\\n            else:\\n                raise TypeError(\\n                    f\"Invalid type for default value: {type(default_value)} for \"\\n                    + f\"actuator on joints {self.joint_names}. Expected float or Tensor.\"\\n                )\\n        else:\\n            raise ValueError(\"The parameter value is None and no default value is provided.\")\\n\\n        return param\\n\\n    def _clip_effort(self, effort: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Clip the desired torques based on the motor limits.\\n\\n        Args:\\n            desired_torques: The desired torques to clip.\\n\\n        Returns:\\n            The clipped torques.\\n        \"\"\"\\n        return torch.clip(effort, min=-self.effort_limit, max=self.effort_limit)'),\n",
       " Document(metadata={}, page_content='class ActuatorBaseCfg:\\n    \"\"\"Configuration for default actuators in an articulation.\"\"\"\\n\\n    class_type: type[ActuatorBase] = MISSING\\n    \"\"\"The associated actuator class.\\n\\n    The class should inherit from :class:`isaaclab.actuators.ActuatorBase`.\\n    \"\"\"\\n\\n    joint_names_expr: list[str] = MISSING\\n    \"\"\"Articulation\\'s joint names that are part of the group.\\n\\n    Note:\\n        This can be a list of joint names or a list of regex expressions (e.g. \".*\").\\n    \"\"\"\\n\\n    effort_limit: dict[str, float] | float | None = None\\n    \"\"\"Force/Torque limit of the joints in the group. Defaults to None.\\n\\n    This limit is used to clip the computed torque sent to the simulation. If None, the\\n    limit is set to the value specified in the USD joint prim.\\n\\n    .. attention::\\n\\n        The :attr:`effort_limit_sim` attribute should be used to set the effort limit for\\n        the simulation physics solver.\\n\\n        The :attr:`effort_limit` attribute is used for clipping the effort output of the\\n        actuator model **only** in the case of explicit actuators, such as the\\n        :class:`~isaaclab.actuators.IdealPDActuator`.\\n\\n    .. note::\\n\\n        For implicit actuators, the attributes :attr:`effort_limit` and :attr:`effort_limit_sim`\\n        are equivalent. However, we suggest using the :attr:`effort_limit_sim` attribute because\\n        it is more intuitive.\\n\\n    \"\"\"\\n\\n    velocity_limit: dict[str, float] | float | None = None\\n    \"\"\"Velocity limit of the joints in the group. Defaults to None.\\n\\n    This limit is used by the actuator model. If None, the limit is set to the value specified\\n    in the USD joint prim.\\n\\n    .. attention::\\n\\n        The :attr:`velocity_limit_sim` attribute should be used to set the velocity limit for\\n        the simulation physics solver.\\n\\n        The :attr:`velocity_limit` attribute is used for clipping the effort output of the\\n        actuator model **only** in the case of explicit actuators, such as the\\n        :class:`~isaaclab.actuators.IdealPDActuator`.\\n\\n    .. note::\\n\\n        For implicit actuators, the attribute :attr:`velocity_limit` is not used. This is to stay\\n        backwards compatible with previous versions of the Isaac Lab, where this parameter was\\n        unused since PhysX did not support setting the velocity limit for the joints using the\\n        PhysX Tensor API.\\n    \"\"\"\\n\\n    effort_limit_sim: dict[str, float] | float | None = None\\n    \"\"\"Effort limit of the joints in the group applied to the simulation physics solver. Defaults to None.\\n\\n    The effort limit is used to constrain the computed joint efforts in the physics engine. If the\\n    computed effort exceeds this limit, the physics engine will clip the effort to this value.\\n\\n    Since explicit actuators (e.g. DC motor), compute and clip the effort in the actuator model, this\\n    limit is by default set to a large value to prevent the physics engine from any additional clipping.\\n    However, at times, it may be necessary to set this limit to a smaller value as a safety measure.\\n\\n    If None, the limit is resolved based on the type of actuator model:\\n\\n    * For implicit actuators, the limit is set to the value specified in the USD joint prim.\\n    * For explicit actuators, the limit is set to 1.0e9.\\n\\n    \"\"\"\\n\\n    velocity_limit_sim: dict[str, float] | float | None = None\\n    \"\"\"Velocity limit of the joints in the group applied to the simulation physics solver. Defaults to None.\\n\\n    The velocity limit is used to constrain the joint velocities in the physics engine. The joint will only\\n    be able to reach this velocity if the joint\\'s effort limit is sufficiently large. If the joint is moving\\n    faster than this velocity, the physics engine will actually try to brake the joint to reach this velocity.\\n\\n    If None, the limit is set to the value specified in the USD joint prim for both implicit and explicit actuators.\\n\\n    .. tip::\\n        If the velocity limit is too tight, the physics engine may have trouble converging to a solution.\\n        In such cases, we recommend either keeping this value sufficiently large or tuning the stiffness and\\n        damping parameters of the joint to ensure the limits are not violated.\\n\\n    \"\"\"\\n\\n    stiffness: dict[str, float] | float | None = MISSING\\n    \"\"\"Stiffness gains (also known as p-gain) of the joints in the group.\\n\\n    The behavior of the stiffness is different for implicit and explicit actuators. For implicit actuators,\\n    the stiffness gets set into the physics engine directly. For explicit actuators, the stiffness is used\\n    by the actuator model to compute the joint efforts.\\n\\n    If None, the stiffness is set to the value from the USD joint prim.\\n    \"\"\"\\n\\n    damping: dict[str, float] | float | None = MISSING\\n    \"\"\"Damping gains (also known as d-gain) of the joints in the group.\\n\\n    The behavior of the damping is different for implicit and explicit actuators. For implicit actuators,\\n    the damping gets set into the physics engine directly. For explicit actuators, the damping gain is used\\n    by the actuator model to compute the joint efforts.\\n\\n    If None, the damping is set to the value from the USD joint prim.\\n    \"\"\"\\n\\n    armature: dict[str, float] | float | None = None\\n    \"\"\"Armature of the joints in the group. Defaults to None.\\n\\n    The armature is directly added to the corresponding joint-space inertia. It helps improve the\\n    simulation stability by reducing the joint velocities.\\n\\n    It is a physics engine solver parameter that gets set into the simulation.\\n\\n    If None, the armature is set to the value from the USD joint prim.\\n    \"\"\"\\n\\n    friction: dict[str, float] | float | None = None\\n    r\"\"\"The friction coefficient of the joints in the group. Defaults to None.\\n\\n    The joint friction is a unitless quantity. It relates the magnitude of the spatial force transmitted\\n    from the parent body to the child body to the maximal friction force that may be applied by the solver\\n    to resist the joint motion.\\n\\n    Mathematically, this means that: :math:`F_{resist} \\\\leq \\\\mu F_{spatial}`, where :math:`F_{resist}`\\n    is the resisting force applied by the solver and :math:`F_{spatial}` is the spatial force\\n    transmitted from the parent body to the child body. The simulated friction effect is therefore\\n    similar to static and Coulomb friction.\\n\\n    If None, the joint friction is set to the value from the USD joint prim.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ImplicitActuatorCfg(ActuatorBaseCfg):\\n    \"\"\"Configuration for an implicit actuator.\\n\\n    Note:\\n        The PD control is handled implicitly by the simulation.\\n    \"\"\"\\n\\n    class_type: type = actuator_pd.ImplicitActuator'),\n",
       " Document(metadata={}, page_content='class IdealPDActuatorCfg(ActuatorBaseCfg):\\n    \"\"\"Configuration for an ideal PD actuator.\"\"\"\\n\\n    class_type: type = actuator_pd.IdealPDActuator'),\n",
       " Document(metadata={}, page_content='class DCMotorCfg(IdealPDActuatorCfg):\\n    \"\"\"Configuration for direct control (DC) motor actuator model.\"\"\"\\n\\n    class_type: type = actuator_pd.DCMotor\\n\\n    saturation_effort: float = MISSING\\n    \"\"\"Peak motor force/torque of the electric DC motor (in N-m).\"\"\"'),\n",
       " Document(metadata={}, page_content='class ActuatorNetLSTMCfg(DCMotorCfg):\\n    \"\"\"Configuration for LSTM-based actuator model.\"\"\"\\n\\n    class_type: type = actuator_net.ActuatorNetLSTM\\n    # we don\\'t use stiffness and damping for actuator net\\n    stiffness = None\\n    damping = None\\n\\n    network_file: str = MISSING\\n    \"\"\"Path to the file containing network weights.\"\"\"'),\n",
       " Document(metadata={}, page_content='class ActuatorNetMLPCfg(DCMotorCfg):\\n    \"\"\"Configuration for MLP-based actuator model.\"\"\"\\n\\n    class_type: type = actuator_net.ActuatorNetMLP\\n    # we don\\'t use stiffness and damping for actuator net\\n    stiffness = None\\n    damping = None\\n\\n    network_file: str = MISSING\\n    \"\"\"Path to the file containing network weights.\"\"\"\\n\\n    pos_scale: float = MISSING\\n    \"\"\"Scaling of the joint position errors input to the network.\"\"\"\\n    vel_scale: float = MISSING\\n    \"\"\"Scaling of the joint velocities input to the network.\"\"\"\\n    torque_scale: float = MISSING\\n    \"\"\"Scaling of the joint efforts output from the network.\"\"\"\\n\\n    input_order: Literal[\"pos_vel\", \"vel_pos\"] = MISSING\\n    \"\"\"Order of the inputs to the network.\\n\\n    The order can be one of the following:\\n\\n    * ``\"pos_vel\"``: joint position errors followed by joint velocities\\n    * ``\"vel_pos\"``: joint velocities followed by joint position errors\\n    \"\"\"\\n\\n    input_idx: Iterable[int] = MISSING\\n    \"\"\"\\n    Indices of the actuator history buffer passed as inputs to the network.\\n\\n    The index *0* corresponds to current time-step, while *n* corresponds to n-th\\n    time-step in the past. The allocated history length is `max(input_idx) + 1`.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class DelayedPDActuatorCfg(IdealPDActuatorCfg):\\n    \"\"\"Configuration for a delayed PD actuator.\"\"\"\\n\\n    class_type: type = actuator_pd.DelayedPDActuator\\n\\n    min_delay: int = 0\\n    \"\"\"Minimum number of physics time-steps with which the actuator command may be delayed. Defaults to 0.\"\"\"\\n\\n    max_delay: int = 0\\n    \"\"\"Maximum number of physics time-steps with which the actuator command may be delayed. Defaults to 0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class RemotizedPDActuatorCfg(DelayedPDActuatorCfg):\\n    \"\"\"Configuration for a remotized PD actuator.\\n\\n    Note:\\n        The torque output limits for this actuator is derived from a linear interpolation of a lookup table\\n        in :attr:`joint_parameter_lookup`. This table describes the relationship between joint angles and\\n        the output torques.\\n    \"\"\"\\n\\n    class_type: type = actuator_pd.RemotizedPDActuator\\n\\n    joint_parameter_lookup: list[list[float]] = MISSING\\n    \"\"\"Joint parameter lookup table. Shape is (num_lookup_points, 3).\\n\\n    This tensor describes the relationship between the joint angle (rad), the transmission ratio (in/out),\\n    and the output torque (N*m). The table is used to interpolate the output torque based on the joint angle.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ActuatorNetLSTM(DCMotor):\\n    \"\"\"Actuator model based on recurrent neural network (LSTM).\\n\\n    Unlike the MLP implementation :cite:t:`hwangbo2019learning`, this class implements\\n    the learned model as a temporal neural network (LSTM) based on the work from\\n    :cite:t:`rudin2022learning`. This removes the need of storing a history as the\\n    hidden states of the recurrent network captures the history.\\n\\n    Note:\\n        Only the desired joint positions are used as inputs to the network.\\n    \"\"\"\\n\\n    cfg: ActuatorNetLSTMCfg\\n    \"\"\"The configuration of the actuator model.\"\"\"\\n\\n    def __init__(self, cfg: ActuatorNetLSTMCfg, *args, **kwargs):\\n        super().__init__(cfg, *args, **kwargs)\\n\\n        # load the model from JIT file\\n        file_bytes = read_file(self.cfg.network_file)\\n        self.network = torch.jit.load(file_bytes, map_location=self._device).eval()\\n\\n        # extract number of lstm layers and hidden dim from the shape of weights\\n        num_layers = len(self.network.lstm.state_dict()) // 4\\n        hidden_dim = self.network.lstm.state_dict()[\"weight_hh_l0\"].shape[1]\\n        # create buffers for storing LSTM inputs\\n        self.sea_input = torch.zeros(self._num_envs * self.num_joints, 1, 2, device=self._device)\\n        self.sea_hidden_state = torch.zeros(\\n            num_layers, self._num_envs * self.num_joints, hidden_dim, device=self._device\\n        )\\n        self.sea_cell_state = torch.zeros(num_layers, self._num_envs * self.num_joints, hidden_dim, device=self._device)\\n        # reshape via views (doesn\\'t change the actual memory layout)\\n        layer_shape_per_env = (num_layers, self._num_envs, self.num_joints, hidden_dim)\\n        self.sea_hidden_state_per_env = self.sea_hidden_state.view(layer_shape_per_env)\\n        self.sea_cell_state_per_env = self.sea_cell_state.view(layer_shape_per_env)\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int]):\\n        # reset the hidden and cell states for the specified environments\\n        with torch.no_grad():\\n            self.sea_hidden_state_per_env[:, env_ids] = 0.0\\n            self.sea_cell_state_per_env[:, env_ids] = 0.0\\n\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        # compute network inputs\\n        self.sea_input[:, 0, 0] = (control_action.joint_positions - joint_pos).flatten()\\n        self.sea_input[:, 0, 1] = joint_vel.flatten()\\n        # save current joint vel for dc-motor clipping\\n        self._joint_vel[:] = joint_vel\\n\\n        # run network inference\\n        with torch.inference_mode():\\n            torques, (self.sea_hidden_state[:], self.sea_cell_state[:]) = self.network(\\n                self.sea_input, (self.sea_hidden_state, self.sea_cell_state)\\n            )\\n        self.computed_effort = torques.reshape(self._num_envs, self.num_joints)\\n\\n        # clip the computed effort based on the motor limits\\n        self.applied_effort = self._clip_effort(self.computed_effort)\\n\\n        # return torques\\n        control_action.joint_efforts = self.applied_effort\\n        control_action.joint_positions = None\\n        control_action.joint_velocities = None\\n        return control_action'),\n",
       " Document(metadata={}, page_content='class ActuatorNetMLP(DCMotor):\\n    \"\"\"Actuator model based on multi-layer perceptron and joint history.\\n\\n    Many times the analytical model is not sufficient to capture the actuator dynamics, the\\n    delay in the actuator response, or the non-linearities in the actuator. In these cases,\\n    a neural network model can be used to approximate the actuator dynamics. This model is\\n    trained using data collected from the physical actuator and maps the joint state and the\\n    desired joint command to the produced torque by the actuator.\\n\\n    This class implements the learned model as a neural network based on the work from\\n    :cite:t:`hwangbo2019learning`. The class stores the history of the joint positions errors\\n    and velocities which are used to provide input to the neural network. The model is loaded\\n    as a TorchScript.\\n\\n    Note:\\n        Only the desired joint positions are used as inputs to the network.\\n\\n    \"\"\"\\n\\n    cfg: ActuatorNetMLPCfg\\n    \"\"\"The configuration of the actuator model.\"\"\"\\n\\n    def __init__(self, cfg: ActuatorNetMLPCfg, *args, **kwargs):\\n        super().__init__(cfg, *args, **kwargs)\\n\\n        # load the model from JIT file\\n        file_bytes = read_file(self.cfg.network_file)\\n        self.network = torch.jit.load(file_bytes, map_location=self._device).eval()\\n\\n        # create buffers for MLP history\\n        history_length = max(self.cfg.input_idx) + 1\\n        self._joint_pos_error_history = torch.zeros(\\n            self._num_envs, history_length, self.num_joints, device=self._device\\n        )\\n        self._joint_vel_history = torch.zeros(self._num_envs, history_length, self.num_joints, device=self._device)\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int]):\\n        # reset the history for the specified environments\\n        self._joint_pos_error_history[env_ids] = 0.0\\n        self._joint_vel_history[env_ids] = 0.0\\n\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        # move history queue by 1 and update top of history\\n        # -- positions\\n        self._joint_pos_error_history = self._joint_pos_error_history.roll(1, 1)\\n        self._joint_pos_error_history[:, 0] = control_action.joint_positions - joint_pos\\n        # -- velocity\\n        self._joint_vel_history = self._joint_vel_history.roll(1, 1)\\n        self._joint_vel_history[:, 0] = joint_vel\\n        # save current joint vel for dc-motor clipping\\n        self._joint_vel[:] = joint_vel\\n\\n        # compute network inputs\\n        # -- positions\\n        pos_input = torch.cat([self._joint_pos_error_history[:, i].unsqueeze(2) for i in self.cfg.input_idx], dim=2)\\n        pos_input = pos_input.view(self._num_envs * self.num_joints, -1)\\n        # -- velocity\\n        vel_input = torch.cat([self._joint_vel_history[:, i].unsqueeze(2) for i in self.cfg.input_idx], dim=2)\\n        vel_input = vel_input.view(self._num_envs * self.num_joints, -1)\\n        # -- scale and concatenate inputs\\n        if self.cfg.input_order == \"pos_vel\":\\n            network_input = torch.cat([pos_input * self.cfg.pos_scale, vel_input * self.cfg.vel_scale], dim=1)\\n        elif self.cfg.input_order == \"vel_pos\":\\n            network_input = torch.cat([vel_input * self.cfg.vel_scale, pos_input * self.cfg.pos_scale], dim=1)\\n        else:\\n            raise ValueError(\\n                f\"Invalid input order for MLP actuator net: {self.cfg.input_order}. Must be \\'pos_vel\\' or \\'vel_pos\\'.\"\\n            )\\n\\n        # run network inference\\n        with torch.inference_mode():\\n            torques = self.network(network_input).view(self._num_envs, self.num_joints)\\n        self.computed_effort = torques.view(self._num_envs, self.num_joints) * self.cfg.torque_scale\\n\\n        # clip the computed effort based on the motor limits\\n        self.applied_effort = self._clip_effort(self.computed_effort)\\n\\n        # return torques\\n        control_action.joint_efforts = self.applied_effort\\n        control_action.joint_positions = None\\n        control_action.joint_velocities = None\\n        return control_action'),\n",
       " Document(metadata={}, page_content='class ImplicitActuator(ActuatorBase):\\n    \"\"\"Implicit actuator model that is handled by the simulation.\\n\\n    This performs a similar function as the :class:`IdealPDActuator` class. However, the PD control is handled\\n    implicitly by the simulation which performs continuous-time integration of the PD control law. This is\\n    generally more accurate than the explicit PD control law used in :class:`IdealPDActuator` when the simulation\\n    time-step is large.\\n\\n    The articulation class sets the stiffness and damping parameters from the implicit actuator configuration\\n    into the simulation. Thus, the class does not perform its own computations on the joint action that\\n    needs to be applied to the simulation. However, it computes the approximate torques for the actuated joint\\n    since PhysX does not expose this quantity explicitly.\\n\\n    .. caution::\\n\\n        The class is only provided for consistency with the other actuator models. It does not implement any\\n        functionality and should not be used. All values should be set to the simulation directly.\\n    \"\"\"\\n\\n    cfg: ImplicitActuatorCfg\\n    \"\"\"The configuration for the actuator model.\"\"\"\\n\\n    def __init__(self, cfg: ImplicitActuatorCfg, *args, **kwargs):\\n        # effort limits\\n        if cfg.effort_limit_sim is None and cfg.effort_limit is not None:\\n            # throw a warning that we have a replacement for the deprecated parameter\\n            omni.log.warn(\\n                \"The <ImplicitActuatorCfg> object has a value for \\'effort_limit\\'.\"\\n                \" This parameter will be removed in the future.\"\\n                \" To set the effort limit, please use \\'effort_limit_sim\\' instead.\"\\n            )\\n            cfg.effort_limit_sim = cfg.effort_limit\\n        elif cfg.effort_limit_sim is not None and cfg.effort_limit is None:\\n            # TODO: Eventually we want to get rid of \\'effort_limit\\' for implicit actuators.\\n            #   We should do this once all parameters have an \"_sim\" suffix.\\n            cfg.effort_limit = cfg.effort_limit_sim\\n        elif cfg.effort_limit_sim is not None and cfg.effort_limit is not None:\\n            if cfg.effort_limit_sim != cfg.effort_limit:\\n                raise ValueError(\\n                    \"The <ImplicitActuatorCfg> object has set both \\'effort_limit_sim\\' and \\'effort_limit\\'\"\\n                    f\" and they have different values {cfg.effort_limit_sim} != {cfg.effort_limit}.\"\\n                    \" Please only set \\'effort_limit_sim\\' for implicit actuators.\"\\n                )\\n\\n        # velocity limits\\n        if cfg.velocity_limit_sim is None and cfg.velocity_limit is not None:\\n            # throw a warning that previously this was not set\\n            # it leads to different simulation behavior so we want to remain backwards compatible\\n            omni.log.warn(\\n                \"The <ImplicitActuatorCfg> object has a value for \\'velocity_limit\\'.\"\\n                \" Previously, although this value was specified, it was not getting used by implicit\"\\n                \" actuators. Since this parameter affects the simulation behavior, we continue to not\"\\n                \" use it. This parameter will be removed in the future.\"\\n                \" To set the velocity limit, please use \\'velocity_limit_sim\\' instead.\"\\n            )\\n            cfg.velocity_limit = None\\n        elif cfg.velocity_limit_sim is not None and cfg.velocity_limit is None:\\n            # TODO: Eventually we want to get rid of \\'velocity_limit\\' for implicit actuators.\\n            #   We should do this once all parameters have an \"_sim\" suffix.\\n            cfg.velocity_limit = cfg.velocity_limit_sim\\n        elif cfg.velocity_limit_sim is not None and cfg.velocity_limit is not None:\\n            if cfg.velocity_limit_sim != cfg.velocity_limit:\\n                raise ValueError(\\n                    \"The <ImplicitActuatorCfg> object has set both \\'velocity_limit_sim\\' and \\'velocity_limit\\'\"\\n                    f\" and they have different values {cfg.velocity_limit_sim} != {cfg.velocity_limit}.\"\\n                    \" Please only set \\'velocity_limit_sim\\' for implicit actuators.\"\\n                )\\n\\n        # set implicit actuator model flag\\n        ImplicitActuator.is_implicit_model = True\\n        # call the base class\\n        super().__init__(cfg, *args, **kwargs)\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, *args, **kwargs):\\n        # This is a no-op. There is no state to reset for implicit actuators.\\n        pass\\n\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        \"\"\"Process the actuator group actions and compute the articulation actions.\\n\\n        In case of implicit actuator, the control action is directly returned as the computed action.\\n        This function is a no-op and does not perform any computation on the input control action.\\n        However, it computes the approximate torques for the actuated joint since PhysX does not compute\\n        this quantity explicitly.\\n\\n        Args:\\n            control_action: The joint action instance comprising of the desired joint positions, joint velocities\\n                and (feed-forward) joint efforts.\\n            joint_pos: The current joint positions of the joints in the group. Shape is (num_envs, num_joints).\\n            joint_vel: The current joint velocities of the joints in the group. Shape is (num_envs, num_joints).\\n\\n        Returns:\\n            The computed desired joint positions, joint velocities and joint efforts.\\n        \"\"\"\\n        # store approximate torques for reward computation\\n        error_pos = control_action.joint_positions - joint_pos\\n        error_vel = control_action.joint_velocities - joint_vel\\n        self.computed_effort = self.stiffness * error_pos + self.damping * error_vel + control_action.joint_efforts\\n        # clip the torques based on the motor limits\\n        self.applied_effort = self._clip_effort(self.computed_effort)\\n        return control_action'),\n",
       " Document(metadata={}, page_content='class IdealPDActuator(ActuatorBase):\\n    r\"\"\"Ideal torque-controlled actuator model with a simple saturation model.\\n\\n    It employs the following model for computing torques for the actuated joint :math:`j`:\\n\\n    .. math::\\n\\n        \\\\tau_{j, computed} = k_p * (q - q_{des}) + k_d * (\\\\dot{q} - \\\\dot{q}_{des}) + \\\\tau_{ff}\\n\\n    where, :math:`k_p` and :math:`k_d` are joint stiffness and damping gains, :math:`q` and :math:`\\\\dot{q}`\\n    are the current joint positions and velocities, :math:`q_{des}`, :math:`\\\\dot{q}_{des}` and :math:`\\\\tau_{ff}`\\n    are the desired joint positions, velocities and torques commands.\\n\\n    The clipping model is based on the maximum torque applied by the motor. It is implemented as:\\n\\n    .. math::\\n\\n        \\\\tau_{j, max} & = \\\\gamma \\\\times \\\\tau_{motor, max} \\\\\\\\\\n        \\\\tau_{j, applied} & = clip(\\\\tau_{computed}, -\\\\tau_{j, max}, \\\\tau_{j, max})\\n\\n    where the clipping function is defined as :math:`clip(x, x_{min}, x_{max}) = min(max(x, x_{min}), x_{max})`.\\n    The parameters :math:`\\\\gamma` is the gear ratio of the gear box connecting the motor and the actuated joint ends,\\n    and :math:`\\\\tau_{motor, max}` is the maximum motor effort possible. These parameters are read from\\n    the configuration instance passed to the class.\\n    \"\"\"\\n\\n    cfg: IdealPDActuatorCfg\\n    \"\"\"The configuration for the actuator model.\"\"\"\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int]):\\n        pass\\n\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        # compute errors\\n        error_pos = control_action.joint_positions - joint_pos\\n        error_vel = control_action.joint_velocities - joint_vel\\n        # calculate the desired joint torques\\n        self.computed_effort = self.stiffness * error_pos + self.damping * error_vel + control_action.joint_efforts\\n        # clip the torques based on the motor limits\\n        self.applied_effort = self._clip_effort(self.computed_effort)\\n        # set the computed actions back into the control action\\n        control_action.joint_efforts = self.applied_effort\\n        control_action.joint_positions = None\\n        control_action.joint_velocities = None\\n        return control_action'),\n",
       " Document(metadata={}, page_content='class DCMotor(IdealPDActuator):\\n    r\"\"\"Direct control (DC) motor actuator model with velocity-based saturation model.\\n\\n    It uses the same model as the :class:`IdealActuator` for computing the torques from input commands.\\n    However, it implements a saturation model defined by DC motor characteristics.\\n\\n    A DC motor is a type of electric motor that is powered by direct current electricity. In most cases,\\n    the motor is connected to a constant source of voltage supply, and the current is controlled by a rheostat.\\n    Depending on various design factors such as windings and materials, the motor can draw a limited maximum power\\n    from the electronic source, which limits the produced motor torque and speed.\\n\\n    A DC motor characteristics are defined by the following parameters:\\n\\n    * Continuous-rated speed (:math:`\\\\dot{q}_{motor, max}`) : The maximum-rated speed of the motor.\\n    * Continuous-stall torque (:math:`\\\\tau_{motor, max}`): The maximum-rated torque produced at 0 speed.\\n    * Saturation torque (:math:`\\\\tau_{motor, sat}`): The maximum torque that can be outputted for a short period.\\n\\n    Based on these parameters, the instantaneous minimum and maximum torques are defined as follows:\\n\\n    .. math::\\n\\n        \\\\tau_{j, max}(\\\\dot{q}) & = clip \\\\left (\\\\tau_{j, sat} \\\\times \\\\left(1 -\\n            \\\\frac{\\\\dot{q}}{\\\\dot{q}_{j, max}}\\\\right), 0.0, \\\\tau_{j, max} \\\\right) \\\\\\\\\\n        \\\\tau_{j, min}(\\\\dot{q}) & = clip \\\\left (\\\\tau_{j, sat} \\\\times \\\\left( -1 -\\n            \\\\frac{\\\\dot{q}}{\\\\dot{q}_{j, max}}\\\\right), - \\\\tau_{j, max}, 0.0 \\\\right)\\n\\n    where :math:`\\\\gamma` is the gear ratio of the gear box connecting the motor and the actuated joint ends,\\n    :math:`\\\\dot{q}_{j, max} = \\\\gamma^{-1} \\\\times  \\\\dot{q}_{motor, max}`, :math:`\\\\tau_{j, max} =\\n    \\\\gamma \\\\times \\\\tau_{motor, max}` and :math:`\\\\tau_{j, peak} = \\\\gamma \\\\times \\\\tau_{motor, peak}`\\n    are the maximum joint velocity, maximum joint torque and peak torque, respectively. These parameters\\n    are read from the configuration instance passed to the class.\\n\\n    Using these values, the computed torques are clipped to the minimum and maximum values based on the\\n    instantaneous joint velocity:\\n\\n    .. math::\\n\\n        \\\\tau_{j, applied} = clip(\\\\tau_{computed}, \\\\tau_{j, min}(\\\\dot{q}), \\\\tau_{j, max}(\\\\dot{q}))\\n\\n    \"\"\"\\n\\n    cfg: DCMotorCfg\\n    \"\"\"The configuration for the actuator model.\"\"\"\\n\\n    def __init__(self, cfg: DCMotorCfg, *args, **kwargs):\\n        super().__init__(cfg, *args, **kwargs)\\n        # parse configuration\\n        if self.cfg.saturation_effort is not None:\\n            self._saturation_effort = self.cfg.saturation_effort\\n        else:\\n            self._saturation_effort = torch.inf\\n        # prepare joint vel buffer for max effort computation\\n        self._joint_vel = torch.zeros_like(self.computed_effort)\\n        # create buffer for zeros effort\\n        self._zeros_effort = torch.zeros_like(self.computed_effort)\\n        # check that quantities are provided\\n        if self.cfg.velocity_limit is None:\\n            raise ValueError(\"The velocity limit must be provided for the DC motor actuator model.\")\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        # save current joint vel\\n        self._joint_vel[:] = joint_vel\\n        # calculate the desired joint torques\\n        return super().compute(control_action, joint_pos, joint_vel)\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _clip_effort(self, effort: torch.Tensor) -> torch.Tensor:\\n        # compute torque limits\\n        # -- max limit\\n        max_effort = self._saturation_effort * (1.0 - self._joint_vel / self.velocity_limit)\\n        max_effort = torch.clip(max_effort, min=self._zeros_effort, max=self.effort_limit)\\n        # -- min limit\\n        min_effort = self._saturation_effort * (-1.0 - self._joint_vel / self.velocity_limit)\\n        min_effort = torch.clip(min_effort, min=-self.effort_limit, max=self._zeros_effort)\\n\\n        # clip the torques based on the motor limits\\n        return torch.clip(effort, min=min_effort, max=max_effort)'),\n",
       " Document(metadata={}, page_content='class DelayedPDActuator(IdealPDActuator):\\n    \"\"\"Ideal PD actuator with delayed command application.\\n\\n    This class extends the :class:`IdealPDActuator` class by adding a delay to the actuator commands. The delay\\n    is implemented using a circular buffer that stores the actuator commands for a certain number of physics steps.\\n    The most recent actuation value is pushed to the buffer at every physics step, but the final actuation value\\n    applied to the simulation is lagged by a certain number of physics steps.\\n\\n    The amount of time lag is configurable and can be set to a random value between the minimum and maximum time\\n    lag bounds at every reset. The minimum and maximum time lag values are set in the configuration instance passed\\n    to the class.\\n    \"\"\"\\n\\n    cfg: DelayedPDActuatorCfg\\n    \"\"\"The configuration for the actuator model.\"\"\"\\n\\n    def __init__(self, cfg: DelayedPDActuatorCfg, *args, **kwargs):\\n        super().__init__(cfg, *args, **kwargs)\\n        # instantiate the delay buffers\\n        self.positions_delay_buffer = DelayBuffer(cfg.max_delay, self._num_envs, device=self._device)\\n        self.velocities_delay_buffer = DelayBuffer(cfg.max_delay, self._num_envs, device=self._device)\\n        self.efforts_delay_buffer = DelayBuffer(cfg.max_delay, self._num_envs, device=self._device)\\n        # all of the envs\\n        self._ALL_INDICES = torch.arange(self._num_envs, dtype=torch.long, device=self._device)\\n\\n    def reset(self, env_ids: Sequence[int]):\\n        super().reset(env_ids)\\n        # number of environments (since env_ids can be a slice)\\n        if env_ids is None or env_ids == slice(None):\\n            num_envs = self._num_envs\\n        else:\\n            num_envs = len(env_ids)\\n        # set a new random delay for environments in env_ids\\n        time_lags = torch.randint(\\n            low=self.cfg.min_delay,\\n            high=self.cfg.max_delay + 1,\\n            size=(num_envs,),\\n            dtype=torch.int,\\n            device=self._device,\\n        )\\n        # set delays\\n        self.positions_delay_buffer.set_time_lag(time_lags, env_ids)\\n        self.velocities_delay_buffer.set_time_lag(time_lags, env_ids)\\n        self.efforts_delay_buffer.set_time_lag(time_lags, env_ids)\\n        # reset buffers\\n        self.positions_delay_buffer.reset(env_ids)\\n        self.velocities_delay_buffer.reset(env_ids)\\n        self.efforts_delay_buffer.reset(env_ids)\\n\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        # apply delay based on the delay the model for all the setpoints\\n        control_action.joint_positions = self.positions_delay_buffer.compute(control_action.joint_positions)\\n        control_action.joint_velocities = self.velocities_delay_buffer.compute(control_action.joint_velocities)\\n        control_action.joint_efforts = self.efforts_delay_buffer.compute(control_action.joint_efforts)\\n        # compte actuator model\\n        return super().compute(control_action, joint_pos, joint_vel)'),\n",
       " Document(metadata={}, page_content='class RemotizedPDActuator(DelayedPDActuator):\\n    \"\"\"Ideal PD actuator with angle-dependent torque limits.\\n\\n    This class extends the :class:`DelayedPDActuator` class by adding angle-dependent torque limits to the actuator.\\n    The torque limits are applied by querying a lookup table describing the relationship between the joint angle\\n    and the maximum output torque. The lookup table is provided in the configuration instance passed to the class.\\n\\n    The torque limits are interpolated based on the current joint positions and applied to the actuator commands.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        cfg: RemotizedPDActuatorCfg,\\n        joint_names: list[str],\\n        joint_ids: Sequence[int],\\n        num_envs: int,\\n        device: str,\\n        stiffness: torch.Tensor | float = 0.0,\\n        damping: torch.Tensor | float = 0.0,\\n        armature: torch.Tensor | float = 0.0,\\n        friction: torch.Tensor | float = 0.0,\\n        effort_limit: torch.Tensor | float = torch.inf,\\n        velocity_limit: torch.Tensor | float = torch.inf,\\n    ):\\n        # remove effort and velocity box constraints from the base class\\n        cfg.effort_limit = torch.inf\\n        cfg.velocity_limit = torch.inf\\n        # call the base method and set default effort_limit and velocity_limit to inf\\n        super().__init__(\\n            cfg, joint_names, joint_ids, num_envs, device, stiffness, damping, armature, friction, torch.inf, torch.inf\\n        )\\n        self._joint_parameter_lookup = torch.tensor(cfg.joint_parameter_lookup, device=device)\\n        # define remotized joint torque limit\\n        self._torque_limit = LinearInterpolation(self.angle_samples, self.max_torque_samples, device=device)\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def angle_samples(self) -> torch.Tensor:\\n        return self._joint_parameter_lookup[:, 0]\\n\\n    @property\\n    def transmission_ratio_samples(self) -> torch.Tensor:\\n        return self._joint_parameter_lookup[:, 1]\\n\\n    @property\\n    def max_torque_samples(self) -> torch.Tensor:\\n        return self._joint_parameter_lookup[:, 2]\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def compute(\\n        self, control_action: ArticulationActions, joint_pos: torch.Tensor, joint_vel: torch.Tensor\\n    ) -> ArticulationActions:\\n        # call the base method\\n        control_action = super().compute(control_action, joint_pos, joint_vel)\\n        # compute the absolute torque limits for the current joint positions\\n        abs_torque_limits = self._torque_limit.compute(joint_pos)\\n        # apply the limits\\n        control_action.joint_efforts = torch.clamp(\\n            control_action.joint_efforts, min=-abs_torque_limits, max=abs_torque_limits\\n        )\\n        self.applied_effort = control_action.joint_efforts\\n        return control_action'),\n",
       " Document(metadata={}, page_content='class ExplicitAction(argparse.Action):\\n    \"\"\"Custom action to track if an argument was explicitly passed by the user.\"\"\"\\n\\n    def __call__(self, parser, namespace, values, option_string=None):\\n        # Set the parameter value\\n        setattr(namespace, self.dest, values)\\n        # Set a flag indicating the parameter was explicitly passed\\n        setattr(namespace, f\"{self.dest}_explicit\", True)'),\n",
       " Document(metadata={}, page_content='class AppLauncher:\\n    \"\"\"A utility class to launch Isaac Sim application based on command-line arguments and environment variables.\\n\\n    The class resolves the simulation app settings that appear through environments variables,\\n    command-line arguments (CLI) or as input keyword arguments. Based on these settings, it launches the\\n    simulation app and configures the extensions to load (as a part of post-launch setup).\\n\\n    The input arguments provided to the class are given higher priority than the values set\\n    from the corresponding environment variables. This provides flexibility to deal with different\\n    users\\' preferences.\\n\\n    .. note::\\n        Explicitly defined arguments are only given priority when their value is set to something outside\\n        their default configuration. For example, the ``livestream`` argument is -1 by default. It only\\n        overrides the ``LIVESTREAM`` environment variable when ``livestream`` argument is set to a\\n        value >-1. In other words, if ``livestream=-1``, then the value from the environment variable\\n        ``LIVESTREAM`` is used.\\n\\n    \"\"\"\\n\\n    def __init__(self, launcher_args: argparse.Namespace | dict | None = None, **kwargs):\\n        \"\"\"Create a `SimulationApp`_ instance based on the input settings.\\n\\n        Args:\\n            launcher_args: Input arguments to parse using the AppLauncher and set into the SimulationApp.\\n                Defaults to None, which is equivalent to passing an empty dictionary. A detailed description of\\n                the possible arguments is available in the `SimulationApp`_ documentation.\\n            **kwargs : Additional keyword arguments that will be merged into :attr:`launcher_args`.\\n                They serve as a convenience for those who want to pass some arguments using the argparse\\n                interface and others directly into the AppLauncher. Duplicated arguments with\\n                the :attr:`launcher_args` will raise a ValueError.\\n\\n        Raises:\\n            ValueError: If there are common/duplicated arguments between ``launcher_args`` and ``kwargs``.\\n            ValueError: If combination of ``launcher_args`` and ``kwargs`` are missing the necessary arguments\\n                that are needed by the AppLauncher to resolve the desired app configuration.\\n            ValueError: If incompatible or undefined values are assigned to relevant environment values,\\n                such as ``LIVESTREAM``.\\n\\n        .. _argparse.Namespace: https://docs.python.org/3/library/argparse.html?highlight=namespace#argparse.Namespace\\n        .. _SimulationApp: https://docs.omniverse.nvidia.com/py/isaacsim/source/isaacsim.simulation_app/docs/index.html\\n        \"\"\"\\n        # We allow users to pass either a dict or an argparse.Namespace into\\n        # __init__, anticipating that these will be all of the argparse arguments\\n        # used by the calling script. Those which we appended via add_app_launcher_args\\n        # will be used to control extension loading logic. Additional arguments are allowed,\\n        # and will be passed directly to the SimulationApp initialization.\\n        #\\n        # We could potentially require users to enter each argument they want passed here\\n        # as a kwarg, but this would require them to pass livestream, headless, and\\n        # any other options we choose to add here explicitly, and with the correct keywords.\\n        #\\n        # @hunter: I feel that this is cumbersome and could introduce error, and would prefer to do\\n        # some sanity checking in the add_app_launcher_args function\\n        if launcher_args is None:\\n            launcher_args = {}\\n        elif isinstance(launcher_args, argparse.Namespace):\\n            launcher_args = launcher_args.__dict__\\n\\n        # Check that arguments are unique\\n        if len(kwargs) > 0:\\n            if not set(kwargs.keys()).isdisjoint(launcher_args.keys()):\\n                overlapping_args = set(kwargs.keys()).intersection(launcher_args.keys())\\n                raise ValueError(\\n                    f\"Input `launcher_args` and `kwargs` both provided common attributes: {overlapping_args}.\"\\n                    \" Please ensure that each argument is supplied to only one of them, as the AppLauncher cannot\"\\n                    \" discern priority between them.\"\\n                )\\n            launcher_args.update(kwargs)\\n\\n        # Define config members that are read from env-vars or keyword args\\n        self._headless: bool  # 0: GUI, 1: Headless\\n        self._livestream: Literal[0, 1, 2]  # 0: Disabled, 1: Native, 2: WebRTC\\n        self._offscreen_render: bool  # 0: Disabled, 1: Enabled\\n        self._sim_experience_file: str  # Experience file to load\\n\\n        # Exposed to train scripts\\n        self.device_id: int  # device ID for GPU simulation (defaults to 0)\\n        self.local_rank: int  # local rank of GPUs in the current node\\n        self.global_rank: int  # global rank for multi-node training\\n\\n        # Integrate env-vars and input keyword args into simulation app config\\n        self._config_resolution(launcher_args)\\n\\n        # Internal: Override SimulationApp._start_app method to apply patches after app has started.\\n        self.__patch_simulation_start_app(launcher_args)\\n\\n        # Create SimulationApp, passing the resolved self._config to it for initialization\\n        self._create_app()\\n        # Load IsaacSim extensions\\n        self._load_extensions()\\n        # Hide the stop button in the toolbar\\n        self._hide_stop_button()\\n        # Set settings from the given rendering mode\\n        self._set_rendering_mode_settings(launcher_args)\\n\\n        # Hide play button callback if the timeline is stopped\\n        import omni.timeline\\n\\n        self._hide_play_button_callback = (\\n            omni.timeline.get_timeline_interface()\\n            .get_timeline_event_stream()\\n            .create_subscription_to_pop_by_type(\\n                int(omni.timeline.TimelineEventType.STOP), lambda e: self._hide_play_button(True)\\n            )\\n        )\\n        self._unhide_play_button_callback = (\\n            omni.timeline.get_timeline_interface()\\n            .get_timeline_event_stream()\\n            .create_subscription_to_pop_by_type(\\n                int(omni.timeline.TimelineEventType.PLAY), lambda e: self._hide_play_button(False)\\n            )\\n        )\\n        # Set up signal handlers for graceful shutdown\\n        # -- during explicit `kill` commands\\n        signal.signal(signal.SIGTERM, self._abort_signal_handle_callback)\\n        # -- during segfaults\\n        signal.signal(signal.SIGABRT, self._abort_signal_handle_callback)\\n        signal.signal(signal.SIGSEGV, self._abort_signal_handle_callback)\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def app(self) -> SimulationApp:\\n        \"\"\"The launched SimulationApp.\"\"\"\\n        if self._app is not None:\\n            return self._app\\n        else:\\n            raise RuntimeError(\"The `AppLauncher.app` member cannot be retrieved until the class is initialized.\")\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    @staticmethod\\n    def add_app_launcher_args(parser: argparse.ArgumentParser) -> None:\\n        \"\"\"Utility function to configure AppLauncher arguments with an existing argument parser object.\\n\\n        This function takes an ``argparse.ArgumentParser`` object and does some sanity checking on the existing\\n        arguments for ingestion by the SimulationApp. It then appends custom command-line arguments relevant\\n        to the SimulationApp to the input :class:`argparse.ArgumentParser` instance. This allows overriding the\\n        environment variables using command-line arguments.\\n\\n        Currently, it adds the following parameters to the argparser object:\\n\\n        * ``headless`` (bool): If True, the app will be launched in headless (no-gui) mode. The values map the same\\n          as that for the ``HEADLESS`` environment variable. If False, then headless mode is determined by the\\n          ``HEADLESS`` environment variable.\\n        * ``livestream`` (int): If one of {1, 2}, then livestreaming and headless mode is enabled. The values\\n          map the same as that for the ``LIVESTREAM`` environment variable. If :obj:`-1`, then livestreaming is\\n          determined by the ``LIVESTREAM`` environment variable.\\n          Valid options are:\\n\\n          - ``0``: Disabled\\n          - ``1``: `Native [DEPRECATED] <https://docs.isaacsim.omniverse.nvidia.com/latest/installation/manual_livestream_clients.html#omniverse-streaming-client-deprecated>`_\\n          - ``2``: `WebRTC <https://docs.isaacsim.omniverse.nvidia.com/latest/installation/manual_livestream_clients.html#isaac-sim-short-webrtc-streaming-client>`_\\n\\n        * ``enable_cameras`` (bool): If True, the app will enable camera sensors and render them, even when in\\n          headless mode. This flag must be set to True if the environments contains any camera sensors.\\n          The values map the same as that for the ``ENABLE_CAMERAS`` environment variable.\\n          If False, then enable_cameras mode is determined by the ``ENABLE_CAMERAS`` environment variable.\\n        * ``device`` (str): The device to run the simulation on.\\n          Valid options are:\\n\\n          - ``cpu``: Use CPU.\\n          - ``cuda``: Use GPU with device ID ``0``.\\n          - ``cuda:N``: Use GPU, where N is the device ID. For example, \"cuda:0\".\\n\\n        * ``experience`` (str): The experience file to load when launching the SimulationApp. If a relative path\\n          is provided, it is resolved relative to the ``apps`` folder in Isaac Sim and Isaac Lab (in that order).\\n\\n          If provided as an empty string, the experience file is determined based on the command-line flags:\\n\\n          * If headless and enable_cameras are True, the experience file is set to ``isaaclab.python.headless.rendering.kit``.\\n          * If headless is False and enable_cameras is True, the experience file is set to ``isaaclab.python.rendering.kit``.\\n          * If headless and enable_cameras are False, the experience file is set to ``isaaclab.python.kit``.\\n          * If headless is True and enable_cameras is False, the experience file is set to ``isaaclab.python.headless.kit``.\\n\\n        * ``kit_args`` (str): Optional command line arguments to be passed to Omniverse Kit directly.\\n          Arguments should be combined into a single string separated by space.\\n          Example usage: --kit_args \"--ext-folder=/path/to/ext1 --ext-folder=/path/to/ext2\"\\n\\n        Args:\\n            parser: An argument parser instance to be extended with the AppLauncher specific options.\\n        \"\"\"\\n        # If the passed parser has an existing _HelpAction when passed,\\n        # we here remove the options which would invoke it,\\n        # to be added back after the additional AppLauncher args\\n        # have been added. This is equivalent to\\n        # initially constructing the ArgParser with add_help=False,\\n        # but this means we don\\'t have to require that behavior\\n        # in users and can handle it on our end.\\n        # We do this because calling parse_known_args() will handle\\n        # any -h/--help options being passed and then exit immediately,\\n        # before the additional arguments can be added to the help readout.\\n        parser_help = None\\n        if len(parser._actions) > 0 and isinstance(parser._actions[0], argparse._HelpAction):  # type: ignore\\n            parser_help = parser._actions[0]\\n            parser._option_string_actions.pop(\"-h\")\\n            parser._option_string_actions.pop(\"--help\")\\n\\n        # Parse known args for potential name collisions/type mismatches\\n        # between the config fields SimulationApp expects and the ArgParse\\n        # arguments that the user passed.\\n        known, _ = parser.parse_known_args()\\n        config = vars(known)\\n        if len(config) == 0:\\n            print(\\n                \"[WARN][AppLauncher]: There are no arguments attached to the ArgumentParser object.\"\\n                \" If you have your own arguments, please load your own arguments before calling the\"\\n                \" `AppLauncher.add_app_launcher_args` method. This allows the method to check the validity\"\\n                \" of the arguments and perform checks for argument names.\"\\n            )\\n        else:\\n            AppLauncher._check_argparser_config_params(config)\\n\\n        # Add custom arguments to the parser\\n        arg_group = parser.add_argument_group(\\n            \"app_launcher arguments\",\\n            description=\"Arguments for the AppLauncher. For more details, please check the documentation.\",\\n        )\\n        arg_group.add_argument(\\n            \"--headless\",\\n            action=\"store_true\",\\n            default=AppLauncher._APPLAUNCHER_CFG_INFO[\"headless\"][1],\\n            help=\"Force display off at all times.\",\\n        )\\n        arg_group.add_argument(\\n            \"--livestream\",\\n            type=int,\\n            default=AppLauncher._APPLAUNCHER_CFG_INFO[\"livestream\"][1],\\n            choices={0, 1, 2},\\n            help=\"Force enable livestreaming. Mapping corresponds to that for the `LIVESTREAM` environment variable.\",\\n        )\\n        arg_group.add_argument(\\n            \"--enable_cameras\",\\n            action=\"store_true\",\\n            default=AppLauncher._APPLAUNCHER_CFG_INFO[\"enable_cameras\"][1],\\n            help=\"Enable camera sensors and relevant extension dependencies.\",\\n        )\\n        arg_group.add_argument(\\n            \"--xr\",\\n            action=\"store_true\",\\n            default=AppLauncher._APPLAUNCHER_CFG_INFO[\"xr\"][1],\\n            help=\"Enable XR mode for VR/AR applications.\",\\n        )\\n        arg_group.add_argument(\\n            \"--device\",\\n            type=str,\\n            action=ExplicitAction,\\n            default=AppLauncher._APPLAUNCHER_CFG_INFO[\"device\"][1],\\n            help=\\'The device to run the simulation on. Can be \"cpu\", \"cuda\", \"cuda:N\", where N is the device ID\\',\\n        )\\n        # Add the deprecated cpu flag to raise an error if it is used\\n        arg_group.add_argument(\"--cpu\", action=\"store_true\", help=argparse.SUPPRESS)\\n        arg_group.add_argument(\\n            \"--verbose\",  # Note: This is read by SimulationApp through sys.argv\\n            action=\"store_true\",\\n            help=\"Enable verbose-level log output from the SimulationApp.\",\\n        )\\n        arg_group.add_argument(\\n            \"--info\",  # Note: This is read by SimulationApp through sys.argv\\n            action=\"store_true\",\\n            help=\"Enable info-level log output from the SimulationApp.\",\\n        )\\n        arg_group.add_argument(\\n            \"--experience\",\\n            type=str,\\n            default=\"\",\\n            help=(\\n                \"The experience file to load when launching the SimulationApp. If an empty string is provided,\"\\n                \" the experience file is determined based on the headless flag. If a relative path is provided,\"\\n                \" it is resolved relative to the `apps` folder in Isaac Sim and Isaac Lab (in that order).\"\\n            ),\\n        )\\n        arg_group.add_argument(\\n            \"--rendering_mode\",\\n            type=str,\\n            action=ExplicitAction,\\n            choices={\"performance\", \"balanced\", \"quality\", \"xr\"},\\n            help=(\\n                \"Sets the rendering mode. Preset settings files can be found in apps/rendering_modes.\"\\n                \\' Can be \"performance\", \"balanced\", \"quality\", or \"xr\".\\'\\n                \" Individual settings can be overwritten by using the RenderCfg class.\"\\n            ),\\n        )\\n        arg_group.add_argument(\\n            \"--kit_args\",\\n            type=str,\\n            default=\"\",\\n            help=(\\n                \"Command line arguments for Omniverse Kit as a string separated by a space delimiter.\"\\n                \\' Example usage: --kit_args \"--ext-folder=/path/to/ext1 --ext-folder=/path/to/ext2\"\\'\\n            ),\\n        )\\n\\n        # Corresponding to the beginning of the function,\\n        # if we have removed -h/--help handling, we add it back.\\n        if parser_help is not None:\\n            parser._option_string_actions[\"-h\"] = parser_help\\n            parser._option_string_actions[\"--help\"] = parser_help\\n\\n    \"\"\"\\n    Internal functions.\\n    \"\"\"\\n\\n    _APPLAUNCHER_CFG_INFO: dict[str, tuple[list[type], Any]] = {\\n        \"headless\": ([bool], False),\\n        \"livestream\": ([int], -1),\\n        \"enable_cameras\": ([bool], False),\\n        \"xr\": ([bool], False),\\n        \"device\": ([str], \"cuda:0\"),\\n        \"experience\": ([str], \"\"),\\n        \"rendering_mode\": ([str], \"balanced\"),\\n    }\\n    \"\"\"A dictionary of arguments added manually by the :meth:`AppLauncher.add_app_launcher_args` method.\\n\\n    The values are a tuple of the expected type and default value. This is used to check against name collisions\\n    for arguments passed to the :class:`AppLauncher` class as well as for type checking.\\n\\n    They have corresponding environment variables as detailed in the documentation.\\n    \"\"\"\\n\\n    # TODO: Find some internally managed NVIDIA list of these types.\\n    # SimulationApp.DEFAULT_LAUNCHER_CONFIG almost works, except that\\n    # it is ambiguous where the default types are None\\n    _SIM_APP_CFG_TYPES: dict[str, list[type]] = {\\n        \"headless\": [bool],\\n        \"hide_ui\": [bool, type(None)],\\n        \"active_gpu\": [int, type(None)],\\n        \"physics_gpu\": [int],\\n        \"multi_gpu\": [bool],\\n        \"sync_loads\": [bool],\\n        \"width\": [int],\\n        \"height\": [int],\\n        \"window_width\": [int],\\n        \"window_height\": [int],\\n        \"display_options\": [int],\\n        \"subdiv_refinement_level\": [int],\\n        \"renderer\": [str],\\n        \"anti_aliasing\": [int],\\n        \"samples_per_pixel_per_frame\": [int],\\n        \"denoiser\": [bool],\\n        \"max_bounces\": [int],\\n        \"max_specular_transmission_bounces\": [int],\\n        \"max_volume_bounces\": [int],\\n        \"open_usd\": [str, type(None)],\\n        \"livesync_usd\": [str, type(None)],\\n        \"fast_shutdown\": [bool],\\n        \"experience\": [str],\\n    }\\n    \"\"\"A dictionary containing the type of arguments passed to SimulationApp.\\n\\n    This is used to check against name collisions for arguments passed to the :class:`AppLauncher` class\\n    as well as for type checking. It corresponds closely to the :attr:`SimulationApp.DEFAULT_LAUNCHER_CONFIG`,\\n    but specifically denotes where None types are allowed.\\n    \"\"\"\\n\\n    @staticmethod\\n    def _check_argparser_config_params(config: dict) -> None:\\n        \"\"\"Checks that input argparser object has parameters with valid settings with no name conflicts.\\n\\n        First, we inspect the dictionary to ensure that the passed ArgParser object is not attempting to add arguments\\n        which should be assigned by calling :meth:`AppLauncher.add_app_launcher_args`.\\n\\n        Then, we check that if the key corresponds to a config setting expected by SimulationApp, then the type of\\n        that key\\'s value corresponds to the type expected by the SimulationApp. If it passes the check, the function\\n        prints out that the setting with be passed to the SimulationApp. Otherwise, we raise a ValueError exception.\\n\\n        Args:\\n            config: A configuration parameters which will be passed to the SimulationApp constructor.\\n\\n        Raises:\\n            ValueError: If a key is an already existing field in the configuration parameters but\\n                should be added by calling the :meth:`AppLauncher.add_app_launcher_args.\\n            ValueError: If keys corresponding to those used to initialize SimulationApp\\n                (as found in :attr:`_SIM_APP_CFG_TYPES`) are of the wrong value type.\\n        \"\"\"\\n        # check that no config key conflicts with AppLauncher config names\\n        applauncher_keys = set(AppLauncher._APPLAUNCHER_CFG_INFO.keys())\\n        for key, value in config.items():\\n            if key in applauncher_keys:\\n                raise ValueError(\\n                    f\"The passed ArgParser object already has the field \\'{key}\\'. This field will be added by\"\\n                    \" `AppLauncher.add_app_launcher_args()`, and should not be added directly. Please remove the\"\\n                    \" argument or rename it to a non-conflicting name.\"\\n                )\\n        # check that type of the passed keys are valid\\n        simulationapp_keys = set(AppLauncher._SIM_APP_CFG_TYPES.keys())\\n        for key, value in config.items():\\n            if key in simulationapp_keys:\\n                given_type = type(value)\\n                expected_types = AppLauncher._SIM_APP_CFG_TYPES[key]\\n                if type(value) not in set(expected_types):\\n                    raise ValueError(\\n                        f\"Invalid value type for the argument \\'{key}\\': {given_type}. Expected one of {expected_types},\"\\n                        \" if intended to be ingested by the SimulationApp object. Please change the type if this\"\\n                        \" intended for the SimulationApp or change the name of the argument to avoid name conflicts.\"\\n                    )\\n                # Print out values which will be used\\n                print(f\"[INFO][AppLauncher]: The argument \\'{key}\\' will be used to configure the SimulationApp.\")\\n\\n    def _config_resolution(self, launcher_args: dict):\\n        \"\"\"Resolve the input arguments and environment variables.\\n\\n        Args:\\n            launcher_args: A dictionary of all input arguments passed to the class object.\\n        \"\"\"\\n        # Handle core settings\\n        livestream_arg, livestream_env = self._resolve_livestream_settings(launcher_args)\\n        self._resolve_headless_settings(launcher_args, livestream_arg, livestream_env)\\n        self._resolve_camera_settings(launcher_args)\\n        self._resolve_xr_settings(launcher_args)\\n        self._resolve_viewport_settings(launcher_args)\\n\\n        # Handle device and distributed settings\\n        self._resolve_device_settings(launcher_args)\\n\\n        # Handle experience file settings\\n        self._resolve_experience_file(launcher_args)\\n\\n        # Handle additional arguments\\n        self._resolve_kit_args(launcher_args)\\n\\n        # Prepare final simulation app config\\n        # Remove all values from input keyword args which are not meant for SimulationApp\\n        # Assign all the passed settings to a dictionary for the simulation app\\n        self._sim_app_config = {\\n            key: launcher_args[key] for key in set(AppLauncher._SIM_APP_CFG_TYPES.keys()) & set(launcher_args.keys())\\n        }\\n\\n    def _resolve_livestream_settings(self, launcher_args: dict) -> tuple[int, int]:\\n        \"\"\"Resolve livestream related settings.\"\"\"\\n        livestream_env = int(os.environ.get(\"LIVESTREAM\", 0))\\n        livestream_arg = launcher_args.pop(\"livestream\", AppLauncher._APPLAUNCHER_CFG_INFO[\"livestream\"][1])\\n        livestream_valid_vals = {0, 1, 2}\\n        # Value checking on LIVESTREAM\\n        if livestream_env not in livestream_valid_vals:\\n            raise ValueError(\\n                f\"Invalid value for environment variable `LIVESTREAM`: {livestream_env} .\"\\n                f\" Expected: {livestream_valid_vals}.\"\\n            )\\n        # We allow livestream kwarg to supersede LIVESTREAM envvar\\n        if livestream_arg >= 0:\\n            if livestream_arg in livestream_valid_vals:\\n                self._livestream = livestream_arg\\n                # print info that we overrode the env-var\\n                print(\\n                    f\"[INFO][AppLauncher]: Input keyword argument `livestream={livestream_arg}` has overridden\"\\n                    f\" the environment variable `LIVESTREAM={livestream_env}`.\"\\n                )\\n            else:\\n                raise ValueError(\\n                    f\"Invalid value for input keyword argument `livestream`: {livestream_arg} .\"\\n                    f\" Expected: {livestream_valid_vals}.\"\\n                )\\n        else:\\n            self._livestream = livestream_env\\n\\n        # Process livestream here before launching kit because some of the extensions only work when launched with the kit file\\n        self._livestream_args = []\\n        if self._livestream >= 1:\\n            # Note: Only one livestream extension can be enabled at a time\\n            if self._livestream == 1:\\n                warnings.warn(\\n                    \"Native Livestream is deprecated. Please use WebRTC Livestream instead with --livestream 2.\"\\n                )\\n                self._livestream_args += [\\n                    \\'--/app/livestream/proto=\"ws\"\\',\\n                    \"--/app/livestream/allowResize=true\",\\n                    \"--enable\",\\n                    \"omni.kit.livestream.core-4.1.2\",\\n                    \"--enable\",\\n                    \"omni.kit.livestream.native-5.0.1\",\\n                    \"--enable\",\\n                    \"omni.kit.streamsdk.plugins-4.1.1\",\\n                ]\\n            elif self._livestream == 2:\\n                self._livestream_args += [\\n                    \"--/app/livestream/allowResize=false\",\\n                    \"--enable\",\\n                    \"omni.kit.livestream.webrtc\",\\n                ]\\n            else:\\n                raise ValueError(f\"Invalid value for livestream: {self._livestream}. Expected: 1, 2 .\")\\n            sys.argv += self._livestream_args\\n\\n        return livestream_arg, livestream_env\\n\\n    def _resolve_headless_settings(self, launcher_args: dict, livestream_arg: int, livestream_env: int):\\n        \"\"\"Resolve headless related settings.\"\"\"\\n        # Resolve headless execution of simulation app\\n        # HEADLESS is initially passed as an int instead of\\n        # the bool of headless_arg to avoid messy string processing,\\n        headless_env = int(os.environ.get(\"HEADLESS\", 0))\\n        headless_arg = launcher_args.pop(\"headless\", AppLauncher._APPLAUNCHER_CFG_INFO[\"headless\"][1])\\n        headless_valid_vals = {0, 1}\\n        # Value checking on HEADLESS\\n        if headless_env not in headless_valid_vals:\\n            raise ValueError(\\n                f\"Invalid value for environment variable `HEADLESS`: {headless_env} . Expected: {headless_valid_vals}.\"\\n            )\\n        # We allow headless kwarg to supersede HEADLESS envvar if headless_arg does not have the default value\\n        # Note: Headless is always true when livestreaming\\n        if headless_arg is True:\\n            self._headless = headless_arg\\n        elif self._livestream in {1, 2}:\\n            # we are always headless on the host machine\\n            self._headless = True\\n            # inform who has toggled the headless flag\\n            if self._livestream == livestream_arg:\\n                print(\\n                    f\"[INFO][AppLauncher]: Input keyword argument `livestream={self._livestream}` has implicitly\"\\n                    f\" overridden the environment variable `HEADLESS={headless_env}` to True.\"\\n                )\\n            elif self._livestream == livestream_env:\\n                print(\\n                    f\"[INFO][AppLauncher]: Environment variable `LIVESTREAM={self._livestream}` has implicitly\"\\n                    f\" overridden the environment variable `HEADLESS={headless_env}` to True.\"\\n                )\\n        else:\\n            # Headless needs to be a bool to be ingested by SimulationApp\\n            self._headless = bool(headless_env)\\n        # Headless needs to be passed to the SimulationApp so we keep it here\\n        launcher_args[\"headless\"] = self._headless\\n\\n    def _resolve_camera_settings(self, launcher_args: dict):\\n        \"\"\"Resolve camera related settings.\"\"\"\\n        enable_cameras_env = int(os.environ.get(\"ENABLE_CAMERAS\", 0))\\n        enable_cameras_arg = launcher_args.pop(\"enable_cameras\", AppLauncher._APPLAUNCHER_CFG_INFO[\"enable_cameras\"][1])\\n        enable_cameras_valid_vals = {0, 1}\\n        if enable_cameras_env not in enable_cameras_valid_vals:\\n            raise ValueError(\\n                f\"Invalid value for environment variable `ENABLE_CAMERAS`: {enable_cameras_env} .\"\\n                f\"Expected: {enable_cameras_valid_vals} .\"\\n            )\\n        # We allow enable_cameras kwarg to supersede ENABLE_CAMERAS envvar\\n        if enable_cameras_arg is True:\\n            self._enable_cameras = enable_cameras_arg\\n        else:\\n            self._enable_cameras = bool(enable_cameras_env)\\n        self._offscreen_render = False\\n        if self._enable_cameras and self._headless:\\n            self._offscreen_render = True\\n\\n    def _resolve_xr_settings(self, launcher_args: dict):\\n        \"\"\"Resolve XR related settings.\"\"\"\\n        xr_env = int(os.environ.get(\"XR\", 0))\\n        xr_arg = launcher_args.pop(\"xr\", AppLauncher._APPLAUNCHER_CFG_INFO[\"xr\"][1])\\n        xr_valid_vals = {0, 1}\\n        if xr_env not in xr_valid_vals:\\n            raise ValueError(f\"Invalid value for environment variable `XR`: {xr_env} .Expected: {xr_valid_vals} .\")\\n        # We allow xr kwarg to supersede XR envvar\\n        if xr_arg is True:\\n            self._xr = xr_arg\\n        else:\\n            self._xr = bool(xr_env)\\n\\n    def _resolve_viewport_settings(self, launcher_args: dict):\\n        \"\"\"Resolve viewport related settings.\"\"\"\\n        # Check if we can disable the viewport to improve performance\\n        #   This should only happen if we are running headless and do not require livestreaming or video recording\\n        #   This is different from offscreen_render because this only affects the default viewport and not other renderproducts in the scene\\n        self._render_viewport = True\\n        if self._headless and not self._livestream and not launcher_args.get(\"video\", False):\\n            self._render_viewport = False\\n\\n        # hide_ui flag\\n        launcher_args[\"hide_ui\"] = False\\n        if self._headless and not self._livestream:\\n            launcher_args[\"hide_ui\"] = True\\n\\n        # avoid creating new stage at startup by default for performance reasons\\n        launcher_args[\"create_new_stage\"] = False\\n\\n    def _resolve_device_settings(self, launcher_args: dict):\\n        \"\"\"Resolve simulation GPU device related settings.\"\"\"\\n        self.device_id = 0\\n        device = launcher_args.get(\"device\", AppLauncher._APPLAUNCHER_CFG_INFO[\"device\"][1])\\n\\n        device_explicitly_passed = launcher_args.pop(\"device_explicit\", False)\\n        if self._xr and not device_explicitly_passed:\\n            # If no device is specified, default to the CPU device if we are running in XR\\n            device = \"cpu\"\\n\\n            # Overwrite for downstream consumers\\n            launcher_args[\"device\"] = \"cpu\"\\n\\n        if \"cuda\" not in device and \"cpu\" not in device:\\n            raise ValueError(\\n                f\"Invalid value for input keyword argument `device`: {device}.\"\\n                \" Expected: a string with the format \\'cuda\\', \\'cuda:<device_id>\\', or \\'cpu\\'.\"\\n            )\\n\\n        if \"cuda:\" in device:\\n            self.device_id = int(device.split(\":\")[-1])\\n\\n        # Raise an error for the deprecated cpu flag\\n        if launcher_args.get(\"cpu\", False):\\n            raise ValueError(\"The `--cpu` flag is deprecated. Please use `--device cpu` instead.\")\\n\\n        if \"distributed\" in launcher_args and launcher_args[\"distributed\"]:\\n            # local rank (GPU id) in a current multi-gpu mode\\n            self.local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\")) + int(os.getenv(\"JAX_LOCAL_RANK\", \"0\"))\\n            # global rank (GPU id) in multi-gpu multi-node mode\\n            self.global_rank = int(os.getenv(\"RANK\", \"0\")) + int(os.getenv(\"JAX_RANK\", \"0\"))\\n\\n            self.device_id = self.local_rank\\n            launcher_args[\"multi_gpu\"] = False\\n            # limit CPU threads to minimize thread context switching\\n            # this ensures processes do not take up all available threads and fight for resources\\n            num_cpu_cores = os.cpu_count()\\n            num_threads_per_process = num_cpu_cores // int(os.getenv(\"WORLD_SIZE\", 1))\\n            # set environment variables to limit CPU threads\\n            os.environ[\"PXR_WORK_THREAD_LIMIT\"] = str(num_threads_per_process)\\n            os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_threads_per_process)\\n            # pass command line variable to kit\\n            sys.argv.append(f\"--/plugins/carb.tasking.plugin/threadCount={num_threads_per_process}\")\\n\\n        # set physics and rendering device\\n        launcher_args[\"physics_gpu\"] = self.device_id\\n        launcher_args[\"active_gpu\"] = self.device_id\\n\\n        print(f\"[INFO][AppLauncher]: Using device: {device}\")\\n\\n    def _resolve_experience_file(self, launcher_args: dict):\\n        \"\"\"Resolve experience file related settings.\"\"\"\\n        # Check if input keywords contain an \\'experience\\' file setting\\n        # Note: since experience is taken as a separate argument by Simulation App, we store it separately\\n        self._sim_experience_file = launcher_args.pop(\"experience\", \"\")\\n\\n        # If nothing is provided resolve the experience file based on the headless flag\\n        kit_app_exp_path = os.environ[\"EXP_PATH\"]\\n        isaaclab_app_exp_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), *[\"..\"] * 4, \"apps\")\\n        if self._sim_experience_file == \"\":\\n            # check if the headless flag is set\\n            if self._enable_cameras:\\n                if self._headless and not self._livestream:\\n                    self._sim_experience_file = os.path.join(\\n                        isaaclab_app_exp_path, \"isaaclab.python.headless.rendering.kit\"\\n                    )\\n                else:\\n                    self._sim_experience_file = os.path.join(isaaclab_app_exp_path, \"isaaclab.python.rendering.kit\")\\n            elif self._xr:\\n                if self._headless:\\n                    self._sim_experience_file = os.path.join(\\n                        isaaclab_app_exp_path, \"isaaclab.python.xr.openxr.headless.kit\"\\n                    )\\n                else:\\n                    self._sim_experience_file = os.path.join(isaaclab_app_exp_path, \"isaaclab.python.xr.openxr.kit\")\\n            elif self._headless and not self._livestream:\\n                self._sim_experience_file = os.path.join(isaaclab_app_exp_path, \"isaaclab.python.headless.kit\")\\n            else:\\n                self._sim_experience_file = os.path.join(isaaclab_app_exp_path, \"isaaclab.python.kit\")\\n        elif not os.path.isabs(self._sim_experience_file):\\n            option_1_app_exp_path = os.path.join(kit_app_exp_path, self._sim_experience_file)\\n            option_2_app_exp_path = os.path.join(isaaclab_app_exp_path, self._sim_experience_file)\\n            if os.path.exists(option_1_app_exp_path):\\n                self._sim_experience_file = option_1_app_exp_path\\n            elif os.path.exists(option_2_app_exp_path):\\n                self._sim_experience_file = option_2_app_exp_path\\n            else:\\n                raise FileNotFoundError(\\n                    f\"Invalid value for input keyword argument `experience`: {self._sim_experience_file}.\"\\n                    \"\\\\n No such file exists in either the Kit or Isaac Lab experience paths. Checked paths:\"\\n                    f\"\\\\n\\\\t [1]: {option_1_app_exp_path}\"\\n                    f\"\\\\n\\\\t [2]: {option_2_app_exp_path}\"\\n                )\\n        elif not os.path.exists(self._sim_experience_file):\\n            raise FileNotFoundError(\\n                f\"Invalid value for input keyword argument `experience`: {self._sim_experience_file}.\"\\n                \" The file does not exist.\"\\n            )\\n\\n        # Set public IP address of a remote instance\\n        public_ip_env = os.environ.get(\"PUBLIC_IP\", \"127.0.0.1\")\\n\\n        # Process livestream here before launching kit because some of the extensions only work when launched with the kit file\\n        self._livestream_args = []\\n        if self._livestream >= 1:\\n            # Note: Only one livestream extension can be enabled at a time\\n            if self._livestream == 1:\\n                warnings.warn(\\n                    \"Native Livestream is deprecated. Please use WebRTC Livestream instead with --livestream 2.\"\\n                )\\n                self._livestream_args += [\\n                    \\'--/app/livestream/proto=\"ws\"\\',\\n                    \"--/app/livestream/allowResize=true\",\\n                    \"--enable\",\\n                    \"omni.kit.livestream.core-4.1.2\",\\n                    \"--enable\",\\n                    \"omni.kit.livestream.native-5.0.1\",\\n                    \"--enable\",\\n                    \"omni.kit.streamsdk.plugins-4.1.1\",\\n                ]\\n            elif self._livestream == 2:\\n                self._livestream_args += [\\n                    f\"--/app/livestream/publicEndpointAddress={public_ip_env}\",\\n                    \"--/app/livestream/port=49100\",\\n                    \"--enable\",\\n                    \"omni.services.livestream.nvcf\",\\n                ]\\n            else:\\n                raise ValueError(f\"Invalid value for livestream: {self._livestream}. Expected: 1, 2 .\")\\n            sys.argv += self._livestream_args\\n        # Resolve the absolute path of the experience file\\n        self._sim_experience_file = os.path.abspath(self._sim_experience_file)\\n        print(f\"[INFO][AppLauncher]: Loading experience file: {self._sim_experience_file}\")\\n\\n    def _resolve_kit_args(self, launcher_args: dict):\\n        \"\"\"Resolve additional arguments passed to Kit.\"\"\"\\n        # Resolve additional arguments passed to Kit\\n        self._kit_args = []\\n        if \"kit_args\" in launcher_args:\\n            self._kit_args = [arg for arg in launcher_args[\"kit_args\"].split()]\\n            sys.argv += self._kit_args\\n\\n    def _create_app(self):\\n        \"\"\"Launch and create the SimulationApp based on the parsed simulation config.\"\"\"\\n        # Initialize SimulationApp\\n        # hack sys module to make sure that the SimulationApp is initialized correctly\\n        # this is to avoid the warnings from the simulation app about not ok modules\\n        r = re.compile(\".*lab.*\")\\n        found_modules = list(filter(r.match, list(sys.modules.keys())))\\n        # remove Isaac Lab modules from sys.modules\\n        hacked_modules = dict()\\n        for key in found_modules:\\n            hacked_modules[key] = sys.modules[key]\\n            del sys.modules[key]\\n\\n        # disable sys stdout and stderr to avoid printing the warning messages\\n        # this is mainly done to purge the print statements from the simulation app\\n        if \"--verbose\" not in sys.argv and \"--info\" not in sys.argv:\\n            sys.stdout = open(os.devnull, \"w\")  # noqa: SIM115\\n        # launch simulation app\\n        self._app = SimulationApp(self._sim_app_config, experience=self._sim_experience_file)\\n        # enable sys stdout and stderr\\n        sys.stdout = sys.__stdout__\\n\\n        # add Isaac Lab modules back to sys.modules\\n        for key, value in hacked_modules.items():\\n            sys.modules[key] = value\\n        # remove the threadCount argument from sys.argv if it was added for distributed training\\n        pattern = r\"--/plugins/carb\\\\.tasking\\\\.plugin/threadCount=\\\\d+\"\\n        sys.argv = [arg for arg in sys.argv if not re.match(pattern, arg)]\\n        # remove additional OV args from sys.argv\\n        if len(self._kit_args) > 0:\\n            sys.argv = [arg for arg in sys.argv if arg not in self._kit_args]\\n        if len(self._livestream_args) > 0:\\n            sys.argv = [arg for arg in sys.argv if arg not in self._livestream_args]\\n\\n    def _rendering_enabled(self) -> bool:\\n        \"\"\"Check if rendering is required by the app.\"\"\"\\n        # Indicates whether rendering is required by the app.\\n        # Extensions required for rendering bring startup and simulation costs, so we do not enable them if not required.\\n        return not self._headless or self._livestream >= 1 or self._enable_cameras or self._xr\\n\\n    def _load_extensions(self):\\n        \"\"\"Load correct extensions based on AppLauncher\\'s resolved config member variables.\"\"\"\\n        # These have to be loaded after SimulationApp is initialized\\n        import carb\\n        import omni.physx.bindings._physx as physx_impl\\n\\n        # Retrieve carb settings for modification\\n        carb_settings_iface = carb.settings.get_settings()\\n\\n        # set carb setting to indicate Isaac Lab\\'s offscreen_render pipeline should be enabled\\n        # this flag is used by the SimulationContext class to enable the offscreen_render pipeline\\n        # when the render() method is called.\\n        carb_settings_iface.set_bool(\"/isaaclab/render/offscreen\", self._offscreen_render)\\n\\n        # set carb setting to indicate Isaac Lab\\'s render_viewport pipeline should be enabled\\n        # this flag is used by the SimulationContext class to enable the render_viewport pipeline\\n        # when the render() method is called.\\n        carb_settings_iface.set_bool(\"/isaaclab/render/active_viewport\", self._render_viewport)\\n\\n        # set carb setting to indicate no RTX sensors are used\\n        # this flag is set to True when an RTX-rendering related sensor is created\\n        # for example: the `Camera` sensor class\\n        carb_settings_iface.set_bool(\"/isaaclab/render/rtx_sensors\", False)\\n\\n        # set fabric update flag to disable updating transforms when rendering is disabled\\n        carb_settings_iface.set_bool(\"/physics/fabricUpdateTransformations\", self._rendering_enabled())\\n\\n        # disable physics backwards compatibility check\\n        carb_settings_iface.set_int(physx_impl.SETTING_BACKWARD_COMPATIBILITY, 0)\\n\\n    def _hide_stop_button(self):\\n        \"\"\"Hide the stop button in the toolbar.\\n\\n        For standalone executions, having a stop button is confusing since it invalidates the whole simulation.\\n        Thus, we hide the button so that users don\\'t accidentally click it.\\n        \"\"\"\\n        # when we are truly headless, then we can\\'t import the widget toolbar\\n        # thus, we only hide the stop button when we are not headless (i.e. GUI is enabled)\\n        if self._livestream >= 1 or not self._headless:\\n            import omni.kit.widget.toolbar\\n\\n            # grey out the stop button because we don\\'t want to stop the simulation manually in standalone mode\\n            toolbar = omni.kit.widget.toolbar.get_instance()\\n            play_button_group = toolbar._builtin_tools._play_button_group  # type: ignore\\n            if play_button_group is not None:\\n                play_button_group._stop_button.visible = False  # type: ignore\\n                play_button_group._stop_button.enabled = False  # type: ignore\\n                play_button_group._stop_button = None  # type: ignore\\n\\n    def _set_rendering_mode_settings(self, launcher_args: dict) -> None:\\n        \"\"\"Set RTX rendering settings to the values from the selected preset.\"\"\"\\n        import carb\\n        from isaacsim.core.utils.carb import set_carb_setting\\n\\n        rendering_mode = launcher_args.get(\"rendering_mode\")\\n\\n        # use default kit rendering settings if cameras are disabled and a rendering mode is not selected\\n        if not self._enable_cameras and rendering_mode is None:\\n            return\\n\\n        # default to balanced mode\\n        if rendering_mode is None:\\n            rendering_mode = \"balanced\"\\n\\n        rendering_mode_explicitly_passed = launcher_args.pop(\"rendering_mode_explicit\", False)\\n        if self._xr and not rendering_mode_explicitly_passed:\\n            # If no rendering mode is specified, default to the xr mode if we are running in XR\\n            rendering_mode = \"xr\"\\n\\n            # Overwrite for downstream consumers\\n            launcher_args[\"rendering_mode\"] = \"xr\"\\n\\n        # parse preset file\\n        repo_path = os.path.join(carb.tokens.get_tokens_interface().resolve(\"${app}\"), \"..\")\\n        preset_filename = os.path.join(repo_path, f\"apps/rendering_modes/{rendering_mode}.kit\")\\n        with open(preset_filename) as file:\\n            preset_dict = toml.load(file)\\n        preset_dict = dict(flatdict.FlatDict(preset_dict, delimiter=\".\"))\\n\\n        # set presets\\n        carb_setting = carb.settings.get_settings()\\n        for key, value in preset_dict.items():\\n            key = \"/\" + key.replace(\".\", \"/\")  # convert to carb setting format\\n            set_carb_setting(carb_setting, key, value)\\n\\n    def _interrupt_signal_handle_callback(self, signal, frame):\\n        \"\"\"Handle the interrupt signal from the keyboard.\"\"\"\\n        # close the app\\n        self._app.close()\\n        # raise the error for keyboard interrupt\\n        raise KeyboardInterrupt\\n\\n    def _hide_play_button(self, flag):\\n        \"\"\"Hide/Unhide the play button in the toolbar.\\n\\n        This is used if the timeline is stopped by a GUI action like \"save as\" to not allow the user to\\n        resume the timeline afterwards.\\n        \"\"\"\\n        # when we are truly headless, then we can\\'t import the widget toolbar\\n        # thus, we only hide the play button when we are not headless (i.e. GUI is enabled)\\n        if self._livestream >= 1 or not self._headless:\\n            import omni.kit.widget.toolbar\\n\\n            toolbar = omni.kit.widget.toolbar.get_instance()\\n            play_button_group = toolbar._builtin_tools._play_button_group  # type: ignore\\n            if play_button_group is not None:\\n                play_button_group._play_button.visible = not flag  # type: ignore\\n                play_button_group._play_button.enabled = not flag  # type: ignore\\n\\n    def _abort_signal_handle_callback(self, signal, frame):\\n        \"\"\"Handle the abort/segmentation/kill signals.\"\"\"\\n        # close the app\\n        self._app.close()\\n\\n    def __patch_simulation_start_app(self, launcher_args: dict):\\n        if not launcher_args.get(\"enable_pinocchio\", False):\\n            return\\n\\n        if launcher_args.get(\"disable_pinocchio_patch\", False):\\n            return\\n\\n        original_start_app = SimulationApp._start_app\\n\\n        def _start_app_patch(sim_app_instance, *args, **kwargs):\\n            original_start_app(sim_app_instance, *args, **kwargs)\\n            self.__patch_pxr_gf_matrix4d(launcher_args)\\n\\n        SimulationApp._start_app = _start_app_patch\\n\\n    def __patch_pxr_gf_matrix4d(self, launcher_args: dict):\\n        import traceback\\n\\n        import carb\\n        from pxr import Gf\\n\\n        carb.log_warn(\\n            \"Due to an issue with Pinocchio and pxr.Gf.Matrix4d, patching the Matrix4d constructor to convert arguments\"\\n            \" into a list of floats.\"\\n        )\\n\\n        # Store the original Matrix4d constructor\\n        original_matrix4d = Gf.Matrix4d.__init__\\n\\n        # Define a wrapper function to handle different input types\\n        def patch_matrix4d(self, *args, **kwargs):\\n            try:\\n                # Case 1: No arguments (identity matrix)\\n                if len(args) == 0:\\n                    original_matrix4d(self, *args, **kwargs)\\n                    return\\n\\n                # Case 2: Single argument\\n                elif len(args) == 1:\\n                    arg = args[0]\\n\\n                    # Case 2a: Already a Matrix4d\\n                    if isinstance(arg, Gf.Matrix4d):\\n                        original_matrix4d(self, arg)\\n                        return\\n\\n                    # Case 2b: Tuple of tuples (4x4 matrix) OR List of lists (4x4 matrix)\\n                    elif (isinstance(arg, tuple) and len(arg) == 4 and all(isinstance(row, tuple) for row in arg)) or (\\n                        isinstance(arg, list) and len(arg) == 4 and all(isinstance(row, list) for row in arg)\\n                    ):\\n                        float_list = [float(item) for row in arg for item in row]\\n                        original_matrix4d(self, *float_list)\\n                        return\\n\\n                    # Case 2c: Flat list of 16 elements\\n                    elif isinstance(arg, (list, tuple)) and len(arg) == 16:\\n                        float_list = [float(item) for item in arg]\\n                        original_matrix4d(self, *float_list)\\n                        return\\n\\n                    # Case 2d: Another matrix-like object with elements accessible via indexing\\n                    elif hasattr(arg, \"__getitem__\") and hasattr(arg, \"__len__\"):\\n                        with contextlib.suppress(IndexError, TypeError):\\n                            if len(arg) == 16:\\n                                float_list = [float(arg[i]) for i in range(16)]\\n                                original_matrix4d(self, *float_list)\\n                                return\\n                            # Try to extract as 4x4 matrix\\n                            elif len(arg) == 4 and all(len(row) == 4 for row in arg):\\n                                float_list = [float(arg[i][j]) for i in range(4) for j in range(4)]\\n                                original_matrix4d(self, *float_list)\\n                                return\\n\\n                # Case 3: 16 separate arguments (individual matrix elements)\\n                elif len(args) == 16:\\n                    float_list = [float(arg) for arg in args]\\n                    original_matrix4d(self, *float_list)\\n                    return\\n\\n                # Default: Use original constructor\\n                original_matrix4d(self, *args, **kwargs)\\n\\n            except Exception as e:\\n                carb.log_error(f\"Matrix4d wrapper error: {e}\")\\n                traceback.print_stack()\\n                # Fall back to original constructor as last resort\\n                try:\\n                    original_matrix4d(self, *args, **kwargs)\\n                except Exception as inner_e:\\n                    carb.log_error(f\"Original Matrix4d constructor also failed: {inner_e}\")\\n                    # Initialize as identity matrix if all else fails\\n                    original_matrix4d(self)\\n\\n        Gf.Matrix4d.__init__ = patch_matrix4d'),\n",
       " Document(metadata={}, page_content='class AssetBase(ABC):\\n    \"\"\"The base interface class for assets.\\n\\n    An asset corresponds to any physics-enabled object that can be spawned in the simulation. These include\\n    rigid objects, articulated objects, deformable objects etc. The core functionality of an asset is to\\n    provide a set of buffers that can be used to interact with the simulator. The buffers are updated\\n    by the asset class and can be written into the simulator using the their respective ``write`` methods.\\n    This allows a convenient way to perform post-processing operations on the buffers before writing them\\n    into the simulator and obtaining the corresponding simulation results.\\n\\n    The class handles both the spawning of the asset into the USD stage as well as initialization of necessary\\n    physics handles to interact with the asset. Upon construction of the asset instance, the prim corresponding\\n    to the asset is spawned into the USD stage if the spawn configuration is not None. The spawn configuration\\n    is defined in the :attr:`AssetBaseCfg.spawn` attribute. In case the configured :attr:`AssetBaseCfg.prim_path`\\n    is an expression, then the prim is spawned at all the matching paths. Otherwise, a single prim is spawned\\n    at the configured path. For more information on the spawn configuration, see the\\n    :mod:`isaaclab.sim.spawners` module.\\n\\n    Unlike Isaac Sim interface, where one usually needs to call the\\n    :meth:`isaacsim.core.prims.XFormPrim.initialize` method to initialize the PhysX handles, the asset\\n    class automatically initializes and invalidates the PhysX handles when the stage is played/stopped. This\\n    is done by registering callbacks for the stage play/stop events.\\n\\n    Additionally, the class registers a callback for debug visualization of the asset if a debug visualization\\n    is implemented in the asset class. This can be enabled by setting the :attr:`AssetBaseCfg.debug_vis` attribute\\n    to True. The debug visualization is implemented through the :meth:`_set_debug_vis_impl` and\\n    :meth:`_debug_vis_callback` methods.\\n    \"\"\"\\n\\n    def __init__(self, cfg: AssetBaseCfg):\\n        \"\"\"Initialize the asset base.\\n\\n        Args:\\n            cfg: The configuration class for the asset.\\n\\n        Raises:\\n            RuntimeError: If no prims found at input prim path or prim path expression.\\n        \"\"\"\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs\\n        self.cfg = cfg.copy()\\n        # flag for whether the asset is initialized\\n        self._is_initialized = False\\n\\n        # check if base asset path is valid\\n        # note: currently the spawner does not work if there is a regex pattern in the leaf\\n        #   For example, if the prim path is \"/World/Robot_[1,2]\" since the spawner will not\\n        #   know which prim to spawn. This is a limitation of the spawner and not the asset.\\n        asset_path = self.cfg.prim_path.split(\"/\")[-1]\\n        asset_path_is_regex = re.match(r\"^[a-zA-Z0-9/_]+$\", asset_path) is None\\n        # spawn the asset\\n        if self.cfg.spawn is not None and not asset_path_is_regex:\\n            self.cfg.spawn.func(\\n                self.cfg.prim_path,\\n                self.cfg.spawn,\\n                translation=self.cfg.init_state.pos,\\n                orientation=self.cfg.init_state.rot,\\n            )\\n        # check that spawn was successful\\n        matching_prims = sim_utils.find_matching_prims(self.cfg.prim_path)\\n        if len(matching_prims) == 0:\\n            raise RuntimeError(f\"Could not find prim with path {self.cfg.prim_path}.\")\\n\\n        # note: Use weakref on all callbacks to ensure that this object can be deleted when its destructor is called.\\n        # add callbacks for stage play/stop\\n        # The order is set to 10 which is arbitrary but should be lower priority than the default order of 0\\n        timeline_event_stream = omni.timeline.get_timeline_interface().get_timeline_event_stream()\\n        self._initialize_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n            int(omni.timeline.TimelineEventType.PLAY),\\n            lambda event, obj=weakref.proxy(self): obj._initialize_callback(event),\\n            order=10,\\n        )\\n        self._invalidate_initialize_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n            int(omni.timeline.TimelineEventType.STOP),\\n            lambda event, obj=weakref.proxy(self): obj._invalidate_initialize_callback(event),\\n            order=10,\\n        )\\n        self._prim_deletion_callback_id = SimulationManager.register_callback(\\n            self._on_prim_deletion, event=IsaacEvents.PRIM_DELETION\\n        )\\n        # add handle for debug visualization (this is set to a valid handle inside set_debug_vis)\\n        self._debug_vis_handle = None\\n        # set initial state of debug visualization\\n        self.set_debug_vis(self.cfg.debug_vis)\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribe from the callbacks.\"\"\"\\n        # clear events handles\\n        self._clear_callbacks()\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def is_initialized(self) -> bool:\\n        \"\"\"Whether the asset is initialized.\\n\\n        Returns True if the asset is initialized, False otherwise.\\n        \"\"\"\\n        return self._is_initialized\\n\\n    @property\\n    @abstractmethod\\n    def num_instances(self) -> int:\\n        \"\"\"Number of instances of the asset.\\n\\n        This is equal to the number of asset instances per environment multiplied by the number of environments.\\n        \"\"\"\\n        return NotImplementedError\\n\\n    @property\\n    def device(self) -> str:\\n        \"\"\"Memory device for computation.\"\"\"\\n        return self._device\\n\\n    @property\\n    @abstractmethod\\n    def data(self) -> Any:\\n        \"\"\"Data related to the asset.\"\"\"\\n        return NotImplementedError\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the asset has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_debug_vis_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def set_visibility(self, visible: bool, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the visibility of the prims corresponding to the asset.\\n\\n        This operation affects the visibility of the prims corresponding to the asset in the USD stage.\\n        It is useful for toggling the visibility of the asset in the simulator. For instance, one can\\n        hide the asset when it is not being used to reduce the rendering overhead.\\n\\n        Note:\\n            This operation uses the PXR API to set the visibility of the prims. Thus, the operation\\n            may have an overhead if the number of prims is large.\\n\\n        Args:\\n            visible: Whether to make the prims visible or not.\\n            env_ids: The indices of the object to set visibility. Defaults to None (all instances).\\n        \"\"\"\\n        # resolve the environment ids\\n        if env_ids is None:\\n            env_ids = range(len(self._prims))\\n        elif isinstance(env_ids, torch.Tensor):\\n            env_ids = env_ids.detach().cpu().tolist()\\n\\n        # obtain the prims corresponding to the asset\\n        # note: we only want to find the prims once since this is a costly operation\\n        if not hasattr(self, \"_prims\"):\\n            self._prims = sim_utils.find_matching_prims(self.cfg.prim_path)\\n\\n        # iterate over the environment ids\\n        for env_id in env_ids:\\n            prim_utils.set_prim_visibility(self._prims[env_id], visible)\\n\\n    def set_debug_vis(self, debug_vis: bool) -> bool:\\n        \"\"\"Sets whether to visualize the asset data.\\n\\n        Args:\\n            debug_vis: Whether to visualize the asset data.\\n\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the asset\\n            does not support debug visualization.\\n        \"\"\"\\n        # check if debug visualization is supported\\n        if not self.has_debug_vis_implementation:\\n            return False\\n        # toggle debug visualization objects\\n        self._set_debug_vis_impl(debug_vis)\\n        # toggle debug visualization handles\\n        if debug_vis:\\n            # create a subscriber for the post update event if it doesn\\'t exist\\n            if self._debug_vis_handle is None:\\n                app_interface = omni.kit.app.get_app_interface()\\n                self._debug_vis_handle = app_interface.get_post_update_event_stream().create_subscription_to_pop(\\n                    lambda event, obj=weakref.proxy(self): obj._debug_vis_callback(event)\\n                )\\n        else:\\n            # remove the subscriber if it exists\\n            if self._debug_vis_handle is not None:\\n                self._debug_vis_handle.unsubscribe()\\n                self._debug_vis_handle = None\\n        # return success\\n        return True\\n\\n    @abstractmethod\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        \"\"\"Resets all internal buffers of selected environments.\\n\\n        Args:\\n            env_ids: The indices of the object to reset. Defaults to None (all instances).\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def write_data_to_sim(self):\\n        \"\"\"Writes data to the simulator.\"\"\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def update(self, dt: float):\\n        \"\"\"Update the internal buffers.\\n\\n        The time step ``dt`` is used to compute numerical derivatives of quantities such as joint\\n        accelerations which are not provided by the simulator.\\n\\n        Args:\\n            dt: The amount of time passed from last ``update`` call.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    \"\"\"\\n    Implementation specific.\\n    \"\"\"\\n\\n    @abstractmethod\\n    def _initialize_impl(self):\\n        \"\"\"Initializes the PhysX handles and internal buffers.\"\"\"\\n        raise NotImplementedError\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set debug visualization into visualization objects.\\n\\n        This function is responsible for creating the visualization objects if they don\\'t exist\\n        and input ``debug_vis`` is True. If the visualization objects exist, the function should\\n        set their visibility into the stage.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")\\n\\n    def _debug_vis_callback(self, event):\\n        \"\"\"Callback for debug visualization.\\n\\n        This function calls the visualization objects and sets the data to visualize into them.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _initialize_callback(self, event):\\n        \"\"\"Initializes the scene elements.\\n\\n        Note:\\n            PhysX handles are only enabled once the simulator starts playing. Hence, this function needs to be\\n            called whenever the simulator \"plays\" from a \"stop\" state.\\n        \"\"\"\\n        if not self._is_initialized:\\n            # obtain simulation related information\\n            self._backend = SimulationManager.get_backend()\\n            self._device = SimulationManager.get_physics_sim_device()\\n            # initialize the asset\\n            try:\\n                self._initialize_impl()\\n            except Exception as e:\\n                if builtins.ISAACLAB_CALLBACK_EXCEPTION is None:\\n                    builtins.ISAACLAB_CALLBACK_EXCEPTION = e\\n            # set flag\\n            self._is_initialized = True\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        self._is_initialized = False\\n        if self._debug_vis_handle is not None:\\n            self._debug_vis_handle.unsubscribe()\\n            self._debug_vis_handle = None\\n\\n    def _on_prim_deletion(self, prim_path: str) -> None:\\n        \"\"\"Invalidates and deletes the callbacks when the prim is deleted.\\n\\n        Args:\\n            prim_path: The path to the prim that is being deleted.\\n\\n        Note:\\n            This function is called when the prim is deleted.\\n        \"\"\"\\n        if prim_path == \"/\":\\n            self._clear_callbacks()\\n            return\\n        result = re.match(\\n            pattern=\"^\" + \"/\".join(self.cfg.prim_path.split(\"/\")[: prim_path.count(\"/\") + 1]) + \"$\", string=prim_path\\n        )\\n        if result:\\n            self._clear_callbacks()\\n\\n    def _clear_callbacks(self) -> None:\\n        \"\"\"Clears the callbacks.\"\"\"\\n        if self._prim_deletion_callback_id:\\n            SimulationManager.deregister_callback(self._prim_deletion_callback_id)\\n            self._prim_deletion_callback_id = None\\n        if self._initialize_handle:\\n            self._initialize_handle.unsubscribe()\\n            self._initialize_handle = None\\n        if self._invalidate_initialize_handle:\\n            self._invalidate_initialize_handle.unsubscribe()\\n            self._invalidate_initialize_handle = None\\n        # clear debug visualization\\n        if self._debug_vis_handle:\\n            self._debug_vis_handle.unsubscribe()\\n            self._debug_vis_handle = None'),\n",
       " Document(metadata={}, page_content='class AssetBaseCfg:\\n    \"\"\"The base configuration class for an asset\\'s parameters.\\n\\n    Please see the :class:`AssetBase` class for more information on the asset class.\\n    \"\"\"\\n\\n    @configclass\\n    class InitialStateCfg:\\n        \"\"\"Initial state of the asset.\\n\\n        This defines the default initial state of the asset when it is spawned into the simulation, as\\n        well as the default state when the simulation is reset.\\n\\n        After parsing the initial state, the asset class stores this information in the :attr:`data`\\n        attribute of the asset class. This can then be accessed by the user to modify the state of the asset\\n        during the simulation, for example, at resets.\\n        \"\"\"\\n\\n        # root position\\n        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Position of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n        \"\"\"Quaternion rotation (w, x, y, z) of the root in simulation world frame.\\n        Defaults to (1.0, 0.0, 0.0, 0.0).\\n        \"\"\"\\n\\n    class_type: type[AssetBase] = None\\n    \"\"\"The associated asset class. Defaults to None, which means that the asset will be spawned\\n    but cannot be interacted with via the asset class.\\n\\n    The class should inherit from :class:`isaaclab.assets.asset_base.AssetBase`.\\n    \"\"\"\\n\\n    prim_path: str = MISSING\\n    \"\"\"Prim path (or expression) to the asset.\\n\\n    .. note::\\n        The expression can contain the environment namespace regex ``{ENV_REGEX_NS}`` which\\n        will be replaced with the environment namespace.\\n\\n        Example: ``{ENV_REGEX_NS}/Robot`` will be replaced with ``/World/envs/env_.*/Robot``.\\n    \"\"\"\\n\\n    spawn: SpawnerCfg | None = None\\n    \"\"\"Spawn configuration for the asset. Defaults to None.\\n\\n    If None, then no prims are spawned by the asset class. Instead, it is assumed that the\\n    asset is already present in the scene.\\n    \"\"\"\\n\\n    init_state: InitialStateCfg = InitialStateCfg()\\n    \"\"\"Initial state of the rigid object. Defaults to identity pose.\"\"\"\\n\\n    collision_group: Literal[0, -1] = 0\\n    \"\"\"Collision group of the asset. Defaults to ``0``.\\n\\n    * ``-1``: global collision group (collides with all assets in the scene).\\n    * ``0``: local collision group (collides with other assets in the same environment).\\n    \"\"\"\\n\\n    debug_vis: bool = False\\n    \"\"\"Whether to enable debug visualization for the asset. Defaults to ``False``.\"\"\"'),\n",
       " Document(metadata={}, page_content='class Articulation(AssetBase):\\n    \"\"\"An articulation asset class.\\n\\n    An articulation is a collection of rigid bodies connected by joints. The joints can be either\\n    fixed or actuated. The joints can be of different types, such as revolute, prismatic, D-6, etc.\\n    However, the articulation class has currently been tested with revolute and prismatic joints.\\n    The class supports both floating-base and fixed-base articulations. The type of articulation\\n    is determined based on the root joint of the articulation. If the root joint is fixed, then\\n    the articulation is considered a fixed-base system. Otherwise, it is considered a floating-base\\n    system. This can be checked using the :attr:`Articulation.is_fixed_base` attribute.\\n\\n    For an asset to be considered an articulation, the root prim of the asset must have the\\n    `USD ArticulationRootAPI`_. This API is used to define the sub-tree of the articulation using\\n    the reduced coordinate formulation. On playing the simulation, the physics engine parses the\\n    articulation root prim and creates the corresponding articulation in the physics engine. The\\n    articulation root prim can be specified using the :attr:`AssetBaseCfg.prim_path` attribute.\\n\\n    The articulation class also provides the functionality to augment the simulation of an articulated\\n    system with custom actuator models. These models can either be explicit or implicit, as detailed in\\n    the :mod:`isaaclab.actuators` module. The actuator models are specified using the\\n    :attr:`ArticulationCfg.actuators` attribute. These are then parsed and used to initialize the\\n    corresponding actuator models, when the simulation is played.\\n\\n    During the simulation step, the articulation class first applies the actuator models to compute\\n    the joint commands based on the user-specified targets. These joint commands are then applied\\n    into the simulation. The joint commands can be either position, velocity, or effort commands.\\n    As an example, the following snippet shows how this can be used for position commands:\\n\\n    .. code-block:: python\\n\\n        # an example instance of the articulation class\\n        my_articulation = Articulation(cfg)\\n\\n        # set joint position targets\\n        my_articulation.set_joint_position_target(position)\\n        # propagate the actuator models and apply the computed commands into the simulation\\n        my_articulation.write_data_to_sim()\\n\\n        # step the simulation using the simulation context\\n        sim_context.step()\\n\\n        # update the articulation state, where dt is the simulation time step\\n        my_articulation.update(dt)\\n\\n    .. _`USD ArticulationRootAPI`: https://openusd.org/dev/api/class_usd_physics_articulation_root_a_p_i.html\\n\\n    \"\"\"\\n\\n    cfg: ArticulationCfg\\n    \"\"\"Configuration instance for the articulations.\"\"\"\\n\\n    actuators: dict[str, ActuatorBase]\\n    \"\"\"Dictionary of actuator instances for the articulation.\\n\\n    The keys are the actuator names and the values are the actuator instances. The actuator instances\\n    are initialized based on the actuator configurations specified in the :attr:`ArticulationCfg.actuators`\\n    attribute. They are used to compute the joint commands during the :meth:`write_data_to_sim` function.\\n    \"\"\"\\n\\n    def __init__(self, cfg: ArticulationCfg):\\n        \"\"\"Initialize the articulation.\\n\\n        Args:\\n            cfg: A configuration instance.\\n        \"\"\"\\n        super().__init__(cfg)\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def data(self) -> ArticulationData:\\n        return self._data\\n\\n    @property\\n    def num_instances(self) -> int:\\n        return self.root_physx_view.count\\n\\n    @property\\n    def is_fixed_base(self) -> bool:\\n        \"\"\"Whether the articulation is a fixed-base or floating-base system.\"\"\"\\n        return self.root_physx_view.shared_metatype.fixed_base\\n\\n    @property\\n    def num_joints(self) -> int:\\n        \"\"\"Number of joints in articulation.\"\"\"\\n        return self.root_physx_view.shared_metatype.dof_count\\n\\n    @property\\n    def num_fixed_tendons(self) -> int:\\n        \"\"\"Number of fixed tendons in articulation.\"\"\"\\n        return self.root_physx_view.max_fixed_tendons\\n\\n    @property\\n    def num_bodies(self) -> int:\\n        \"\"\"Number of bodies in articulation.\"\"\"\\n        return self.root_physx_view.shared_metatype.link_count\\n\\n    @property\\n    def joint_names(self) -> list[str]:\\n        \"\"\"Ordered names of joints in articulation.\"\"\"\\n        return self.root_physx_view.shared_metatype.dof_names\\n\\n    @property\\n    def fixed_tendon_names(self) -> list[str]:\\n        \"\"\"Ordered names of fixed tendons in articulation.\"\"\"\\n        return self._fixed_tendon_names\\n\\n    @property\\n    def body_names(self) -> list[str]:\\n        \"\"\"Ordered names of bodies in articulation.\"\"\"\\n        return self.root_physx_view.shared_metatype.link_names\\n\\n    @property\\n    def root_physx_view(self) -> physx.ArticulationView:\\n        \"\"\"Articulation view for the asset (PhysX).\\n\\n        Note:\\n            Use this view with caution. It requires handling of tensors in a specific way.\\n        \"\"\"\\n        return self._root_physx_view\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # use ellipses object to skip initial indices.\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset actuators\\n        for actuator in self.actuators.values():\\n            actuator.reset(env_ids)\\n        # reset external wrench\\n        self._external_force_b[env_ids] = 0.0\\n        self._external_torque_b[env_ids] = 0.0\\n\\n    def write_data_to_sim(self):\\n        \"\"\"Write external wrenches and joint commands to the simulation.\\n\\n        If any explicit actuators are present, then the actuator models are used to compute the\\n        joint commands. Otherwise, the joint commands are directly set into the simulation.\\n\\n        Note:\\n            We write external wrench to the simulation here since this function is called before the simulation step.\\n            This ensures that the external wrench is applied at every simulation step.\\n        \"\"\"\\n        # write external wrench\\n        if self.has_external_wrench:\\n            self.root_physx_view.apply_forces_and_torques_at_position(\\n                force_data=self._external_force_b.view(-1, 3),\\n                torque_data=self._external_torque_b.view(-1, 3),\\n                position_data=None,\\n                indices=self._ALL_INDICES,\\n                is_global=False,\\n            )\\n\\n        # apply actuator models\\n        self._apply_actuator_model()\\n        # write actions into simulation\\n        self.root_physx_view.set_dof_actuation_forces(self._joint_effort_target_sim, self._ALL_INDICES)\\n        # position and velocity targets only for implicit actuators\\n        if self._has_implicit_actuators:\\n            self.root_physx_view.set_dof_position_targets(self._joint_pos_target_sim, self._ALL_INDICES)\\n            self.root_physx_view.set_dof_velocity_targets(self._joint_vel_target_sim, self._ALL_INDICES)\\n\\n    def update(self, dt: float):\\n        self._data.update(dt)\\n\\n    \"\"\"\\n    Operations - Finders.\\n    \"\"\"\\n\\n    def find_bodies(self, name_keys: str | Sequence[str], preserve_order: bool = False) -> tuple[list[int], list[str]]:\\n        \"\"\"Find bodies in the articulation based on the name keys.\\n\\n        Please check the :meth:`isaaclab.utils.string_utils.resolve_matching_names` function for more\\n        information on the name matching.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the body names.\\n            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.\\n\\n        Returns:\\n            A tuple of lists containing the body indices and names.\\n        \"\"\"\\n        return string_utils.resolve_matching_names(name_keys, self.body_names, preserve_order)\\n\\n    def find_joints(\\n        self, name_keys: str | Sequence[str], joint_subset: list[str] | None = None, preserve_order: bool = False\\n    ) -> tuple[list[int], list[str]]:\\n        \"\"\"Find joints in the articulation based on the name keys.\\n\\n        Please see the :func:`isaaclab.utils.string.resolve_matching_names` function for more information\\n        on the name matching.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the joint names.\\n            joint_subset: A subset of joints to search for. Defaults to None, which means all joints\\n                in the articulation are searched.\\n            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.\\n\\n        Returns:\\n            A tuple of lists containing the joint indices and names.\\n        \"\"\"\\n        if joint_subset is None:\\n            joint_subset = self.joint_names\\n        # find joints\\n        return string_utils.resolve_matching_names(name_keys, joint_subset, preserve_order)\\n\\n    def find_fixed_tendons(\\n        self, name_keys: str | Sequence[str], tendon_subsets: list[str] | None = None, preserve_order: bool = False\\n    ) -> tuple[list[int], list[str]]:\\n        \"\"\"Find fixed tendons in the articulation based on the name keys.\\n\\n        Please see the :func:`isaaclab.utils.string.resolve_matching_names` function for more information\\n        on the name matching.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the joint\\n                names with fixed tendons.\\n            tendon_subsets: A subset of joints with fixed tendons to search for. Defaults to None, which means\\n                all joints in the articulation are searched.\\n            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.\\n\\n        Returns:\\n            A tuple of lists containing the tendon indices and names.\\n        \"\"\"\\n        if tendon_subsets is None:\\n            # tendons follow the joint names they are attached to\\n            tendon_subsets = self.fixed_tendon_names\\n        # find tendons\\n        return string_utils.resolve_matching_names(name_keys, tendon_subsets, preserve_order)\\n\\n    \"\"\"\\n    Operations - State Writers.\\n    \"\"\"\\n\\n    def write_root_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root state over selected environment indices into the simulation.\\n\\n        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n\\n        # set into simulation\\n        self.write_root_pose_to_sim(root_state[:, :7], env_ids=env_ids)\\n        self.write_root_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)\\n\\n    def write_root_com_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass state over selected environment indices into the simulation.\\n\\n        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # set into simulation\\n        self.write_root_com_pose_to_sim(root_state[:, :7], env_ids=env_ids)\\n        self.write_root_com_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)\\n\\n    def write_root_link_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root link state over selected environment indices into the simulation.\\n\\n        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # set into simulation\\n        self.write_root_link_pose_to_sim(root_state[:, :7], env_ids=env_ids)\\n        self.write_root_link_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)\\n\\n    def write_root_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root pose over selected environment indices into the simulation.\\n\\n        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n\\n        Args:\\n            root_pose: Root poses in simulation frame. Shape is (len(env_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_state_w[env_ids, :7] = root_pose.clone()\\n        # convert root quaternion from wxyz to xyzw\\n        root_poses_xyzw = self._data.root_state_w[:, :7].clone()\\n        root_poses_xyzw[:, 3:] = math_utils.convert_quat(root_poses_xyzw[:, 3:], to=\"xyzw\")\\n        # Need to invalidate the buffer to trigger the update with the new root pose.\\n        self._data._body_state_w.timestamp = -1.0\\n        self._data._body_link_state_w.timestamp = -1.0\\n        self._data._body_com_state_w.timestamp = -1.0\\n        # set into simulation\\n        self.root_physx_view.set_root_transforms(root_poses_xyzw, indices=physx_env_ids)\\n\\n    def write_root_link_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root link pose over selected environment indices into the simulation.\\n\\n        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n\\n        Args:\\n            root_pose: Root poses in simulation frame. Shape is (len(env_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_link_state_w[env_ids, :7] = root_pose.clone()\\n        self._data.root_state_w[env_ids, :7] = self._data.root_link_state_w[env_ids, :7]\\n        # convert root quaternion from wxyz to xyzw\\n        root_poses_xyzw = self._data.root_link_state_w[:, :7].clone()\\n        root_poses_xyzw[:, 3:] = math_utils.convert_quat(root_poses_xyzw[:, 3:], to=\"xyzw\")\\n        # Need to invalidate the buffer to trigger the update with the new root pose.\\n        self._data._body_state_w.timestamp = -1.0\\n        self._data._body_link_state_w.timestamp = -1.0\\n        self._data._body_com_state_w.timestamp = -1.0\\n        # set into simulation\\n        self.root_physx_view.set_root_transforms(root_poses_xyzw, indices=physx_env_ids)\\n\\n    def write_root_com_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass pose over selected environment indices into the simulation.\\n\\n        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n        The orientation is the orientation of the principle axes of inertia.\\n\\n        Args:\\n            root_pose: Root center of mass poses in simulation frame. Shape is (len(env_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n\\n        com_pos = self.data.com_pos_b[env_ids, 0, :]\\n        com_quat = self.data.com_quat_b[env_ids, 0, :]\\n\\n        root_link_pos, root_link_quat = math_utils.combine_frame_transforms(\\n            root_pose[..., :3],\\n            root_pose[..., 3:7],\\n            math_utils.quat_apply(math_utils.quat_inv(com_quat), -com_pos),\\n            math_utils.quat_inv(com_quat),\\n        )\\n\\n        root_link_pose = torch.cat((root_link_pos, root_link_quat), dim=-1)\\n        self.write_root_link_pose_to_sim(root_pose=root_link_pose, env_ids=physx_env_ids)\\n\\n    def write_root_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass velocity over selected environment indices into the simulation.\\n\\n        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.\\n        NOTE: This sets the velocity of the root\\'s center of mass rather than the roots frame.\\n\\n        Args:\\n            root_velocity: Root center of mass velocities in simulation world frame. Shape is (len(env_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_state_w[env_ids, 7:] = root_velocity.clone()\\n        self._data.body_acc_w[env_ids] = 0.0\\n        # set into simulation\\n        self.root_physx_view.set_root_velocities(self._data.root_state_w[:, 7:], indices=physx_env_ids)\\n\\n    def write_root_com_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass velocity over selected environment indices into the simulation.\\n\\n        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.\\n        NOTE: This sets the velocity of the root\\'s center of mass rather than the roots frame.\\n\\n        Args:\\n            root_velocity: Root center of mass velocities in simulation world frame. Shape is (len(env_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_com_state_w[env_ids, 7:] = root_velocity.clone()\\n        self._data.root_state_w[env_ids, 7:] = self._data.root_com_state_w[env_ids, 7:]\\n        self._data.body_acc_w[env_ids] = 0.0\\n        # set into simulation\\n        self.root_physx_view.set_root_velocities(self._data.root_com_state_w[:, 7:], indices=physx_env_ids)\\n\\n    def write_root_link_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root link velocity over selected environment indices into the simulation.\\n\\n        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.\\n        NOTE: This sets the velocity of the root\\'s frame rather than the roots center of mass.\\n\\n        Args:\\n            root_velocity: Root frame velocities in simulation world frame. Shape is (len(env_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n\\n        root_com_velocity = root_velocity.clone()\\n        quat = self.data.root_link_state_w[env_ids, 3:7]\\n        com_pos_b = self.data.com_pos_b[env_ids, 0, :]\\n        # transform given velocity to center of mass\\n        root_com_velocity[:, :3] += torch.linalg.cross(\\n            root_com_velocity[:, 3:], math_utils.quat_apply(quat, com_pos_b), dim=-1\\n        )\\n        # write center of mass velocity to sim\\n        self.write_root_com_velocity_to_sim(root_velocity=root_com_velocity, env_ids=physx_env_ids)\\n\\n    def write_joint_state_to_sim(\\n        self,\\n        position: torch.Tensor,\\n        velocity: torch.Tensor,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | slice | None = None,\\n    ):\\n        \"\"\"Write joint positions and velocities to the simulation.\\n\\n        Args:\\n            position: Joint positions. Shape is (len(env_ids), len(joint_ids)).\\n            velocity: Joint velocities. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the targets for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the targets for. Defaults to None (all environments).\\n        \"\"\"\\n        # set into simulation\\n        self.write_joint_position_to_sim(position, joint_ids=joint_ids, env_ids=env_ids)\\n        self.write_joint_velocity_to_sim(velocity, joint_ids=joint_ids, env_ids=env_ids)\\n\\n    def write_joint_position_to_sim(\\n        self,\\n        position: torch.Tensor,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | slice | None = None,\\n    ):\\n        \"\"\"Write joint positions to the simulation.\\n\\n        Args:\\n            position: Joint positions. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the targets for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the targets for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._data.joint_pos[env_ids, joint_ids] = position\\n        # Need to invalidate the buffer to trigger the update with the new root pose.\\n        self._data._body_state_w.timestamp = -1.0\\n        self._data._body_link_state_w.timestamp = -1.0\\n        self._data._body_com_state_w.timestamp = -1.0\\n        # set into simulation\\n        self.root_physx_view.set_dof_positions(self._data.joint_pos, indices=physx_env_ids)\\n\\n    def write_joint_velocity_to_sim(\\n        self,\\n        velocity: torch.Tensor,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | slice | None = None,\\n    ):\\n        \"\"\"Write joint velocities to the simulation.\\n\\n        Args:\\n            velocity: Joint velocities. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the targets for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the targets for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._data.joint_vel[env_ids, joint_ids] = velocity\\n        self._data._previous_joint_vel[env_ids, joint_ids] = velocity\\n        self._data.joint_acc[env_ids, joint_ids] = 0.0\\n        # set into simulation\\n        self.root_physx_view.set_dof_velocities(self._data.joint_vel, indices=physx_env_ids)\\n\\n    \"\"\"\\n    Operations - Simulation Parameters Writers.\\n    \"\"\"\\n\\n    def write_joint_stiffness_to_sim(\\n        self,\\n        stiffness: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Write joint stiffness into the simulation.\\n\\n        Args:\\n            stiffness: Joint stiffness. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the stiffness for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the stiffness for. Defaults to None (all environments).\\n        \"\"\"\\n        # note: This function isn\\'t setting the values for actuator models. (#128)\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._data.joint_stiffness[env_ids, joint_ids] = stiffness\\n        # set into simulation\\n        self.root_physx_view.set_dof_stiffnesses(self._data.joint_stiffness.cpu(), indices=physx_env_ids.cpu())\\n\\n    def write_joint_damping_to_sim(\\n        self,\\n        damping: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Write joint damping into the simulation.\\n\\n        Args:\\n            damping: Joint damping. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the damping for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the damping for. Defaults to None (all environments).\\n        \"\"\"\\n        # note: This function isn\\'t setting the values for actuator models. (#128)\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._data.joint_damping[env_ids, joint_ids] = damping\\n        # set into simulation\\n        self.root_physx_view.set_dof_dampings(self._data.joint_damping.cpu(), indices=physx_env_ids.cpu())\\n\\n    def write_joint_position_limit_to_sim(\\n        self,\\n        limits: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n        warn_limit_violation: bool = True,\\n    ):\\n        \"\"\"Write joint position limits into the simulation.\\n\\n        Args:\\n            limits: Joint limits. Shape is (len(env_ids), len(joint_ids), 2).\\n            joint_ids: The joint indices to set the limits for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the limits for. Defaults to None (all environments).\\n            warn_limit_violation: Whether to use warning or info level logging when default joint positions\\n                exceed the new limits. Defaults to True.\\n        \"\"\"\\n        # note: This function isn\\'t setting the values for actuator models. (#128)\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._data.joint_pos_limits[env_ids, joint_ids] = limits\\n        # update default joint pos to stay within the new limits\\n        if torch.any(\\n            (self._data.default_joint_pos[env_ids, joint_ids] < limits[..., 0])\\n            | (self._data.default_joint_pos[env_ids, joint_ids] > limits[..., 1])\\n        ):\\n            self._data.default_joint_pos[env_ids, joint_ids] = torch.clamp(\\n                self._data.default_joint_pos[env_ids, joint_ids], limits[..., 0], limits[..., 1]\\n            )\\n            violation_message = (\\n                \"Some default joint positions are outside of the range of the new joint limits. Default joint positions\"\\n                \" will be clamped to be within the new joint limits.\"\\n            )\\n            if warn_limit_violation:\\n                # warn level will show in console\\n                omni.log.warn(violation_message)\\n            else:\\n                # info level is only written to log file\\n                omni.log.info(violation_message)\\n        # set into simulation\\n        self.root_physx_view.set_dof_limits(self._data.joint_pos_limits.cpu(), indices=physx_env_ids.cpu())\\n\\n        # compute the soft limits based on the joint limits\\n        # TODO: Optimize this computation for only selected joints\\n        # soft joint position limits (recommended not to be too close to limits).\\n        joint_pos_mean = (self._data.joint_pos_limits[..., 0] + self._data.joint_pos_limits[..., 1]) / 2\\n        joint_pos_range = self._data.joint_pos_limits[..., 1] - self._data.joint_pos_limits[..., 0]\\n        soft_limit_factor = self.cfg.soft_joint_pos_limit_factor\\n        # add to data\\n        self._data.soft_joint_pos_limits[..., 0] = joint_pos_mean - 0.5 * joint_pos_range * soft_limit_factor\\n        self._data.soft_joint_pos_limits[..., 1] = joint_pos_mean + 0.5 * joint_pos_range * soft_limit_factor\\n\\n    def write_joint_velocity_limit_to_sim(\\n        self,\\n        limits: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Write joint max velocity to the simulation.\\n\\n        The velocity limit is used to constrain the joint velocities in the physics engine. The joint will only\\n        be able to reach this velocity if the joint\\'s effort limit is sufficiently large. If the joint is moving\\n        faster than this velocity, the physics engine will actually try to brake the joint to reach this velocity.\\n\\n        Args:\\n            limits: Joint max velocity. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the max velocity for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the max velocity for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # move tensor to cpu if needed\\n        if isinstance(limits, torch.Tensor):\\n            limits = limits.to(self.device)\\n        # set into internal buffers\\n        self._data.joint_vel_limits[env_ids, joint_ids] = limits\\n        # set into simulation\\n        self.root_physx_view.set_dof_max_velocities(self._data.joint_vel_limits.cpu(), indices=physx_env_ids.cpu())\\n\\n    def write_joint_effort_limit_to_sim(\\n        self,\\n        limits: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Write joint effort limits into the simulation.\\n\\n        The effort limit is used to constrain the computed joint efforts in the physics engine. If the\\n        computed effort exceeds this limit, the physics engine will clip the effort to this value.\\n\\n        Args:\\n            limits: Joint torque limits. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the joint torque limits for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the joint torque limits for. Defaults to None (all environments).\\n        \"\"\"\\n        # note: This function isn\\'t setting the values for actuator models. (#128)\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # move tensor to cpu if needed\\n        if isinstance(limits, torch.Tensor):\\n            limits = limits.to(self.device)\\n        # set into internal buffers\\n        self._data.joint_effort_limits[env_ids, joint_ids] = limits\\n        # set into simulation\\n        self.root_physx_view.set_dof_max_forces(self._data.joint_effort_limits.cpu(), indices=physx_env_ids.cpu())\\n\\n    def write_joint_armature_to_sim(\\n        self,\\n        armature: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Write joint armature into the simulation.\\n\\n        The armature is directly added to the corresponding joint-space inertia. It helps improve the\\n        simulation stability by reducing the joint velocities.\\n\\n        Args:\\n            armature: Joint armature. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the joint torque limits for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the joint torque limits for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._data.joint_armature[env_ids, joint_ids] = armature\\n        # set into simulation\\n        self.root_physx_view.set_dof_armatures(self._data.joint_armature.cpu(), indices=physx_env_ids.cpu())\\n\\n    def write_joint_friction_coefficient_to_sim(\\n        self,\\n        joint_friction_coeff: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        r\"\"\"Write joint friction coefficients into the simulation.\\n\\n        The joint friction is a unitless quantity. It relates the magnitude of the spatial force transmitted\\n        from the parent body to the child body to the maximal friction force that may be applied by the solver\\n        to resist the joint motion.\\n\\n        Mathematically, this means that: :math:`F_{resist} \\\\leq \\\\mu F_{spatial}`, where :math:`F_{resist}`\\n        is the resisting force applied by the solver and :math:`F_{spatial}` is the spatial force\\n        transmitted from the parent body to the child body. The simulated friction effect is therefore\\n        similar to static and Coulomb friction.\\n\\n        Args:\\n            joint_friction: Joint friction. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the joint torque limits for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the joint torque limits for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._data.joint_friction_coeff[env_ids, joint_ids] = joint_friction_coeff\\n        # set into simulation\\n        self.root_physx_view.set_dof_friction_coefficients(\\n            self._data.joint_friction_coeff.cpu(), indices=physx_env_ids.cpu()\\n        )\\n\\n    \"\"\"\\n    Operations - Setters.\\n    \"\"\"\\n\\n    def set_external_force_and_torque(\\n        self,\\n        forces: torch.Tensor,\\n        torques: torch.Tensor,\\n        body_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set external force and torque to apply on the asset\\'s bodies in their local frame.\\n\\n        For many applications, we want to keep the applied external force on rigid bodies constant over a period of\\n        time (for instance, during the policy control). This function allows us to store the external force and torque\\n        into buffers which are then applied to the simulation at every step.\\n\\n        .. caution::\\n            If the function is called with empty forces and torques, then this function disables the application\\n            of external wrench to the simulation.\\n\\n            .. code-block:: python\\n\\n                # example of disabling external wrench\\n                asset.set_external_force_and_torque(forces=torch.zeros(0, 3), torques=torch.zeros(0, 3))\\n\\n        .. note::\\n            This function does not apply the external wrench to the simulation. It only fills the buffers with\\n            the desired values. To apply the external wrench, call the :meth:`write_data_to_sim` function\\n            right before the simulation step.\\n\\n        Args:\\n            forces: External forces in bodies\\' local frame. Shape is (len(env_ids), len(body_ids), 3).\\n            torques: External torques in bodies\\' local frame. Shape is (len(env_ids), len(body_ids), 3).\\n            body_ids: Body indices to apply external wrench to. Defaults to None (all bodies).\\n            env_ids: Environment indices to apply external wrench to. Defaults to None (all instances).\\n        \"\"\"\\n        if forces.any() or torques.any():\\n            self.has_external_wrench = True\\n        else:\\n            self.has_external_wrench = False\\n\\n        # resolve all indices\\n        # -- env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_INDICES\\n        elif not isinstance(env_ids, torch.Tensor):\\n            env_ids = torch.tensor(env_ids, dtype=torch.long, device=self.device)\\n        # -- body_ids\\n        if body_ids is None:\\n            body_ids = torch.arange(self.num_bodies, dtype=torch.long, device=self.device)\\n        elif isinstance(body_ids, slice):\\n            body_ids = torch.arange(self.num_bodies, dtype=torch.long, device=self.device)[body_ids]\\n        elif not isinstance(body_ids, torch.Tensor):\\n            body_ids = torch.tensor(body_ids, dtype=torch.long, device=self.device)\\n\\n        # note: we need to do this complicated indexing since torch doesn\\'t support multi-indexing\\n        # create global body indices from env_ids and env_body_ids\\n        # (env_id * total_bodies_per_env) + body_id\\n        indices = body_ids.repeat(len(env_ids), 1) + env_ids.unsqueeze(1) * self.num_bodies\\n        indices = indices.view(-1)\\n        # set into internal buffers\\n        # note: these are applied in the write_to_sim function\\n        self._external_force_b.flatten(0, 1)[indices] = forces.flatten(0, 1)\\n        self._external_torque_b.flatten(0, 1)[indices] = torques.flatten(0, 1)\\n\\n    def set_joint_position_target(\\n        self, target: torch.Tensor, joint_ids: Sequence[int] | slice | None = None, env_ids: Sequence[int] | None = None\\n    ):\\n        \"\"\"Set joint position targets into internal buffers.\\n\\n        This function does not apply the joint targets to the simulation. It only fills the buffers with\\n        the desired values. To apply the joint targets, call the :meth:`write_data_to_sim` function.\\n\\n        Args:\\n            target: Joint position targets. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the targets for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the targets for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set targets\\n        self._data.joint_pos_target[env_ids, joint_ids] = target\\n\\n    def set_joint_velocity_target(\\n        self, target: torch.Tensor, joint_ids: Sequence[int] | slice | None = None, env_ids: Sequence[int] | None = None\\n    ):\\n        \"\"\"Set joint velocity targets into internal buffers.\\n\\n        This function does not apply the joint targets to the simulation. It only fills the buffers with\\n        the desired values. To apply the joint targets, call the :meth:`write_data_to_sim` function.\\n\\n        Args:\\n            target: Joint velocity targets. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the targets for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the targets for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set targets\\n        self._data.joint_vel_target[env_ids, joint_ids] = target\\n\\n    def set_joint_effort_target(\\n        self, target: torch.Tensor, joint_ids: Sequence[int] | slice | None = None, env_ids: Sequence[int] | None = None\\n    ):\\n        \"\"\"Set joint efforts into internal buffers.\\n\\n        This function does not apply the joint targets to the simulation. It only fills the buffers with\\n        the desired values. To apply the joint targets, call the :meth:`write_data_to_sim` function.\\n\\n        Args:\\n            target: Joint effort targets. Shape is (len(env_ids), len(joint_ids)).\\n            joint_ids: The joint indices to set the targets for. Defaults to None (all joints).\\n            env_ids: The environment indices to set the targets for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if joint_ids is None:\\n            joint_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and joint_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set targets\\n        self._data.joint_effort_target[env_ids, joint_ids] = target\\n\\n    \"\"\"\\n    Operations - Tendons.\\n    \"\"\"\\n\\n    def set_fixed_tendon_stiffness(\\n        self,\\n        stiffness: torch.Tensor,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set fixed tendon stiffness into internal buffers.\\n\\n        This function does not apply the tendon stiffness to the simulation. It only fills the buffers with\\n        the desired values. To apply the tendon stiffness, call the :meth:`write_fixed_tendon_properties_to_sim` function.\\n\\n        Args:\\n            stiffness: Fixed tendon stiffness. Shape is (len(env_ids), len(fixed_tendon_ids)).\\n            fixed_tendon_ids: The tendon indices to set the stiffness for. Defaults to None (all fixed tendons).\\n            env_ids: The environment indices to set the stiffness for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if fixed_tendon_ids is None:\\n            fixed_tendon_ids = slice(None)\\n        if env_ids != slice(None) and fixed_tendon_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set stiffness\\n        self._data.fixed_tendon_stiffness[env_ids, fixed_tendon_ids] = stiffness\\n\\n    def set_fixed_tendon_damping(\\n        self,\\n        damping: torch.Tensor,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set fixed tendon damping into internal buffers.\\n\\n        This function does not apply the tendon damping to the simulation. It only fills the buffers with\\n        the desired values. To apply the tendon damping, call the :meth:`write_fixed_tendon_properties_to_sim` function.\\n\\n        Args:\\n            damping: Fixed tendon damping. Shape is (len(env_ids), len(fixed_tendon_ids)).\\n            fixed_tendon_ids: The tendon indices to set the damping for. Defaults to None (all fixed tendons).\\n            env_ids: The environment indices to set the damping for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if fixed_tendon_ids is None:\\n            fixed_tendon_ids = slice(None)\\n        if env_ids != slice(None) and fixed_tendon_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set damping\\n        self._data.fixed_tendon_damping[env_ids, fixed_tendon_ids] = damping\\n\\n    def set_fixed_tendon_limit_stiffness(\\n        self,\\n        limit_stiffness: torch.Tensor,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set fixed tendon limit stiffness efforts into internal buffers.\\n\\n        This function does not apply the tendon limit stiffness to the simulation. It only fills the buffers with\\n        the desired values. To apply the tendon limit stiffness, call the :meth:`write_fixed_tendon_properties_to_sim` function.\\n\\n        Args:\\n            limit_stiffness: Fixed tendon limit stiffness. Shape is (len(env_ids), len(fixed_tendon_ids)).\\n            fixed_tendon_ids: The tendon indices to set the limit stiffness for. Defaults to None (all fixed tendons).\\n            env_ids: The environment indices to set the limit stiffness for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if fixed_tendon_ids is None:\\n            fixed_tendon_ids = slice(None)\\n        if env_ids != slice(None) and fixed_tendon_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set limit_stiffness\\n        self._data.fixed_tendon_limit_stiffness[env_ids, fixed_tendon_ids] = limit_stiffness\\n\\n    def set_fixed_tendon_position_limit(\\n        self,\\n        limit: torch.Tensor,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set fixed tendon limit efforts into internal buffers.\\n\\n        This function does not apply the tendon limit to the simulation. It only fills the buffers with\\n        the desired values. To apply the tendon limit, call the :meth:`write_fixed_tendon_properties_to_sim` function.\\n\\n         Args:\\n             limit: Fixed tendon limit. Shape is (len(env_ids), len(fixed_tendon_ids)).\\n             fixed_tendon_ids: The tendon indices to set the limit for. Defaults to None (all fixed tendons).\\n             env_ids: The environment indices to set the limit for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if fixed_tendon_ids is None:\\n            fixed_tendon_ids = slice(None)\\n        if env_ids != slice(None) and fixed_tendon_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set limit\\n        self._data.fixed_tendon_pos_limits[env_ids, fixed_tendon_ids] = limit\\n\\n    def set_fixed_tendon_rest_length(\\n        self,\\n        rest_length: torch.Tensor,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set fixed tendon rest length efforts into internal buffers.\\n\\n        This function does not apply the tendon rest length to the simulation. It only fills the buffers with\\n        the desired values. To apply the tendon rest length, call the :meth:`write_fixed_tendon_properties_to_sim` function.\\n\\n        Args:\\n            rest_length: Fixed tendon rest length. Shape is (len(env_ids), len(fixed_tendon_ids)).\\n            fixed_tendon_ids: The tendon indices to set the rest length for. Defaults to None (all fixed tendons).\\n            env_ids: The environment indices to set the rest length for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if fixed_tendon_ids is None:\\n            fixed_tendon_ids = slice(None)\\n        if env_ids != slice(None) and fixed_tendon_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set rest_length\\n        self._data.fixed_tendon_rest_length[env_ids, fixed_tendon_ids] = rest_length\\n\\n    def set_fixed_tendon_offset(\\n        self,\\n        offset: torch.Tensor,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set fixed tendon offset efforts into internal buffers.\\n\\n        This function does not apply the tendon offset to the simulation. It only fills the buffers with\\n        the desired values. To apply the tendon offset, call the :meth:`write_fixed_tendon_properties_to_sim` function.\\n\\n        Args:\\n            offset: Fixed tendon offset. Shape is (len(env_ids), len(fixed_tendon_ids)).\\n            fixed_tendon_ids: The tendon indices to set the offset for. Defaults to None (all fixed tendons).\\n            env_ids: The environment indices to set the offset for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        if fixed_tendon_ids is None:\\n            fixed_tendon_ids = slice(None)\\n        if env_ids != slice(None) and fixed_tendon_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set offset\\n        self._data.fixed_tendon_offset[env_ids, fixed_tendon_ids] = offset\\n\\n    def write_fixed_tendon_properties_to_sim(\\n        self,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Write fixed tendon properties into the simulation.\\n\\n        Args:\\n            fixed_tendon_ids: The fixed tendon indices to set the limits for. Defaults to None (all fixed tendons).\\n            env_ids: The environment indices to set the limits for. Defaults to None (all environments).\\n        \"\"\"\\n        # resolve indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            physx_env_ids = self._ALL_INDICES\\n        if fixed_tendon_ids is None:\\n            fixed_tendon_ids = slice(None)\\n\\n        # set into simulation\\n        self.root_physx_view.set_fixed_tendon_properties(\\n            self._data.fixed_tendon_stiffness,\\n            self._data.fixed_tendon_damping,\\n            self._data.fixed_tendon_limit_stiffness,\\n            self._data.fixed_tendon_pos_limits,\\n            self._data.fixed_tendon_rest_length,\\n            self._data.fixed_tendon_offset,\\n            indices=physx_env_ids,\\n        )\\n\\n    \"\"\"\\n    Internal helper.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n\\n        if self.cfg.articulation_root_prim_path is not None:\\n            # The articulation root prim path is specified explicitly, so we can just use this.\\n            root_prim_path_expr = self.cfg.prim_path + self.cfg.articulation_root_prim_path\\n        else:\\n            # No articulation root prim path was specified, so we need to search\\n            # for it. We search for this in the first environment and then\\n            # create a regex that matches all environments.\\n            first_env_matching_prim = sim_utils.find_first_matching_prim(self.cfg.prim_path)\\n            if first_env_matching_prim is None:\\n                raise RuntimeError(f\"Failed to find prim for expression: \\'{self.cfg.prim_path}\\'.\")\\n            first_env_matching_prim_path = first_env_matching_prim.GetPath().pathString\\n\\n            # Find all articulation root prims in the first environment.\\n            first_env_root_prims = sim_utils.get_all_matching_child_prims(\\n                first_env_matching_prim_path,\\n                predicate=lambda prim: prim.HasAPI(UsdPhysics.ArticulationRootAPI),\\n            )\\n            if len(first_env_root_prims) == 0:\\n                raise RuntimeError(\\n                    f\"Failed to find an articulation when resolving \\'{first_env_matching_prim_path}\\'.\"\\n                    \" Please ensure that the prim has \\'USD ArticulationRootAPI\\' applied.\"\\n                )\\n            if len(first_env_root_prims) > 1:\\n                raise RuntimeError(\\n                    f\"Failed to find a single articulation when resolving \\'{first_env_matching_prim_path}\\'.\"\\n                    f\" Found multiple \\'{first_env_root_prims}\\' under \\'{first_env_matching_prim_path}\\'.\"\\n                    \" Please ensure that there is only one articulation in the prim path tree.\"\\n                )\\n\\n            # Now we convert the found articulation root from the first\\n            # environment back into a regex that matches all environments.\\n            first_env_root_prim_path = first_env_root_prims[0].GetPath().pathString\\n            root_prim_path_relative_to_prim_path = first_env_root_prim_path[len(first_env_matching_prim_path) :]\\n            root_prim_path_expr = self.cfg.prim_path + root_prim_path_relative_to_prim_path\\n\\n        # -- articulation\\n        self._root_physx_view = self._physics_sim_view.create_articulation_view(root_prim_path_expr.replace(\".*\", \"*\"))\\n\\n        # check if the articulation was created\\n        if self._root_physx_view._backend is None:\\n            raise RuntimeError(f\"Failed to create articulation at: {root_prim_path_expr}. Please check PhysX logs.\")\\n\\n        # log information about the articulation\\n        omni.log.info(f\"Articulation initialized at: {self.cfg.prim_path} with root \\'{root_prim_path_expr}\\'.\")\\n        omni.log.info(f\"Is fixed root: {self.is_fixed_base}\")\\n        omni.log.info(f\"Number of bodies: {self.num_bodies}\")\\n        omni.log.info(f\"Body names: {self.body_names}\")\\n        omni.log.info(f\"Number of joints: {self.num_joints}\")\\n        omni.log.info(f\"Joint names: {self.joint_names}\")\\n        omni.log.info(f\"Number of fixed tendons: {self.num_fixed_tendons}\")\\n\\n        # container for data access\\n        self._data = ArticulationData(self.root_physx_view, self.device)\\n\\n        # create buffers\\n        self._create_buffers()\\n        # process configuration\\n        self._process_cfg()\\n        self._process_actuators_cfg()\\n        self._process_fixed_tendons()\\n        # validate configuration\\n        self._validate_cfg()\\n        # update the robot data\\n        self.update(0.0)\\n        # log joint information\\n        self._log_articulation_info()\\n\\n    def _create_buffers(self):\\n        # constants\\n        self._ALL_INDICES = torch.arange(self.num_instances, dtype=torch.long, device=self.device)\\n\\n        # external forces and torques\\n        self.has_external_wrench = False\\n        self._external_force_b = torch.zeros((self.num_instances, self.num_bodies, 3), device=self.device)\\n        self._external_torque_b = torch.zeros_like(self._external_force_b)\\n\\n        # asset named data\\n        self._data.joint_names = self.joint_names\\n        self._data.body_names = self.body_names\\n        # tendon names are set in _process_fixed_tendons function\\n\\n        # -- joint properties\\n        self._data.default_joint_pos_limits = self.root_physx_view.get_dof_limits().to(self.device).clone()\\n        self._data.default_joint_stiffness = self.root_physx_view.get_dof_stiffnesses().to(self.device).clone()\\n        self._data.default_joint_damping = self.root_physx_view.get_dof_dampings().to(self.device).clone()\\n        self._data.default_joint_armature = self.root_physx_view.get_dof_armatures().to(self.device).clone()\\n        self._data.default_joint_friction_coeff = (\\n            self.root_physx_view.get_dof_friction_coefficients().to(self.device).clone()\\n        )\\n\\n        self._data.joint_pos_limits = self._data.default_joint_pos_limits.clone()\\n        self._data.joint_vel_limits = self.root_physx_view.get_dof_max_velocities().to(self.device).clone()\\n        self._data.joint_effort_limits = self.root_physx_view.get_dof_max_forces().to(self.device).clone()\\n        self._data.joint_stiffness = self._data.default_joint_stiffness.clone()\\n        self._data.joint_damping = self._data.default_joint_damping.clone()\\n        self._data.joint_armature = self._data.default_joint_armature.clone()\\n        self._data.joint_friction_coeff = self._data.default_joint_friction_coeff.clone()\\n\\n        # -- body properties\\n        self._data.default_mass = self.root_physx_view.get_masses().clone()\\n        self._data.default_inertia = self.root_physx_view.get_inertias().clone()\\n\\n        # -- joint commands (sent to the actuator from the user)\\n        self._data.joint_pos_target = torch.zeros(self.num_instances, self.num_joints, device=self.device)\\n        self._data.joint_vel_target = torch.zeros_like(self._data.joint_pos_target)\\n        self._data.joint_effort_target = torch.zeros_like(self._data.joint_pos_target)\\n        # -- joint commands (sent to the simulation after actuator processing)\\n        self._joint_pos_target_sim = torch.zeros_like(self._data.joint_pos_target)\\n        self._joint_vel_target_sim = torch.zeros_like(self._data.joint_pos_target)\\n        self._joint_effort_target_sim = torch.zeros_like(self._data.joint_pos_target)\\n\\n        # -- computed joint efforts from the actuator models\\n        self._data.computed_torque = torch.zeros_like(self._data.joint_pos_target)\\n        self._data.applied_torque = torch.zeros_like(self._data.joint_pos_target)\\n\\n        # -- other data that are filled based on explicit actuator models\\n        self._data.soft_joint_vel_limits = torch.zeros(self.num_instances, self.num_joints, device=self.device)\\n        self._data.gear_ratio = torch.ones(self.num_instances, self.num_joints, device=self.device)\\n\\n        # soft joint position limits (recommended not to be too close to limits).\\n        joint_pos_mean = (self._data.joint_pos_limits[..., 0] + self._data.joint_pos_limits[..., 1]) / 2\\n        joint_pos_range = self._data.joint_pos_limits[..., 1] - self._data.joint_pos_limits[..., 0]\\n        soft_limit_factor = self.cfg.soft_joint_pos_limit_factor\\n        # add to data\\n        self._data.soft_joint_pos_limits = torch.zeros(self.num_instances, self.num_joints, 2, device=self.device)\\n        self._data.soft_joint_pos_limits[..., 0] = joint_pos_mean - 0.5 * joint_pos_range * soft_limit_factor\\n        self._data.soft_joint_pos_limits[..., 1] = joint_pos_mean + 0.5 * joint_pos_range * soft_limit_factor\\n\\n    def _process_cfg(self):\\n        \"\"\"Post processing of configuration parameters.\"\"\"\\n        # default state\\n        # -- root state\\n        # note: we cast to tuple to avoid torch/numpy type mismatch.\\n        default_root_state = (\\n            tuple(self.cfg.init_state.pos)\\n            + tuple(self.cfg.init_state.rot)\\n            + tuple(self.cfg.init_state.lin_vel)\\n            + tuple(self.cfg.init_state.ang_vel)\\n        )\\n        default_root_state = torch.tensor(default_root_state, dtype=torch.float, device=self.device)\\n        self._data.default_root_state = default_root_state.repeat(self.num_instances, 1)\\n\\n        # -- joint state\\n        self._data.default_joint_pos = torch.zeros(self.num_instances, self.num_joints, device=self.device)\\n        self._data.default_joint_vel = torch.zeros_like(self._data.default_joint_pos)\\n        # joint pos\\n        indices_list, _, values_list = string_utils.resolve_matching_names_values(\\n            self.cfg.init_state.joint_pos, self.joint_names\\n        )\\n        self._data.default_joint_pos[:, indices_list] = torch.tensor(values_list, device=self.device)\\n        # joint vel\\n        indices_list, _, values_list = string_utils.resolve_matching_names_values(\\n            self.cfg.init_state.joint_vel, self.joint_names\\n        )\\n        self._data.default_joint_vel[:, indices_list] = torch.tensor(values_list, device=self.device)\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        self._root_physx_view = None\\n\\n    \"\"\"\\n    Internal helpers -- Actuators.\\n    \"\"\"\\n\\n    def _process_actuators_cfg(self):\\n        \"\"\"Process and apply articulation joint properties.\"\"\"\\n        # create actuators\\n        self.actuators = dict()\\n        # flag for implicit actuators\\n        # if this is false, we by-pass certain checks when doing actuator-related operations\\n        self._has_implicit_actuators = False\\n\\n        # iterate over all actuator configurations\\n        for actuator_name, actuator_cfg in self.cfg.actuators.items():\\n            # type annotation for type checkers\\n            actuator_cfg: ActuatorBaseCfg\\n            # create actuator group\\n            joint_ids, joint_names = self.find_joints(actuator_cfg.joint_names_expr)\\n            # check if any joints are found\\n            if len(joint_names) == 0:\\n                raise ValueError(\\n                    f\"No joints found for actuator group: {actuator_name} with joint name expression:\"\\n                    f\" {actuator_cfg.joint_names_expr}.\"\\n                )\\n            # resolve joint indices\\n            # we pass a slice if all joints are selected to avoid indexing overhead\\n            if len(joint_names) == self.num_joints:\\n                joint_ids = slice(None)\\n            else:\\n                joint_ids = torch.tensor(joint_ids, device=self.device)\\n            # create actuator collection\\n            # note: for efficiency avoid indexing when over all indices\\n            actuator: ActuatorBase = actuator_cfg.class_type(\\n                cfg=actuator_cfg,\\n                joint_names=joint_names,\\n                joint_ids=joint_ids,\\n                num_envs=self.num_instances,\\n                device=self.device,\\n                stiffness=self._data.default_joint_stiffness[:, joint_ids],\\n                damping=self._data.default_joint_damping[:, joint_ids],\\n                armature=self._data.default_joint_armature[:, joint_ids],\\n                friction=self._data.default_joint_friction_coeff[:, joint_ids],\\n                effort_limit=self._data.joint_effort_limits[:, joint_ids],\\n                velocity_limit=self._data.joint_vel_limits[:, joint_ids],\\n            )\\n            # log information on actuator groups\\n            model_type = \"implicit\" if actuator.is_implicit_model else \"explicit\"\\n            omni.log.info(\\n                f\"Actuator collection: {actuator_name} with model \\'{actuator_cfg.class_type.__name__}\\'\"\\n                f\" (type: {model_type}) and joint names: {joint_names} [{joint_ids}].\"\\n            )\\n            # store actuator group\\n            self.actuators[actuator_name] = actuator\\n            # set the passed gains and limits into the simulation\\n            if isinstance(actuator, ImplicitActuator):\\n                self._has_implicit_actuators = True\\n                # the gains and limits are set into the simulation since actuator model is implicit\\n                self.write_joint_stiffness_to_sim(actuator.stiffness, joint_ids=actuator.joint_indices)\\n                self.write_joint_damping_to_sim(actuator.damping, joint_ids=actuator.joint_indices)\\n            else:\\n                # the gains and limits are processed by the actuator model\\n                # we set gains to zero, and torque limit to a high value in simulation to avoid any interference\\n                self.write_joint_stiffness_to_sim(0.0, joint_ids=actuator.joint_indices)\\n                self.write_joint_damping_to_sim(0.0, joint_ids=actuator.joint_indices)\\n\\n            # Set common properties into the simulation\\n            self.write_joint_effort_limit_to_sim(actuator.effort_limit_sim, joint_ids=actuator.joint_indices)\\n            self.write_joint_velocity_limit_to_sim(actuator.velocity_limit_sim, joint_ids=actuator.joint_indices)\\n            self.write_joint_armature_to_sim(actuator.armature, joint_ids=actuator.joint_indices)\\n            self.write_joint_friction_coefficient_to_sim(actuator.friction, joint_ids=actuator.joint_indices)\\n\\n            # Store the configured values from the actuator model\\n            # note: this is the value configured in the actuator model (for implicit and explicit actuators)\\n            self._data.default_joint_stiffness[:, actuator.joint_indices] = actuator.stiffness\\n            self._data.default_joint_damping[:, actuator.joint_indices] = actuator.damping\\n            self._data.default_joint_armature[:, actuator.joint_indices] = actuator.armature\\n            self._data.default_joint_friction_coeff[:, actuator.joint_indices] = actuator.friction\\n\\n        # perform some sanity checks to ensure actuators are prepared correctly\\n        total_act_joints = sum(actuator.num_joints for actuator in self.actuators.values())\\n        if total_act_joints != (self.num_joints - self.num_fixed_tendons):\\n            omni.log.warn(\\n                \"Not all actuators are configured! Total number of actuated joints not equal to number of\"\\n                f\" joints available: {total_act_joints} != {self.num_joints - self.num_fixed_tendons}.\"\\n            )\\n\\n    def _process_fixed_tendons(self):\\n        \"\"\"Process fixed tendons.\"\"\"\\n        # create a list to store the fixed tendon names\\n        self._fixed_tendon_names = list()\\n\\n        # parse fixed tendons properties if they exist\\n        if self.num_fixed_tendons > 0:\\n            stage = stage_utils.get_current_stage()\\n            joint_paths = self.root_physx_view.dof_paths[0]\\n\\n            # iterate over all joints to find tendons attached to them\\n            for j in range(self.num_joints):\\n                usd_joint_path = joint_paths[j]\\n                # check whether joint has tendons - tendon name follows the joint name it is attached to\\n                joint = UsdPhysics.Joint.Get(stage, usd_joint_path)\\n                if joint.GetPrim().HasAPI(PhysxSchema.PhysxTendonAxisRootAPI):\\n                    joint_name = usd_joint_path.split(\"/\")[-1]\\n                    self._fixed_tendon_names.append(joint_name)\\n\\n            # store the fixed tendon names\\n            self._data.fixed_tendon_names = self._fixed_tendon_names\\n\\n            # store the current USD fixed tendon properties\\n            self._data.default_fixed_tendon_stiffness = self.root_physx_view.get_fixed_tendon_stiffnesses().clone()\\n            self._data.default_fixed_tendon_damping = self.root_physx_view.get_fixed_tendon_dampings().clone()\\n            self._data.default_fixed_tendon_limit_stiffness = (\\n                self.root_physx_view.get_fixed_tendon_limit_stiffnesses().clone()\\n            )\\n            self._data.default_fixed_tendon_pos_limits = self.root_physx_view.get_fixed_tendon_limits().clone()\\n            self._data.default_fixed_tendon_rest_length = self.root_physx_view.get_fixed_tendon_rest_lengths().clone()\\n            self._data.default_fixed_tendon_offset = self.root_physx_view.get_fixed_tendon_offsets().clone()\\n\\n            # store a copy of the default values for the fixed tendons\\n            self._data.fixed_tendon_stiffness = self._data.default_fixed_tendon_stiffness.clone()\\n            self._data.fixed_tendon_damping = self._data.default_fixed_tendon_damping.clone()\\n            self._data.fixed_tendon_limit_stiffness = self._data.default_fixed_tendon_limit_stiffness.clone()\\n            self._data.fixed_tendon_pos_limits = self._data.default_fixed_tendon_pos_limits.clone()\\n            self._data.fixed_tendon_rest_length = self._data.default_fixed_tendon_rest_length.clone()\\n            self._data.fixed_tendon_offset = self._data.default_fixed_tendon_offset.clone()\\n\\n    def _apply_actuator_model(self):\\n        \"\"\"Processes joint commands for the articulation by forwarding them to the actuators.\\n\\n        The actions are first processed using actuator models. Depending on the robot configuration,\\n        the actuator models compute the joint level simulation commands and sets them into the PhysX buffers.\\n        \"\"\"\\n        # process actions per group\\n        for actuator in self.actuators.values():\\n            # prepare input for actuator model based on cached data\\n            # TODO : A tensor dict would be nice to do the indexing of all tensors together\\n            control_action = ArticulationActions(\\n                joint_positions=self._data.joint_pos_target[:, actuator.joint_indices],\\n                joint_velocities=self._data.joint_vel_target[:, actuator.joint_indices],\\n                joint_efforts=self._data.joint_effort_target[:, actuator.joint_indices],\\n                joint_indices=actuator.joint_indices,\\n            )\\n            # compute joint command from the actuator model\\n            control_action = actuator.compute(\\n                control_action,\\n                joint_pos=self._data.joint_pos[:, actuator.joint_indices],\\n                joint_vel=self._data.joint_vel[:, actuator.joint_indices],\\n            )\\n            # update targets (these are set into the simulation)\\n            if control_action.joint_positions is not None:\\n                self._joint_pos_target_sim[:, actuator.joint_indices] = control_action.joint_positions\\n            if control_action.joint_velocities is not None:\\n                self._joint_vel_target_sim[:, actuator.joint_indices] = control_action.joint_velocities\\n            if control_action.joint_efforts is not None:\\n                self._joint_effort_target_sim[:, actuator.joint_indices] = control_action.joint_efforts\\n            # update state of the actuator model\\n            # -- torques\\n            self._data.computed_torque[:, actuator.joint_indices] = actuator.computed_effort\\n            self._data.applied_torque[:, actuator.joint_indices] = actuator.applied_effort\\n            # -- actuator data\\n            self._data.soft_joint_vel_limits[:, actuator.joint_indices] = actuator.velocity_limit\\n            # TODO: find a cleaner way to handle gear ratio. Only needed for variable gear ratio actuators.\\n            if hasattr(actuator, \"gear_ratio\"):\\n                self._data.gear_ratio[:, actuator.joint_indices] = actuator.gear_ratio\\n\\n    \"\"\"\\n    Internal helpers -- Debugging.\\n    \"\"\"\\n\\n    def _validate_cfg(self):\\n        \"\"\"Validate the configuration after processing.\\n\\n        Note:\\n            This function should be called only after the configuration has been processed and the buffers have been\\n            created. Otherwise, some settings that are altered during processing may not be validated.\\n            For instance, the actuator models may change the joint max velocity limits.\\n        \"\"\"\\n        # check that the default values are within the limits\\n        joint_pos_limits = self.root_physx_view.get_dof_limits()[0].to(self.device)\\n        out_of_range = self._data.default_joint_pos[0] < joint_pos_limits[:, 0]\\n        out_of_range |= self._data.default_joint_pos[0] > joint_pos_limits[:, 1]\\n        violated_indices = torch.nonzero(out_of_range, as_tuple=False).squeeze(-1)\\n        # throw error if any of the default joint positions are out of the limits\\n        if len(violated_indices) > 0:\\n            # prepare message for violated joints\\n            msg = \"The following joints have default positions out of the limits: \\\\n\"\\n            for idx in violated_indices:\\n                joint_name = self.data.joint_names[idx]\\n                joint_limit = joint_pos_limits[idx]\\n                joint_pos = self.data.default_joint_pos[0, idx]\\n                # add to message\\n                msg += f\"\\\\t- \\'{joint_name}\\': {joint_pos:.3f} not in [{joint_limit[0]:.3f}, {joint_limit[1]:.3f}]\\\\n\"\\n            raise ValueError(msg)\\n\\n        # check that the default joint velocities are within the limits\\n        joint_max_vel = self.root_physx_view.get_dof_max_velocities()[0].to(self.device)\\n        out_of_range = torch.abs(self._data.default_joint_vel[0]) > joint_max_vel\\n        violated_indices = torch.nonzero(out_of_range, as_tuple=False).squeeze(-1)\\n        if len(violated_indices) > 0:\\n            # prepare message for violated joints\\n            msg = \"The following joints have default velocities out of the limits: \\\\n\"\\n            for idx in violated_indices:\\n                joint_name = self.data.joint_names[idx]\\n                joint_limit = [-joint_max_vel[idx], joint_max_vel[idx]]\\n                joint_vel = self.data.default_joint_vel[0, idx]\\n                # add to message\\n                msg += f\"\\\\t- \\'{joint_name}\\': {joint_vel:.3f} not in [{joint_limit[0]:.3f}, {joint_limit[1]:.3f}]\\\\n\"\\n            raise ValueError(msg)\\n\\n    def _log_articulation_info(self):\\n        \"\"\"Log information about the articulation.\\n\\n        Note: We purposefully read the values from the simulator to ensure that the values are configured as expected.\\n        \"\"\"\\n        # read out all joint parameters from simulation\\n        # -- gains\\n        stiffnesses = self.root_physx_view.get_dof_stiffnesses()[0].tolist()\\n        dampings = self.root_physx_view.get_dof_dampings()[0].tolist()\\n        # -- properties\\n        armatures = self.root_physx_view.get_dof_armatures()[0].tolist()\\n        frictions = self.root_physx_view.get_dof_friction_coefficients()[0].tolist()\\n        # -- limits\\n        position_limits = self.root_physx_view.get_dof_limits()[0].tolist()\\n        velocity_limits = self.root_physx_view.get_dof_max_velocities()[0].tolist()\\n        effort_limits = self.root_physx_view.get_dof_max_forces()[0].tolist()\\n        # create table for term information\\n        joint_table = PrettyTable()\\n        joint_table.title = f\"Simulation Joint Information (Prim path: {self.cfg.prim_path})\"\\n        joint_table.field_names = [\\n            \"Index\",\\n            \"Name\",\\n            \"Stiffness\",\\n            \"Damping\",\\n            \"Armature\",\\n            \"Friction\",\\n            \"Position Limits\",\\n            \"Velocity Limits\",\\n            \"Effort Limits\",\\n        ]\\n        joint_table.float_format = \".3\"\\n        joint_table.custom_format[\"Position Limits\"] = lambda f, v: f\"[{v[0]:.3f}, {v[1]:.3f}]\"\\n        # set alignment of table columns\\n        joint_table.align[\"Name\"] = \"l\"\\n        # add info on each term\\n        for index, name in enumerate(self.joint_names):\\n            joint_table.add_row([\\n                index,\\n                name,\\n                stiffnesses[index],\\n                dampings[index],\\n                armatures[index],\\n                frictions[index],\\n                position_limits[index],\\n                velocity_limits[index],\\n                effort_limits[index],\\n            ])\\n        # convert table to string\\n        omni.log.info(f\"Simulation parameters for joints in {self.cfg.prim_path}:\\\\n\" + joint_table.get_string())\\n\\n        # read out all tendon parameters from simulation\\n        if self.num_fixed_tendons > 0:\\n            # -- gains\\n            ft_stiffnesses = self.root_physx_view.get_fixed_tendon_stiffnesses()[0].tolist()\\n            ft_dampings = self.root_physx_view.get_fixed_tendon_dampings()[0].tolist()\\n            # -- limits\\n            ft_limit_stiffnesses = self.root_physx_view.get_fixed_tendon_limit_stiffnesses()[0].tolist()\\n            ft_limits = self.root_physx_view.get_fixed_tendon_limits()[0].tolist()\\n            ft_rest_lengths = self.root_physx_view.get_fixed_tendon_rest_lengths()[0].tolist()\\n            ft_offsets = self.root_physx_view.get_fixed_tendon_offsets()[0].tolist()\\n            # create table for term information\\n            tendon_table = PrettyTable()\\n            tendon_table.title = f\"Simulation Fixed Tendon Information (Prim path: {self.cfg.prim_path})\"\\n            tendon_table.field_names = [\\n                \"Index\",\\n                \"Stiffness\",\\n                \"Damping\",\\n                \"Limit Stiffness\",\\n                \"Limits\",\\n                \"Rest Length\",\\n                \"Offset\",\\n            ]\\n            tendon_table.float_format = \".3\"\\n            joint_table.custom_format[\"Limits\"] = lambda f, v: f\"[{v[0]:.3f}, {v[1]:.3f}]\"\\n            # add info on each term\\n            for index in range(self.num_fixed_tendons):\\n                tendon_table.add_row([\\n                    index,\\n                    ft_stiffnesses[index],\\n                    ft_dampings[index],\\n                    ft_limit_stiffnesses[index],\\n                    ft_limits[index],\\n                    ft_rest_lengths[index],\\n                    ft_offsets[index],\\n                ])\\n            # convert table to string\\n            omni.log.info(f\"Simulation parameters for tendons in {self.cfg.prim_path}:\\\\n\" + tendon_table.get_string())\\n\\n    \"\"\"\\n    Deprecated methods.\\n    \"\"\"\\n\\n    def write_joint_friction_to_sim(\\n        self,\\n        joint_friction: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Write joint friction coefficients into the simulation.\\n\\n        .. deprecated:: 2.1.0\\n            Please use :meth:`write_joint_friction_coefficient_to_sim` instead.\\n        \"\"\"\\n        omni.log.warn(\\n            \"The function \\'write_joint_friction_to_sim\\' will be deprecated in a future release. Please\"\\n            \" use \\'write_joint_friction_coefficient_to_sim\\' instead.\"\\n        )\\n        self.write_joint_friction_coefficient_to_sim(joint_friction, joint_ids=joint_ids, env_ids=env_ids)\\n\\n    def write_joint_limits_to_sim(\\n        self,\\n        limits: torch.Tensor | float,\\n        joint_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n        warn_limit_violation: bool = True,\\n    ):\\n        \"\"\"Write joint limits into the simulation.\\n\\n        .. deprecated:: 2.1.0\\n            Please use :meth:`write_joint_position_limit_to_sim` instead.\\n        \"\"\"\\n        omni.log.warn(\\n            \"The function \\'write_joint_limits_to_sim\\' will be deprecated in a future release. Please\"\\n            \" use \\'write_joint_position_limit_to_sim\\' instead.\"\\n        )\\n        self.write_joint_position_limit_to_sim(\\n            limits, joint_ids=joint_ids, env_ids=env_ids, warn_limit_violation=warn_limit_violation\\n        )\\n\\n    def set_fixed_tendon_limit(\\n        self,\\n        limit: torch.Tensor,\\n        fixed_tendon_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set fixed tendon position limits into internal buffers.\\n\\n        .. deprecated:: 2.1.0\\n            Please use :meth:`set_fixed_tendon_position_limit` instead.\\n        \"\"\"\\n        omni.log.warn(\\n            \"The function \\'set_fixed_tendon_limit\\' will be deprecated in a future release. Please\"\\n            \" use \\'set_fixed_tendon_position_limit\\' instead.\"\\n        )\\n        self.set_fixed_tendon_position_limit(limit, fixed_tendon_ids=fixed_tendon_ids, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='class ArticulationCfg(AssetBaseCfg):\\n    \"\"\"Configuration parameters for an articulation.\"\"\"\\n\\n    @configclass\\n    class InitialStateCfg(AssetBaseCfg.InitialStateCfg):\\n        \"\"\"Initial state of the articulation.\"\"\"\\n\\n        # root velocity\\n        lin_vel: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Linear velocity of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n        ang_vel: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Angular velocity of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n\\n        # joint state\\n        joint_pos: dict[str, float] = {\".*\": 0.0}\\n        \"\"\"Joint positions of the joints. Defaults to 0.0 for all joints.\"\"\"\\n        joint_vel: dict[str, float] = {\".*\": 0.0}\\n        \"\"\"Joint velocities of the joints. Defaults to 0.0 for all joints.\"\"\"\\n\\n    ##\\n    # Initialize configurations.\\n    ##\\n\\n    class_type: type = Articulation\\n\\n    articulation_root_prim_path: str | None = None\\n    \"\"\"Path to the articulation root prim in the USD file.\\n\\n    If not provided will search for a prim with the ArticulationRootAPI. Should start with a slash.\\n    \"\"\"\\n\\n    init_state: InitialStateCfg = InitialStateCfg()\\n    \"\"\"Initial state of the articulated object. Defaults to identity pose with zero velocity and zero joint state.\"\"\"\\n\\n    soft_joint_pos_limit_factor: float = 1.0\\n    \"\"\"Fraction specifying the range of joint position limits (parsed from the asset) to use. Defaults to 1.0.\\n\\n    The soft joint position limits are scaled by this factor to specify a safety region within the simulated\\n    joint position limits. This isn\\'t used by the simulation, but is useful for learning agents to prevent the joint\\n    positions from violating the limits, such as for termination conditions.\\n\\n    The soft joint position limits are accessible through the :attr:`ArticulationData.soft_joint_pos_limits` attribute.\\n    \"\"\"\\n\\n    actuators: dict[str, ActuatorBaseCfg] = MISSING\\n    \"\"\"Actuators for the robot with corresponding joint names.\"\"\"'),\n",
       " Document(metadata={}, page_content='class ArticulationData:\\n    \"\"\"Data container for an articulation.\\n\\n    This class contains the data for an articulation in the simulation. The data includes the state of\\n    the root rigid body, the state of all the bodies in the articulation, and the joint state. The data is\\n    stored in the simulation world frame unless otherwise specified.\\n\\n    An articulation is comprised of multiple rigid bodies or links. For a rigid body, there are two frames\\n    of reference that are used:\\n\\n    - Actor frame: The frame of reference of the rigid body prim. This typically corresponds to the Xform prim\\n      with the rigid body schema.\\n    - Center of mass frame: The frame of reference of the center of mass of the rigid body.\\n\\n    Depending on the settings, the two frames may not coincide with each other. In the robotics sense, the actor frame\\n    can be interpreted as the link frame.\\n    \"\"\"\\n\\n    def __init__(self, root_physx_view: physx.ArticulationView, device: str):\\n        \"\"\"Initializes the articulation data.\\n\\n        Args:\\n            root_physx_view: The root articulation view.\\n            device: The device used for processing.\\n        \"\"\"\\n        # Set the parameters\\n        self.device = device\\n        # Set the root articulation view\\n        # note: this is stored as a weak reference to avoid circular references between the asset class\\n        #  and the data container. This is important to avoid memory leaks.\\n        self._root_physx_view: physx.ArticulationView = weakref.proxy(root_physx_view)\\n\\n        # Set initial time stamp\\n        self._sim_timestamp = 0.0\\n\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        gravity = self._physics_sim_view.get_gravity()\\n        # Convert to direction vector\\n        gravity_dir = torch.tensor((gravity[0], gravity[1], gravity[2]), device=self.device)\\n        gravity_dir = math_utils.normalize(gravity_dir.unsqueeze(0)).squeeze(0)\\n\\n        # Initialize constants\\n        self.GRAVITY_VEC_W = gravity_dir.repeat(self._root_physx_view.count, 1)\\n        self.FORWARD_VEC_B = torch.tensor((1.0, 0.0, 0.0), device=self.device).repeat(self._root_physx_view.count, 1)\\n\\n        # Initialize history for finite differencing\\n        self._previous_joint_vel = self._root_physx_view.get_dof_velocities().clone()\\n\\n        # Initialize the lazy buffers.\\n        self._root_state_w = TimestampedBuffer()\\n        self._root_link_state_w = TimestampedBuffer()\\n        self._root_com_state_w = TimestampedBuffer()\\n        self._body_state_w = TimestampedBuffer()\\n        self._body_link_state_w = TimestampedBuffer()\\n        self._body_com_state_w = TimestampedBuffer()\\n        self._body_acc_w = TimestampedBuffer()\\n        self._joint_pos = TimestampedBuffer()\\n        self._joint_acc = TimestampedBuffer()\\n        self._joint_vel = TimestampedBuffer()\\n        self._body_incoming_joint_wrench_b = TimestampedBuffer()\\n\\n    def update(self, dt: float):\\n        # update the simulation timestamp\\n        self._sim_timestamp += dt\\n        # Trigger an update of the joint acceleration buffer at a higher frequency\\n        # since we do finite differencing.\\n        self.joint_acc\\n\\n    ##\\n    # Names.\\n    ##\\n\\n    body_names: list[str] = None\\n    \"\"\"Body names in the order parsed by the simulation view.\"\"\"\\n\\n    joint_names: list[str] = None\\n    \"\"\"Joint names in the order parsed by the simulation view.\"\"\"\\n\\n    fixed_tendon_names: list[str] = None\\n    \"\"\"Fixed tendon names in the order parsed by the simulation view.\"\"\"\\n\\n    ##\\n    # Defaults - Initial state.\\n    ##\\n\\n    default_root_state: torch.Tensor = None\\n    \"\"\"Default root state ``[pos, quat, lin_vel, ang_vel]`` in the local environment frame. Shape is (num_instances, 13).\\n\\n    The position and quaternion are of the articulation root\\'s actor frame. Meanwhile, the linear and angular\\n    velocities are of its center of mass frame.\\n\\n    This quantity is configured through the :attr:`isaaclab.assets.ArticulationCfg.init_state` parameter.\\n    \"\"\"\\n\\n    default_joint_pos: torch.Tensor = None\\n    \"\"\"Default joint positions of all joints. Shape is (num_instances, num_joints).\\n\\n    This quantity is configured through the :attr:`isaaclab.assets.ArticulationCfg.init_state` parameter.\\n    \"\"\"\\n\\n    default_joint_vel: torch.Tensor = None\\n    \"\"\"Default joint velocities of all joints. Shape is (num_instances, num_joints).\\n\\n    This quantity is configured through the :attr:`isaaclab.assets.ArticulationCfg.init_state` parameter.\\n    \"\"\"\\n\\n    ##\\n    # Defaults - Physical properties.\\n    ##\\n\\n    default_mass: torch.Tensor = None\\n    \"\"\"Default mass for all the bodies in the articulation. Shape is (num_instances, num_bodies).\\n\\n    This quantity is parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_inertia: torch.Tensor = None\\n    \"\"\"Default inertia for all the bodies in the articulation. Shape is (num_instances, num_bodies, 9).\\n\\n    The inertia is the inertia tensor relative to the center of mass frame. The values are stored in\\n    the order :math:`[I_{xx}, I_{xy}, I_{xz}, I_{yx}, I_{yy}, I_{yz}, I_{zx}, I_{zy}, I_{zz}]`.\\n\\n    This quantity is parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_joint_stiffness: torch.Tensor = None\\n    \"\"\"Default joint stiffness of all joints. Shape is (num_instances, num_joints).\\n\\n    This quantity is configured through the actuator model\\'s :attr:`isaaclab.actuators.ActuatorBaseCfg.stiffness`\\n    parameter. If the parameter\\'s value is None, the value parsed from the USD schema, at the time of initialization,\\n    is used.\\n\\n    .. attention::\\n        The default stiffness is the value configured by the user or the value parsed from the USD schema.\\n        It should not be confused with :attr:`joint_stiffness`, which is the value set into the simulation.\\n    \"\"\"\\n\\n    default_joint_damping: torch.Tensor = None\\n    \"\"\"Default joint damping of all joints. Shape is (num_instances, num_joints).\\n\\n    This quantity is configured through the actuator model\\'s :attr:`isaaclab.actuators.ActuatorBaseCfg.damping`\\n    parameter. If the parameter\\'s value is None, the value parsed from the USD schema, at the time of initialization,\\n    is used.\\n\\n    .. attention::\\n        The default stiffness is the value configured by the user or the value parsed from the USD schema.\\n        It should not be confused with :attr:`joint_damping`, which is the value set into the simulation.\\n    \"\"\"\\n\\n    default_joint_armature: torch.Tensor = None\\n    \"\"\"Default joint armature of all joints. Shape is (num_instances, num_joints).\\n\\n    This quantity is configured through the actuator model\\'s :attr:`isaaclab.actuators.ActuatorBaseCfg.armature`\\n    parameter. If the parameter\\'s value is None, the value parsed from the USD schema, at the time of initialization,\\n    is used.\\n    \"\"\"\\n\\n    default_joint_friction_coeff: torch.Tensor = None\\n    \"\"\"Default joint friction coefficient of all joints. Shape is (num_instances, num_joints).\\n\\n    This quantity is configured through the actuator model\\'s :attr:`isaaclab.actuators.ActuatorBaseCfg.friction`\\n    parameter. If the parameter\\'s value is None, the value parsed from the USD schema, at the time of initialization,\\n    is used.\\n    \"\"\"\\n\\n    default_joint_pos_limits: torch.Tensor = None\\n    \"\"\"Default joint position limits of all joints. Shape is (num_instances, num_joints, 2).\\n\\n    The limits are in the order :math:`[lower, upper]`. They are parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_fixed_tendon_stiffness: torch.Tensor = None\\n    \"\"\"Default tendon stiffness of all tendons. Shape is (num_instances, num_fixed_tendons).\\n\\n    This quantity is parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_fixed_tendon_damping: torch.Tensor = None\\n    \"\"\"Default tendon damping of all tendons. Shape is (num_instances, num_fixed_tendons).\\n\\n    This quantity is parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_fixed_tendon_limit_stiffness: torch.Tensor = None\\n    \"\"\"Default tendon limit stiffness of all tendons. Shape is (num_instances, num_fixed_tendons).\\n\\n    This quantity is parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_fixed_tendon_rest_length: torch.Tensor = None\\n    \"\"\"Default tendon rest length of all tendons. Shape is (num_instances, num_fixed_tendons).\\n\\n    This quantity is parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_fixed_tendon_offset: torch.Tensor = None\\n    \"\"\"Default tendon offset of all tendons. Shape is (num_instances, num_fixed_tendons).\\n\\n    This quantity is parsed from the USD schema at the time of initialization.\\n    \"\"\"\\n\\n    default_fixed_tendon_pos_limits: torch.Tensor = None\\n    \"\"\"Default tendon position limits of all tendons. Shape is (num_instances, num_fixed_tendons, 2).\\n\\n    The position limits are in the order :math:`[lower, upper]`. They are parsed from the USD schema at the time of\\n    initialization.\\n    \"\"\"\\n\\n    ##\\n    # Joint commands -- Set into simulation.\\n    ##\\n\\n    joint_pos_target: torch.Tensor = None\\n    \"\"\"Joint position targets commanded by the user. Shape is (num_instances, num_joints).\\n\\n    For an implicit actuator model, the targets are directly set into the simulation.\\n    For an explicit actuator model, the targets are used to compute the joint torques (see :attr:`applied_torque`),\\n    which are then set into the simulation.\\n    \"\"\"\\n\\n    joint_vel_target: torch.Tensor = None\\n    \"\"\"Joint velocity targets commanded by the user. Shape is (num_instances, num_joints).\\n\\n    For an implicit actuator model, the targets are directly set into the simulation.\\n    For an explicit actuator model, the targets are used to compute the joint torques (see :attr:`applied_torque`),\\n    which are then set into the simulation.\\n    \"\"\"\\n\\n    joint_effort_target: torch.Tensor = None\\n    \"\"\"Joint effort targets commanded by the user. Shape is (num_instances, num_joints).\\n\\n    For an implicit actuator model, the targets are directly set into the simulation.\\n    For an explicit actuator model, the targets are used to compute the joint torques (see :attr:`applied_torque`),\\n    which are then set into the simulation.\\n    \"\"\"\\n\\n    ##\\n    # Joint commands -- Explicit actuators.\\n    ##\\n\\n    computed_torque: torch.Tensor = None\\n    \"\"\"Joint torques computed from the actuator model (before clipping). Shape is (num_instances, num_joints).\\n\\n    This quantity is the raw torque output from the actuator mode, before any clipping is applied.\\n    It is exposed for users who want to inspect the computations inside the actuator model.\\n    For instance, to penalize the learning agent for a difference between the computed and applied torques.\\n    \"\"\"\\n\\n    applied_torque: torch.Tensor = None\\n    \"\"\"Joint torques applied from the actuator model (after clipping). Shape is (num_instances, num_joints).\\n\\n    These torques are set into the simulation, after clipping the :attr:`computed_torque` based on the\\n    actuator model.\\n    \"\"\"\\n\\n    ##\\n    # Joint properties.\\n    ##\\n\\n    joint_stiffness: torch.Tensor = None\\n    \"\"\"Joint stiffness provided to the simulation. Shape is (num_instances, num_joints).\\n\\n    In the case of explicit actuators, the value for the corresponding joints is zero.\\n    \"\"\"\\n\\n    joint_damping: torch.Tensor = None\\n    \"\"\"Joint damping provided to the simulation. Shape is (num_instances, num_joints)\\n\\n    In the case of explicit actuators, the value for the corresponding joints is zero.\\n    \"\"\"\\n\\n    joint_armature: torch.Tensor = None\\n    \"\"\"Joint armature provided to the simulation. Shape is (num_instances, num_joints).\"\"\"\\n\\n    joint_friction_coeff: torch.Tensor = None\\n    \"\"\"Joint friction coefficient provided to the simulation. Shape is (num_instances, num_joints).\"\"\"\\n\\n    joint_pos_limits: torch.Tensor = None\\n    \"\"\"Joint position limits provided to the simulation. Shape is (num_instances, num_joints, 2).\\n\\n    The limits are in the order :math:`[lower, upper]`.\\n    \"\"\"\\n\\n    joint_vel_limits: torch.Tensor = None\\n    \"\"\"Joint maximum velocity provided to the simulation. Shape is (num_instances, num_joints).\"\"\"\\n\\n    joint_effort_limits: torch.Tensor = None\\n    \"\"\"Joint maximum effort provided to the simulation. Shape is (num_instances, num_joints).\"\"\"\\n\\n    ##\\n    # Joint properties - Custom.\\n    ##\\n\\n    soft_joint_pos_limits: torch.Tensor = None\\n    r\"\"\"Soft joint positions limits for all joints. Shape is (num_instances, num_joints, 2).\\n\\n    The limits are in the order :math:`[lower, upper]`.The soft joint position limits are computed as\\n    a sub-region of the :attr:`joint_pos_limits` based on the\\n    :attr:`~isaaclab.assets.ArticulationCfg.soft_joint_pos_limit_factor` parameter.\\n\\n    Consider the joint position limits :math:`[lower, upper]` and the soft joint position limits\\n    :math:`[soft_lower, soft_upper]`. The soft joint position limits are computed as:\\n\\n    .. math::\\n\\n        soft\\\\_lower = (lower + upper) / 2 - factor * (upper - lower) / 2\\n        soft\\\\_upper = (lower + upper) / 2 + factor * (upper - lower) / 2\\n\\n    The soft joint position limits help specify a safety region around the joint limits. It isn\\'t used by the\\n    simulation, but is useful for learning agents to prevent the joint positions from violating the limits.\\n    \"\"\"\\n\\n    soft_joint_vel_limits: torch.Tensor = None\\n    \"\"\"Soft joint velocity limits for all joints. Shape is (num_instances, num_joints).\\n\\n    These are obtained from the actuator model. It may differ from :attr:`joint_vel_limits` if the actuator model\\n    has a variable velocity limit model. For instance, in a variable gear ratio actuator model.\\n    \"\"\"\\n\\n    gear_ratio: torch.Tensor = None\\n    \"\"\"Gear ratio for relating motor torques to applied Joint torques. Shape is (num_instances, num_joints).\"\"\"\\n\\n    ##\\n    # Fixed tendon properties.\\n    ##\\n\\n    fixed_tendon_stiffness: torch.Tensor = None\\n    \"\"\"Fixed tendon stiffness provided to the simulation. Shape is (num_instances, num_fixed_tendons).\"\"\"\\n\\n    fixed_tendon_damping: torch.Tensor = None\\n    \"\"\"Fixed tendon damping provided to the simulation. Shape is (num_instances, num_fixed_tendons).\"\"\"\\n\\n    fixed_tendon_limit_stiffness: torch.Tensor = None\\n    \"\"\"Fixed tendon limit stiffness provided to the simulation. Shape is (num_instances, num_fixed_tendons).\"\"\"\\n\\n    fixed_tendon_rest_length: torch.Tensor = None\\n    \"\"\"Fixed tendon rest length provided to the simulation. Shape is (num_instances, num_fixed_tendons).\"\"\"\\n\\n    fixed_tendon_offset: torch.Tensor = None\\n    \"\"\"Fixed tendon offset provided to the simulation. Shape is (num_instances, num_fixed_tendons).\"\"\"\\n\\n    fixed_tendon_pos_limits: torch.Tensor = None\\n    \"\"\"Fixed tendon position limits provided to the simulation. Shape is (num_instances, num_fixed_tendons, 2).\"\"\"\\n\\n    ##\\n    # Properties.\\n    ##\\n\\n    @property\\n    def root_state_w(self):\\n        \"\"\"Root state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame. Shape is (num_instances, 13).\\n\\n        The position and quaternion are of the articulation root\\'s actor frame relative to the world. Meanwhile,\\n        the linear and angular velocities are of the articulation root\\'s center of mass frame.\\n        \"\"\"\\n        if self._root_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._root_physx_view.get_root_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_root_velocities()\\n            # set the buffer data and timestamp\\n            self._root_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._root_state_w.timestamp = self._sim_timestamp\\n        return self._root_state_w.data\\n\\n    @property\\n    def root_link_state_w(self):\\n        \"\"\"Root state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame. Shape is (num_instances, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the articulation root\\'s actor frame relative to the\\n        world.\\n        \"\"\"\\n        if self._root_link_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._root_physx_view.get_root_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_root_velocities().clone()\\n\\n            # adjust linear velocity to link from center of mass\\n            velocity[:, :3] += torch.linalg.cross(\\n                velocity[:, 3:], math_utils.quat_apply(pose[:, 3:7], -self.com_pos_b[:, 0, :]), dim=-1\\n            )\\n            # set the buffer data and timestamp\\n            self._root_link_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._root_link_state_w.timestamp = self._sim_timestamp\\n\\n        return self._root_link_state_w.data\\n\\n    @property\\n    def root_com_state_w(self):\\n        \"\"\"Root center of mass state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame. Shape is (num_instances, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the articulation root link\\'s center of mass frame\\n        relative to the world. Center of mass frame is assumed to be the same orientation as the link rather than the\\n        orientation of the principle inertia.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation (pose is of link)\\n            pose = self._root_physx_view.get_root_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_root_velocities()\\n\\n            # adjust pose to center of mass\\n            pos, quat = math_utils.combine_frame_transforms(\\n                pose[:, :3], pose[:, 3:7], self.com_pos_b[:, 0, :], self.com_quat_b[:, 0, :]\\n            )\\n            pose = torch.cat((pos, quat), dim=-1)\\n            # set the buffer data and timestamp\\n            self._root_com_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._root_com_state_w.timestamp = self._sim_timestamp\\n        return self._root_com_state_w.data\\n\\n    @property\\n    def body_state_w(self):\\n        \"\"\"State of all bodies `[pos, quat, lin_vel, ang_vel]` in simulation world frame.\\n        Shape is (num_instances, num_bodies, 13).\\n\\n        The position and quaternion are of all the articulation links\\'s actor frame. Meanwhile, the linear and angular\\n        velocities are of the articulation links\\'s center of mass frame.\\n        \"\"\"\\n\\n        if self._body_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation\\n            poses = self._root_physx_view.get_link_transforms().clone()\\n            poses[..., 3:7] = math_utils.convert_quat(poses[..., 3:7], to=\"wxyz\")\\n            velocities = self._root_physx_view.get_link_velocities()\\n            # set the buffer data and timestamp\\n            self._body_state_w.data = torch.cat((poses, velocities), dim=-1)\\n            self._body_state_w.timestamp = self._sim_timestamp\\n        return self._body_state_w.data\\n\\n    @property\\n    def body_link_state_w(self):\\n        \"\"\"State of all bodies\\' link frame`[pos, quat, lin_vel, ang_vel]` in simulation world frame.\\n        Shape is (num_instances, num_bodies, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the body\\'s link frame relative to the world.\\n        \"\"\"\\n        if self._body_link_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation\\n            pose = self._root_physx_view.get_link_transforms().clone()\\n            pose[..., 3:7] = math_utils.convert_quat(pose[..., 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_link_velocities()\\n\\n            # adjust linear velocity to link from center of mass\\n            velocity[..., :3] += torch.linalg.cross(\\n                velocity[..., 3:], math_utils.quat_apply(pose[..., 3:7], -self.com_pos_b), dim=-1\\n            )\\n            # set the buffer data and timestamp\\n            self._body_link_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._body_link_state_w.timestamp = self._sim_timestamp\\n\\n        return self._body_link_state_w.data\\n\\n    @property\\n    def body_com_state_w(self):\\n        \"\"\"State of all bodies center of mass `[pos, quat, lin_vel, ang_vel]` in simulation world frame.\\n        Shape is (num_instances, num_bodies, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the body\\'s center of mass frame relative to the\\n        world. Center of mass frame is assumed to be the same orientation as the link rather than the orientation of the\\n        principle inertia.\\n        \"\"\"\\n        if self._body_com_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation (pose is of link)\\n            pose = self._root_physx_view.get_link_transforms().clone()\\n            pose[..., 3:7] = math_utils.convert_quat(pose[..., 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_link_velocities()\\n\\n            # adjust pose to center of mass\\n            pos, quat = math_utils.combine_frame_transforms(\\n                pose[..., :3], pose[..., 3:7], self.com_pos_b, self.com_quat_b\\n            )\\n            pose = torch.cat((pos, quat), dim=-1)\\n            # set the buffer data and timestamp\\n            self._body_com_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._body_com_state_w.timestamp = self._sim_timestamp\\n        return self._body_com_state_w.data\\n\\n    @property\\n    def body_acc_w(self):\\n        \"\"\"Acceleration of all bodies (center of mass). Shape is (num_instances, num_bodies, 6).\\n\\n        All values are relative to the world.\\n        \"\"\"\\n        if self._body_acc_w.timestamp < self._sim_timestamp:\\n            # read data from simulation and set the buffer data and timestamp\\n            self._body_acc_w.data = self._root_physx_view.get_link_accelerations()\\n\\n            self._body_acc_w.timestamp = self._sim_timestamp\\n        return self._body_acc_w.data\\n\\n    @property\\n    def body_incoming_joint_wrench_b(self) -> torch.Tensor:\\n        \"\"\"Joint reaction wrench applied from body parent to child body in parent body frame.\\n\\n        Shape is (num_instances, num_bodies, 6). All body reaction wrenches are provided including the root body to the\\n        world of an articulation.\\n\\n        For more information on joint wrenches, please check the`PhysX documentation <https://nvidia-omniverse.github.io/PhysX/physx/5.5.1/docs/Articulations.html#link-incoming-joint-force>`__\\n        and the underlying `PhysX Tensor API <https://docs.omniverse.nvidia.com/kit/docs/omni_physics/latest/extensions/runtime/source/omni.physics.tensors/docs/api/python.html#omni.physics.tensors.impl.api.ArticulationView.get_link_incoming_joint_force>`__ .\\n        \"\"\"\\n\\n        if self._body_incoming_joint_wrench_b.timestamp < self._sim_timestamp:\\n            self._body_incoming_joint_wrench_b.data = self._root_physx_view.get_link_incoming_joint_force()\\n            self._body_incoming_joint_wrench_b.time_stamp = self._sim_timestamp\\n        return self._body_incoming_joint_wrench_b.data\\n\\n    @property\\n    def projected_gravity_b(self):\\n        \"\"\"Projection of the gravity direction on base frame. Shape is (num_instances, 3).\"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.GRAVITY_VEC_W)\\n\\n    @property\\n    def heading_w(self):\\n        \"\"\"Yaw heading of the base frame (in radians). Shape is (num_instances,).\\n\\n        Note:\\n            This quantity is computed by assuming that the forward-direction of the base\\n            frame is along x-direction, i.e. :math:`(1, 0, 0)`.\\n        \"\"\"\\n        forward_w = math_utils.quat_apply(self.root_link_quat_w, self.FORWARD_VEC_B)\\n        return torch.atan2(forward_w[:, 1], forward_w[:, 0])\\n\\n    @property\\n    def joint_pos(self):\\n        \"\"\"Joint positions of all joints. Shape is (num_instances, num_joints).\"\"\"\\n        if self._joint_pos.timestamp < self._sim_timestamp:\\n            # read data from simulation and set the buffer data and timestamp\\n            self._joint_pos.data = self._root_physx_view.get_dof_positions()\\n            self._joint_pos.timestamp = self._sim_timestamp\\n        return self._joint_pos.data\\n\\n    @property\\n    def joint_vel(self):\\n        \"\"\"Joint velocities of all joints. Shape is (num_instances, num_joints).\"\"\"\\n        if self._joint_vel.timestamp < self._sim_timestamp:\\n            # read data from simulation and set the buffer data and timestamp\\n            self._joint_vel.data = self._root_physx_view.get_dof_velocities()\\n            self._joint_vel.timestamp = self._sim_timestamp\\n        return self._joint_vel.data\\n\\n    @property\\n    def joint_acc(self):\\n        \"\"\"Joint acceleration of all joints. Shape is (num_instances, num_joints).\"\"\"\\n        if self._joint_acc.timestamp < self._sim_timestamp:\\n            # note: we use finite differencing to compute acceleration\\n            time_elapsed = self._sim_timestamp - self._joint_acc.timestamp\\n            self._joint_acc.data = (self.joint_vel - self._previous_joint_vel) / time_elapsed\\n            self._joint_acc.timestamp = self._sim_timestamp\\n            # update the previous joint velocity\\n            self._previous_joint_vel[:] = self.joint_vel\\n        return self._joint_acc.data\\n\\n    ##\\n    # Derived properties.\\n    ##\\n\\n    @property\\n    def root_pos_w(self) -> torch.Tensor:\\n        \"\"\"Root position in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the position of the actor frame of the articulation root relative to the world.\\n        \"\"\"\\n        return self.root_state_w[:, :3]\\n\\n    @property\\n    def root_quat_w(self) -> torch.Tensor:\\n        \"\"\"Root orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, 4).\\n\\n        This quantity is the orientation of the actor frame of the articulation root relative to the world.\\n        \"\"\"\\n        return self.root_state_w[:, 3:7]\\n\\n    @property\\n    def root_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root velocity in simulation world frame. Shape is (num_instances, 6).\\n\\n        This quantity contains the linear and angular velocities of the articulation root\\'s center of mass frame\\n        relative to the world.\\n        \"\"\"\\n        return self.root_state_w[:, 7:13]\\n\\n    @property\\n    def root_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root linear velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the articulation root\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        return self.root_state_w[:, 7:10]\\n\\n    @property\\n    def root_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root angular velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the articulation root\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        return self.root_state_w[:, 10:13]\\n\\n    @property\\n    def root_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root linear velocity in base frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the articulation root\\'s center of mass frame relative to the world\\n        with respect to the articulation root\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_quat_w, self.root_lin_vel_w)\\n\\n    @property\\n    def root_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root angular velocity in base world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the articulation root\\'s center of mass frame relative to the world with\\n        respect to the articulation root\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_quat_w, self.root_ang_vel_w)\\n\\n    ##\\n    # Derived Root Link Frame Properties\\n    ##\\n\\n    @property\\n    def root_link_pos_w(self) -> torch.Tensor:\\n        \"\"\"Root link position in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the position of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        if self._root_link_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation (pose is of link)\\n            pose = self._root_physx_view.get_root_transforms()\\n            return pose[:, :3]\\n        return self.root_link_state_w[:, :3]\\n\\n    @property\\n    def root_link_quat_w(self) -> torch.Tensor:\\n        \"\"\"Root link orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, 4).\\n\\n        This quantity is the orientation of the actor frame of the root rigid body.\\n        \"\"\"\\n        if self._root_link_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation (pose is of link)\\n            pose = self._root_physx_view.get_root_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            return pose[:, 3:7]\\n        return self.root_link_state_w[:, 3:7]\\n\\n    @property\\n    def root_link_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root link velocity in simulation world frame. Shape is (num_instances, 6).\\n\\n        This quantity contains the linear and angular velocities of the actor frame of the root\\n        rigid body relative to the world.\\n        \"\"\"\\n        return self.root_link_state_w[:, 7:13]\\n\\n    @property\\n    def root_link_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root linear velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s actor frame relative to the world.\\n        \"\"\"\\n        return self.root_link_state_w[:, 7:10]\\n\\n    @property\\n    def root_link_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root link angular velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        return self.root_link_state_w[:, 10:13]\\n\\n    @property\\n    def root_link_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root link linear velocity in base frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the actor frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_link_lin_vel_w)\\n\\n    @property\\n    def root_link_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root link angular velocity in base world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the actor frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_link_ang_vel_w)\\n\\n    ##\\n    # Root Center of Mass state properties\\n    ##\\n\\n    @property\\n    def root_com_pos_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass position in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the position of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        return self.root_com_state_w[:, :3]\\n\\n    @property\\n    def root_com_quat_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, 4).\\n\\n        This quantity is the orientation of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        return self.root_com_state_w[:, 3:7]\\n\\n    @property\\n    def root_com_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass velocity in simulation world frame. Shape is (num_instances, 6).\\n\\n        This quantity contains the linear and angular velocities of the root rigid body\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation (pose is of link)\\n            velocity = self._root_physx_view.get_root_velocities()\\n            return velocity\\n        return self.root_com_state_w[:, 7:13]\\n\\n    @property\\n    def root_com_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass linear velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation (pose is of link)\\n            velocity = self._root_physx_view.get_root_velocities()\\n            return velocity[:, 0:3]\\n        return self.root_com_state_w[:, 7:10]\\n\\n    @property\\n    def root_com_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass angular velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the root rigid body\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation (pose is of link)\\n            velocity = self._root_physx_view.get_root_velocities()\\n            return velocity[:, 3:6]\\n        return self.root_com_state_w[:, 10:13]\\n\\n    @property\\n    def root_com_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root center of mass linear velocity in base frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_com_lin_vel_w)\\n\\n    @property\\n    def root_com_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root center of mass angular velocity in base world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the root rigid body\\'s center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_com_ang_vel_w)\\n\\n    @property\\n    def body_pos_w(self) -> torch.Tensor:\\n        \"\"\"Positions of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the position of the rigid bodies\\' actor frame relative to the world.\\n        \"\"\"\\n        return self.body_state_w[..., :3]\\n\\n    @property\\n    def body_quat_w(self) -> torch.Tensor:\\n        \"\"\"Orientation (w, x, y, z) of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 4).\\n\\n        This quantity is the orientation of the rigid bodies\\' actor frame relative to the world.\\n        \"\"\"\\n        return self.body_state_w[..., 3:7]\\n\\n    @property\\n    def body_vel_w(self) -> torch.Tensor:\\n        \"\"\"Velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame relative\\n        to the world.\\n        \"\"\"\\n        return self.body_state_w[..., 7:13]\\n\\n    @property\\n    def body_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Linear velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_state_w[..., 7:10]\\n\\n    @property\\n    def body_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Angular velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_state_w[..., 10:13]\\n\\n    @property\\n    def body_lin_acc_w(self) -> torch.Tensor:\\n        \"\"\"Linear acceleration of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the linear acceleration of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_acc_w[..., 0:3]\\n\\n    @property\\n    def body_ang_acc_w(self) -> torch.Tensor:\\n        \"\"\"Angular acceleration of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the angular acceleration of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_acc_w[..., 3:6]\\n\\n    ##\\n    # Link body properties\\n    ##\\n\\n    @property\\n    def body_link_pos_w(self) -> torch.Tensor:\\n        \"\"\"Positions of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the position of the rigid bodies\\' actor frame relative to the world.\\n        \"\"\"\\n        if self._body_link_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation\\n            pose = self._root_physx_view.get_link_transforms()\\n            return pose[..., :3]\\n        return self._body_link_state_w.data[..., :3]\\n\\n    @property\\n    def body_link_quat_w(self) -> torch.Tensor:\\n        \"\"\"Orientation (w, x, y, z) of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 4).\\n\\n        This quantity is the orientation of the rigid bodies\\' actor frame  relative to the world.\\n        \"\"\"\\n        if self._body_link_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation\\n            pose = self._root_physx_view.get_link_transforms().clone()\\n            pose[..., 3:7] = math_utils.convert_quat(pose[..., 3:7], to=\"wxyz\")\\n            return pose[..., 3:7]\\n        return self.body_link_state_w[..., 3:7]\\n\\n    @property\\n    def body_link_vel_w(self) -> torch.Tensor:\\n        \"\"\"Velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame\\n        relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., 7:13]\\n\\n    @property\\n    def body_link_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Linear velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., 7:10]\\n\\n    @property\\n    def body_link_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Angular velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., 10:13]\\n\\n    ##\\n    # Center of mass body properties\\n    ##\\n\\n    @property\\n    def body_com_pos_w(self) -> torch.Tensor:\\n        \"\"\"Positions of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the position of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.body_com_state_w[..., :3]\\n\\n    @property\\n    def body_com_quat_w(self) -> torch.Tensor:\\n        \"\"\"Orientation (w, x, y, z) of the prinicple axies of inertia of all bodies in simulation world frame.\\n\\n        Shape is (num_instances, num_bodies, 4). This quantity is the orientation of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.body_com_state_w[..., 3:7]\\n\\n    @property\\n    def body_com_vel_w(self) -> torch.Tensor:\\n        \"\"\"Velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._body_com_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation (velocity is of com)\\n            velocity = self._root_physx_view.get_link_velocities()\\n            return velocity\\n        return self.body_com_state_w[..., 7:13]\\n\\n    @property\\n    def body_com_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Linear velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._body_com_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation (velocity is of com)\\n            velocity = self._root_physx_view.get_link_velocities()\\n            return velocity[..., 0:3]\\n        return self.body_com_state_w[..., 7:10]\\n\\n    @property\\n    def body_com_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Angular velocity of all bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._body_com_state_w.timestamp < self._sim_timestamp:\\n            self._physics_sim_view.update_articulations_kinematic()\\n            # read data from simulation (velocity is of com)\\n            velocity = self._root_physx_view.get_link_velocities()\\n            return velocity[..., 3:6]\\n        return self.body_com_state_w[..., 10:13]\\n\\n    @property\\n    def com_pos_b(self) -> torch.Tensor:\\n        \"\"\"Center of mass of all of the bodies in simulation world frame. Shape is (num_instances, num_bodies, 3).\\n\\n        This quantity is the center of mass location relative to its body frame.\\n        \"\"\"\\n        return self._root_physx_view.get_coms().to(self.device)[..., :3]\\n\\n    @property\\n    def com_quat_b(self) -> torch.Tensor:\\n        \"\"\"Orientation (w,x,y,z) of the principle axies of inertia of all of the bodies in simulation world frame. Shape is (num_instances, num_bodies, 4).\\n\\n        This quantity is the orientation of the principles axes of inertia relative to its body frame.\\n        \"\"\"\\n        quat = self._root_physx_view.get_coms().to(self.device)[..., 3:7]\\n        return math_utils.convert_quat(quat, to=\"wxyz\")\\n\\n    ##\\n    # Backward compatibility.\\n    ##\\n\\n    @property\\n    def joint_limits(self) -> torch.Tensor:\\n        \"\"\"Deprecated property. Please use :attr:`joint_pos_limits` instead.\"\"\"\\n        omni.log.warn(\\n            \"The `joint_limits` property will be deprecated in a future release. Please use `joint_pos_limits` instead.\"\\n        )\\n        return self.joint_pos_limits\\n\\n    @property\\n    def default_joint_limits(self) -> torch.Tensor:\\n        \"\"\"Deprecated property. Please use :attr:`default_joint_pos_limits` instead.\"\"\"\\n        omni.log.warn(\\n            \"The `default_joint_limits` property will be deprecated in a future release. Please use\"\\n            \" `default_joint_pos_limits` instead.\"\\n        )\\n        return self.default_joint_pos_limits\\n\\n    @property\\n    def joint_velocity_limits(self) -> torch.Tensor:\\n        \"\"\"Deprecated property. Please use :attr:`joint_vel_limits` instead.\"\"\"\\n        omni.log.warn(\\n            \"The `joint_velocity_limits` property will be deprecated in a future release. Please use\"\\n            \" `joint_vel_limits` instead.\"\\n        )\\n        return self.joint_vel_limits\\n\\n    @property\\n    def joint_friction(self) -> torch.Tensor:\\n        \"\"\"Deprecated property. Please use :attr:`joint_friction_coeff` instead.\"\"\"\\n        omni.log.warn(\\n            \"The `joint_friction` property will be deprecated in a future release. Please use\"\\n            \" `joint_friction_coeff` instead.\"\\n        )\\n        return self.joint_friction_coeff\\n\\n    @property\\n    def default_joint_friction(self) -> torch.Tensor:\\n        \"\"\"Deprecated property. Please use :attr:`default_joint_friction_coeff` instead.\"\"\"\\n        omni.log.warn(\\n            \"The `default_joint_friction` property will be deprecated in a future release. Please use\"\\n            \" `default_joint_friction_coeff` instead.\"\\n        )\\n        return self.default_joint_friction_coeff\\n\\n    @property\\n    def fixed_tendon_limit(self) -> torch.Tensor:\\n        \"\"\"Deprecated property. Please use :attr:`fixed_tendon_pos_limits` instead.\"\"\"\\n        omni.log.warn(\\n            \"The `fixed_tendon_limit` property will be deprecated in a future release. Please use\"\\n            \" `fixed_tendon_pos_limits` instead.\"\\n        )\\n        return self.fixed_tendon_pos_limits\\n\\n    @property\\n    def default_fixed_tendon_limit(self) -> torch.Tensor:\\n        \"\"\"Deprecated property. Please use :attr:`default_fixed_tendon_pos_limits` instead.\"\"\"\\n        omni.log.warn(\\n            \"The `default_fixed_tendon_limit` property will be deprecated in a future release. Please use\"\\n            \" `default_fixed_tendon_pos_limits` instead.\"\\n        )\\n        return self.default_fixed_tendon_pos_limits'),\n",
       " Document(metadata={}, page_content='class DeformableObject(AssetBase):\\n    \"\"\"A deformable object asset class.\\n\\n    Deformable objects are assets that can be deformed in the simulation. They are typically used for\\n    soft bodies, such as stuffed animals and food items.\\n\\n    Unlike rigid object assets, deformable objects have a more complex structure and require additional\\n    handling for simulation. The simulation of deformable objects follows a finite element approach, where\\n    the object is discretized into a mesh of nodes and elements. The nodes are connected by elements, which\\n    define the material properties of the object. The nodes can be moved and deformed, and the elements\\n    respond to these changes.\\n\\n    The state of a deformable object comprises of its nodal positions and velocities, and not the object\\'s root\\n    position and orientation. The nodal positions and velocities are in the simulation frame.\\n\\n    Soft bodies can be `partially kinematic`_, where some nodes are driven by kinematic targets, and the rest are\\n    simulated. The kinematic targets are the desired positions of the nodes, and the simulation drives the nodes\\n    towards these targets. This is useful for partial control of the object, such as moving a stuffed animal\\'s\\n    head while the rest of the body is simulated.\\n\\n    .. attention::\\n        This class is experimental and subject to change due to changes on the underlying PhysX API on which\\n        it depends. We will try to maintain backward compatibility as much as possible but some changes may be\\n        necessary.\\n\\n    .. _partially kinematic: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/SoftBodies.html#kinematic-soft-bodies\\n    \"\"\"\\n\\n    cfg: DeformableObjectCfg\\n    \"\"\"Configuration instance for the deformable object.\"\"\"\\n\\n    def __init__(self, cfg: DeformableObjectCfg):\\n        \"\"\"Initialize the deformable object.\\n\\n        Args:\\n            cfg: A configuration instance.\\n        \"\"\"\\n        super().__init__(cfg)\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def data(self) -> DeformableObjectData:\\n        return self._data\\n\\n    @property\\n    def num_instances(self) -> int:\\n        return self.root_physx_view.count\\n\\n    @property\\n    def num_bodies(self) -> int:\\n        \"\"\"Number of bodies in the asset.\\n\\n        This is always 1 since each object is a single deformable body.\\n        \"\"\"\\n        return 1\\n\\n    @property\\n    def root_physx_view(self) -> physx.SoftBodyView:\\n        \"\"\"Deformable body view for the asset (PhysX).\\n\\n        Note:\\n            Use this view with caution. It requires handling of tensors in a specific way.\\n        \"\"\"\\n        return self._root_physx_view\\n\\n    @property\\n    def material_physx_view(self) -> physx.SoftBodyMaterialView | None:\\n        \"\"\"Deformable material view for the asset (PhysX).\\n\\n        This view is optional and may not be available if the material is not bound to the deformable body.\\n        If the material is not available, then the material properties will be set to default values.\\n\\n        Note:\\n            Use this view with caution. It requires handling of tensors in a specific way.\\n        \"\"\"\\n        return self._material_physx_view\\n\\n    @property\\n    def max_sim_elements_per_body(self) -> int:\\n        \"\"\"The maximum number of simulation mesh elements per deformable body.\"\"\"\\n        return self.root_physx_view.max_sim_elements_per_body\\n\\n    @property\\n    def max_collision_elements_per_body(self) -> int:\\n        \"\"\"The maximum number of collision mesh elements per deformable body.\"\"\"\\n        return self.root_physx_view.max_elements_per_body\\n\\n    @property\\n    def max_sim_vertices_per_body(self) -> int:\\n        \"\"\"The maximum number of simulation mesh vertices per deformable body.\"\"\"\\n        return self.root_physx_view.max_sim_vertices_per_body\\n\\n    @property\\n    def max_collision_vertices_per_body(self) -> int:\\n        \"\"\"The maximum number of collision mesh vertices per deformable body.\"\"\"\\n        return self.root_physx_view.max_vertices_per_body\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # Think: Should we reset the kinematic targets when resetting the object?\\n        #  This is not done in the current implementation. We assume users will reset the kinematic targets.\\n        pass\\n\\n    def write_data_to_sim(self):\\n        pass\\n\\n    def update(self, dt: float):\\n        self._data.update(dt)\\n\\n    \"\"\"\\n    Operations - Write to simulation.\\n    \"\"\"\\n\\n    def write_nodal_state_to_sim(self, nodal_state: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the nodal state over selected environment indices into the simulation.\\n\\n        The nodal state comprises of the nodal positions and velocities. Since these are nodes, the velocity only has\\n        a translational component. All the quantities are in the simulation frame.\\n\\n        Args:\\n            nodal_state: Nodal state in simulation frame.\\n                Shape is (len(env_ids), max_sim_vertices_per_body, 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # set into simulation\\n        self.write_nodal_pos_to_sim(nodal_state[..., :3], env_ids=env_ids)\\n        self.write_nodal_velocity_to_sim(nodal_state[..., 3:], env_ids=env_ids)\\n\\n    def write_nodal_pos_to_sim(self, nodal_pos: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the nodal positions over selected environment indices into the simulation.\\n\\n        The nodal position comprises of individual nodal positions of the simulation mesh for the deformable body.\\n        The positions are in the simulation frame.\\n\\n        Args:\\n            nodal_pos: Nodal positions in simulation frame.\\n                Shape is (len(env_ids), max_sim_vertices_per_body, 3).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.nodal_pos_w[env_ids] = nodal_pos.clone()\\n        # set into simulation\\n        self.root_physx_view.set_sim_nodal_positions(self._data.nodal_pos_w, indices=physx_env_ids)\\n\\n    def write_nodal_velocity_to_sim(self, nodal_vel: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the nodal velocity over selected environment indices into the simulation.\\n\\n        The nodal velocity comprises of individual nodal velocities of the simulation mesh for the deformable\\n        body. Since these are nodes, the velocity only has a translational component. The velocities are in the\\n        simulation frame.\\n\\n        Args:\\n            nodal_vel: Nodal velocities in simulation frame.\\n                Shape is (len(env_ids), max_sim_vertices_per_body, 3).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.nodal_vel_w[env_ids] = nodal_vel.clone()\\n        # set into simulation\\n        self.root_physx_view.set_sim_nodal_velocities(self._data.nodal_vel_w, indices=physx_env_ids)\\n\\n    def write_nodal_kinematic_target_to_sim(self, targets: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the kinematic targets of the simulation mesh for the deformable bodies indicated by the indices.\\n\\n        The kinematic targets comprise of individual nodal positions of the simulation mesh for the deformable body\\n        and a flag indicating whether the node is kinematically driven or not. The positions are in the simulation frame.\\n\\n        Note:\\n            The flag is set to 0.0 for kinematically driven nodes and 1.0 for free nodes.\\n\\n        Args:\\n            targets: The kinematic targets comprising of nodal positions and flags.\\n                Shape is (len(env_ids), max_sim_vertices_per_body, 4).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # store into internal buffers\\n        self._data.nodal_kinematic_target[env_ids] = targets.clone()\\n        # set into simulation\\n        self.root_physx_view.set_sim_kinematic_targets(self._data.nodal_kinematic_target, indices=physx_env_ids)\\n\\n    \"\"\"\\n    Operations - Helper.\\n    \"\"\"\\n\\n    def transform_nodal_pos(\\n        self, nodal_pos: torch.tensor, pos: torch.Tensor | None = None, quat: torch.Tensor | None = None\\n    ) -> torch.Tensor:\\n        \"\"\"Transform the nodal positions based on the pose transformation.\\n\\n        This function computes the transformation of the nodal positions based on the pose transformation.\\n        It multiplies the nodal positions with the rotation matrix of the pose and adds the translation.\\n        Internally, it calls the :meth:`isaaclab.utils.math.transform_points` function.\\n\\n        Args:\\n            nodal_pos: The nodal positions in the simulation frame. Shape is (N, max_sim_vertices_per_body, 3).\\n            pos: The position transformation. Shape is (N, 3).\\n                Defaults to None, in which case the position is assumed to be zero.\\n            quat: The orientation transformation as quaternion (w, x, y, z). Shape is (N, 4).\\n                Defaults to None, in which case the orientation is assumed to be identity.\\n\\n        Returns:\\n            The transformed nodal positions. Shape is (N, max_sim_vertices_per_body, 3).\\n        \"\"\"\\n        # offset the nodal positions to center them around the origin\\n        mean_nodal_pos = nodal_pos.mean(dim=1, keepdim=True)\\n        nodal_pos = nodal_pos - mean_nodal_pos\\n        # transform the nodal positions based on the pose around the origin\\n        return math_utils.transform_points(nodal_pos, pos, quat) + mean_nodal_pos\\n\\n    \"\"\"\\n    Internal helper.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        # obtain the first prim in the regex expression (all others are assumed to be a copy of this)\\n        template_prim = sim_utils.find_first_matching_prim(self.cfg.prim_path)\\n        if template_prim is None:\\n            raise RuntimeError(f\"Failed to find prim for expression: \\'{self.cfg.prim_path}\\'.\")\\n        template_prim_path = template_prim.GetPath().pathString\\n\\n        # find deformable root prims\\n        root_prims = sim_utils.get_all_matching_child_prims(\\n            template_prim_path, predicate=lambda prim: prim.HasAPI(PhysxSchema.PhysxDeformableBodyAPI)\\n        )\\n        if len(root_prims) == 0:\\n            raise RuntimeError(\\n                f\"Failed to find a deformable body when resolving \\'{self.cfg.prim_path}\\'.\"\\n                \" Please ensure that the prim has \\'PhysxSchema.PhysxDeformableBodyAPI\\' applied.\"\\n            )\\n        if len(root_prims) > 1:\\n            raise RuntimeError(\\n                f\"Failed to find a single deformable body when resolving \\'{self.cfg.prim_path}\\'.\"\\n                f\" Found multiple \\'{root_prims}\\' under \\'{template_prim_path}\\'.\"\\n                \" Please ensure that there is only one deformable body in the prim path tree.\"\\n            )\\n        # we only need the first one from the list\\n        root_prim = root_prims[0]\\n\\n        # find deformable material prims\\n        material_prim = None\\n        # obtain material prim from the root prim\\n        # note: here we assume that all the root prims have their material prims at similar paths\\n        #   and we only need to find the first one. This may not be the case for all scenarios.\\n        #   However, the checks in that case get cumbersome and are not included here.\\n        if root_prim.HasAPI(UsdShade.MaterialBindingAPI):\\n            # check the materials that are bound with the purpose \\'physics\\'\\n            material_paths = UsdShade.MaterialBindingAPI(root_prim).GetDirectBindingRel(\"physics\").GetTargets()\\n            # iterate through targets and find the deformable body material\\n            if len(material_paths) > 0:\\n                for mat_path in material_paths:\\n                    mat_prim = root_prim.GetStage().GetPrimAtPath(mat_path)\\n                    if mat_prim.HasAPI(PhysxSchema.PhysxDeformableBodyMaterialAPI):\\n                        material_prim = mat_prim\\n                        break\\n        if material_prim is None:\\n            omni.log.info(\\n                f\"Failed to find a deformable material binding for \\'{root_prim.GetPath().pathString}\\'.\"\\n                \" The material properties will be set to default values and are not modifiable at runtime.\"\\n                \" If you want to modify the material properties, please ensure that the material is bound\"\\n                \" to the deformable body.\"\\n            )\\n\\n        # resolve root path back into regex expression\\n        # -- root prim expression\\n        root_prim_path = root_prim.GetPath().pathString\\n        root_prim_path_expr = self.cfg.prim_path + root_prim_path[len(template_prim_path) :]\\n        # -- object view\\n        self._root_physx_view = self._physics_sim_view.create_soft_body_view(root_prim_path_expr.replace(\".*\", \"*\"))\\n\\n        # Return if the asset is not found\\n        if self._root_physx_view._backend is None:\\n            raise RuntimeError(f\"Failed to create deformable body at: {self.cfg.prim_path}. Please check PhysX logs.\")\\n\\n        # resolve material path back into regex expression\\n        if material_prim is not None:\\n            # -- material prim expression\\n            material_prim_path = material_prim.GetPath().pathString\\n            # check if the material prim is under the template prim\\n            # if not then we are assuming that the single material prim is used for all the deformable bodies\\n            if template_prim_path in material_prim_path:\\n                material_prim_path_expr = self.cfg.prim_path + material_prim_path[len(template_prim_path) :]\\n            else:\\n                material_prim_path_expr = material_prim_path\\n            # -- material view\\n            self._material_physx_view = self._physics_sim_view.create_soft_body_material_view(\\n                material_prim_path_expr.replace(\".*\", \"*\")\\n            )\\n        else:\\n            self._material_physx_view = None\\n\\n        # log information about the deformable body\\n        omni.log.info(f\"Deformable body initialized at: {root_prim_path_expr}\")\\n        omni.log.info(f\"Number of instances: {self.num_instances}\")\\n        omni.log.info(f\"Number of bodies: {self.num_bodies}\")\\n        if self._material_physx_view is not None:\\n            omni.log.info(f\"Deformable material initialized at: {material_prim_path_expr}\")\\n            omni.log.info(f\"Number of instances: {self._material_physx_view.count}\")\\n        else:\\n            omni.log.info(\"No deformable material found. Material properties will be set to default values.\")\\n\\n        # container for data access\\n        self._data = DeformableObjectData(self.root_physx_view, self.device)\\n\\n        # create buffers\\n        self._create_buffers()\\n        # update the deformable body data\\n        self.update(0.0)\\n\\n    def _create_buffers(self):\\n        \"\"\"Create buffers for storing data.\"\"\"\\n        # constants\\n        self._ALL_INDICES = torch.arange(self.num_instances, dtype=torch.long, device=self.device)\\n\\n        # default state\\n        # we use the initial nodal positions at spawn time as the default state\\n        # note: these are all in the simulation frame\\n        nodal_positions = self.root_physx_view.get_sim_nodal_positions()\\n        nodal_velocities = torch.zeros_like(nodal_positions)\\n        self._data.default_nodal_state_w = torch.cat((nodal_positions, nodal_velocities), dim=-1)\\n\\n        # kinematic targets\\n        self._data.nodal_kinematic_target = self.root_physx_view.get_sim_kinematic_targets()\\n        # set all nodes as non-kinematic targets by default\\n        self._data.nodal_kinematic_target[..., -1] = 1.0\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # set visibility of markers\\n        # note: parent only deals with callbacks. not their visibility\\n        if debug_vis:\\n            if not hasattr(self, \"target_visualizer\"):\\n                self.target_visualizer = VisualizationMarkers(self.cfg.visualizer_cfg)\\n            # set their visibility to true\\n            self.target_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"target_visualizer\"):\\n                self.target_visualizer.set_visibility(False)\\n\\n    def _debug_vis_callback(self, event):\\n        # check where to visualize\\n        targets_enabled = self.data.nodal_kinematic_target[:, :, 3] == 0.0\\n        num_enabled = int(torch.sum(targets_enabled).item())\\n        # get positions if any targets are enabled\\n        if num_enabled == 0:\\n            # create a marker below the ground\\n            positions = torch.tensor([[0.0, 0.0, -10.0]], device=self.device)\\n        else:\\n            positions = self.data.nodal_kinematic_target[targets_enabled][..., :3]\\n        # show target visualizer\\n        self.target_visualizer.visualize(positions)\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        self._root_physx_view = None'),\n",
       " Document(metadata={}, page_content='class DeformableObjectCfg(AssetBaseCfg):\\n    \"\"\"Configuration parameters for a deformable object.\"\"\"\\n\\n    class_type: type = DeformableObject\\n\\n    visualizer_cfg: VisualizationMarkersCfg = DEFORMABLE_TARGET_MARKER_CFG.replace(\\n        prim_path=\"/Visuals/DeformableTarget\"\\n    )\\n    \"\"\"The configuration object for the visualization markers. Defaults to DEFORMABLE_TARGET_MARKER_CFG.\\n\\n    Note:\\n        This attribute is only used when debug visualization is enabled.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class DeformableObjectData:\\n    \"\"\"Data container for a deformable object.\\n\\n    This class contains the data for a deformable object in the simulation. The data includes the nodal states of\\n    the root deformable body in the object. The data is stored in the simulation world frame unless otherwise specified.\\n\\n    A deformable object in PhysX uses two tetrahedral meshes to represent the object:\\n\\n    1. **Simulation mesh**: This mesh is used for the simulation and is the one that is deformed by the solver.\\n    2. **Collision mesh**: This mesh only needs to match the surface of the simulation mesh and is used for\\n       collision detection.\\n\\n    The APIs exposed provides the data for both the simulation and collision meshes. These are specified\\n    by the `sim` and `collision` prefixes in the property names.\\n\\n    The data is lazily updated, meaning that the data is only updated when it is accessed. This is useful\\n    when the data is expensive to compute or retrieve. The data is updated when the timestamp of the buffer\\n    is older than the current simulation timestamp. The timestamp is updated whenever the data is updated.\\n    \"\"\"\\n\\n    def __init__(self, root_physx_view: physx.SoftBodyView, device: str):\\n        \"\"\"Initializes the deformable object data.\\n\\n        Args:\\n            root_physx_view: The root deformable body view of the object.\\n            device: The device used for processing.\\n        \"\"\"\\n        # Set the parameters\\n        self.device = device\\n        # Set the root deformable body view\\n        # note: this is stored as a weak reference to avoid circular references between the asset class\\n        #  and the data container. This is important to avoid memory leaks.\\n        self._root_physx_view: physx.SoftBodyView = weakref.proxy(root_physx_view)\\n\\n        # Set initial time stamp\\n        self._sim_timestamp = 0.0\\n\\n        # Initialize the lazy buffers.\\n        # -- node state in simulation world frame\\n        self._nodal_pos_w = TimestampedBuffer()\\n        self._nodal_vel_w = TimestampedBuffer()\\n        self._nodal_state_w = TimestampedBuffer()\\n        # -- mesh element-wise rotations\\n        self._sim_element_quat_w = TimestampedBuffer()\\n        self._collision_element_quat_w = TimestampedBuffer()\\n        # -- mesh element-wise deformation gradients\\n        self._sim_element_deform_gradient_w = TimestampedBuffer()\\n        self._collision_element_deform_gradient_w = TimestampedBuffer()\\n        # -- mesh element-wise stresses\\n        self._sim_element_stress_w = TimestampedBuffer()\\n        self._collision_element_stress_w = TimestampedBuffer()\\n\\n    def update(self, dt: float):\\n        \"\"\"Updates the data for the deformable object.\\n\\n        Args:\\n            dt: The time step for the update. This must be a positive value.\\n        \"\"\"\\n        # update the simulation timestamp\\n        self._sim_timestamp += dt\\n\\n    ##\\n    # Defaults.\\n    ##\\n\\n    default_nodal_state_w: torch.Tensor = None\\n    \"\"\"Default nodal state ``[nodal_pos, nodal_vel]`` in simulation world frame.\\n    Shape is (num_instances, max_sim_vertices_per_body, 6).\\n    \"\"\"\\n\\n    ##\\n    # Kinematic commands\\n    ##\\n\\n    nodal_kinematic_target: torch.Tensor = None\\n    \"\"\"Simulation mesh kinematic targets for the deformable bodies.\\n    Shape is (num_instances, max_sim_vertices_per_body, 4).\\n\\n    The kinematic targets are used to drive the simulation mesh vertices to the target positions.\\n    The targets are stored as (x, y, z, is_not_kinematic) where \"is_not_kinematic\" is a binary\\n    flag indicating whether the vertex is kinematic or not. The flag is set to 0 for kinematic vertices\\n    and 1 for non-kinematic vertices.\\n    \"\"\"\\n\\n    ##\\n    # Properties.\\n    ##\\n\\n    @property\\n    def nodal_pos_w(self):\\n        \"\"\"Nodal positions in simulation world frame. Shape is (num_instances, max_sim_vertices_per_body, 3).\"\"\"\\n        if self._nodal_pos_w.timestamp < self._sim_timestamp:\\n            self._nodal_pos_w.data = self._root_physx_view.get_sim_nodal_positions()\\n            self._nodal_pos_w.timestamp = self._sim_timestamp\\n        return self._nodal_pos_w.data\\n\\n    @property\\n    def nodal_vel_w(self):\\n        \"\"\"Nodal velocities in simulation world frame. Shape is (num_instances, max_sim_vertices_per_body, 3).\"\"\"\\n        if self._nodal_vel_w.timestamp < self._sim_timestamp:\\n            self._nodal_vel_w.data = self._root_physx_view.get_sim_nodal_velocities()\\n            self._nodal_vel_w.timestamp = self._sim_timestamp\\n        return self._nodal_vel_w.data\\n\\n    @property\\n    def nodal_state_w(self):\\n        \"\"\"Nodal state ``[nodal_pos, nodal_vel]`` in simulation world frame.\\n        Shape is (num_instances, max_sim_vertices_per_body, 6).\\n        \"\"\"\\n        if self._nodal_state_w.timestamp < self._sim_timestamp:\\n            nodal_positions = self.nodal_pos_w\\n            nodal_velocities = self.nodal_vel_w\\n            # set the buffer data and timestamp\\n            self._nodal_state_w.data = torch.cat((nodal_positions, nodal_velocities), dim=-1)\\n            self._nodal_state_w.timestamp = self._sim_timestamp\\n        return self._nodal_state_w.data\\n\\n    @property\\n    def sim_element_quat_w(self):\\n        \"\"\"Simulation mesh element-wise rotations as quaternions for the deformable bodies in simulation world frame.\\n        Shape is (num_instances, max_sim_elements_per_body, 4).\\n\\n        The rotations are stored as quaternions in the order (w, x, y, z).\\n        \"\"\"\\n        if self._sim_element_quat_w.timestamp < self._sim_timestamp:\\n            # convert from xyzw to wxyz\\n            quats = self._root_physx_view.get_sim_element_rotations().view(self._root_physx_view.count, -1, 4)\\n            quats = math_utils.convert_quat(quats, to=\"wxyz\")\\n            # set the buffer data and timestamp\\n            self._sim_element_quat_w.data = quats\\n            self._sim_element_quat_w.timestamp = self._sim_timestamp\\n        return self._sim_element_quat_w.data\\n\\n    @property\\n    def collision_element_quat_w(self):\\n        \"\"\"Collision mesh element-wise rotations as quaternions for the deformable bodies in simulation world frame.\\n        Shape is (num_instances, max_collision_elements_per_body, 4).\\n\\n        The rotations are stored as quaternions in the order (w, x, y, z).\\n        \"\"\"\\n        if self._collision_element_quat_w.timestamp < self._sim_timestamp:\\n            # convert from xyzw to wxyz\\n            quats = self._root_physx_view.get_element_rotations().view(self._root_physx_view.count, -1, 4)\\n            quats = math_utils.convert_quat(quats, to=\"wxyz\")\\n            # set the buffer data and timestamp\\n            self._collision_element_quat_w.data = quats\\n            self._collision_element_quat_w.timestamp = self._sim_timestamp\\n        return self._collision_element_quat_w.data\\n\\n    @property\\n    def sim_element_deform_gradient_w(self):\\n        \"\"\"Simulation mesh element-wise second-order deformation gradient tensors for the deformable bodies\\n        in simulation world frame. Shape is (num_instances, max_sim_elements_per_body, 3, 3).\\n        \"\"\"\\n        if self._sim_element_deform_gradient_w.timestamp < self._sim_timestamp:\\n            # set the buffer data and timestamp\\n            self._sim_element_deform_gradient_w.data = (\\n                self._root_physx_view.get_sim_element_deformation_gradients().view(\\n                    self._root_physx_view.count, -1, 3, 3\\n                )\\n            )\\n            self._sim_element_deform_gradient_w.timestamp = self._sim_timestamp\\n        return self._sim_element_deform_gradient_w.data\\n\\n    @property\\n    def collision_element_deform_gradient_w(self):\\n        \"\"\"Collision mesh element-wise second-order deformation gradient tensors for the deformable bodies\\n        in simulation world frame. Shape is (num_instances, max_collision_elements_per_body, 3, 3).\\n        \"\"\"\\n        if self._collision_element_deform_gradient_w.timestamp < self._sim_timestamp:\\n            # set the buffer data and timestamp\\n            self._collision_element_deform_gradient_w.data = (\\n                self._root_physx_view.get_element_deformation_gradients().view(self._root_physx_view.count, -1, 3, 3)\\n            )\\n            self._collision_element_deform_gradient_w.timestamp = self._sim_timestamp\\n        return self._collision_element_deform_gradient_w.data\\n\\n    @property\\n    def sim_element_stress_w(self):\\n        \"\"\"Simulation mesh element-wise second-order Cauchy stress tensors for the deformable bodies\\n        in simulation world frame. Shape is (num_instances, max_sim_elements_per_body, 3, 3).\\n        \"\"\"\\n        if self._sim_element_stress_w.timestamp < self._sim_timestamp:\\n            # set the buffer data and timestamp\\n            self._sim_element_stress_w.data = self._root_physx_view.get_sim_element_stresses().view(\\n                self._root_physx_view.count, -1, 3, 3\\n            )\\n            self._sim_element_stress_w.timestamp = self._sim_timestamp\\n        return self._sim_element_stress_w.data\\n\\n    @property\\n    def collision_element_stress_w(self):\\n        \"\"\"Collision mesh element-wise second-order Cauchy stress tensors for the deformable bodies\\n        in simulation world frame. Shape is (num_instances, max_collision_elements_per_body, 3, 3).\\n        \"\"\"\\n        if self._collision_element_stress_w.timestamp < self._sim_timestamp:\\n            # set the buffer data and timestamp\\n            self._collision_element_stress_w.data = self._root_physx_view.get_element_stresses().view(\\n                self._root_physx_view.count, -1, 3, 3\\n            )\\n            self._collision_element_stress_w.timestamp = self._sim_timestamp\\n        return self._collision_element_stress_w.data\\n\\n    ##\\n    # Derived properties.\\n    ##\\n\\n    @property\\n    def root_pos_w(self) -> torch.Tensor:\\n        \"\"\"Root position from nodal positions of the simulation mesh for the deformable bodies in simulation world frame.\\n        Shape is (num_instances, 3).\\n\\n        This quantity is computed as the mean of the nodal positions.\\n        \"\"\"\\n        return self.nodal_pos_w.mean(dim=1)\\n\\n    @property\\n    def root_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root velocity from vertex velocities for the deformable bodies in simulation world frame.\\n        Shape is (num_instances, 3).\\n\\n        This quantity is computed as the mean of the nodal velocities.\\n        \"\"\"\\n        return self.nodal_vel_w.mean(dim=1)'),\n",
       " Document(metadata={}, page_content='class RigidObject(AssetBase):\\n    \"\"\"A rigid object asset class.\\n\\n    Rigid objects are assets comprising of rigid bodies. They can be used to represent dynamic objects\\n    such as boxes, spheres, etc. A rigid body is described by its pose, velocity and mass distribution.\\n\\n    For an asset to be considered a rigid object, the root prim of the asset must have the `USD RigidBodyAPI`_\\n    applied to it. This API is used to define the simulation properties of the rigid body. On playing the\\n    simulation, the physics engine will automatically register the rigid body and create a corresponding\\n    rigid body handle. This handle can be accessed using the :attr:`root_physx_view` attribute.\\n\\n    .. note::\\n\\n        For users familiar with Isaac Sim, the PhysX view class API is not the exactly same as Isaac Sim view\\n        class API. Similar to Isaac Lab, Isaac Sim wraps around the PhysX view API. However, as of now (2023.1 release),\\n        we see a large difference in initializing the view classes in Isaac Sim. This is because the view classes\\n        in Isaac Sim perform additional USD-related operations which are slow and also not required.\\n\\n    .. _`USD RigidBodyAPI`: https://openusd.org/dev/api/class_usd_physics_rigid_body_a_p_i.html\\n    \"\"\"\\n\\n    cfg: RigidObjectCfg\\n    \"\"\"Configuration instance for the rigid object.\"\"\"\\n\\n    def __init__(self, cfg: RigidObjectCfg):\\n        \"\"\"Initialize the rigid object.\\n\\n        Args:\\n            cfg: A configuration instance.\\n        \"\"\"\\n        super().__init__(cfg)\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def data(self) -> RigidObjectData:\\n        return self._data\\n\\n    @property\\n    def num_instances(self) -> int:\\n        return self.root_physx_view.count\\n\\n    @property\\n    def num_bodies(self) -> int:\\n        \"\"\"Number of bodies in the asset.\\n\\n        This is always 1 since each object is a single rigid body.\\n        \"\"\"\\n        return 1\\n\\n    @property\\n    def body_names(self) -> list[str]:\\n        \"\"\"Ordered names of bodies in the rigid object.\"\"\"\\n        prim_paths = self.root_physx_view.prim_paths[: self.num_bodies]\\n        return [path.split(\"/\")[-1] for path in prim_paths]\\n\\n    @property\\n    def root_physx_view(self) -> physx.RigidBodyView:\\n        \"\"\"Rigid body view for the asset (PhysX).\\n\\n        Note:\\n            Use this view with caution. It requires handling of tensors in a specific way.\\n        \"\"\"\\n        return self._root_physx_view\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # resolve all indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset external wrench\\n        self._external_force_b[env_ids] = 0.0\\n        self._external_torque_b[env_ids] = 0.0\\n\\n    def write_data_to_sim(self):\\n        \"\"\"Write external wrench to the simulation.\\n\\n        Note:\\n            We write external wrench to the simulation here since this function is called before the simulation step.\\n            This ensures that the external wrench is applied at every simulation step.\\n        \"\"\"\\n        # write external wrench\\n        if self.has_external_wrench:\\n            self.root_physx_view.apply_forces_and_torques_at_position(\\n                force_data=self._external_force_b.view(-1, 3),\\n                torque_data=self._external_torque_b.view(-1, 3),\\n                position_data=None,\\n                indices=self._ALL_INDICES,\\n                is_global=False,\\n            )\\n\\n    def update(self, dt: float):\\n        self._data.update(dt)\\n\\n    \"\"\"\\n    Operations - Finders.\\n    \"\"\"\\n\\n    def find_bodies(self, name_keys: str | Sequence[str], preserve_order: bool = False) -> tuple[list[int], list[str]]:\\n        \"\"\"Find bodies in the rigid body based on the name keys.\\n\\n        Please check the :meth:`isaaclab.utils.string_utils.resolve_matching_names` function for more\\n        information on the name matching.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the body names.\\n            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.\\n\\n        Returns:\\n            A tuple of lists containing the body indices and names.\\n        \"\"\"\\n        return string_utils.resolve_matching_names(name_keys, self.body_names, preserve_order)\\n\\n    \"\"\"\\n    Operations - Write to simulation.\\n    \"\"\"\\n\\n    def write_root_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root state over selected environment indices into the simulation.\\n\\n        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n\\n        # set into simulation\\n        self.write_root_pose_to_sim(root_state[:, :7], env_ids=env_ids)\\n        self.write_root_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)\\n\\n    def write_root_com_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass state over selected environment indices into the simulation.\\n\\n        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # set into simulation\\n        self.write_root_com_pose_to_sim(root_state[:, :7], env_ids=env_ids)\\n        self.write_root_com_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)\\n\\n    def write_root_link_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root link state over selected environment indices into the simulation.\\n\\n        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # set into simulation\\n        self.write_root_link_pose_to_sim(root_state[:, :7], env_ids=env_ids)\\n        self.write_root_link_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)\\n\\n    def write_root_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root pose over selected environment indices into the simulation.\\n\\n        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n\\n        Args:\\n            root_pose: Root poses in simulation frame. Shape is (len(env_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_state_w[env_ids, :7] = root_pose.clone()\\n        # convert root quaternion from wxyz to xyzw\\n        root_poses_xyzw = self._data.root_state_w[:, :7].clone()\\n        root_poses_xyzw[:, 3:] = math_utils.convert_quat(root_poses_xyzw[:, 3:], to=\"xyzw\")\\n        # set into simulation\\n        self.root_physx_view.set_transforms(root_poses_xyzw, indices=physx_env_ids)\\n\\n    def write_root_link_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root link pose over selected environment indices into the simulation.\\n\\n        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n\\n        Args:\\n            root_pose: Root poses in simulation frame. Shape is (len(env_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_link_state_w[env_ids, :7] = root_pose.clone()\\n        self._data.root_state_w[env_ids, :7] = self._data.root_link_state_w[env_ids, :7]\\n        # convert root quaternion from wxyz to xyzw\\n        root_poses_xyzw = self._data.root_link_state_w[:, :7].clone()\\n        root_poses_xyzw[:, 3:] = math_utils.convert_quat(root_poses_xyzw[:, 3:], to=\"xyzw\")\\n        # set into simulation\\n        self.root_physx_view.set_transforms(root_poses_xyzw, indices=physx_env_ids)\\n\\n    def write_root_com_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass pose over selected environment indices into the simulation.\\n\\n        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n        The orientation is the orientation of the principle axes of inertia.\\n\\n        Args:\\n            root_pose: Root center of mass poses in simulation frame. Shape is (len(env_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        if env_ids is None:\\n            local_env_ids = slice(env_ids)\\n        else:\\n            local_env_ids = env_ids\\n\\n        com_pos = self.data.com_pos_b[local_env_ids, 0, :]\\n        com_quat = self.data.com_quat_b[local_env_ids, 0, :]\\n\\n        root_link_pos, root_link_quat = math_utils.combine_frame_transforms(\\n            root_pose[..., :3],\\n            root_pose[..., 3:7],\\n            math_utils.quat_apply(math_utils.quat_inv(com_quat), -com_pos),\\n            math_utils.quat_inv(com_quat),\\n        )\\n\\n        root_link_pose = torch.cat((root_link_pos, root_link_quat), dim=-1)\\n        self.write_root_link_pose_to_sim(root_pose=root_link_pose, env_ids=env_ids)\\n\\n    def write_root_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass velocity over selected environment indices into the simulation.\\n\\n        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.\\n        NOTE: This sets the velocity of the root\\'s center of mass rather than the roots frame.\\n\\n        Args:\\n            root_velocity: Root center of mass velocities in simulation world frame. Shape is (len(env_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_state_w[env_ids, 7:] = root_velocity.clone()\\n        self._data.body_acc_w[env_ids] = 0.0\\n        # set into simulation\\n        self.root_physx_view.set_velocities(self._data.root_state_w[:, 7:], indices=physx_env_ids)\\n\\n    def write_root_com_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root center of mass velocity over selected environment indices into the simulation.\\n\\n        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.\\n        NOTE: This sets the velocity of the root\\'s center of mass rather than the roots frame.\\n\\n        Args:\\n            root_velocity: Root center of mass velocities in simulation world frame. Shape is (len(env_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n\\n        # resolve all indices\\n        physx_env_ids = env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n            physx_env_ids = self._ALL_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.root_com_state_w[env_ids, 7:] = root_velocity.clone()\\n        self._data.root_state_w[env_ids, 7:] = self._data.root_com_state_w[env_ids, 7:]\\n        self._data.body_acc_w[env_ids] = 0.0\\n        # set into simulation\\n        self.root_physx_view.set_velocities(self._data.root_com_state_w[:, 7:], indices=physx_env_ids)\\n\\n    def write_root_link_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):\\n        \"\"\"Set the root link velocity over selected environment indices into the simulation.\\n\\n        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.\\n        NOTE: This sets the velocity of the root\\'s frame rather than the roots center of mass.\\n\\n        Args:\\n            root_velocity: Root frame velocities in simulation world frame. Shape is (len(env_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        if env_ids is None:\\n            local_env_ids = slice(env_ids)\\n        else:\\n            local_env_ids = env_ids\\n\\n        root_com_velocity = root_velocity.clone()\\n        quat = self.data.root_link_state_w[local_env_ids, 3:7]\\n        com_pos_b = self.data.com_pos_b[local_env_ids, 0, :]\\n        # transform given velocity to center of mass\\n        root_com_velocity[:, :3] += torch.linalg.cross(\\n            root_com_velocity[:, 3:], math_utils.quat_apply(quat, com_pos_b), dim=-1\\n        )\\n        # write center of mass velocity to sim\\n        self.write_root_com_velocity_to_sim(root_velocity=root_com_velocity, env_ids=env_ids)\\n\\n    \"\"\"\\n    Operations - Setters.\\n    \"\"\"\\n\\n    def set_external_force_and_torque(\\n        self,\\n        forces: torch.Tensor,\\n        torques: torch.Tensor,\\n        body_ids: Sequence[int] | slice | None = None,\\n        env_ids: Sequence[int] | None = None,\\n    ):\\n        \"\"\"Set external force and torque to apply on the asset\\'s bodies in their local frame.\\n\\n        For many applications, we want to keep the applied external force on rigid bodies constant over a period of\\n        time (for instance, during the policy control). This function allows us to store the external force and torque\\n        into buffers which are then applied to the simulation at every step.\\n\\n        .. caution::\\n            If the function is called with empty forces and torques, then this function disables the application\\n            of external wrench to the simulation.\\n\\n            .. code-block:: python\\n\\n                # example of disabling external wrench\\n                asset.set_external_force_and_torque(forces=torch.zeros(0, 3), torques=torch.zeros(0, 3))\\n\\n        .. note::\\n            This function does not apply the external wrench to the simulation. It only fills the buffers with\\n            the desired values. To apply the external wrench, call the :meth:`write_data_to_sim` function\\n            right before the simulation step.\\n\\n        Args:\\n            forces: External forces in bodies\\' local frame. Shape is (len(env_ids), len(body_ids), 3).\\n            torques: External torques in bodies\\' local frame. Shape is (len(env_ids), len(body_ids), 3).\\n            body_ids: Body indices to apply external wrench to. Defaults to None (all bodies).\\n            env_ids: Environment indices to apply external wrench to. Defaults to None (all instances).\\n        \"\"\"\\n        if forces.any() or torques.any():\\n            self.has_external_wrench = True\\n        else:\\n            self.has_external_wrench = False\\n            # to be safe, explicitly set value to zero\\n            forces = torques = 0.0\\n\\n        # resolve all indices\\n        # -- env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # -- body_ids\\n        if body_ids is None:\\n            body_ids = slice(None)\\n        # broadcast env_ids if needed to allow double indexing\\n        if env_ids != slice(None) and body_ids != slice(None):\\n            env_ids = env_ids[:, None]\\n        # set into internal buffers\\n        self._external_force_b[env_ids, body_ids] = forces\\n        self._external_torque_b[env_ids, body_ids] = torques\\n\\n    \"\"\"\\n    Internal helper.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        # obtain the first prim in the regex expression (all others are assumed to be a copy of this)\\n        template_prim = sim_utils.find_first_matching_prim(self.cfg.prim_path)\\n        if template_prim is None:\\n            raise RuntimeError(f\"Failed to find prim for expression: \\'{self.cfg.prim_path}\\'.\")\\n        template_prim_path = template_prim.GetPath().pathString\\n\\n        # find rigid root prims\\n        root_prims = sim_utils.get_all_matching_child_prims(\\n            template_prim_path, predicate=lambda prim: prim.HasAPI(UsdPhysics.RigidBodyAPI)\\n        )\\n        if len(root_prims) == 0:\\n            raise RuntimeError(\\n                f\"Failed to find a rigid body when resolving \\'{self.cfg.prim_path}\\'.\"\\n                \" Please ensure that the prim has \\'USD RigidBodyAPI\\' applied.\"\\n            )\\n        if len(root_prims) > 1:\\n            raise RuntimeError(\\n                f\"Failed to find a single rigid body when resolving \\'{self.cfg.prim_path}\\'.\"\\n                f\" Found multiple \\'{root_prims}\\' under \\'{template_prim_path}\\'.\"\\n                \" Please ensure that there is only one rigid body in the prim path tree.\"\\n            )\\n\\n        articulation_prims = sim_utils.get_all_matching_child_prims(\\n            template_prim_path, predicate=lambda prim: prim.HasAPI(UsdPhysics.ArticulationRootAPI)\\n        )\\n        if len(articulation_prims) != 0:\\n            if articulation_prims[0].GetAttribute(\"physxArticulation:articulationEnabled\").Get():\\n                raise RuntimeError(\\n                    f\"Found an articulation root when resolving \\'{self.cfg.prim_path}\\' for rigid objects. These are\"\\n                    f\" located at: \\'{articulation_prims}\\' under \\'{template_prim_path}\\'. Please disable the articulation\"\\n                    \" root in the USD or from code by setting the parameter\"\\n                    \" \\'ArticulationRootPropertiesCfg.articulation_enabled\\' to False in the spawn configuration.\"\\n                )\\n\\n        # resolve root prim back into regex expression\\n        root_prim_path = root_prims[0].GetPath().pathString\\n        root_prim_path_expr = self.cfg.prim_path + root_prim_path[len(template_prim_path) :]\\n        # -- object view\\n        self._root_physx_view = self._physics_sim_view.create_rigid_body_view(root_prim_path_expr.replace(\".*\", \"*\"))\\n\\n        # check if the rigid body was created\\n        if self._root_physx_view._backend is None:\\n            raise RuntimeError(f\"Failed to create rigid body at: {self.cfg.prim_path}. Please check PhysX logs.\")\\n\\n        # log information about the rigid body\\n        omni.log.info(f\"Rigid body initialized at: {self.cfg.prim_path} with root \\'{root_prim_path_expr}\\'.\")\\n        omni.log.info(f\"Number of instances: {self.num_instances}\")\\n        omni.log.info(f\"Number of bodies: {self.num_bodies}\")\\n        omni.log.info(f\"Body names: {self.body_names}\")\\n\\n        # container for data access\\n        self._data = RigidObjectData(self.root_physx_view, self.device)\\n\\n        # create buffers\\n        self._create_buffers()\\n        # process configuration\\n        self._process_cfg()\\n        # update the rigid body data\\n        self.update(0.0)\\n\\n    def _create_buffers(self):\\n        \"\"\"Create buffers for storing data.\"\"\"\\n        # constants\\n        self._ALL_INDICES = torch.arange(self.num_instances, dtype=torch.long, device=self.device)\\n\\n        # external forces and torques\\n        self.has_external_wrench = False\\n        self._external_force_b = torch.zeros((self.num_instances, self.num_bodies, 3), device=self.device)\\n        self._external_torque_b = torch.zeros_like(self._external_force_b)\\n\\n        # set information about rigid body into data\\n        self._data.body_names = self.body_names\\n        self._data.default_mass = self.root_physx_view.get_masses().clone()\\n        self._data.default_inertia = self.root_physx_view.get_inertias().clone()\\n\\n    def _process_cfg(self):\\n        \"\"\"Post processing of configuration parameters.\"\"\"\\n        # default state\\n        # -- root state\\n        # note: we cast to tuple to avoid torch/numpy type mismatch.\\n        default_root_state = (\\n            tuple(self.cfg.init_state.pos)\\n            + tuple(self.cfg.init_state.rot)\\n            + tuple(self.cfg.init_state.lin_vel)\\n            + tuple(self.cfg.init_state.ang_vel)\\n        )\\n        default_root_state = torch.tensor(default_root_state, dtype=torch.float, device=self.device)\\n        self._data.default_root_state = default_root_state.repeat(self.num_instances, 1)\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        # set all existing views to None to invalidate them\\n        self._root_physx_view = None'),\n",
       " Document(metadata={}, page_content='class RigidObjectCfg(AssetBaseCfg):\\n    \"\"\"Configuration parameters for a rigid object.\"\"\"\\n\\n    @configclass\\n    class InitialStateCfg(AssetBaseCfg.InitialStateCfg):\\n        \"\"\"Initial state of the rigid body.\"\"\"\\n\\n        lin_vel: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Linear velocity of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n        ang_vel: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Angular velocity of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n\\n    ##\\n    # Initialize configurations.\\n    ##\\n\\n    class_type: type = RigidObject\\n\\n    init_state: InitialStateCfg = InitialStateCfg()\\n    \"\"\"Initial state of the rigid object. Defaults to identity pose with zero velocity.\"\"\"'),\n",
       " Document(metadata={}, page_content='class RigidObjectData:\\n    \"\"\"Data container for a rigid object.\\n\\n    This class contains the data for a rigid object in the simulation. The data includes the state of\\n    the root rigid body and the state of all the bodies in the object. The data is stored in the simulation\\n    world frame unless otherwise specified.\\n\\n    For a rigid body, there are two frames of reference that are used:\\n\\n    - Actor frame: The frame of reference of the rigid body prim. This typically corresponds to the Xform prim\\n      with the rigid body schema.\\n    - Center of mass frame: The frame of reference of the center of mass of the rigid body.\\n\\n    Depending on the settings of the simulation, the actor frame and the center of mass frame may be the same.\\n    This needs to be taken into account when interpreting the data.\\n\\n    The data is lazily updated, meaning that the data is only updated when it is accessed. This is useful\\n    when the data is expensive to compute or retrieve. The data is updated when the timestamp of the buffer\\n    is older than the current simulation timestamp. The timestamp is updated whenever the data is updated.\\n    \"\"\"\\n\\n    def __init__(self, root_physx_view: physx.RigidBodyView, device: str):\\n        \"\"\"Initializes the rigid object data.\\n\\n        Args:\\n            root_physx_view: The root rigid body view.\\n            device: The device used for processing.\\n        \"\"\"\\n        # Set the parameters\\n        self.device = device\\n        # Set the root rigid body view\\n        # note: this is stored as a weak reference to avoid circular references between the asset class\\n        #  and the data container. This is important to avoid memory leaks.\\n        self._root_physx_view: physx.RigidBodyView = weakref.proxy(root_physx_view)\\n\\n        # Set initial time stamp\\n        self._sim_timestamp = 0.0\\n\\n        # Obtain global physics sim view\\n        physics_sim_view = physx.create_simulation_view(\"torch\")\\n        physics_sim_view.set_subspace_roots(\"/\")\\n        gravity = physics_sim_view.get_gravity()\\n        # Convert to direction vector\\n        gravity_dir = torch.tensor((gravity[0], gravity[1], gravity[2]), device=self.device)\\n        gravity_dir = math_utils.normalize(gravity_dir.unsqueeze(0)).squeeze(0)\\n\\n        # Initialize constants\\n        self.GRAVITY_VEC_W = gravity_dir.repeat(self._root_physx_view.count, 1)\\n        self.FORWARD_VEC_B = torch.tensor((1.0, 0.0, 0.0), device=self.device).repeat(self._root_physx_view.count, 1)\\n\\n        # Initialize the lazy buffers.\\n        self._root_state_w = TimestampedBuffer()\\n        self._root_link_state_w = TimestampedBuffer()\\n        self._root_com_state_w = TimestampedBuffer()\\n        self._body_acc_w = TimestampedBuffer()\\n\\n    def update(self, dt: float):\\n        \"\"\"Updates the data for the rigid object.\\n\\n        Args:\\n            dt: The time step for the update. This must be a positive value.\\n        \"\"\"\\n        # update the simulation timestamp\\n        self._sim_timestamp += dt\\n\\n    ##\\n    # Names.\\n    ##\\n\\n    body_names: list[str] = None\\n    \"\"\"Body names in the order parsed by the simulation view.\"\"\"\\n\\n    ##\\n    # Defaults.\\n    ##\\n\\n    default_root_state: torch.Tensor = None\\n    \"\"\"Default root state ``[pos, quat, lin_vel, ang_vel]`` in local environment frame. Shape is (num_instances, 13).\\n\\n    The position and quaternion are of the rigid body\\'s actor frame. Meanwhile, the linear and angular velocities are\\n    of the center of mass frame.\\n    \"\"\"\\n\\n    default_mass: torch.Tensor = None\\n    \"\"\"Default mass read from the simulation. Shape is (num_instances, 1).\"\"\"\\n\\n    default_inertia: torch.Tensor = None\\n    \"\"\"Default inertia tensor read from the simulation. Shape is (num_instances, 9).\\n\\n    The inertia is the inertia tensor relative to the center of mass frame. The values are stored in\\n    the order :math:`[I_{xx}, I_{xy}, I_{xz}, I_{yx}, I_{yy}, I_{yz}, I_{zx}, I_{zy}, I_{zz}]`.\\n    \"\"\"\\n\\n    ##\\n    # Properties.\\n    ##\\n\\n    @property\\n    def root_state_w(self):\\n        \"\"\"Root state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame. Shape is (num_instances, 13).\\n\\n        The position and orientation are of the rigid body\\'s actor frame. Meanwhile, the linear and angular\\n        velocities are of the rigid body\\'s center of mass frame.\\n        \"\"\"\\n\\n        if self._root_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._root_physx_view.get_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_velocities()\\n            # set the buffer data and timestamp\\n            self._root_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._root_state_w.timestamp = self._sim_timestamp\\n        return self._root_state_w.data\\n\\n    @property\\n    def root_link_state_w(self):\\n        \"\"\"Root state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame. Shape is (num_instances, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the rigid body root frame relative to the\\n        world.\\n        \"\"\"\\n        if self._root_link_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._root_physx_view.get_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_velocities().clone()\\n\\n            # adjust linear velocity to link from center of mass\\n            velocity[:, :3] += torch.linalg.cross(\\n                velocity[:, 3:], math_utils.quat_apply(pose[:, 3:7], -self.com_pos_b[:, 0, :]), dim=-1\\n            )\\n            # set the buffer data and timestamp\\n            self._root_link_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._root_link_state_w.timestamp = self._sim_timestamp\\n\\n        return self._root_link_state_w.data\\n\\n    @property\\n    def root_com_state_w(self):\\n        \"\"\"Root center of mass state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame. Shape is (num_instances, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the rigid body\\'s center of mass frame\\n        relative to the world. Center of mass frame is the orientation principle axes of inertia.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation (pose is of link)\\n            pose = self._root_physx_view.get_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            velocity = self._root_physx_view.get_velocities()\\n\\n            # adjust pose to center of mass\\n            pos, quat = math_utils.combine_frame_transforms(\\n                pose[:, :3], pose[:, 3:7], self.com_pos_b[:, 0, :], self.com_quat_b[:, 0, :]\\n            )\\n            pose = torch.cat((pos, quat), dim=-1)\\n            # set the buffer data and timestamp\\n            self._root_com_state_w.data = torch.cat((pos, quat, velocity), dim=-1)\\n            self._root_com_state_w.timestamp = self._sim_timestamp\\n        return self._root_com_state_w.data\\n\\n    @property\\n    def body_state_w(self):\\n        \"\"\"State of all bodies `[pos, quat, lin_vel, ang_vel]` in simulation world frame. Shape is (num_instances, 1, 13).\\n\\n        The position and orientation are of the rigid bodies\\' actor frame. Meanwhile, the linear and angular\\n        velocities are of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n\\n        return self.root_state_w.view(-1, 1, 13)\\n\\n    @property\\n    def body_link_state_w(self):\\n        \"\"\"State of all bodies `[pos, quat, lin_vel, ang_vel]` in simulation world frame.\\n        Shape is (num_instances, 1, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the body\\'s link frame relative to the world.\\n        \"\"\"\\n        return self.root_link_state_w.view(-1, 1, 13)\\n\\n    @property\\n    def body_com_state_w(self):\\n        \"\"\"State of all bodies `[pos, quat, lin_vel, ang_vel]` in simulation world frame.\\n        Shape is (num_instances, num_bodies, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the body\\'s center of mass frame relative to the\\n        world. Center of mass frame is assumed to be the same orientation as the link rather than the orientation of the\\n        principle inertia.\\n        \"\"\"\\n        return self.root_com_state_w.view(-1, 1, 13)\\n\\n    @property\\n    def body_acc_w(self):\\n        \"\"\"Acceleration of all bodies. Shape is (num_instances, 1, 6).\\n\\n        This quantity is the acceleration of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._body_acc_w.timestamp < self._sim_timestamp:\\n            # note: we use finite differencing to compute acceleration\\n            self._body_acc_w.data = self._root_physx_view.get_accelerations().unsqueeze(1)\\n            self._body_acc_w.timestamp = self._sim_timestamp\\n        return self._body_acc_w.data\\n\\n    @property\\n    def projected_gravity_b(self):\\n        \"\"\"Projection of the gravity direction on base frame. Shape is (num_instances, 3).\"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.GRAVITY_VEC_W)\\n\\n    @property\\n    def heading_w(self):\\n        \"\"\"Yaw heading of the base frame (in radians). Shape is (num_instances,).\\n\\n        Note:\\n            This quantity is computed by assuming that the forward-direction of the base\\n            frame is along x-direction, i.e. :math:`(1, 0, 0)`.\\n        \"\"\"\\n        forward_w = math_utils.quat_apply(self.root_link_quat_w, self.FORWARD_VEC_B)\\n        return torch.atan2(forward_w[:, 1], forward_w[:, 0])\\n\\n    ##\\n    # Derived properties.\\n    ##\\n\\n    @property\\n    def root_pos_w(self) -> torch.Tensor:\\n        \"\"\"Root position in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the position of the actor frame of the root rigid body.\\n        \"\"\"\\n        return self.root_state_w[:, :3]\\n\\n    @property\\n    def root_quat_w(self) -> torch.Tensor:\\n        \"\"\"Root orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, 4).\\n\\n        This quantity is the orientation of the actor frame of the root rigid body.\\n        \"\"\"\\n        return self.root_state_w[:, 3:7]\\n\\n    @property\\n    def root_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root velocity in simulation world frame. Shape is (num_instances, 6).\\n\\n        This quantity contains the linear and angular velocities of the root rigid body\\'s center of mass frame.\\n        \"\"\"\\n        return self.root_state_w[:, 7:13]\\n\\n    @property\\n    def root_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root linear velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        return self.root_state_w[:, 7:10]\\n\\n    @property\\n    def root_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root angular velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the root rigid body\\'s center of mass frame.\\n        \"\"\"\\n        return self.root_state_w[:, 10:13]\\n\\n    @property\\n    def root_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root linear velocity in base frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_lin_vel_w)\\n\\n    @property\\n    def root_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root angular velocity in base world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the root rigid body\\'s center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_ang_vel_w)\\n\\n    @property\\n    def root_link_pos_w(self) -> torch.Tensor:\\n        \"\"\"Root link position in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the position of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        if self._root_link_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._root_physx_view.get_transforms()\\n            return pose[:, :3]\\n        return self.root_link_state_w[:, :3]\\n\\n    @property\\n    def root_link_quat_w(self) -> torch.Tensor:\\n        \"\"\"Root link orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, 4).\\n\\n        This quantity is the orientation of the actor frame of the root rigid body.\\n        \"\"\"\\n        if self._root_link_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._root_physx_view.get_transforms().clone()\\n            pose[:, 3:7] = math_utils.convert_quat(pose[:, 3:7], to=\"wxyz\")\\n            return pose[:, 3:7]\\n        return self.root_link_state_w[:, 3:7]\\n\\n    @property\\n    def root_link_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root link velocity in simulation world frame. Shape is (num_instances, 6).\\n\\n        This quantity contains the linear and angular velocities of the actor frame of the root\\n        rigid body relative to the world.\\n        \"\"\"\\n        return self.root_link_state_w[:, 7:13]\\n\\n    @property\\n    def root_link_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root linear velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s actor frame relative to the world.\\n        \"\"\"\\n        return self.root_link_state_w[:, 7:10]\\n\\n    @property\\n    def root_link_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root link angular velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        return self.root_link_state_w[:, 10:13]\\n\\n    @property\\n    def root_link_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root link linear velocity in base frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the actor frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_link_lin_vel_w)\\n\\n    @property\\n    def root_link_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root link angular velocity in base world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the actor frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_link_ang_vel_w)\\n\\n    @property\\n    def root_com_pos_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass position in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the position of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        return self.root_com_state_w[:, :3]\\n\\n    @property\\n    def root_com_quat_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, 4).\\n\\n        This quantity is the orientation of the actor frame of the root rigid body relative to the world.\\n        \"\"\"\\n        return self.root_com_state_w[:, 3:7]\\n\\n    @property\\n    def root_com_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass velocity in simulation world frame. Shape is (num_instances, 6).\\n\\n        This quantity contains the linear and angular velocities of the root rigid body\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            velocity = self._root_physx_view.get_velocities()\\n            return velocity\\n        return self.root_com_state_w[:, 7:13]\\n\\n    @property\\n    def root_com_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass linear velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            velocity = self._root_physx_view.get_velocities()\\n            return velocity[:, 0:3]\\n        return self.root_com_state_w[:, 7:10]\\n\\n    @property\\n    def root_com_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Root center of mass angular velocity in simulation world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the root rigid body\\'s center of mass frame relative to the world.\\n        \"\"\"\\n        if self._root_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            velocity = self._root_physx_view.get_velocities()\\n            return velocity[:, 3:6]\\n        return self.root_com_state_w[:, 10:13]\\n\\n    @property\\n    def root_com_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root center of mass linear velocity in base frame. Shape is (num_instances, 3).\\n\\n        This quantity is the linear velocity of the root rigid body\\'s center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_com_lin_vel_w)\\n\\n    @property\\n    def root_com_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Root center of mass angular velocity in base world frame. Shape is (num_instances, 3).\\n\\n        This quantity is the angular velocity of the root rigid body\\'s center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.root_link_quat_w, self.root_com_ang_vel_w)\\n\\n    @property\\n    def body_pos_w(self) -> torch.Tensor:\\n        \"\"\"Positions of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the position of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.body_state_w[..., :3]\\n\\n    @property\\n    def body_quat_w(self) -> torch.Tensor:\\n        \"\"\"Orientation (w, x, y, z) of all bodies in simulation world frame. Shape is (num_instances, 1, 4).\\n\\n        This quantity is the orientation of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.body_state_w[..., 3:7]\\n\\n    @property\\n    def body_vel_w(self) -> torch.Tensor:\\n        \"\"\"Velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_state_w[..., 7:13]\\n\\n    @property\\n    def body_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Linear velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_state_w[..., 7:10]\\n\\n    @property\\n    def body_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Angular velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_state_w[..., 10:13]\\n\\n    @property\\n    def body_lin_acc_w(self) -> torch.Tensor:\\n        \"\"\"Linear acceleration of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the linear acceleration of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_acc_w[..., 0:3]\\n\\n    @property\\n    def body_ang_acc_w(self) -> torch.Tensor:\\n        \"\"\"Angular acceleration of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the angular acceleration of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_acc_w[..., 3:6]\\n\\n    #\\n    # Link body properties\\n    #\\n\\n    @property\\n    def body_link_pos_w(self) -> torch.Tensor:\\n        \"\"\"Positions of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the position of the rigid bodies\\' actor frame relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., :3]\\n\\n    @property\\n    def body_link_quat_w(self) -> torch.Tensor:\\n        \"\"\"Orientation (w, x, y, z) of all bodies in simulation world frame. Shape is (num_instances, 1, 4).\\n\\n        This quantity is the orientation of the rigid bodies\\' actor frame  relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., 3:7]\\n\\n    @property\\n    def body_link_vel_w(self) -> torch.Tensor:\\n        \"\"\"Velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame\\n        relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., 7:13]\\n\\n    @property\\n    def body_link_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Linear velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., 7:10]\\n\\n    @property\\n    def body_link_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Angular velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame relative to the world.\\n        \"\"\"\\n        return self.body_link_state_w[..., 10:13]\\n\\n    #\\n    # Center of mass body properties\\n    #\\n\\n    @property\\n    def body_com_pos_w(self) -> torch.Tensor:\\n        \"\"\"Positions of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the position of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.body_com_state_w[..., :3]\\n\\n    @property\\n    def body_com_quat_w(self) -> torch.Tensor:\\n        \"\"\"Orientation (w, x, y, z) of the prinicple axies of inertia of all bodies in simulation world frame.\\n\\n        Shape is (num_instances, 1, 4). This quantity is the orientation of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.body_com_state_w[..., 3:7]\\n\\n    @property\\n    def body_com_vel_w(self) -> torch.Tensor:\\n        \"\"\"Velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_com_state_w[..., 7:13]\\n\\n    @property\\n    def body_com_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Linear velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_com_state_w[..., 7:10]\\n\\n    @property\\n    def body_com_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Angular velocity of all bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.body_com_state_w[..., 10:13]\\n\\n    @property\\n    def com_pos_b(self) -> torch.Tensor:\\n        \"\"\"Center of mass of all of the bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the center of mass location relative to its body frame.\\n        \"\"\"\\n        return self._root_physx_view.get_coms().to(self.device)[..., :3].view(-1, 1, 3)\\n\\n    @property\\n    def com_quat_b(self) -> torch.Tensor:\\n        \"\"\"Orientation (w,x,y,z) of the prinicple axies of inertia of all of the bodies in simulation world frame. Shape is (num_instances, 1, 4).\\n\\n        This quantity is the orientation of the principles axes of inertia relative to its body frame.\\n        \"\"\"\\n        quat = self._root_physx_view.get_coms().to(self.device)[..., 3:7]\\n        return math_utils.convert_quat(quat, to=\"wxyz\").view(-1, 1, 4)'),\n",
       " Document(metadata={}, page_content='class RigidObjectCollection(AssetBase):\\n    \"\"\"A rigid object collection class.\\n\\n    This class represents a collection of rigid objects in the simulation, where the state of the\\n    rigid objects can be accessed and modified using a batched ``(env_ids, object_ids)`` API.\\n\\n    For each rigid body in the collection, the root prim of the asset must have the `USD RigidBodyAPI`_\\n    applied to it. This API is used to define the simulation properties of the rigid bodies. On playing the\\n    simulation, the physics engine will automatically register the rigid bodies and create a corresponding\\n    rigid body handle. This handle can be accessed using the :attr:`root_physx_view` attribute.\\n\\n    Rigid objects in the collection are uniquely identified via the key of the dictionary\\n    :attr:`~isaaclab.assets.RigidObjectCollectionCfg.rigid_objects` in the\\n    :class:`~isaaclab.assets.RigidObjectCollectionCfg` configuration class.\\n    This differs from the :class:`~isaaclab.assets.RigidObject` class, where a rigid object is identified by\\n    the name of the Xform where the `USD RigidBodyAPI`_ is applied. This would not be possible for the rigid\\n    object collection since the :attr:`~isaaclab.assets.RigidObjectCollectionCfg.rigid_objects` dictionary\\n    could contain the same rigid object multiple times, leading to ambiguity.\\n\\n    .. _`USD RigidBodyAPI`: https://openusd.org/dev/api/class_usd_physics_rigid_body_a_p_i.html\\n    \"\"\"\\n\\n    cfg: RigidObjectCollectionCfg\\n    \"\"\"Configuration instance for the rigid object collection.\"\"\"\\n\\n    def __init__(self, cfg: RigidObjectCollectionCfg):\\n        \"\"\"Initialize the rigid object collection.\\n\\n        Args:\\n            cfg: A configuration instance.\\n        \"\"\"\\n        # Note: We never call the parent constructor as it tries to call its own spawning which we don\\'t want.\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs\\n        self.cfg = cfg.copy()\\n        # flag for whether the asset is initialized\\n        self._is_initialized = False\\n        self._prim_paths = []\\n        # spawn the rigid objects\\n        for rigid_object_cfg in self.cfg.rigid_objects.values():\\n            # check if the rigid object path is valid\\n            # note: currently the spawner does not work if there is a regex pattern in the leaf\\n            #   For example, if the prim path is \"/World/Object_[1,2]\" since the spawner will not\\n            #   know which prim to spawn. This is a limitation of the spawner and not the asset.\\n            asset_path = rigid_object_cfg.prim_path.split(\"/\")[-1]\\n            asset_path_is_regex = re.match(r\"^[a-zA-Z0-9/_]+$\", asset_path) is None\\n            # spawn the asset\\n            if rigid_object_cfg.spawn is not None and not asset_path_is_regex:\\n                rigid_object_cfg.spawn.func(\\n                    rigid_object_cfg.prim_path,\\n                    rigid_object_cfg.spawn,\\n                    translation=rigid_object_cfg.init_state.pos,\\n                    orientation=rigid_object_cfg.init_state.rot,\\n                )\\n            # check that spawn was successful\\n            matching_prims = sim_utils.find_matching_prims(rigid_object_cfg.prim_path)\\n            if len(matching_prims) == 0:\\n                raise RuntimeError(f\"Could not find prim with path {rigid_object_cfg.prim_path}.\")\\n            self._prim_paths.append(rigid_object_cfg.prim_path)\\n        # stores object names\\n        self._object_names_list = []\\n\\n        # note: Use weakref on all callbacks to ensure that this object can be deleted when its destructor is called.\\n        # add callbacks for stage play/stop\\n        # The order is set to 10 which is arbitrary but should be lower priority than the default order of 0\\n        timeline_event_stream = omni.timeline.get_timeline_interface().get_timeline_event_stream()\\n        self._initialize_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n            int(omni.timeline.TimelineEventType.PLAY),\\n            lambda event, obj=weakref.proxy(self): obj._initialize_callback(event),\\n            order=10,\\n        )\\n        self._invalidate_initialize_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n            int(omni.timeline.TimelineEventType.STOP),\\n            lambda event, obj=weakref.proxy(self): obj._invalidate_initialize_callback(event),\\n            order=10,\\n        )\\n        self._prim_deletion_callback_id = SimulationManager.register_callback(\\n            self._on_prim_deletion, event=IsaacEvents.PRIM_DELETION\\n        )\\n        self._debug_vis_handle = None\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def data(self) -> RigidObjectCollectionData:\\n        return self._data\\n\\n    @property\\n    def num_instances(self) -> int:\\n        \"\"\"Number of instances of the collection.\"\"\"\\n        return self.root_physx_view.count // self.num_objects\\n\\n    @property\\n    def num_objects(self) -> int:\\n        \"\"\"Number of objects in the collection.\\n\\n        This corresponds to the distinct number of rigid bodies in the collection.\\n        \"\"\"\\n        return len(self.object_names)\\n\\n    @property\\n    def object_names(self) -> list[str]:\\n        \"\"\"Ordered names of objects in the rigid object collection.\"\"\"\\n        return self._object_names_list\\n\\n    @property\\n    def root_physx_view(self) -> physx.RigidBodyView:\\n        \"\"\"Rigid body view for the rigid body collection (PhysX).\\n\\n        Note:\\n            Use this view with caution. It requires handling of tensors in a specific way.\\n        \"\"\"\\n        return self._root_physx_view  # type: ignore\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: torch.Tensor | None = None, object_ids: slice | torch.Tensor | None = None):\\n        \"\"\"Resets all internal buffers of selected environments and objects.\\n\\n        Args:\\n            env_ids: The indices of the object to reset. Defaults to None (all instances).\\n            object_ids: The indices of the object to reset. Defaults to None (all objects).\\n        \"\"\"\\n        # resolve all indices\\n        if env_ids is None:\\n            env_ids = self._ALL_ENV_INDICES\\n        if object_ids is None:\\n            object_ids = self._ALL_OBJ_INDICES\\n        # reset external wrench\\n        self._external_force_b[env_ids[:, None], object_ids] = 0.0\\n        self._external_torque_b[env_ids[:, None], object_ids] = 0.0\\n\\n    def write_data_to_sim(self):\\n        \"\"\"Write external wrench to the simulation.\\n\\n        Note:\\n            We write external wrench to the simulation here since this function is called before the simulation step.\\n            This ensures that the external wrench is applied at every simulation step.\\n        \"\"\"\\n        # write external wrench\\n        if self.has_external_wrench:\\n            self.root_physx_view.apply_forces_and_torques_at_position(\\n                force_data=self.reshape_data_to_view(self._external_force_b),\\n                torque_data=self.reshape_data_to_view(self._external_torque_b),\\n                position_data=None,\\n                indices=self._env_obj_ids_to_view_ids(self._ALL_ENV_INDICES, self._ALL_OBJ_INDICES),\\n                is_global=False,\\n            )\\n\\n    def update(self, dt: float):\\n        self._data.update(dt)\\n\\n    \"\"\"\\n    Operations - Finders.\\n    \"\"\"\\n\\n    def find_objects(\\n        self, name_keys: str | Sequence[str], preserve_order: bool = False\\n    ) -> tuple[torch.Tensor, list[str]]:\\n        \"\"\"Find objects in the collection based on the name keys.\\n\\n        Please check the :meth:`isaaclab.utils.string_utils.resolve_matching_names` function for more\\n        information on the name matching.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the object names.\\n            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.\\n\\n        Returns:\\n            A tuple containing the object indices and names.\\n        \"\"\"\\n        obj_ids, obj_names = string_utils.resolve_matching_names(name_keys, self.object_names, preserve_order)\\n        return torch.tensor(obj_ids, device=self.device), obj_names\\n\\n    \"\"\"\\n    Operations - Write to simulation.\\n    \"\"\"\\n\\n    def write_object_state_to_sim(\\n        self,\\n        object_state: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object state over selected environment and object indices into the simulation.\\n\\n        The object state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            object_state: Object state in simulation frame. Shape is (len(env_ids), len(object_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n\\n        # set into simulation\\n        self.write_object_pose_to_sim(object_state[..., :7], env_ids=env_ids, object_ids=object_ids)\\n        self.write_object_velocity_to_sim(object_state[..., 7:], env_ids=env_ids, object_ids=object_ids)\\n\\n    def write_object_com_state_to_sim(\\n        self,\\n        object_state: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object center of mass state over selected environment indices into the simulation.\\n        The object state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            object_state: Object state in simulation frame. Shape is (len(env_ids), len(object_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n        # set into simulation\\n        self.write_object_com_pose_to_sim(object_state[..., :7], env_ids=env_ids, object_ids=object_ids)\\n        self.write_object_com_velocity_to_sim(object_state[..., 7:], env_ids=env_ids, object_ids=object_ids)\\n\\n    def write_object_link_state_to_sim(\\n        self,\\n        object_state: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object link state over selected environment indices into the simulation.\\n        The object state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear\\n        and angular velocity. All the quantities are in the simulation frame.\\n\\n        Args:\\n            object_state: Object state in simulation frame. Shape is (len(env_ids), len(object_ids), 13).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n        # set into simulation\\n        self.write_object_link_pose_to_sim(object_state[..., :7], env_ids=env_ids, object_ids=object_ids)\\n        self.write_object_link_velocity_to_sim(object_state[..., 7:], env_ids=env_ids, object_ids=object_ids)\\n\\n    def write_object_pose_to_sim(\\n        self,\\n        object_pose: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object pose over selected environment and object indices into the simulation.\\n\\n        The object pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n\\n        Args:\\n            object_pose: Object poses in simulation frame. Shape is (len(env_ids), len(object_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        # -- env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_ENV_INDICES\\n        # -- object_ids\\n        if object_ids is None:\\n            object_ids = self._ALL_OBJ_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.object_state_w[env_ids[:, None], object_ids, :7] = object_pose.clone()\\n        # convert the quaternion from wxyz to xyzw\\n        poses_xyzw = self._data.object_state_w[..., :7].clone()\\n        poses_xyzw[..., 3:] = math_utils.convert_quat(poses_xyzw[..., 3:], to=\"xyzw\")\\n        # set into simulation\\n        view_ids = self._env_obj_ids_to_view_ids(env_ids, object_ids)\\n        self.root_physx_view.set_transforms(self.reshape_data_to_view(poses_xyzw), indices=view_ids)\\n\\n    def write_object_link_pose_to_sim(\\n        self,\\n        object_pose: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object pose over selected environment and object indices into the simulation.\\n\\n        The object pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n\\n        Args:\\n            object_pose: Object poses in simulation frame. Shape is (len(env_ids), len(object_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n\\n        # resolve all indices\\n        # -- env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_ENV_INDICES\\n        # -- object_ids\\n        if object_ids is None:\\n            object_ids = self._ALL_OBJ_INDICES\\n        # note: we need to do this here since tensors are not set into simulation until step.\\n        # set into internal buffers\\n        self._data.object_link_state_w[env_ids[:, None], object_ids, :7] = object_pose.clone()\\n        self._data.object_state_w[env_ids[:, None], object_ids, :7] = object_pose.clone()\\n        # convert the quaternion from wxyz to xyzw\\n        poses_xyzw = self._data.object_link_state_w[..., :7].clone()\\n        poses_xyzw[..., 3:] = math_utils.convert_quat(poses_xyzw[..., 3:], to=\"xyzw\")\\n        # set into simulation\\n        view_ids = self._env_obj_ids_to_view_ids(env_ids, object_ids)\\n        self.root_physx_view.set_transforms(self.reshape_data_to_view(poses_xyzw), indices=view_ids)\\n\\n    def write_object_com_pose_to_sim(\\n        self,\\n        object_pose: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object center of mass pose over selected environment indices into the simulation.\\n        The object pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).\\n        The orientation is the orientation of the principle axes of inertia.\\n        Args:\\n            object_pose: Object poses in simulation frame. Shape is (len(env_ids), len(object_ids), 7).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n\\n        # resolve all indices\\n        if env_ids is None:\\n            local_env_ids = slice(env_ids)\\n        else:\\n            local_env_ids = env_ids\\n        if object_ids is None:\\n            local_object_ids = slice(object_ids)\\n        else:\\n            local_object_ids = object_ids\\n\\n        com_pos = self.data.com_pos_b[local_env_ids][:, local_object_ids, :]\\n        com_quat = self.data.com_quat_b[local_env_ids][:, local_object_ids, :]\\n\\n        object_link_pos, object_link_quat = math_utils.combine_frame_transforms(\\n            object_pose[..., :3],\\n            object_pose[..., 3:7],\\n            math_utils.quat_apply(math_utils.quat_inv(com_quat), -com_pos),\\n            math_utils.quat_inv(com_quat),\\n        )\\n\\n        object_link_pose = torch.cat((object_link_pos, object_link_quat), dim=-1)\\n        self.write_object_link_pose_to_sim(object_pose=object_link_pose, env_ids=env_ids, object_ids=object_ids)\\n\\n    def write_object_velocity_to_sim(\\n        self,\\n        object_velocity: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object velocity over selected environment and object indices into the simulation.\\n\\n        Args:\\n            object_velocity: Object velocities in simulation frame. Shape is (len(env_ids), len(object_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        # -- env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_ENV_INDICES\\n        # -- object_ids\\n        if object_ids is None:\\n            object_ids = self._ALL_OBJ_INDICES\\n\\n        self._data.object_state_w[env_ids[:, None], object_ids, 7:] = object_velocity.clone()\\n        self._data.object_acc_w[env_ids[:, None], object_ids] = 0.0\\n\\n        # set into simulation\\n        view_ids = self._env_obj_ids_to_view_ids(env_ids, object_ids)\\n        self.root_physx_view.set_velocities(\\n            self.reshape_data_to_view(self._data.object_state_w[..., 7:]), indices=view_ids\\n        )\\n\\n    def write_object_com_velocity_to_sim(\\n        self,\\n        object_velocity: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object center of mass velocity over selected environment and object indices into the simulation.\\n\\n        Args:\\n            object_velocity: Object velocities in simulation frame. Shape is (len(env_ids), len(object_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        # -- env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_ENV_INDICES\\n        # -- object_ids\\n        if object_ids is None:\\n            object_ids = self._ALL_OBJ_INDICES\\n\\n        self._data.object_com_state_w[env_ids[:, None], object_ids, 7:] = object_velocity.clone()\\n        self._data.object_state_w[env_ids[:, None], object_ids, 7:] = object_velocity.clone()\\n        self._data.object_acc_w[env_ids[:, None], object_ids] = 0.0\\n\\n        # set into simulation\\n        view_ids = self._env_obj_ids_to_view_ids(env_ids, object_ids)\\n        self.root_physx_view.set_velocities(\\n            self.reshape_data_to_view(self._data.object_com_state_w[..., 7:]), indices=view_ids\\n        )\\n\\n    def write_object_link_velocity_to_sim(\\n        self,\\n        object_velocity: torch.Tensor,\\n        env_ids: torch.Tensor | None = None,\\n        object_ids: slice | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the object link velocity over selected environment indices into the simulation.\\n        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.\\n        NOTE: This sets the velocity of the object\\'s frame rather than the objects center of mass.\\n        Args:\\n            object_velocity: Object velocities in simulation frame. Shape is (len(env_ids), len(object_ids), 6).\\n            env_ids: Environment indices. If None, then all indices are used.\\n            object_ids: Object indices. If None, then all indices are used.\\n        \"\"\"\\n        # resolve all indices\\n        if env_ids is None:\\n            local_env_ids = slice(env_ids)\\n        else:\\n            local_env_ids = env_ids\\n        if object_ids is None:\\n            local_object_ids = slice(object_ids)\\n        else:\\n            local_object_ids = object_ids\\n\\n        object_com_velocity = object_velocity.clone()\\n        quat = self.data.object_link_state_w[local_env_ids][:, local_object_ids, 3:7]\\n        com_pos_b = self.data.com_pos_b[local_env_ids][:, local_object_ids, :]\\n        # transform given velocity to center of mass\\n        object_com_velocity[..., :3] += torch.linalg.cross(\\n            object_com_velocity[..., 3:], math_utils.quat_apply(quat, com_pos_b), dim=-1\\n        )\\n        # write center of mass velocity to sim\\n        self.write_object_com_velocity_to_sim(\\n            object_velocity=object_com_velocity, env_ids=env_ids, object_ids=object_ids\\n        )\\n\\n    \"\"\"\\n    Operations - Setters.\\n    \"\"\"\\n\\n    def set_external_force_and_torque(\\n        self,\\n        forces: torch.Tensor,\\n        torques: torch.Tensor,\\n        object_ids: slice | torch.Tensor | None = None,\\n        env_ids: torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set external force and torque to apply on the objects\\' bodies in their local frame.\\n\\n        For many applications, we want to keep the applied external force on rigid bodies constant over a period of\\n        time (for instance, during the policy control). This function allows us to store the external force and torque\\n        into buffers which are then applied to the simulation at every step.\\n\\n        .. caution::\\n            If the function is called with empty forces and torques, then this function disables the application\\n            of external wrench to the simulation.\\n\\n            .. code-block:: python\\n\\n                # example of disabling external wrench\\n                asset.set_external_force_and_torque(forces=torch.zeros(0, 0, 3), torques=torch.zeros(0, 0, 3))\\n\\n        .. note::\\n            This function does not apply the external wrench to the simulation. It only fills the buffers with\\n            the desired values. To apply the external wrench, call the :meth:`write_data_to_sim` function\\n            right before the simulation step.\\n\\n        Args:\\n            forces: External forces in bodies\\' local frame. Shape is (len(env_ids), len(object_ids), 3).\\n            torques: External torques in bodies\\' local frame. Shape is (len(env_ids), len(object_ids), 3).\\n            object_ids: Object indices to apply external wrench to. Defaults to None (all objects).\\n            env_ids: Environment indices to apply external wrench to. Defaults to None (all instances).\\n        \"\"\"\\n        if forces.any() or torques.any():\\n            self.has_external_wrench = True\\n        else:\\n            self.has_external_wrench = False\\n            # to be safe, explicitly set value to zero\\n            forces = torques = 0.0\\n\\n        # resolve all indices\\n        # -- env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_ENV_INDICES\\n        # -- object_ids\\n        if object_ids is None:\\n            object_ids = self._ALL_OBJ_INDICES\\n        # set into internal buffers\\n        self._external_force_b[env_ids[:, None], object_ids] = forces\\n        self._external_torque_b[env_ids[:, None], object_ids] = torques\\n\\n    \"\"\"\\n    Internal helper.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        root_prim_path_exprs = []\\n        for name, rigid_object_cfg in self.cfg.rigid_objects.items():\\n            # obtain the first prim in the regex expression (all others are assumed to be a copy of this)\\n            template_prim = sim_utils.find_first_matching_prim(rigid_object_cfg.prim_path)\\n            if template_prim is None:\\n                raise RuntimeError(f\"Failed to find prim for expression: \\'{rigid_object_cfg.prim_path}\\'.\")\\n            template_prim_path = template_prim.GetPath().pathString\\n\\n            # find rigid root prims\\n            root_prims = sim_utils.get_all_matching_child_prims(\\n                template_prim_path, predicate=lambda prim: prim.HasAPI(UsdPhysics.RigidBodyAPI)\\n            )\\n            if len(root_prims) == 0:\\n                raise RuntimeError(\\n                    f\"Failed to find a rigid body when resolving \\'{rigid_object_cfg.prim_path}\\'.\"\\n                    \" Please ensure that the prim has \\'USD RigidBodyAPI\\' applied.\"\\n                )\\n            if len(root_prims) > 1:\\n                raise RuntimeError(\\n                    f\"Failed to find a single rigid body when resolving \\'{rigid_object_cfg.prim_path}\\'.\"\\n                    f\" Found multiple \\'{root_prims}\\' under \\'{template_prim_path}\\'.\"\\n                    \" Please ensure that there is only one rigid body in the prim path tree.\"\\n                )\\n\\n            # check that no rigid object has an articulation root API, which decreases simulation performance\\n            articulation_prims = sim_utils.get_all_matching_child_prims(\\n                template_prim_path, predicate=lambda prim: prim.HasAPI(UsdPhysics.ArticulationRootAPI)\\n            )\\n            if len(articulation_prims) != 0:\\n                if articulation_prims[0].GetAttribute(\"physxArticulation:articulationEnabled\").Get():\\n                    raise RuntimeError(\\n                        f\"Found an articulation root when resolving \\'{rigid_object_cfg.prim_path}\\' in the rigid object\"\\n                        f\" collection. These are located at: \\'{articulation_prims}\\' under \\'{template_prim_path}\\'.\"\\n                        \" Please disable the articulation root in the USD or from code by setting the parameter\"\\n                        \" \\'ArticulationRootPropertiesCfg.articulation_enabled\\' to False in the spawn configuration.\"\\n                    )\\n\\n            # resolve root prim back into regex expression\\n            root_prim_path = root_prims[0].GetPath().pathString\\n            root_prim_path_expr = rigid_object_cfg.prim_path + root_prim_path[len(template_prim_path) :]\\n            root_prim_path_exprs.append(root_prim_path_expr.replace(\".*\", \"*\"))\\n\\n            self._object_names_list.append(name)\\n\\n        # -- object view\\n        self._root_physx_view = self._physics_sim_view.create_rigid_body_view(root_prim_path_exprs)\\n\\n        # check if the rigid body was created\\n        if self._root_physx_view._backend is None:\\n            raise RuntimeError(\"Failed to create rigid body collection. Please check PhysX logs.\")\\n\\n        # log information about the rigid body\\n        omni.log.info(f\"Number of instances: {self.num_instances}\")\\n        omni.log.info(f\"Number of distinct objects: {self.num_objects}\")\\n        omni.log.info(f\"Object names: {self.object_names}\")\\n\\n        # container for data access\\n        self._data = RigidObjectCollectionData(self.root_physx_view, self.num_objects, self.device)\\n\\n        # create buffers\\n        self._create_buffers()\\n        # process configuration\\n        self._process_cfg()\\n        # update the rigid body data\\n        self.update(0.0)\\n\\n    def _create_buffers(self):\\n        \"\"\"Create buffers for storing data.\"\"\"\\n        # constants\\n        self._ALL_ENV_INDICES = torch.arange(self.num_instances, dtype=torch.long, device=self.device)\\n        self._ALL_OBJ_INDICES = torch.arange(self.num_objects, dtype=torch.long, device=self.device)\\n\\n        # external forces and torques\\n        self.has_external_wrench = False\\n        self._external_force_b = torch.zeros((self.num_instances, self.num_objects, 3), device=self.device)\\n        self._external_torque_b = torch.zeros_like(self._external_force_b)\\n\\n        # set information about rigid body into data\\n        self._data.object_names = self.object_names\\n        self._data.default_mass = self.reshape_view_to_data(self.root_physx_view.get_masses().clone())\\n        self._data.default_inertia = self.reshape_view_to_data(self.root_physx_view.get_inertias().clone())\\n\\n    def _process_cfg(self):\\n        \"\"\"Post processing of configuration parameters.\"\"\"\\n        # default state\\n        # -- object state\\n        default_object_states = []\\n        for rigid_object_cfg in self.cfg.rigid_objects.values():\\n            default_object_state = (\\n                tuple(rigid_object_cfg.init_state.pos)\\n                + tuple(rigid_object_cfg.init_state.rot)\\n                + tuple(rigid_object_cfg.init_state.lin_vel)\\n                + tuple(rigid_object_cfg.init_state.ang_vel)\\n            )\\n            default_object_state = (\\n                torch.tensor(default_object_state, dtype=torch.float, device=self.device)\\n                .repeat(self.num_instances, 1)\\n                .unsqueeze(1)\\n            )\\n            default_object_states.append(default_object_state)\\n        # concatenate the default state for each object\\n        default_object_states = torch.cat(default_object_states, dim=1)\\n        self._data.default_object_state = default_object_states\\n\\n    def reshape_view_to_data(self, data: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Reshapes and arranges the data coming from the :attr:`root_physx_view` to (num_instances, num_objects, data_size).\\n\\n        Args:\\n            data: The data coming from the :attr:`root_physx_view`. Shape is (num_instances*num_objects, data_size).\\n\\n        Returns:\\n            The reshaped data. Shape is (num_instances, num_objects, data_size).\\n        \"\"\"\\n        return torch.einsum(\"ijk -> jik\", data.reshape(self.num_objects, self.num_instances, -1))\\n\\n    def reshape_data_to_view(self, data: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Reshapes and arranges the data to the be consistent with data from the :attr:`root_physx_view`.\\n\\n        Args:\\n            data: The data to be reshaped. Shape is (num_instances, num_objects, data_size).\\n\\n        Returns:\\n            The reshaped data. Shape is (num_instances*num_objects, data_size).\\n        \"\"\"\\n        return torch.einsum(\"ijk -> jik\", data).reshape(self.num_objects * self.num_instances, *data.shape[2:])\\n\\n    def _env_obj_ids_to_view_ids(\\n        self, env_ids: torch.Tensor, object_ids: Sequence[int] | slice | torch.Tensor\\n    ) -> torch.Tensor:\\n        \"\"\"Converts environment and object indices to indices consistent with data from :attr:`root_physx_view`.\\n\\n        Args:\\n            env_ids: Environment indices.\\n            object_ids: Object indices.\\n\\n        Returns:\\n            The view indices.\\n        \"\"\"\\n        # the order is env_0/object_0, env_0/object_1, env_0/object_..., env_1/object_0, env_1/object_1, ...\\n        # return a flat tensor of indices\\n        if isinstance(object_ids, slice):\\n            object_ids = self._ALL_OBJ_INDICES\\n        elif isinstance(object_ids, Sequence):\\n            object_ids = torch.tensor(object_ids, device=self.device)\\n        return (object_ids.unsqueeze(1) * self.num_instances + env_ids).flatten()\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        # set all existing views to None to invalidate them\\n        self._root_physx_view = None\\n\\n    def _on_prim_deletion(self, prim_path: str) -> None:\\n        \"\"\"Invalidates and deletes the callbacks when the prim is deleted.\\n\\n        Args:\\n            prim_path: The path to the prim that is being deleted.\\n\\n        Note:\\n            This function is called when the prim is deleted.\\n        \"\"\"\\n        if prim_path == \"/\":\\n            self._clear_callbacks()\\n            return\\n        for prim_path_expr in self._prim_paths:\\n            result = re.match(\\n                pattern=\"^\" + \"/\".join(prim_path_expr.split(\"/\")[: prim_path.count(\"/\") + 1]) + \"$\", string=prim_path\\n            )\\n            if result:\\n                self._clear_callbacks()\\n                return'),\n",
       " Document(metadata={}, page_content='class RigidObjectCollectionCfg:\\n    \"\"\"Configuration parameters for a rigid object collection.\"\"\"\\n\\n    class_type: type = RigidObjectCollection\\n    \"\"\"The associated asset class.\\n\\n    The class should inherit from :class:`isaaclab.assets.asset_base.AssetBase`.\\n    \"\"\"\\n\\n    rigid_objects: dict[str, RigidObjectCfg] = MISSING\\n    \"\"\"Dictionary of rigid object configurations to spawn.\\n\\n    The keys are the names for the objects, which are used as unique identifiers throughout the code.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RigidObjectCollectionData:\\n    \"\"\"Data container for a rigid object collection.\\n\\n    This class contains the data for a rigid object collection in the simulation. The data includes the state of\\n    all the bodies in the collection. The data is stored in the simulation world frame unless otherwise specified.\\n    The data is in the order ``(num_instances, num_objects, data_size)``, where data_size is the size of the data.\\n\\n    For a rigid body, there are two frames of reference that are used:\\n\\n    - Actor frame: The frame of reference of the rigid body prim. This typically corresponds to the Xform prim\\n      with the rigid body schema.\\n    - Center of mass frame: The frame of reference of the center of mass of the rigid body.\\n\\n    Depending on the settings of the simulation, the actor frame and the center of mass frame may be the same.\\n    This needs to be taken into account when interpreting the data.\\n\\n    The data is lazily updated, meaning that the data is only updated when it is accessed. This is useful\\n    when the data is expensive to compute or retrieve. The data is updated when the timestamp of the buffer\\n    is older than the current simulation timestamp. The timestamp is updated whenever the data is updated.\\n    \"\"\"\\n\\n    def __init__(self, root_physx_view: physx.RigidBodyView, num_objects: int, device: str):\\n        \"\"\"Initializes the data.\\n\\n        Args:\\n            root_physx_view: The root rigid body view.\\n            num_objects: The number of objects in the collection.\\n            device: The device used for processing.\\n        \"\"\"\\n        # Set the parameters\\n        self.device = device\\n        self.num_objects = num_objects\\n        # Set the root rigid body view\\n        # note: this is stored as a weak reference to avoid circular references between the asset class\\n        #  and the data container. This is important to avoid memory leaks.\\n        self._root_physx_view: physx.RigidBodyView = weakref.proxy(root_physx_view)\\n        self.num_instances = self._root_physx_view.count // self.num_objects\\n\\n        # Set initial time stamp\\n        self._sim_timestamp = 0.0\\n\\n        # Obtain global physics sim view\\n        physics_sim_view = physx.create_simulation_view(\"torch\")\\n        physics_sim_view.set_subspace_roots(\"/\")\\n        gravity = physics_sim_view.get_gravity()\\n        # Convert to direction vector\\n        gravity_dir = torch.tensor((gravity[0], gravity[1], gravity[2]), device=self.device)\\n        gravity_dir = math_utils.normalize(gravity_dir.unsqueeze(0)).squeeze(0)\\n\\n        # Initialize constants\\n        self.GRAVITY_VEC_W = gravity_dir.repeat(self.num_instances, self.num_objects, 1)\\n        self.FORWARD_VEC_B = torch.tensor((1.0, 0.0, 0.0), device=self.device).repeat(\\n            self.num_instances, self.num_objects, 1\\n        )\\n\\n        # Initialize the lazy buffers.\\n        self._object_state_w = TimestampedBuffer()\\n        self._object_link_state_w = TimestampedBuffer()\\n        self._object_com_state_w = TimestampedBuffer()\\n        self._object_acc_w = TimestampedBuffer()\\n\\n    def update(self, dt: float):\\n        \"\"\"Updates the data for the rigid object collection.\\n\\n        Args:\\n            dt: The time step for the update. This must be a positive value.\\n        \"\"\"\\n        # update the simulation timestamp\\n        self._sim_timestamp += dt\\n\\n    ##\\n    # Names.\\n    ##\\n\\n    object_names: list[str] = None\\n    \"\"\"Object names in the order parsed by the simulation view.\"\"\"\\n\\n    ##\\n    # Defaults.\\n    ##\\n\\n    default_object_state: torch.Tensor = None\\n    \"\"\"Default object state ``[pos, quat, lin_vel, ang_vel]`` in local environment frame.\\n    Shape is (num_instances, num_objects, 13).\\n\\n    The position and quaternion are of each object\\'s rigid body\\'s actor frame. Meanwhile, the linear and\\n    angular velocities are of the center of mass frame.\\n    \"\"\"\\n\\n    default_mass: torch.Tensor = None\\n    \"\"\"Default object mass read from the simulation. Shape is (num_instances, num_objects, 1).\"\"\"\\n\\n    default_inertia: torch.Tensor = None\\n    \"\"\"Default object inertia tensor read from the simulation. Shape is (num_instances, num_objects, 9).\\n\\n    The inertia is the inertia tensor relative to the center of mass frame. The values are stored in\\n    the order :math:`[I_{xx}, I_{xy}, I_{xz}, I_{yx}, I_{yy}, I_{yz}, I_{zx}, I_{zy}, I_{zz}]`.\\n    \"\"\"\\n\\n    ##\\n    # Properties.\\n    ##\\n\\n    @property\\n    def object_state_w(self):\\n        \"\"\"Object state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame.\\n        Shape is (num_instances, num_objects, 13).\\n\\n        The position and orientation are of the rigid body\\'s actor frame. Meanwhile, the linear and angular\\n        velocities are of the rigid body\\'s center of mass frame.\\n        \"\"\"\\n\\n        if self._object_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._reshape_view_to_data(self._root_physx_view.get_transforms().clone())\\n            pose[..., 3:7] = math_utils.convert_quat(pose[..., 3:7], to=\"wxyz\")\\n            velocity = self._reshape_view_to_data(self._root_physx_view.get_velocities())\\n            # set the buffer data and timestamp\\n            self._object_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._object_state_w.timestamp = self._sim_timestamp\\n        return self._object_state_w.data\\n\\n    @property\\n    def object_link_state_w(self):\\n        \"\"\"Object center of mass state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame.\\n        Shape is (num_instances, num_objects, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the rigid body root frame relative to the\\n        world.\\n        \"\"\"\\n        if self._object_link_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._reshape_view_to_data(self._root_physx_view.get_transforms().clone())\\n            pose[..., 3:7] = math_utils.convert_quat(pose[..., 3:7], to=\"wxyz\")\\n            velocity = self._reshape_view_to_data(self._root_physx_view.get_velocities())\\n\\n            # adjust linear velocity to link from center of mass\\n            velocity[..., :3] += torch.linalg.cross(\\n                velocity[..., 3:], math_utils.quat_apply(pose[..., 3:7], -self.com_pos_b[..., :]), dim=-1\\n            )\\n\\n            # set the buffer data and timestamp\\n            self._object_link_state_w.data = torch.cat((pose, velocity), dim=-1)\\n            self._object_link_state_w.timestamp = self._sim_timestamp\\n        return self._object_link_state_w.data\\n\\n    @property\\n    def object_com_state_w(self):\\n        \"\"\"Object state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame.\\n        Shape is (num_instances, num_objects, 13).\\n\\n        The position, quaternion, and linear/angular velocity are of the rigid body\\'s center of mass frame\\n        relative to the world. Center of mass frame is the orientation principle axes of inertia.\\n        \"\"\"\\n\\n        if self._object_com_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._reshape_view_to_data(self._root_physx_view.get_transforms().clone())\\n            pose[..., 3:7] = math_utils.convert_quat(pose[..., 3:7], to=\"wxyz\")\\n            velocity = self._reshape_view_to_data(self._root_physx_view.get_velocities())\\n\\n            # adjust pose to center of mass\\n            pos, quat = math_utils.combine_frame_transforms(\\n                pose[..., :3], pose[..., 3:7], self.com_pos_b[..., :], self.com_quat_b[..., :]\\n            )\\n\\n            # set the buffer data and timestamp\\n            self._object_com_state_w.data = torch.cat((pos, quat, velocity), dim=-1)\\n            self._object_com_state_w.timestamp = self._sim_timestamp\\n        return self._object_com_state_w.data\\n\\n    @property\\n    def object_acc_w(self):\\n        \"\"\"Acceleration of all objects. Shape is (num_instances, num_objects, 6).\\n\\n        This quantity is the acceleration of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._object_acc_w.timestamp < self._sim_timestamp:\\n            # note: we use finite differencing to compute acceleration\\n            self._object_acc_w.data = self._reshape_view_to_data(self._root_physx_view.get_accelerations().clone())\\n            self._object_acc_w.timestamp = self._sim_timestamp\\n        return self._object_acc_w.data\\n\\n    @property\\n    def projected_gravity_b(self):\\n        \"\"\"Projection of the gravity direction on base frame. Shape is (num_instances, num_objects, 3).\"\"\"\\n        return math_utils.quat_apply_inverse(self.object_link_quat_w, self.GRAVITY_VEC_W)\\n\\n    @property\\n    def heading_w(self):\\n        \"\"\"Yaw heading of the base frame (in radians). Shape is (num_instances, num_objects,).\\n\\n        Note:\\n            This quantity is computed by assuming that the forward-direction of the base\\n            frame is along x-direction, i.e. :math:`(1, 0, 0)`.\\n        \"\"\"\\n        forward_w = math_utils.quat_apply(self.object_link_quat_w, self.FORWARD_VEC_B)\\n        return torch.atan2(forward_w[..., 1], forward_w[..., 0])\\n\\n    ##\\n    # Derived properties.\\n    ##\\n\\n    @property\\n    def object_pos_w(self) -> torch.Tensor:\\n        \"\"\"Object position in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the position of the actor frame of the rigid bodies.\\n        \"\"\"\\n        return self.object_state_w[..., :3]\\n\\n    @property\\n    def object_quat_w(self) -> torch.Tensor:\\n        \"\"\"Object orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, num_objects, 4).\\n\\n        This quantity is the orientation of the actor frame of the rigid bodies.\\n        \"\"\"\\n        return self.object_state_w[..., 3:7]\\n\\n    @property\\n    def object_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object velocity in simulation world frame. Shape is (num_instances, num_objects, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.object_state_w[..., 7:13]\\n\\n    @property\\n    def object_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object linear velocity in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.object_state_w[..., 7:10]\\n\\n    @property\\n    def object_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object angular velocity in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.object_state_w[..., 10:13]\\n\\n    @property\\n    def object_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Object linear velocity in base frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.object_quat_w, self.object_lin_vel_w)\\n\\n    @property\\n    def object_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Object angular velocity in base world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.object_quat_w, self.object_ang_vel_w)\\n\\n    @property\\n    def object_lin_acc_w(self) -> torch.Tensor:\\n        \"\"\"Linear acceleration of all bodies in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the linear acceleration of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.object_acc_w[..., 0:3]\\n\\n    @property\\n    def object_ang_acc_w(self) -> torch.Tensor:\\n        \"\"\"Angular acceleration of all bodies in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the angular acceleration of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        return self.object_acc_w[..., 3:6]\\n\\n    @property\\n    def object_link_pos_w(self) -> torch.Tensor:\\n        \"\"\"Object link position in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the position of the actor frame of the rigid bodies.\\n        \"\"\"\\n        if self._object_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._reshape_view_to_data(self._root_physx_view.get_transforms().clone())\\n            return pose[..., :3]\\n        return self.object_link_state_w[..., :3]\\n\\n    @property\\n    def object_link_quat_w(self) -> torch.Tensor:\\n        \"\"\"Object link orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, num_objects, 4).\\n\\n        This quantity is the orientation of the actor frame of the rigid bodies.\\n        \"\"\"\\n        if self._object_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            pose = self._reshape_view_to_data(self._root_physx_view.get_transforms().clone())\\n            pose[..., 3:7] = math_utils.convert_quat(pose[..., 3:7], to=\"wxyz\")\\n            return pose[..., 3:7]\\n        return self.object_link_state_w[..., 3:7]\\n\\n    @property\\n    def object_link_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object link velocity in simulation world frame. Shape is (num_instances, num_objects, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.object_link_state_w[..., 7:13]\\n\\n    @property\\n    def object_link_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object link linear velocity in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.object_link_state_w[..., 7:10]\\n\\n    @property\\n    def object_link_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object link angular velocity in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' actor frame.\\n        \"\"\"\\n        return self.object_link_state_w[..., 10:13]\\n\\n    @property\\n    def object_link_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Object link linear velocity in base frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the linear velocity of the actor frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.object_link_quat_w, self.object_link_lin_vel_w)\\n\\n    @property\\n    def object_link_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Object link angular velocity in base world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the angular velocity of the actor frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.object_link_quat_w, self.object_link_ang_vel_w)\\n\\n    @property\\n    def object_com_pos_w(self) -> torch.Tensor:\\n        \"\"\"Object center of mass position in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the position of the center of mass frame of the rigid bodies.\\n        \"\"\"\\n        return self.object_com_state_w[..., :3]\\n\\n    @property\\n    def object_com_quat_w(self) -> torch.Tensor:\\n        \"\"\"Object center of mass orientation (w, x, y, z) in simulation world frame. Shape is (num_instances, num_objects, 4).\\n\\n        This quantity is the orientation of the center of mass frame of the rigid bodies.\\n        \"\"\"\\n        return self.object_com_state_w[..., 3:7]\\n\\n    @property\\n    def object_com_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object center of mass velocity in simulation world frame. Shape is (num_instances, num_objects, 6).\\n\\n        This quantity contains the linear and angular velocities of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._object_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            velocity = self._reshape_view_to_data(self._root_physx_view.get_velocities())\\n            return velocity\\n        return self.object_com_state_w[..., 7:13]\\n\\n    @property\\n    def object_com_lin_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object center of mass linear velocity in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the linear velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._object_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            velocity = self._reshape_view_to_data(self._root_physx_view.get_velocities())\\n            return velocity[..., 0:3]\\n        return self.object_com_state_w[..., 7:10]\\n\\n    @property\\n    def object_com_ang_vel_w(self) -> torch.Tensor:\\n        \"\"\"Object center of mass angular velocity in simulation world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the angular velocity of the rigid bodies\\' center of mass frame.\\n        \"\"\"\\n        if self._object_state_w.timestamp < self._sim_timestamp:\\n            # read data from simulation\\n            velocity = self._reshape_view_to_data(self._root_physx_view.get_velocities())\\n            return velocity[..., 3:6]\\n        return self.object_com_state_w[..., 10:13]\\n\\n    @property\\n    def object_com_lin_vel_b(self) -> torch.Tensor:\\n        \"\"\"Object center of mass linear velocity in base frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the linear velocity of the center of mass frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.object_link_quat_w, self.object_com_lin_vel_w)\\n\\n    @property\\n    def object_com_ang_vel_b(self) -> torch.Tensor:\\n        \"\"\"Object center of mass angular velocity in base world frame. Shape is (num_instances, num_objects, 3).\\n\\n        This quantity is the angular velocity of the center of mass frame of the root rigid body frame with respect to the\\n        rigid body\\'s actor frame.\\n        \"\"\"\\n        return math_utils.quat_apply_inverse(self.object_link_quat_w, self.object_com_ang_vel_w)\\n\\n    @property\\n    def com_pos_b(self) -> torch.Tensor:\\n        \"\"\"Center of mass of all of the bodies in simulation world frame. Shape is (num_instances, 1, 3).\\n\\n        This quantity is the center of mass location relative to its body frame.\\n        \"\"\"\\n        pos = self._root_physx_view.get_coms().to(self.device)[..., :3]\\n        return self._reshape_view_to_data(pos)\\n\\n    @property\\n    def com_quat_b(self) -> torch.Tensor:\\n        \"\"\"Orientation (w,x,y,z) of the prinicple axies of inertia of all of the bodies in simulation world frame. Shape is (num_instances, 1, 4).\\n\\n        This quantity is the orientation of the principles axes of inertia relative to its body frame.\\n        \"\"\"\\n        quat = self._root_physx_view.get_coms().to(self.device)[..., 3:7].view(self.num_instances, self.num_objects, 4)\\n        quat_wxyz = math_utils.convert_quat(quat, to=\"wxyz\")\\n        return self._reshape_view_to_data(quat_wxyz)\\n\\n    ##\\n    # Helpers.\\n    ##\\n\\n    def _reshape_view_to_data(self, data: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Reshapes and arranges the data from the physics view to (num_instances, num_objects, data_size).\\n\\n        Args:\\n            data: The data from the physics view. Shape is (num_instances * num_objects, data_size).\\n\\n        Returns:\\n            The reshaped data. Shape is (num_objects, num_instances, data_size).\\n        \"\"\"\\n        return torch.einsum(\"ijk -> jik\", data.reshape(self.num_objects, self.num_instances, -1))'),\n",
       " Document(metadata={}, page_content='class DifferentialIKController:\\n    r\"\"\"Differential inverse kinematics (IK) controller.\\n\\n    This controller is based on the concept of differential inverse kinematics [1, 2] which is a method for computing\\n    the change in joint positions that yields the desired change in pose.\\n\\n    .. math::\\n\\n        \\\\Delta \\\\mathbf{q} &= \\\\mathbf{J}^{\\\\dagger} \\\\Delta \\\\mathbf{x} \\\\\\\\\\n        \\\\mathbf{q}_{\\\\text{desired}} &= \\\\mathbf{q}_{\\\\text{current}} + \\\\Delta \\\\mathbf{q}\\n\\n    where :math:`\\\\mathbf{J}^{\\\\dagger}` is the pseudo-inverse of the Jacobian matrix :math:`\\\\mathbf{J}`,\\n    :math:`\\\\Delta \\\\mathbf{x}` is the desired change in pose, and :math:`\\\\mathbf{q}_{\\\\text{current}}`\\n    is the current joint positions.\\n\\n    To deal with singularity in Jacobian, the following methods are supported for computing inverse of the Jacobian:\\n\\n    - \"pinv\": Moore-Penrose pseudo-inverse\\n    - \"svd\": Adaptive singular-value decomposition (SVD)\\n    - \"trans\": Transpose of matrix\\n    - \"dls\": Damped version of Moore-Penrose pseudo-inverse (also called Levenberg-Marquardt)\\n\\n\\n    .. caution::\\n        The controller does not assume anything about the frames of the current and desired end-effector pose,\\n        or the joint-space velocities. It is up to the user to ensure that these quantities are given\\n        in the correct format.\\n\\n    Reference:\\n\\n    1. `Robot Dynamics Lecture Notes <https://ethz.ch/content/dam/ethz/special-interest/mavt/robotics-n-intelligent-systems/rsl-dam/documents/RobotDynamics2017/RD_HS2017script.pdf>`_\\n       by Marco Hutter (ETH Zurich)\\n    2. `Introduction to Inverse Kinematics <https://www.cs.cmu.edu/~15464-s13/lectures/lecture6/iksurvey.pdf>`_\\n       by Samuel R. Buss (University of California, San Diego)\\n\\n    \"\"\"\\n\\n    def __init__(self, cfg: DifferentialIKControllerCfg, num_envs: int, device: str):\\n        \"\"\"Initialize the controller.\\n\\n        Args:\\n            cfg: The configuration for the controller.\\n            num_envs: The number of environments.\\n            device: The device to use for computations.\\n        \"\"\"\\n        # store inputs\\n        self.cfg = cfg\\n        self.num_envs = num_envs\\n        self._device = device\\n        # create buffers\\n        self.ee_pos_des = torch.zeros(self.num_envs, 3, device=self._device)\\n        self.ee_quat_des = torch.zeros(self.num_envs, 4, device=self._device)\\n        # -- input command\\n        self._command = torch.zeros(self.num_envs, self.action_dim, device=self._device)\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        \"\"\"Dimension of the controller\\'s input command.\"\"\"\\n        if self.cfg.command_type == \"position\":\\n            return 3  # (x, y, z)\\n        elif self.cfg.command_type == \"pose\" and self.cfg.use_relative_mode:\\n            return 6  # (dx, dy, dz, droll, dpitch, dyaw)\\n        else:\\n            return 7  # (x, y, z, qw, qx, qy, qz)\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: torch.Tensor = None):\\n        \"\"\"Reset the internals.\\n\\n        Args:\\n            env_ids: The environment indices to reset. If None, then all environments are reset.\\n        \"\"\"\\n        pass\\n\\n    def set_command(\\n        self, command: torch.Tensor, ee_pos: torch.Tensor | None = None, ee_quat: torch.Tensor | None = None\\n    ):\\n        \"\"\"Set target end-effector pose command.\\n\\n        Based on the configured command type and relative mode, the method computes the desired end-effector pose.\\n        It is up to the user to ensure that the command is given in the correct frame. The method only\\n        applies the relative mode if the command type is ``position_rel`` or ``pose_rel``.\\n\\n        Args:\\n            command: The input command in shape (N, 3) or (N, 6) or (N, 7).\\n            ee_pos: The current end-effector position in shape (N, 3).\\n                This is only needed if the command type is ``position_rel`` or ``pose_rel``.\\n            ee_quat: The current end-effector orientation (w, x, y, z) in shape (N, 4).\\n                This is only needed if the command type is ``position_*`` or ``pose_rel``.\\n\\n        Raises:\\n            ValueError: If the command type is ``position_*`` and :attr:`ee_quat` is None.\\n            ValueError: If the command type is ``position_rel`` and :attr:`ee_pos` is None.\\n            ValueError: If the command type is ``pose_rel`` and either :attr:`ee_pos` or :attr:`ee_quat` is None.\\n        \"\"\"\\n        # store command\\n        self._command[:] = command\\n        # compute the desired end-effector pose\\n        if self.cfg.command_type == \"position\":\\n            # we need end-effector orientation even though we are in position mode\\n            # this is only needed for display purposes\\n            if ee_quat is None:\\n                raise ValueError(\"End-effector orientation can not be None for `position_*` command type!\")\\n            # compute targets\\n            if self.cfg.use_relative_mode:\\n                if ee_pos is None:\\n                    raise ValueError(\"End-effector position can not be None for `position_rel` command type!\")\\n                self.ee_pos_des[:] = ee_pos + self._command\\n                self.ee_quat_des[:] = ee_quat\\n            else:\\n                self.ee_pos_des[:] = self._command\\n                self.ee_quat_des[:] = ee_quat\\n        else:\\n            # compute targets\\n            if self.cfg.use_relative_mode:\\n                if ee_pos is None or ee_quat is None:\\n                    raise ValueError(\\n                        \"Neither end-effector position nor orientation can be None for `pose_rel` command type!\"\\n                    )\\n                self.ee_pos_des, self.ee_quat_des = apply_delta_pose(ee_pos, ee_quat, self._command)\\n            else:\\n                self.ee_pos_des = self._command[:, 0:3]\\n                self.ee_quat_des = self._command[:, 3:7]\\n\\n    def compute(\\n        self, ee_pos: torch.Tensor, ee_quat: torch.Tensor, jacobian: torch.Tensor, joint_pos: torch.Tensor\\n    ) -> torch.Tensor:\\n        \"\"\"Computes the target joint positions that will yield the desired end effector pose.\\n\\n        Args:\\n            ee_pos: The current end-effector position in shape (N, 3).\\n            ee_quat: The current end-effector orientation in shape (N, 4).\\n            jacobian: The geometric jacobian matrix in shape (N, 6, num_joints).\\n            joint_pos: The current joint positions in shape (N, num_joints).\\n\\n        Returns:\\n            The target joint positions commands in shape (N, num_joints).\\n        \"\"\"\\n        # compute the delta in joint-space\\n        if \"position\" in self.cfg.command_type:\\n            position_error = self.ee_pos_des - ee_pos\\n            jacobian_pos = jacobian[:, 0:3]\\n            delta_joint_pos = self._compute_delta_joint_pos(delta_pose=position_error, jacobian=jacobian_pos)\\n        else:\\n            position_error, axis_angle_error = compute_pose_error(\\n                ee_pos, ee_quat, self.ee_pos_des, self.ee_quat_des, rot_error_type=\"axis_angle\"\\n            )\\n            pose_error = torch.cat((position_error, axis_angle_error), dim=1)\\n            delta_joint_pos = self._compute_delta_joint_pos(delta_pose=pose_error, jacobian=jacobian)\\n        # return the desired joint positions\\n        return joint_pos + delta_joint_pos\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _compute_delta_joint_pos(self, delta_pose: torch.Tensor, jacobian: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Computes the change in joint position that yields the desired change in pose.\\n\\n        The method uses the Jacobian mapping from joint-space velocities to end-effector velocities\\n        to compute the delta-change in the joint-space that moves the robot closer to a desired\\n        end-effector position.\\n\\n        Args:\\n            delta_pose: The desired delta pose in shape (N, 3) or (N, 6).\\n            jacobian: The geometric jacobian matrix in shape (N, 3, num_joints) or (N, 6, num_joints).\\n\\n        Returns:\\n            The desired delta in joint space. Shape is (N, num-jointsß).\\n        \"\"\"\\n        if self.cfg.ik_params is None:\\n            raise RuntimeError(f\"Inverse-kinematics parameters for method \\'{self.cfg.ik_method}\\' is not defined!\")\\n        # compute the delta in joint-space\\n        if self.cfg.ik_method == \"pinv\":  # Jacobian pseudo-inverse\\n            # parameters\\n            k_val = self.cfg.ik_params[\"k_val\"]\\n            # computation\\n            jacobian_pinv = torch.linalg.pinv(jacobian)\\n            delta_joint_pos = k_val * jacobian_pinv @ delta_pose.unsqueeze(-1)\\n            delta_joint_pos = delta_joint_pos.squeeze(-1)\\n        elif self.cfg.ik_method == \"svd\":  # adaptive SVD\\n            # parameters\\n            k_val = self.cfg.ik_params[\"k_val\"]\\n            min_singular_value = self.cfg.ik_params[\"min_singular_value\"]\\n            # computation\\n            # U: 6xd, S: dxd, V: d x num-joint\\n            U, S, Vh = torch.linalg.svd(jacobian)\\n            S_inv = 1.0 / S\\n            S_inv = torch.where(S > min_singular_value, S_inv, torch.zeros_like(S_inv))\\n            jacobian_pinv = (\\n                torch.transpose(Vh, dim0=1, dim1=2)[:, :, :6]\\n                @ torch.diag_embed(S_inv)\\n                @ torch.transpose(U, dim0=1, dim1=2)\\n            )\\n            delta_joint_pos = k_val * jacobian_pinv @ delta_pose.unsqueeze(-1)\\n            delta_joint_pos = delta_joint_pos.squeeze(-1)\\n        elif self.cfg.ik_method == \"trans\":  # Jacobian transpose\\n            # parameters\\n            k_val = self.cfg.ik_params[\"k_val\"]\\n            # computation\\n            jacobian_T = torch.transpose(jacobian, dim0=1, dim1=2)\\n            delta_joint_pos = k_val * jacobian_T @ delta_pose.unsqueeze(-1)\\n            delta_joint_pos = delta_joint_pos.squeeze(-1)\\n        elif self.cfg.ik_method == \"dls\":  # damped least squares\\n            # parameters\\n            lambda_val = self.cfg.ik_params[\"lambda_val\"]\\n            # computation\\n            jacobian_T = torch.transpose(jacobian, dim0=1, dim1=2)\\n            lambda_matrix = (lambda_val**2) * torch.eye(n=jacobian.shape[1], device=self._device)\\n            delta_joint_pos = (\\n                jacobian_T @ torch.inverse(jacobian @ jacobian_T + lambda_matrix) @ delta_pose.unsqueeze(-1)\\n            )\\n            delta_joint_pos = delta_joint_pos.squeeze(-1)\\n        else:\\n            raise ValueError(f\"Unsupported inverse-kinematics method: {self.cfg.ik_method}\")\\n\\n        return delta_joint_pos'),\n",
       " Document(metadata={}, page_content='class DifferentialIKControllerCfg:\\n    \"\"\"Configuration for differential inverse kinematics controller.\"\"\"\\n\\n    class_type: type = DifferentialIKController\\n    \"\"\"The associated controller class.\"\"\"\\n\\n    command_type: Literal[\"position\", \"pose\"] = MISSING\\n    \"\"\"Type of task-space command to control the articulation\\'s body.\\n\\n    If \"position\", then the controller only controls the position of the articulation\\'s body.\\n    Otherwise, the controller controls the pose of the articulation\\'s body.\\n    \"\"\"\\n\\n    use_relative_mode: bool = False\\n    \"\"\"Whether to use relative mode for the controller. Defaults to False.\\n\\n    If True, then the controller treats the input command as a delta change in the position/pose.\\n    Otherwise, the controller treats the input command as the absolute position/pose.\\n    \"\"\"\\n\\n    ik_method: Literal[\"pinv\", \"svd\", \"trans\", \"dls\"] = MISSING\\n    \"\"\"Method for computing inverse of Jacobian.\"\"\"\\n\\n    ik_params: dict[str, float] | None = None\\n    \"\"\"Parameters for the inverse-kinematics method. Defaults to None, in which case the default\\n    parameters for the method are used.\\n\\n    - Moore-Penrose pseudo-inverse (\"pinv\"):\\n        - \"k_val\": Scaling of computed delta-joint positions (default: 1.0).\\n    - Adaptive Singular Value Decomposition (\"svd\"):\\n        - \"k_val\": Scaling of computed delta-joint positions (default: 1.0).\\n        - \"min_singular_value\": Single values less than this are suppressed to zero (default: 1e-5).\\n    - Jacobian transpose (\"trans\"):\\n        - \"k_val\": Scaling of computed delta-joint positions (default: 1.0).\\n    - Damped Moore-Penrose pseudo-inverse (\"dls\"):\\n        - \"lambda_val\": Damping coefficient (default: 0.01).\\n    \"\"\"\\n\\n    def __post_init__(self):\\n        # check valid input\\n        if self.command_type not in [\"position\", \"pose\"]:\\n            raise ValueError(f\"Unsupported inverse-kinematics command: {self.command_type}.\")\\n        if self.ik_method not in [\"pinv\", \"svd\", \"trans\", \"dls\"]:\\n            raise ValueError(f\"Unsupported inverse-kinematics method: {self.ik_method}.\")\\n        # default parameters for different inverse kinematics approaches.\\n        default_ik_params = {\\n            \"pinv\": {\"k_val\": 1.0},\\n            \"svd\": {\"k_val\": 1.0, \"min_singular_value\": 1e-5},\\n            \"trans\": {\"k_val\": 1.0},\\n            \"dls\": {\"lambda_val\": 0.01},\\n        }\\n        # update parameters for IK-method if not provided\\n        ik_params = default_ik_params[self.ik_method].copy()\\n        if self.ik_params is not None:\\n            ik_params.update(self.ik_params)\\n        self.ik_params = ik_params'),\n",
       " Document(metadata={}, page_content='class JointImpedanceControllerCfg:\\n    \"\"\"Configuration for joint impedance regulation controller.\"\"\"\\n\\n    command_type: str = \"p_abs\"\\n    \"\"\"Type of command: p_abs (absolute) or p_rel (relative).\"\"\"\\n\\n    dof_pos_offset: Sequence[float] | None = None\\n    \"\"\"Offset to DOF position command given to controller. (default: None).\\n\\n    If None then position offsets are set to zero.\\n    \"\"\"\\n\\n    impedance_mode: str = MISSING\\n    \"\"\"Type of gains: \"fixed\", \"variable\", \"variable_kp\".\"\"\"\\n\\n    inertial_compensation: bool = False\\n    \"\"\"Whether to perform inertial compensation (inverse dynamics).\"\"\"\\n\\n    gravity_compensation: bool = False\\n    \"\"\"Whether to perform gravity compensation.\"\"\"\\n\\n    stiffness: float | Sequence[float] = MISSING\\n    \"\"\"The positional gain for determining desired torques based on joint position error.\"\"\"\\n\\n    damping_ratio: float | Sequence[float] | None = None\\n    \"\"\"The damping ratio is used in-conjunction with positional gain to compute desired torques\\n    based on joint velocity error.\\n\\n    The following math operation is performed for computing velocity gains:\\n        :math:`d_gains = 2 * sqrt(p_gains) * damping_ratio`.\\n    \"\"\"\\n\\n    stiffness_limits: tuple[float, float] = (0, 300)\\n    \"\"\"Minimum and maximum values for positional gains.\\n\\n    Note: Used only when :obj:`impedance_mode` is \"variable\" or \"variable_kp\".\\n    \"\"\"\\n\\n    damping_ratio_limits: tuple[float, float] = (0, 100)\\n    \"\"\"Minimum and maximum values for damping ratios used to compute velocity gains.\\n\\n    Note: Used only when :obj:`impedance_mode` is \"variable\".\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class JointImpedanceController:\\n    \"\"\"Joint impedance regulation control.\\n\\n    Reference:\\n        [1] https://ethz.ch/content/dam/ethz/special-interest/mavt/robotics-n-intelligent-systems/rsl-dam/documents/RobotDynamics2017/RD_HS2017script.pdf\\n    \"\"\"\\n\\n    def __init__(self, cfg: JointImpedanceControllerCfg, num_robots: int, dof_pos_limits: torch.Tensor, device: str):\\n        \"\"\"Initialize joint impedance controller.\\n\\n        Args:\\n            cfg: The configuration for the controller.\\n            num_robots: The number of robots to control.\\n            dof_pos_limits: The joint position limits for each robot. This is a tensor of shape\\n                (num_robots, num_dof, 2) where the last dimension contains the lower and upper limits.\\n            device: The device to use for computations.\\n\\n        Raises:\\n            ValueError: When the shape of :obj:`dof_pos_limits` is not (num_robots, num_dof, 2).\\n        \"\"\"\\n        # check valid inputs\\n        if len(dof_pos_limits.shape) != 3:\\n            raise ValueError(f\"Joint position limits has shape \\'{dof_pos_limits.shape}\\'. Expected length of shape = 3.\")\\n        # store inputs\\n        self.cfg = cfg\\n        self.num_robots = num_robots\\n        self.num_dof = dof_pos_limits.shape[1]  # (num_robots, num_dof, 2)\\n        self._device = device\\n\\n        # create buffers\\n        # -- commands\\n        self._dof_pos_target = torch.zeros(self.num_robots, self.num_dof, device=self._device)\\n        # -- offsets\\n        self._dof_pos_offset = torch.zeros(self.num_robots, self.num_dof, device=self._device)\\n        # -- limits\\n        self._dof_pos_limits = dof_pos_limits\\n        # -- positional gains\\n        self._p_gains = torch.zeros(self.num_robots, self.num_dof, device=self._device)\\n        self._p_gains[:] = torch.tensor(self.cfg.stiffness, device=self._device)\\n        # -- velocity gains\\n        self._d_gains = torch.zeros(self.num_robots, self.num_dof, device=self._device)\\n        self._d_gains[:] = 2 * torch.sqrt(self._p_gains) * torch.tensor(self.cfg.damping_ratio, device=self._device)\\n        # -- position offsets\\n        if self.cfg.dof_pos_offset is not None:\\n            self._dof_pos_offset[:] = torch.tensor(self.cfg.dof_pos_offset, device=self._device)\\n        # -- position gain limits\\n        self._p_gains_limits = torch.zeros_like(self._dof_pos_limits)\\n        self._p_gains_limits[..., 0] = self.cfg.stiffness_limits[0]\\n        self._p_gains_limits[..., 1] = self.cfg.stiffness_limits[1]\\n        # -- damping ratio limits\\n        self._damping_ratio_limits = torch.zeros_like(self._dof_pos_limits)\\n        self._damping_ratio_limits[..., 0] = self.cfg.damping_ratio_limits[0]\\n        self._damping_ratio_limits[..., 1] = self.cfg.damping_ratio_limits[1]\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_actions(self) -> int:\\n        \"\"\"Dimension of the action space of controller.\"\"\"\\n        # impedance mode\\n        if self.cfg.impedance_mode == \"fixed\":\\n            # joint positions\\n            return self.num_dof\\n        elif self.cfg.impedance_mode == \"variable_kp\":\\n            # joint positions + stiffness\\n            return self.num_dof * 2\\n        elif self.cfg.impedance_mode == \"variable\":\\n            # joint positions + stiffness + damping\\n            return self.num_dof * 3\\n        else:\\n            raise ValueError(f\"Invalid impedance mode: {self.cfg.impedance_mode}.\")\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def initialize(self):\\n        \"\"\"Initialize the internals.\"\"\"\\n        pass\\n\\n    def reset_idx(self, robot_ids: torch.Tensor = None):\\n        \"\"\"Reset the internals.\"\"\"\\n        pass\\n\\n    def set_command(self, command: torch.Tensor):\\n        \"\"\"Set target end-effector pose command.\\n\\n        Args:\\n            command: The command to set. This is a tensor of shape (num_robots, num_actions) where\\n                :obj:`num_actions` is the dimension of the action space of the controller.\\n        \"\"\"\\n        # check input size\\n        if command.shape != (self.num_robots, self.num_actions):\\n            raise ValueError(\\n                f\"Invalid command shape \\'{command.shape}\\'. Expected: \\'{(self.num_robots, self.num_actions)}\\'.\"\\n            )\\n        # impedance mode\\n        if self.cfg.impedance_mode == \"fixed\":\\n            # joint positions\\n            self._dof_pos_target[:] = command\\n        elif self.cfg.impedance_mode == \"variable_kp\":\\n            # split input command\\n            dof_pos_command, stiffness = torch.tensor_split(command, 2, dim=-1)\\n            # format command\\n            stiffness = stiffness.clip_(min=self._p_gains_limits[0], max=self._p_gains_limits[1])\\n            # joint positions + stiffness\\n            self._dof_pos_target[:] = dof_pos_command\\n            self._p_gains[:] = stiffness\\n            self._d_gains[:] = 2 * torch.sqrt(self._p_gains)  # critically damped\\n        elif self.cfg.impedance_mode == \"variable\":\\n            # split input command\\n            dof_pos_command, stiffness, damping_ratio = torch.tensor_split(command, 3, dim=-1)\\n            # format command\\n            stiffness = stiffness.clip_(min=self._p_gains_limits[0], max=self._p_gains_limits[1])\\n            damping_ratio = damping_ratio.clip_(min=self._damping_ratio_limits[0], max=self._damping_ratio_limits[1])\\n            # joint positions + stiffness + damping\\n            self._dof_pos_target[:] = dof_pos_command\\n            self._p_gains[:] = stiffness\\n            self._d_gains[:] = 2 * torch.sqrt(self._p_gains) * damping_ratio\\n        else:\\n            raise ValueError(f\"Invalid impedance mode: {self.cfg.impedance_mode}.\")\\n\\n    def compute(\\n        self,\\n        dof_pos: torch.Tensor,\\n        dof_vel: torch.Tensor,\\n        mass_matrix: torch.Tensor | None = None,\\n        gravity: torch.Tensor | None = None,\\n    ) -> torch.Tensor:\\n        \"\"\"Performs inference with the controller.\\n\\n        Args:\\n            dof_pos: The current joint positions.\\n            dof_vel: The current joint velocities.\\n            mass_matrix: The joint-space inertial matrix. Defaults to None.\\n            gravity: The joint-space gravity vector. Defaults to None.\\n\\n        Raises:\\n            ValueError: When the command type is invalid.\\n\\n        Returns:\\n            The target joint torques commands.\\n        \"\"\"\\n        # resolve the command type\\n        if self.cfg.command_type == \"p_abs\":\\n            desired_dof_pos = self._dof_pos_target + self._dof_pos_offset\\n        elif self.cfg.command_type == \"p_rel\":\\n            desired_dof_pos = self._dof_pos_target + dof_pos\\n        else:\\n            raise ValueError(f\"Invalid dof position command mode: {self.cfg.command_type}.\")\\n        # compute errors\\n        desired_dof_pos = desired_dof_pos.clip_(min=self._dof_pos_limits[..., 0], max=self._dof_pos_limits[..., 1])\\n        dof_pos_error = desired_dof_pos - dof_pos\\n        dof_vel_error = -dof_vel\\n        # compute acceleration\\n        des_dof_acc = self._p_gains * dof_pos_error + self._d_gains * dof_vel_error\\n        # compute torques\\n        # -- inertial compensation\\n        if self.cfg.inertial_compensation:\\n            # inverse dynamics control\\n            desired_torques = mass_matrix @ des_dof_acc\\n        else:\\n            # decoupled spring-mass control\\n            desired_torques = des_dof_acc\\n        # -- gravity compensation (bias correction)\\n        if self.cfg.gravity_compensation:\\n            desired_torques += gravity\\n\\n        return desired_torques'),\n",
       " Document(metadata={}, page_content='class OperationalSpaceController:\\n    \"\"\"Operational-space controller.\\n\\n    Reference:\\n\\n    1. `A unified approach for motion and force control of robot manipulators: The operational space formulation <http://dx.doi.org/10.1109/JRA.1987.1087068>`_\\n       by Oussama Khatib (Stanford University)\\n    2. `Robot Dynamics Lecture Notes <https://ethz.ch/content/dam/ethz/special-interest/mavt/robotics-n-intelligent-systems/rsl-dam/documents/RobotDynamics2017/RD_HS2017script.pdf>`_\\n       by Marco Hutter (ETH Zurich)\\n    \"\"\"\\n\\n    def __init__(self, cfg: OperationalSpaceControllerCfg, num_envs: int, device: str):\\n        \"\"\"Initialize operational-space controller.\\n\\n        Args:\\n            cfg: The configuration for operational-space controller.\\n            num_envs: The number of environments.\\n            device: The device to use for computations.\\n\\n        Raises:\\n            ValueError: When invalid control command is provided.\\n        \"\"\"\\n        # store inputs\\n        self.cfg = cfg\\n        self.num_envs = num_envs\\n        self._device = device\\n\\n        # resolve tasks-pace target dimensions\\n        self.target_list = list()\\n        for command_type in self.cfg.target_types:\\n            if command_type == \"pose_rel\":\\n                self.target_list.append(6)\\n            elif command_type == \"pose_abs\":\\n                self.target_list.append(7)\\n            elif command_type == \"wrench_abs\":\\n                self.target_list.append(6)\\n            else:\\n                raise ValueError(f\"Invalid control command: {command_type}.\")\\n        self.target_dim = sum(self.target_list)\\n\\n        # create buffers\\n        # -- selection matrices, which might be defined in the task reference frame different from the root frame\\n        self._selection_matrix_motion_task = torch.diag_embed(\\n            torch.tensor(self.cfg.motion_control_axes_task, dtype=torch.float, device=self._device)\\n            .unsqueeze(0)\\n            .repeat(self.num_envs, 1)\\n        )\\n        self._selection_matrix_force_task = torch.diag_embed(\\n            torch.tensor(self.cfg.contact_wrench_control_axes_task, dtype=torch.float, device=self._device)\\n            .unsqueeze(0)\\n            .repeat(self.num_envs, 1)\\n        )\\n        # -- selection matrices in root frame\\n        self._selection_matrix_motion_b = torch.zeros_like(self._selection_matrix_motion_task)\\n        self._selection_matrix_force_b = torch.zeros_like(self._selection_matrix_force_task)\\n        # -- commands\\n        self._task_space_target_task = torch.zeros(self.num_envs, self.target_dim, device=self._device)\\n        # -- Placeholders for motion/force control\\n        self.desired_ee_pose_task = None\\n        self.desired_ee_pose_b = None\\n        self.desired_ee_wrench_task = None\\n        self.desired_ee_wrench_b = None\\n        # -- buffer for operational space mass matrix\\n        self._os_mass_matrix_b = torch.zeros(self.num_envs, 6, 6, device=self._device)\\n        # -- Placeholder for the inverse of joint space mass matrix\\n        self._mass_matrix_inv = None\\n        # -- motion control gains\\n        self._motion_p_gains_task = torch.diag_embed(\\n            torch.ones(self.num_envs, 6, device=self._device)\\n            * torch.tensor(self.cfg.motion_stiffness_task, dtype=torch.float, device=self._device)\\n        )\\n        # -- -- zero out the axes that are not motion controlled, as keeping them non-zero will cause other axes\\n        # -- -- to act due to coupling\\n        self._motion_p_gains_task[:] = self._selection_matrix_motion_task @ self._motion_p_gains_task[:]\\n        self._motion_d_gains_task = torch.diag_embed(\\n            2\\n            * torch.diagonal(self._motion_p_gains_task, dim1=-2, dim2=-1).sqrt()\\n            * torch.as_tensor(self.cfg.motion_damping_ratio_task, dtype=torch.float, device=self._device).reshape(1, -1)\\n        )\\n        # -- -- motion control gains in root frame\\n        self._motion_p_gains_b = torch.zeros_like(self._motion_p_gains_task)\\n        self._motion_d_gains_b = torch.zeros_like(self._motion_d_gains_task)\\n        # -- force control gains\\n        if self.cfg.contact_wrench_stiffness_task is not None:\\n            self._contact_wrench_p_gains_task = torch.diag_embed(\\n                torch.ones(self.num_envs, 6, device=self._device)\\n                * torch.tensor(self.cfg.contact_wrench_stiffness_task, dtype=torch.float, device=self._device)\\n            )\\n            self._contact_wrench_p_gains_task[:] = (\\n                self._selection_matrix_force_task @ self._contact_wrench_p_gains_task[:]\\n            )\\n            # -- -- force control gains in root frame\\n            self._contact_wrench_p_gains_b = torch.zeros_like(self._contact_wrench_p_gains_task)\\n        else:\\n            self._contact_wrench_p_gains_task = None\\n            self._contact_wrench_p_gains_b = None\\n        # -- position gain limits\\n        self._motion_p_gains_limits = torch.zeros(self.num_envs, 6, 2, device=self._device)\\n        self._motion_p_gains_limits[..., 0], self._motion_p_gains_limits[..., 1] = (\\n            self.cfg.motion_stiffness_limits_task[0],\\n            self.cfg.motion_stiffness_limits_task[1],\\n        )\\n        # -- damping ratio limits\\n        self._motion_damping_ratio_limits = torch.zeros_like(self._motion_p_gains_limits)\\n        self._motion_damping_ratio_limits[..., 0], self._motion_damping_ratio_limits[..., 1] = (\\n            self.cfg.motion_damping_ratio_limits_task[0],\\n            self.cfg.motion_damping_ratio_limits_task[1],\\n        )\\n        # -- end-effector contact wrench\\n        self._ee_contact_wrench_b = torch.zeros(self.num_envs, 6, device=self._device)\\n\\n        # -- buffers for null-space control gains\\n        self._nullspace_p_gain = torch.tensor(self.cfg.nullspace_stiffness, dtype=torch.float, device=self._device)\\n        self._nullspace_d_gain = (\\n            2\\n            * torch.sqrt(self._nullspace_p_gain)\\n            * torch.tensor(self.cfg.nullspace_damping_ratio, dtype=torch.float, device=self._device)\\n        )\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        \"\"\"Dimension of the action space of controller.\"\"\"\\n        # impedance mode\\n        if self.cfg.impedance_mode == \"fixed\":\\n            # task-space targets\\n            return self.target_dim\\n        elif self.cfg.impedance_mode == \"variable_kp\":\\n            # task-space targets + stiffness\\n            return self.target_dim + 6\\n        elif self.cfg.impedance_mode == \"variable\":\\n            # task-space targets + stiffness + damping\\n            return self.target_dim + 6 + 6\\n        else:\\n            raise ValueError(f\"Invalid impedance mode: {self.cfg.impedance_mode}.\")\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self):\\n        \"\"\"Reset the internals.\"\"\"\\n        self.desired_ee_pose_b = None\\n        self.desired_ee_pose_task = None\\n        self.desired_ee_wrench_b = None\\n        self.desired_ee_wrench_task = None\\n\\n    def set_command(\\n        self,\\n        command: torch.Tensor,\\n        current_ee_pose_b: torch.Tensor | None = None,\\n        current_task_frame_pose_b: torch.Tensor | None = None,\\n    ):\\n        \"\"\"Set the task-space targets and impedance parameters.\\n\\n        Args:\\n            command (torch.Tensor): A concatenated tensor of shape (``num_envs``, ``action_dim``) containing task-space\\n                targets (i.e., pose/wrench) and impedance parameters.\\n            current_ee_pose_b (torch.Tensor, optional): Current end-effector pose, in root frame, of shape\\n                (``num_envs``, 7), containing position and quaternion ``(w, x, y, z)``. Required for relative\\n                commands. Defaults to None.\\n            current_task_frame_pose_b: Current pose of the task frame, in root frame, in which the targets and the\\n                (motion/wrench) control axes are defined. It is a tensor of shape (``num_envs``, 7),\\n                containing position and the quaternion ``(w, x, y, z)``. Defaults to None.\\n\\n        Format:\\n            Task-space targets, ordered according to \\'command_types\\':\\n\\n                Absolute pose: shape (``num_envs``, 7), containing position and quaternion ``(w, x, y, z)``.\\n                Relative pose: shape (``num_envs``, 6), containing delta position and rotation in axis-angle form.\\n                Absolute wrench: shape (``num_envs``, 6), containing force and torque.\\n\\n            Impedance parameters: stiffness for ``variable_kp``, or stiffness, followed by damping ratio for\\n            ``variable``:\\n\\n                Stiffness: shape (``num_envs``, 6)\\n                Damping ratio: shape (``num_envs``, 6)\\n\\n        Raises:\\n            ValueError: When the command dimensions are invalid.\\n            ValueError: When an invalid impedance mode is provided.\\n            ValueError: When the current end-effector pose is not provided for the ``pose_rel`` command.\\n            ValueError: When an invalid control command is provided.\\n        \"\"\"\\n        # Check the input dimensions\\n        if command.shape != (self.num_envs, self.action_dim):\\n            raise ValueError(\\n                f\"Invalid command shape \\'{command.shape}\\'. Expected: \\'{(self.num_envs, self.action_dim)}\\'.\"\\n            )\\n\\n        # Resolve the impedance parameters\\n        if self.cfg.impedance_mode == \"fixed\":\\n            # task space targets (i.e., pose/wrench)\\n            self._task_space_target_task[:] = command\\n        elif self.cfg.impedance_mode == \"variable_kp\":\\n            # split input command\\n            task_space_command, stiffness = torch.split(command, [self.target_dim, 6], dim=-1)\\n            # format command\\n            stiffness = stiffness.clip_(\\n                min=self._motion_p_gains_limits[..., 0], max=self._motion_p_gains_limits[..., 1]\\n            )\\n            # task space targets + stiffness\\n            self._task_space_target_task[:] = task_space_command.squeeze(dim=-1)\\n            self._motion_p_gains_task[:] = torch.diag_embed(stiffness)\\n            self._motion_p_gains_task[:] = self._selection_matrix_motion_task @ self._motion_p_gains_task[:]\\n            self._motion_d_gains_task = torch.diag_embed(\\n                2\\n                * torch.diagonal(self._motion_p_gains_task, dim1=-2, dim2=-1).sqrt()\\n                * torch.as_tensor(self.cfg.motion_damping_ratio_task, dtype=torch.float, device=self._device).reshape(\\n                    1, -1\\n                )\\n            )\\n        elif self.cfg.impedance_mode == \"variable\":\\n            # split input command\\n            task_space_command, stiffness, damping_ratio = torch.split(command, [self.target_dim, 6, 6], dim=-1)\\n            # format command\\n            stiffness = stiffness.clip_(\\n                min=self._motion_p_gains_limits[..., 0], max=self._motion_p_gains_limits[..., 1]\\n            )\\n            damping_ratio = damping_ratio.clip_(\\n                min=self._motion_damping_ratio_limits[..., 0], max=self._motion_damping_ratio_limits[..., 1]\\n            )\\n            # task space targets + stiffness + damping\\n            self._task_space_target_task[:] = task_space_command\\n            self._motion_p_gains_task[:] = torch.diag_embed(stiffness)\\n            self._motion_p_gains_task[:] = self._selection_matrix_motion_task @ self._motion_p_gains_task[:]\\n            self._motion_d_gains_task[:] = torch.diag_embed(\\n                2 * torch.diagonal(self._motion_p_gains_task, dim1=-2, dim2=-1).sqrt() * damping_ratio\\n            )\\n        else:\\n            raise ValueError(f\"Invalid impedance mode: {self.cfg.impedance_mode}.\")\\n\\n        if current_task_frame_pose_b is None:\\n            current_task_frame_pose_b = torch.tensor(\\n                [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]] * self.num_envs, device=self._device\\n            )\\n\\n        # Resolve the target commands\\n        target_groups = torch.split(self._task_space_target_task, self.target_list, dim=1)\\n        for command_type, target in zip(self.cfg.target_types, target_groups):\\n            if command_type == \"pose_rel\":\\n                # check input is provided\\n                if current_ee_pose_b is None:\\n                    raise ValueError(\"Current pose is required for \\'pose_rel\\' command.\")\\n                # Transform the current pose from base/root frame to task frame\\n                current_ee_pos_task, current_ee_rot_task = subtract_frame_transforms(\\n                    current_task_frame_pose_b[:, :3],\\n                    current_task_frame_pose_b[:, 3:],\\n                    current_ee_pose_b[:, :3],\\n                    current_ee_pose_b[:, 3:],\\n                )\\n                # compute targets in task frame\\n                desired_ee_pos_task, desired_ee_rot_task = apply_delta_pose(\\n                    current_ee_pos_task, current_ee_rot_task, target\\n                )\\n                self.desired_ee_pose_task = torch.cat([desired_ee_pos_task, desired_ee_rot_task], dim=-1)\\n            elif command_type == \"pose_abs\":\\n                # compute targets\\n                self.desired_ee_pose_task = target.clone()\\n            elif command_type == \"wrench_abs\":\\n                # compute targets\\n                self.desired_ee_wrench_task = target.clone()\\n            else:\\n                raise ValueError(f\"Invalid control command: {command_type}.\")\\n\\n        # Rotation of task frame wrt root frame, converts a coordinate from task frame to root frame.\\n        R_task_b = matrix_from_quat(current_task_frame_pose_b[:, 3:])\\n        # Rotation of root frame wrt task frame, converts a coordinate from root frame to task frame.\\n        R_b_task = R_task_b.mT\\n\\n        # Transform motion control stiffness gains from task frame to root frame\\n        self._motion_p_gains_b[:, 0:3, 0:3] = R_task_b @ self._motion_p_gains_task[:, 0:3, 0:3] @ R_b_task\\n        self._motion_p_gains_b[:, 3:6, 3:6] = R_task_b @ self._motion_p_gains_task[:, 3:6, 3:6] @ R_b_task\\n\\n        # Transform motion control damping gains from task frame to root frame\\n        self._motion_d_gains_b[:, 0:3, 0:3] = R_task_b @ self._motion_d_gains_task[:, 0:3, 0:3] @ R_b_task\\n        self._motion_d_gains_b[:, 3:6, 3:6] = R_task_b @ self._motion_d_gains_task[:, 3:6, 3:6] @ R_b_task\\n\\n        # Transform contact wrench gains from task frame to root frame (if applicable)\\n        if self._contact_wrench_p_gains_task is not None and self._contact_wrench_p_gains_b is not None:\\n            self._contact_wrench_p_gains_b[:, 0:3, 0:3] = (\\n                R_task_b @ self._contact_wrench_p_gains_task[:, 0:3, 0:3] @ R_b_task\\n            )\\n            self._contact_wrench_p_gains_b[:, 3:6, 3:6] = (\\n                R_task_b @ self._contact_wrench_p_gains_task[:, 3:6, 3:6] @ R_b_task\\n            )\\n\\n        # Transform selection matrices from target frame to base frame\\n        self._selection_matrix_motion_b[:, 0:3, 0:3] = (\\n            R_task_b @ self._selection_matrix_motion_task[:, 0:3, 0:3] @ R_b_task\\n        )\\n        self._selection_matrix_motion_b[:, 3:6, 3:6] = (\\n            R_task_b @ self._selection_matrix_motion_task[:, 3:6, 3:6] @ R_b_task\\n        )\\n        self._selection_matrix_force_b[:, 0:3, 0:3] = (\\n            R_task_b @ self._selection_matrix_force_task[:, 0:3, 0:3] @ R_b_task\\n        )\\n        self._selection_matrix_force_b[:, 3:6, 3:6] = (\\n            R_task_b @ self._selection_matrix_force_task[:, 3:6, 3:6] @ R_b_task\\n        )\\n\\n        # Transform desired pose from task frame to root frame\\n        if self.desired_ee_pose_task is not None:\\n            self.desired_ee_pose_b = torch.zeros_like(self.desired_ee_pose_task)\\n            self.desired_ee_pose_b[:, :3], self.desired_ee_pose_b[:, 3:] = combine_frame_transforms(\\n                current_task_frame_pose_b[:, :3],\\n                current_task_frame_pose_b[:, 3:],\\n                self.desired_ee_pose_task[:, :3],\\n                self.desired_ee_pose_task[:, 3:],\\n            )\\n\\n        # Transform desired wrenches to root frame\\n        if self.desired_ee_wrench_task is not None:\\n            self.desired_ee_wrench_b = torch.zeros_like(self.desired_ee_wrench_task)\\n            self.desired_ee_wrench_b[:, :3] = (R_task_b @ self.desired_ee_wrench_task[:, :3].unsqueeze(-1)).squeeze(-1)\\n            self.desired_ee_wrench_b[:, 3:] = (R_task_b @ self.desired_ee_wrench_task[:, 3:].unsqueeze(-1)).squeeze(\\n                -1\\n            ) + torch.cross(current_task_frame_pose_b[:, :3], self.desired_ee_wrench_b[:, :3], dim=-1)\\n\\n    def compute(\\n        self,\\n        jacobian_b: torch.Tensor,\\n        current_ee_pose_b: torch.Tensor | None = None,\\n        current_ee_vel_b: torch.Tensor | None = None,\\n        current_ee_force_b: torch.Tensor | None = None,\\n        mass_matrix: torch.Tensor | None = None,\\n        gravity: torch.Tensor | None = None,\\n        current_joint_pos: torch.Tensor | None = None,\\n        current_joint_vel: torch.Tensor | None = None,\\n        nullspace_joint_pos_target: torch.Tensor | None = None,\\n    ) -> torch.Tensor:\\n        \"\"\"Performs inference with the controller.\\n\\n        Args:\\n            jacobian_b: The Jacobian matrix of the end-effector in root frame. It is a tensor of shape\\n                (``num_envs``, 6, ``num_DoF``).\\n            current_ee_pose_b: The current end-effector pose in root frame. It is a tensor of shape\\n                (``num_envs``, 7), which contains the position and quaternion ``(w, x, y, z)``. Defaults to ``None``.\\n            current_ee_vel_b: The current end-effector velocity in root frame. It is a tensor of shape\\n                (``num_envs``, 6), which contains the linear and angular velocities. Defaults to None.\\n            current_ee_force_b: The current external force on the end-effector in root frame. It is a tensor of\\n                shape (``num_envs``, 3), which contains the linear force. Defaults to ``None``.\\n            mass_matrix: The joint-space mass/inertia matrix. It is a tensor of shape (``num_envs``, ``num_DoF``,\\n                ``num_DoF``). Defaults to ``None``.\\n            gravity: The joint-space gravity vector. It is a tensor of shape (``num_envs``, ``num_DoF``). Defaults\\n                to ``None``.\\n            current_joint_pos: The current joint positions. It is a tensor of shape (``num_envs``, ``num_DoF``).\\n                Defaults to ``None``.\\n            current_joint_vel: The current joint velocities. It is a tensor of shape (``num_envs``, ``num_DoF``).\\n                Defaults to ``None``.\\n            nullspace_joint_pos_target: The target joint positions the null space controller is trying to enforce, when\\n                possible. It is a tensor of shape (``num_envs``, ``num_DoF``).\\n\\n        Raises:\\n            ValueError: When motion-control is enabled but the current end-effector pose or velocity is not provided.\\n            ValueError: When inertial dynamics decoupling is enabled but the mass matrix is not provided.\\n            ValueError: When the current end-effector pose is not provided for the ``pose_rel`` command.\\n            ValueError: When closed-loop force control is enabled but the current end-effector force is not provided.\\n            ValueError: When gravity compensation is enabled but the gravity vector is not provided.\\n            ValueError: When null-space control is enabled but the system is not redundant.\\n            ValueError: When dynamically consistent pseudo-inverse is enabled but the mass matrix inverse is not\\n                provided.\\n            ValueError: When null-space control is enabled but the current joint positions and velocities are not\\n                provided.\\n            ValueError: When target joint positions are provided for null-space control but their dimensions do not\\n                match the current joint positions.\\n            ValueError: When an invalid null-space control method is provided.\\n\\n        Returns:\\n            Tensor: The joint efforts computed by the controller. It is a tensor of shape (``num_envs``, ``num_DoF``).\\n        \"\"\"\\n\\n        # deduce number of DoF\\n        num_DoF = jacobian_b.shape[2]\\n        # create joint effort vector\\n        joint_efforts = torch.zeros(self.num_envs, num_DoF, device=self._device)\\n\\n        # compute joint efforts for motion-control\\n        if self.desired_ee_pose_b is not None:\\n            # check input is provided\\n            if current_ee_pose_b is None or current_ee_vel_b is None:\\n                raise ValueError(\"Current end-effector pose and velocity are required for motion control.\")\\n            # -- end-effector tracking error\\n            pose_error_b = torch.cat(\\n                compute_pose_error(\\n                    current_ee_pose_b[:, :3],\\n                    current_ee_pose_b[:, 3:],\\n                    self.desired_ee_pose_b[:, :3],\\n                    self.desired_ee_pose_b[:, 3:],\\n                    rot_error_type=\"axis_angle\",\\n                ),\\n                dim=-1,\\n            )\\n            velocity_error_b = -current_ee_vel_b  # zero target velocity. The target is assumed to be stationary.\\n            # -- desired end-effector acceleration (spring-damper system)\\n            des_ee_acc_b = self._motion_p_gains_b @ pose_error_b.unsqueeze(\\n                -1\\n            ) + self._motion_d_gains_b @ velocity_error_b.unsqueeze(-1)\\n            # -- Inertial dynamics decoupling\\n            if self.cfg.inertial_dynamics_decoupling:\\n                # check input is provided\\n                if mass_matrix is None:\\n                    raise ValueError(\"Mass matrix is required for inertial decoupling.\")\\n                # Compute operational space mass matrix\\n                self._mass_matrix_inv = torch.inverse(mass_matrix)\\n                if self.cfg.partial_inertial_dynamics_decoupling:\\n                    # Fill in the translational and rotational parts of the inertia separately, ignoring their coupling\\n                    self._os_mass_matrix_b[:, 0:3, 0:3] = torch.inverse(\\n                        jacobian_b[:, 0:3] @ self._mass_matrix_inv @ jacobian_b[:, 0:3].mT\\n                    )\\n                    self._os_mass_matrix_b[:, 3:6, 3:6] = torch.inverse(\\n                        jacobian_b[:, 3:6] @ self._mass_matrix_inv @ jacobian_b[:, 3:6].mT\\n                    )\\n                else:\\n                    # Calculate the operational space mass matrix fully accounting for the couplings\\n                    self._os_mass_matrix_b[:] = torch.inverse(jacobian_b @ self._mass_matrix_inv @ jacobian_b.mT)\\n                # (Generalized) operational space command forces\\n                # F = (J M^(-1) J^T)^(-1) * \\\\ddot(x_des) = M_task * \\\\ddot(x_des)\\n                os_command_forces_b = self._os_mass_matrix_b @ des_ee_acc_b\\n            else:\\n                # Task-space impedance control: command forces = \\\\ddot(x_des).\\n                # Please note that the definition of task-space impedance control varies in literature.\\n                # This implementation ignores the inertial term. For inertial decoupling,\\n                # use inertial_dynamics_decoupling=True.\\n                os_command_forces_b = des_ee_acc_b\\n            # -- joint-space commands\\n            joint_efforts += (jacobian_b.mT @ self._selection_matrix_motion_b @ os_command_forces_b).squeeze(-1)\\n\\n        # compute joint efforts for contact wrench/force control\\n        if self.desired_ee_wrench_b is not None:\\n            # -- task-space contact wrench\\n            if self.cfg.contact_wrench_stiffness_task is not None:\\n                # check input is provided\\n                if current_ee_force_b is None:\\n                    raise ValueError(\"Current end-effector force is required for closed-loop force control.\")\\n                # We can only measure the force component at the contact, so only apply the feedback for only the force\\n                # component, keep the control of moment components open loop\\n                self._ee_contact_wrench_b[:, 0:3] = current_ee_force_b\\n                self._ee_contact_wrench_b[:, 3:6] = self.desired_ee_wrench_b[:, 3:6]\\n                # closed-loop control with feedforward term\\n                os_contact_wrench_command_b = self.desired_ee_wrench_b.unsqueeze(\\n                    -1\\n                ) + self._contact_wrench_p_gains_b @ (self.desired_ee_wrench_b - self._ee_contact_wrench_b).unsqueeze(\\n                    -1\\n                )\\n            else:\\n                # open-loop control\\n                os_contact_wrench_command_b = self.desired_ee_wrench_b.unsqueeze(-1)\\n            # -- joint-space commands\\n            joint_efforts += (jacobian_b.mT @ self._selection_matrix_force_b @ os_contact_wrench_command_b).squeeze(-1)\\n\\n        # add gravity compensation (bias correction)\\n        if self.cfg.gravity_compensation:\\n            # check input is provided\\n            if gravity is None:\\n                raise ValueError(\"Gravity vector is required for gravity compensation.\")\\n            # add gravity compensation\\n            joint_efforts += gravity\\n\\n        # Add null-space control\\n        # -- Free null-space control\\n        if self.cfg.nullspace_control == \"none\":\\n            # No additional control is applied in the null space.\\n            pass\\n        else:\\n            # Check if the system is redundant\\n            if num_DoF <= 6:\\n                raise ValueError(\"Null-space control is only applicable for redundant manipulators.\")\\n\\n            # Calculate the pseudo-inverse of the Jacobian\\n            if self.cfg.inertial_dynamics_decoupling and not self.cfg.partial_inertial_dynamics_decoupling:\\n                # Dynamically consistent pseudo-inverse allows decoupling of null space and task space\\n                if self._mass_matrix_inv is None or mass_matrix is None:\\n                    raise ValueError(\"Mass matrix inverse is required for dynamically consistent pseudo-inverse\")\\n                jacobian_pinv_transpose = self._os_mass_matrix_b @ jacobian_b @ self._mass_matrix_inv\\n            else:\\n                # Moore-Penrose pseudo-inverse if full inertia matrix is not available (e.g., no/partial decoupling)\\n                jacobian_pinv_transpose = torch.pinverse(jacobian_b).mT\\n\\n            # Calculate the null-space projector\\n            nullspace_jacobian_transpose = (\\n                torch.eye(n=num_DoF, device=self._device) - jacobian_b.mT @ jacobian_pinv_transpose\\n            )\\n\\n            # Null space position control\\n            if self.cfg.nullspace_control == \"position\":\\n\\n                # Check if the current joint positions and velocities are provided\\n                if current_joint_pos is None or current_joint_vel is None:\\n                    raise ValueError(\"Current joint positions and velocities are required for null-space control.\")\\n\\n                # Calculate the joint errors for nullspace position control\\n                if nullspace_joint_pos_target is None:\\n                    nullspace_joint_pos_target = torch.zeros_like(current_joint_pos)\\n                # Check if the dimensions of the target nullspace joint positions match the current joint positions\\n                elif nullspace_joint_pos_target.shape != current_joint_pos.shape:\\n                    raise ValueError(\\n                        f\"The target nullspace joint positions shape \\'{nullspace_joint_pos_target.shape}\\' does not\"\\n                        f\"match the current joint positions shape \\'{current_joint_pos.shape}\\'.\"\\n                    )\\n\\n                joint_pos_error_nullspace = nullspace_joint_pos_target - current_joint_pos\\n                joint_vel_error_nullspace = -current_joint_vel\\n\\n                # Calculate the desired joint accelerations\\n                joint_acc_nullspace = (\\n                    self._nullspace_p_gain * joint_pos_error_nullspace\\n                    + self._nullspace_d_gain * joint_vel_error_nullspace\\n                ).unsqueeze(-1)\\n\\n                # Calculate the projected torques in null-space\\n                if mass_matrix is not None:\\n                    tau_null = (nullspace_jacobian_transpose @ mass_matrix @ joint_acc_nullspace).squeeze(-1)\\n                else:\\n                    tau_null = nullspace_jacobian_transpose @ joint_acc_nullspace\\n\\n                # Add the null-space joint efforts to the total joint efforts\\n                joint_efforts += tau_null\\n\\n            else:\\n                raise ValueError(f\"Invalid null-space control method: {self.cfg.nullspace_control}.\")\\n\\n        return joint_efforts'),\n",
       " Document(metadata={}, page_content='class OperationalSpaceControllerCfg:\\n    \"\"\"Configuration for operational-space controller.\"\"\"\\n\\n    class_type: type = OperationalSpaceController\\n    \"\"\"The associated controller class.\"\"\"\\n\\n    target_types: Sequence[str] = MISSING\\n    \"\"\"Type of task-space targets.\\n\\n    It has two sub-strings joined by underscore:\\n        - type of task-space target: ``\"pose\"``, ``\"wrench\"``\\n        - reference for the task-space targets: ``\"abs\"`` (absolute), ``\"rel\"`` (relative, only for pose)\\n    \"\"\"\\n\\n    motion_control_axes_task: Sequence[int] = (1, 1, 1, 1, 1, 1)\\n    \"\"\"Motion direction to control in task reference frame. Mark as ``0/1`` for each axis.\"\"\"\\n\\n    contact_wrench_control_axes_task: Sequence[int] = (0, 0, 0, 0, 0, 0)\\n    \"\"\"Contact wrench direction to control in task reference frame. Mark as 0/1 for each axis.\"\"\"\\n\\n    inertial_dynamics_decoupling: bool = False\\n    \"\"\"Whether to perform inertial dynamics decoupling for motion control (inverse dynamics).\"\"\"\\n\\n    partial_inertial_dynamics_decoupling: bool = False\\n    \"\"\"Whether to ignore the inertial coupling between the translational & rotational motions.\"\"\"\\n\\n    gravity_compensation: bool = False\\n    \"\"\"Whether to perform gravity compensation.\"\"\"\\n\\n    impedance_mode: str = \"fixed\"\\n    \"\"\"Type of gains for motion control: ``\"fixed\"``, ``\"variable\"``, ``\"variable_kp\"``.\"\"\"\\n\\n    motion_stiffness_task: float | Sequence[float] = (100.0, 100.0, 100.0, 100.0, 100.0, 100.0)\\n    \"\"\"The positional gain for determining operational space command forces based on task-space pose error.\"\"\"\\n\\n    motion_damping_ratio_task: float | Sequence[float] = (1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\\n    \"\"\"The damping ratio is used in-conjunction with positional gain to compute operational space command forces\\n    based on task-space velocity error.\\n\\n    The following math operation is performed for computing velocity gains:\\n        :math:`d_gains = 2 * sqrt(p_gains) * damping_ratio`.\\n    \"\"\"\\n\\n    motion_stiffness_limits_task: tuple[float, float] = (0, 1000)\\n    \"\"\"Minimum and maximum values for positional gains.\\n\\n    Note: Used only when :obj:`impedance_mode` is ``\"variable\"`` or ``\"variable_kp\"``.\\n    \"\"\"\\n\\n    motion_damping_ratio_limits_task: tuple[float, float] = (0, 100)\\n    \"\"\"Minimum and maximum values for damping ratios used to compute velocity gains.\\n\\n    Note: Used only when :obj:`impedance_mode` is ``\"variable\"``.\\n    \"\"\"\\n\\n    contact_wrench_stiffness_task: float | Sequence[float] | None = None\\n    \"\"\"The proportional gain for determining operational space command forces for closed-loop contact force control.\\n\\n    If ``None``, then open-loop control of desired contact wrench is performed.\\n\\n    Note: since only the linear forces could be measured at the moment,\\n    only the first three elements are used for the feedback loop.\\n    \"\"\"\\n\\n    nullspace_control: str = \"none\"\\n    \"\"\"The null space control method for redundant manipulators: ``\"none\"``, ``\"position\"``.\\n\\n    Note: ``\"position\"`` is used to drive the redundant manipulator to zero configuration by default. If\\n    ``target_joint_pos`` is provided in the ``compute()`` method, it will be driven to this configuration.\\n    \"\"\"\\n\\n    nullspace_stiffness: float = 10.0\\n    \"\"\"The stiffness for null space control.\"\"\"\\n\\n    nullspace_damping_ratio: float = 1.0\\n    \"\"\"The damping ratio for null space control.\"\"\"'),\n",
       " Document(metadata={}, page_content='class PinkIKController:\\n    \"\"\"Integration of Pink IK controller with Isaac Lab.\\n\\n    The Pink IK controller is available at: https://github.com/stephane-caron/pink\\n    \"\"\"\\n\\n    def __init__(self, cfg: PinkIKControllerCfg, device: str):\\n        \"\"\"Initialize the Pink IK Controller.\\n\\n        Args:\\n            cfg: The configuration for the controller.\\n            device: The device to use for computations (e.g., \\'cuda:0\\').\\n        \"\"\"\\n        # Initialize the robot model from URDF and mesh files\\n        self.robot_wrapper = RobotWrapper.BuildFromURDF(cfg.urdf_path, cfg.mesh_path, root_joint=None)\\n        self.pink_configuration = Configuration(\\n            self.robot_wrapper.model, self.robot_wrapper.data, self.robot_wrapper.q0\\n        )\\n\\n        # Set the default targets for each task from the configuration\\n        for task in cfg.variable_input_tasks:\\n            task.set_target_from_configuration(self.pink_configuration)\\n        for task in cfg.fixed_input_tasks:\\n            task.set_target_from_configuration(self.pink_configuration)\\n\\n        # Map joint names from Isaac Lab to Pink\\'s joint conventions\\n        pink_joint_names = self.robot_wrapper.model.names.tolist()[1:]  # Skip the root and universal joints\\n        isaac_lab_joint_names = cfg.joint_names\\n\\n        # Create reordering arrays for joint indices\\n        self.isaac_lab_to_pink_ordering = [isaac_lab_joint_names.index(pink_joint) for pink_joint in pink_joint_names]\\n        self.pink_to_isaac_lab_ordering = [\\n            pink_joint_names.index(isaac_lab_joint) for isaac_lab_joint in isaac_lab_joint_names\\n        ]\\n\\n        self.cfg = cfg\\n        self.device = device\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reorder_array(self, input_array: list[float], reordering_array: list[int]) -> list[float]:\\n        \"\"\"Reorder the input array based on the provided ordering.\\n\\n        Args:\\n            input_array: The array to reorder.\\n            reordering_array: The indices to use for reordering.\\n\\n        Returns:\\n            Reordered array.\\n        \"\"\"\\n        return [input_array[i] for i in reordering_array]\\n\\n    def initialize(self):\\n        \"\"\"Initialize the internals of the controller.\\n\\n        This method is called during setup but before the first compute call.\\n        \"\"\"\\n        pass\\n\\n    def compute(\\n        self,\\n        curr_joint_pos: np.ndarray,\\n        dt: float,\\n    ) -> torch.Tensor:\\n        \"\"\"Compute the target joint positions based on current state and tasks.\\n\\n        Args:\\n            curr_joint_pos: The current joint positions.\\n            dt: The time step for computing joint position changes.\\n\\n        Returns:\\n            The target joint positions as a tensor.\\n        \"\"\"\\n        # Initialize joint positions for Pink, including the root and universal joints\\n        joint_positions_pink = np.array(self.reorder_array(curr_joint_pos, self.isaac_lab_to_pink_ordering))\\n\\n        # Update Pink\\'s robot configuration with the current joint positions\\n        self.pink_configuration.update(joint_positions_pink)\\n\\n        # pink.solve_ik can raise an exception if the solver fails\\n        try:\\n            velocity = solve_ik(\\n                self.pink_configuration, self.cfg.variable_input_tasks + self.cfg.fixed_input_tasks, dt, solver=\"osqp\"\\n            )\\n            Delta_q = velocity * dt\\n        except (AssertionError, Exception):\\n            # Print warning and return the current joint positions as the target\\n            # Not using omni.log since its not available in CI during docs build\\n            if self.cfg.show_ik_warnings:\\n                print(\\n                    \"Warning: IK quadratic solver could not find a solution! Did not update the target joint positions.\"\\n                )\\n            return torch.tensor(curr_joint_pos, device=self.device, dtype=torch.float32)\\n\\n        # Discard the first 6 values (for root and universal joints)\\n        pink_joint_angle_changes = Delta_q\\n\\n        # Reorder the joint angle changes back to Isaac Lab conventions\\n        joint_vel_isaac_lab = torch.tensor(\\n            self.reorder_array(pink_joint_angle_changes, self.pink_to_isaac_lab_ordering),\\n            device=self.device,\\n            dtype=torch.float,\\n        )\\n\\n        # Add the velocity changes to the current joint positions to get the target joint positions\\n        target_joint_pos = torch.add(\\n            joint_vel_isaac_lab, torch.tensor(curr_joint_pos, device=self.device, dtype=torch.float32)\\n        )\\n\\n        return target_joint_pos'),\n",
       " Document(metadata={}, page_content='class PinkIKControllerCfg:\\n    \"\"\"Configuration settings for the Pink IK Controller.\\n\\n    The Pink IK controller can be found at: https://github.com/stephane-caron/pink\\n    \"\"\"\\n\\n    urdf_path: str | None = None\\n    \"\"\"Path to the robot\\'s URDF file. This file is used by Pinocchio\\'s `robot_wrapper.BuildFromURDF` to load the robot model.\"\"\"\\n\\n    mesh_path: str | None = None\\n    \"\"\"Path to the mesh files associated with the robot. These files are also loaded by Pinocchio\\'s `robot_wrapper.BuildFromURDF`.\"\"\"\\n\\n    num_hand_joints: int = 0\\n    \"\"\"The number of hand joints in the robot. The action space for the controller contains the pose_dim(7)*num_controlled_frames + num_hand_joints.\\n    The last num_hand_joints values of the action are the hand joint angles.\"\"\"\\n\\n    variable_input_tasks: list[FrameTask] = MISSING\\n    \"\"\"\\n    A list of tasks for the Pink IK controller. These tasks are controllable by the env action.\\n\\n    These tasks can be used to control the pose of a frame or the angles of joints.\\n    For more details, visit: https://github.com/stephane-caron/pink\\n    \"\"\"\\n\\n    fixed_input_tasks: list[FrameTask] = MISSING\\n    \"\"\"\\n    A list of tasks for the Pink IK controller. These tasks are fixed and not controllable by the env action.\\n\\n    These tasks can be used to fix the pose of a frame or the angles of joints to a desired configuration.\\n    For more details, visit: https://github.com/stephane-caron/pink\\n    \"\"\"\\n\\n    joint_names: list[str] | None = None\\n    \"\"\"A list of joint names in the USD asset. This is required because the joint naming conventions differ between USD and URDF files.\\n    This value is currently designed to be automatically populated by the action term in a manager based environment.\"\"\"\\n\\n    articulation_name: str = \"robot\"\\n    \"\"\"The name of the articulation USD asset in the scene.\"\"\"\\n\\n    base_link_name: str = \"base_link\"\\n    \"\"\"The name of the base link in the USD asset.\"\"\"\\n\\n    show_ik_warnings: bool = True\\n    \"\"\"Show warning if IK solver fails to find a solution.\"\"\"'),\n",
       " Document(metadata={}, page_content='class RmpFlowControllerCfg:\\n    \"\"\"Configuration for RMP-Flow controller (provided through LULA library).\"\"\"\\n\\n    name: str = \"rmp_flow\"\\n    \"\"\"Name of the controller. Supported: \"rmp_flow\", \"rmp_flow_smoothed\". Defaults to \"rmp_flow\".\"\"\"\\n    config_file: str = MISSING\\n    \"\"\"Path to the configuration file for the controller.\"\"\"\\n    urdf_file: str = MISSING\\n    \"\"\"Path to the URDF model of the robot.\"\"\"\\n    collision_file: str = MISSING\\n    \"\"\"Path to collision model description of the robot.\"\"\"\\n    frame_name: str = MISSING\\n    \"\"\"Name of the robot frame for task space (must be present in the URDF).\"\"\"\\n    evaluations_per_frame: float = MISSING\\n    \"\"\"Number of substeps during Euler integration inside LULA world model.\"\"\"\\n    ignore_robot_state_updates: bool = False\\n    \"\"\"If true, then state of the world model inside controller is rolled out. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='class RmpFlowController:\\n    \"\"\"Wraps around RMPFlow from IsaacSim for batched environments.\"\"\"\\n\\n    def __init__(self, cfg: RmpFlowControllerCfg, device: str):\\n        \"\"\"Initialize the controller.\\n\\n        Args:\\n            cfg: The configuration for the controller.\\n            device: The device to use for computation.\\n        \"\"\"\\n        # store input\\n        self.cfg = cfg\\n        self._device = device\\n        # display info\\n        print(f\"[INFO]: Loading RMPFlow controller URDF from: {self.cfg.urdf_file}\")\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_actions(self) -> int:\\n        \"\"\"Dimension of the action space of controller.\"\"\"\\n        return 7\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def initialize(self, prim_paths_expr: str):\\n        \"\"\"Initialize the controller.\\n\\n        Args:\\n            prim_paths_expr: The expression to find the articulation prim paths.\\n        \"\"\"\\n        # obtain the simulation time\\n        physics_dt = SimulationContext.instance().get_physics_dt()\\n        # find all prims\\n        self._prim_paths = prim_utils.find_matching_prim_paths(prim_paths_expr)\\n        self.num_robots = len(self._prim_paths)\\n        # resolve controller\\n        if self.cfg.name == \"rmp_flow\":\\n            controller_cls = RmpFlow\\n        elif self.cfg.name == \"rmp_flow_smoothed\":\\n            controller_cls = RmpFlowSmoothed\\n        else:\\n            raise ValueError(f\"Unsupported controller in Lula library: {self.cfg.name}\")\\n        # create all franka robots references and their controllers\\n        self.articulation_policies = list()\\n        for prim_path in self._prim_paths:\\n            # add robot reference\\n            robot = SingleArticulation(prim_path)\\n            robot.initialize()\\n            # add controller\\n            rmpflow = controller_cls(\\n                robot_description_path=self.cfg.collision_file,\\n                urdf_path=self.cfg.urdf_file,\\n                rmpflow_config_path=self.cfg.config_file,\\n                end_effector_frame_name=self.cfg.frame_name,\\n                maximum_substep_size=physics_dt / self.cfg.evaluations_per_frame,\\n                ignore_robot_state_updates=self.cfg.ignore_robot_state_updates,\\n            )\\n            # wrap rmpflow to connect to the Franka robot articulation\\n            articulation_policy = ArticulationMotionPolicy(robot, rmpflow, physics_dt)\\n            self.articulation_policies.append(articulation_policy)\\n        # get number of active joints\\n        self.active_dof_names = self.articulation_policies[0].get_motion_policy().get_active_joints()\\n        self.num_dof = len(self.active_dof_names)\\n        # create buffers\\n        # -- for storing command\\n        self._command = torch.zeros(self.num_robots, self.num_actions, device=self._device)\\n        # -- for policy output\\n        self.dof_pos_target = torch.zeros((self.num_robots, self.num_dof), device=self._device)\\n        self.dof_vel_target = torch.zeros((self.num_robots, self.num_dof), device=self._device)\\n\\n    def reset_idx(self, robot_ids: torch.Tensor = None):\\n        \"\"\"Reset the internals.\"\"\"\\n        # if no robot ids are provided, then reset all robots\\n        if robot_ids is None:\\n            robot_ids = torch.arange(self.num_robots, device=self._device)\\n        # reset policies for specified robots\\n        for index in robot_ids:\\n            self.articulation_policies[index].motion_policy.reset()\\n\\n    def set_command(self, command: torch.Tensor):\\n        \"\"\"Set target end-effector pose command.\"\"\"\\n        # store command\\n        self._command[:] = command\\n\\n    def compute(self) -> tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"Performs inference with the controller.\\n\\n        Returns:\\n            The target joint positions and velocity commands.\\n        \"\"\"\\n        # convert command to numpy\\n        command = self._command.cpu().numpy()\\n        # compute control actions\\n        for i, policy in enumerate(self.articulation_policies):\\n            # enable type-hinting\\n            policy: ArticulationMotionPolicy\\n            # set rmpflow target to be the current position of the target cube.\\n            policy.get_motion_policy().set_end_effector_target(\\n                target_position=command[i, 0:3], target_orientation=command[i, 3:7]\\n            )\\n            # apply action on the robot\\n            action = policy.get_next_articulation_action()\\n            # copy actions into buffer\\n            self.dof_pos_target[i, :] = torch.from_numpy(action.joint_positions[:]).to(self.dof_pos_target)\\n            self.dof_vel_target[i, :] = torch.from_numpy(action.joint_velocities[:]).to(self.dof_vel_target)\\n\\n        return self.dof_pos_target, self.dof_vel_target'),\n",
       " Document(metadata={}, page_content='def convert_usd_to_urdf(usd_path: str, output_path: str, force_conversion: bool = True) -> tuple[str, str]:\\n    \"\"\"Convert a USD file to URDF format.\\n\\n    Args:\\n        usd_path: Path to the USD file to convert.\\n        output_path: Directory to save the converted URDF and mesh files.\\n        force_conversion: Whether to force the conversion even if the URDF and mesh files already exist.\\n    Returns:\\n        A tuple containing the paths to the URDF file and the mesh directory.\\n    \"\"\"\\n    usd_to_urdf_kwargs = {\\n        \"node_names_to_remove\": None,\\n        \"edge_names_to_remove\": None,\\n        \"root\": None,\\n        \"parent_link_is_body_1\": None,\\n        \"log_level\": logger.level_from_name(\"ERROR\"),\\n    }\\n\\n    urdf_output_dir = os.path.join(output_path, \"urdf\")\\n    urdf_file_name = os.path.basename(usd_path).split(\".\")[0] + \".urdf\"\\n    urdf_output_path = urdf_output_dir + \"/\" + urdf_file_name\\n    urdf_meshes_output_dir = os.path.join(output_path, \"meshes\")\\n\\n    if not os.path.exists(urdf_output_path) or not os.path.exists(urdf_meshes_output_dir) or force_conversion:\\n        usd_to_urdf = UsdToUrdf.init_from_file(usd_path, **usd_to_urdf_kwargs)\\n        os.makedirs(urdf_output_dir, exist_ok=True)\\n        os.makedirs(urdf_meshes_output_dir, exist_ok=True)\\n\\n        output_path = usd_to_urdf.save_to_file(\\n            urdf_output_path=urdf_output_path,\\n            visualize_collision_meshes=False,\\n            mesh_dir=urdf_meshes_output_dir,\\n            mesh_path_prefix=\"\",\\n        )\\n\\n        # The current version of the usd to urdf converter creates \"inf\" effort,\\n        # This has to be replaced with a max value for the urdf to be valid\\n        # Open the file for reading and writing\\n        with open(urdf_output_path) as file:\\n            # Read the content of the file\\n            content = file.read()\\n\\n        # Replace all occurrences of \\'inf\\' with \\'0\\'\\n        content = content.replace(\"inf\", \"0.\")\\n\\n        # Open the file again to write the modified content\\n        with open(urdf_output_path, \"w\") as file:\\n            # Write the modified content back to the file\\n            file.write(content)\\n    return urdf_output_path, urdf_meshes_output_dir'),\n",
       " Document(metadata={}, page_content='def change_revolute_to_fixed(urdf_path: str, fixed_joints: list[str], verbose: bool = False):\\n    \"\"\"Change revolute joints to fixed joints in a URDF file.\\n\\n    This function modifies a URDF file by changing specified revolute joints to fixed joints.\\n    This is useful when you want to disable certain joints in a robot model.\\n\\n    Args:\\n        urdf_path: Path to the URDF file to modify.\\n        fixed_joints: List of joint names to convert from revolute to fixed.\\n        verbose: Whether to print information about the changes being made.\\n    \"\"\"\\n    with open(urdf_path) as file:\\n        content = file.read()\\n\\n    for joint in fixed_joints:\\n        old_str = f\\'<joint name=\"{joint}\" type=\"revolute\">\\'\\n        new_str = f\\'<joint name=\"{joint}\" type=\"fixed\">\\'\\n        if verbose:\\n            omni.log.warn(f\"Replacing {joint} with fixed joint\")\\n            omni.log.warn(old_str)\\n            omni.log.warn(new_str)\\n            if old_str not in content:\\n                omni.log.warn(f\"Error: Could not find revolute joint named \\'{joint}\\' in URDF file\")\\n        content = content.replace(old_str, new_str)\\n\\n    with open(urdf_path, \"w\") as file:\\n        file.write(content)'),\n",
       " Document(metadata={}, page_content='class DeviceBase(ABC):\\n    \"\"\"An interface class for teleoperation devices.\\n\\n    Derived classes have two implementation options:\\n\\n    1. Override _get_raw_data() and use the base advance() implementation:\\n       This approach is suitable for devices that want to leverage the built-in\\n       retargeting logic but only need to customize the raw data acquisition.\\n\\n    2. Override advance() completely:\\n       This approach gives full control over the command generation process,\\n       and _get_raw_data() can be ignored entirely.\\n    \"\"\"\\n\\n    def __init__(self, retargeters: list[RetargeterBase] | None = None):\\n        \"\"\"Initialize the teleoperation interface.\\n\\n        Args:\\n            retargeters: List of components that transform device data into robot commands.\\n                        If None or empty list, the device will output its native data format.\\n        \"\"\"\\n        # Initialize empty list if None is provided\\n        self._retargeters = retargeters or []\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing the information of joystick.\"\"\"\\n        return f\"{self.__class__.__name__}\"\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    @abstractmethod\\n    def reset(self):\\n        \"\"\"Reset the internals.\"\"\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def add_callback(self, key: Any, func: Callable):\\n        \"\"\"Add additional functions to bind keyboard.\\n\\n        Args:\\n            key: The button to check against.\\n            func: The function to call when key is pressed. The callback function should not\\n                take any arguments.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def _get_raw_data(self) -> Any:\\n        \"\"\"Internal method to get the raw data from the device.\\n\\n        This method is intended for internal use by the advance() implementation.\\n        Derived classes can override this method to customize raw data acquisition\\n        while still using the base class\\'s advance() implementation.\\n\\n        Returns:\\n            Raw device data in a device-specific format\\n\\n        Note:\\n            This is an internal implementation detail. Clients should call advance()\\n            instead of this method.\\n        \"\"\"\\n        raise NotImplementedError(\"Derived class must implement _get_raw_data() or override advance()\")\\n\\n    def advance(self) -> Any:\\n        \"\"\"Process current device state and return control commands.\\n\\n        This method retrieves raw data from the device and optionally applies\\n        retargeting to convert it to robot commands.\\n\\n        Derived classes can either:\\n        1. Override _get_raw_data() and use this base implementation, or\\n        2. Override this method completely for custom command processing\\n\\n        Returns:\\n            Raw device data if no retargeters are configured.\\n            When retargeters are configured, returns a tuple containing each retargeter\\'s processed output.\\n        \"\"\"\\n        raw_data = self._get_raw_data()\\n\\n        # If no retargeters, return raw data directly (not as a tuple)\\n        if not self._retargeters:\\n            return raw_data\\n\\n        # With multiple retargeters, return a tuple of outputs\\n        return tuple(retargeter.retarget(raw_data) for retargeter in self._retargeters)'),\n",
       " Document(metadata={}, page_content='class RetargeterBase(ABC):\\n    \"\"\"Base interface for input data retargeting.\\n\\n    This abstract class defines the interface for components that transform\\n    raw device data into robot control commands. Implementations can handle\\n    various types of transformations including:\\n    - Hand joint data to end-effector poses\\n    - Input device commands to robot movements\\n    - Sensor data to control signals\\n    \"\"\"\\n\\n    @abstractmethod\\n    def retarget(self, data: Any) -> Any:\\n        \"\"\"Retarget input data to desired output format.\\n\\n        Args:\\n            data: Raw input data to be transformed\\n\\n        Returns:\\n            Retargeted data in implementation-specific format\\n        \"\"\"\\n        pass'),\n",
       " Document(metadata={}, page_content='class Se2Gamepad(DeviceBase):\\n    r\"\"\"A gamepad controller for sending SE(2) commands as velocity commands.\\n\\n    This class is designed to provide a gamepad controller for mobile base (such as quadrupeds).\\n    It uses the Omniverse gamepad interface to listen to gamepad events and map them to robot\\'s\\n    task-space commands.\\n\\n    The command comprises of the base linear and angular velocity: :math:`(v_x, v_y, \\\\omega_z)`.\\n\\n    Key bindings:\\n        ====================== ========================= ========================\\n        Command                Key (+ve axis)            Key (-ve axis)\\n        ====================== ========================= ========================\\n        Move along x-axis      left stick up             left stick down\\n        Move along y-axis      left stick right          left stick left\\n        Rotate along z-axis    right stick right         right stick left\\n        ====================== ========================= ========================\\n\\n    .. seealso::\\n\\n        The official documentation for the gamepad interface: `Carb Gamepad Interface <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/gamepad.html>`__.\\n\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        v_x_sensitivity: float = 1.0,\\n        v_y_sensitivity: float = 1.0,\\n        omega_z_sensitivity: float = 1.0,\\n        dead_zone: float = 0.01,\\n    ):\\n        \"\"\"Initialize the gamepad layer.\\n\\n        Args:\\n            v_x_sensitivity: Magnitude of linear velocity along x-direction scaling. Defaults to 1.0.\\n            v_y_sensitivity: Magnitude of linear velocity along y-direction scaling. Defaults to 1.0.\\n            omega_z_sensitivity: Magnitude of angular velocity along z-direction scaling. Defaults to 1.0.\\n            dead_zone: Magnitude of dead zone for gamepad. An event value from the gamepad less than\\n                this value will be ignored. Defaults to 0.01.\\n        \"\"\"\\n        # turn off simulator gamepad control\\n        carb_settings_iface = carb.settings.get_settings()\\n        carb_settings_iface.set_bool(\"/persistent/app/omniverse/gamepadCameraControl\", False)\\n        # store inputs\\n        self.v_x_sensitivity = v_x_sensitivity\\n        self.v_y_sensitivity = v_y_sensitivity\\n        self.omega_z_sensitivity = omega_z_sensitivity\\n        self.dead_zone = dead_zone\\n        # acquire omniverse interfaces\\n        self._appwindow = omni.appwindow.get_default_app_window()\\n        self._input = carb.input.acquire_input_interface()\\n        self._gamepad = self._appwindow.get_gamepad(0)\\n        # note: Use weakref on callbacks to ensure that this object can be deleted when its destructor is called\\n        self._gamepad_sub = self._input.subscribe_to_gamepad_events(\\n            self._gamepad,\\n            lambda event, *args, obj=weakref.proxy(self): obj._on_gamepad_event(event, *args),\\n        )\\n        # bindings for gamepad to command\\n        self._create_key_bindings()\\n        # command buffers\\n        # When using the gamepad, two values are provided for each axis.\\n        # For example: when the left stick is moved down, there are two evens: `left_stick_down = 0.8`\\n        #   and `left_stick_up = 0.0`. If only the value of left_stick_up is used, the value will be 0.0,\\n        #   which is not the desired behavior. Therefore, we save both the values into the buffer and use\\n        #   the maximum value.\\n        # (positive, negative), (x, y, yaw)\\n        self._base_command_raw = np.zeros([2, 3])\\n        # dictionary for additional callbacks\\n        self._additional_callbacks = dict()\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribe from gamepad events.\"\"\"\\n        self._input.unsubscribe_to_gamepad_events(self._gamepad, self._gamepad_sub)\\n        self._gamepad_sub = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing the information of joystick.\"\"\"\\n        msg = f\"Gamepad Controller for SE(2): {self.__class__.__name__}\\\\n\"\\n        msg += f\"\\\\tDevice name: {self._input.get_gamepad_name(self._gamepad)}\\\\n\"\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tMove in X-Y plane: left stick\\\\n\"\\n        msg += \"\\\\tRotate in Z-axis: right stick\\\\n\"\\n        return msg\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self):\\n        # default flags\\n        self._base_command_raw.fill(0.0)\\n\\n    def add_callback(self, key: carb.input.GamepadInput, func: Callable):\\n        \"\"\"Add additional functions to bind gamepad.\\n\\n        A list of available gamepad keys are present in the\\n        `carb documentation <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/gamepad.html>`__.\\n\\n        Args:\\n            key: The gamepad button to check against.\\n            func: The function to call when key is pressed. The callback function should not\\n                take any arguments.\\n        \"\"\"\\n        self._additional_callbacks[key] = func\\n\\n    def advance(self) -> np.ndarray:\\n        \"\"\"Provides the result from gamepad event state.\\n\\n        Returns:\\n            A 3D array containing the linear (x,y) and angular velocity (z).\\n        \"\"\"\\n        return self._resolve_command_buffer(self._base_command_raw)\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _on_gamepad_event(self, event: carb.input.GamepadEvent, *args, **kwargs):\\n        \"\"\"Subscriber callback to when kit is updated.\\n\\n        Reference:\\n            https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/gamepad.html\\n        \"\"\"\\n\\n        # check if the event is a button press\\n        cur_val = event.value\\n        if abs(cur_val) < self.dead_zone:\\n            cur_val = 0\\n        # -- left and right stick\\n        if event.input in self._INPUT_STICK_VALUE_MAPPING:\\n            direction, axis, value = self._INPUT_STICK_VALUE_MAPPING[event.input]\\n            # change the value only if the stick is moved (soft press)\\n            self._base_command_raw[direction, axis] = value * cur_val\\n\\n        # additional callbacks\\n        if event.input in self._additional_callbacks:\\n            self._additional_callbacks[event.input]()\\n\\n        # since no error, we are fine :)\\n        return True\\n\\n    def _create_key_bindings(self):\\n        \"\"\"Creates default key binding.\"\"\"\\n        self._INPUT_STICK_VALUE_MAPPING = {\\n            # forward command\\n            carb.input.GamepadInput.LEFT_STICK_UP: (0, 0, self.v_x_sensitivity),\\n            # backward command\\n            carb.input.GamepadInput.LEFT_STICK_DOWN: (1, 0, self.v_x_sensitivity),\\n            # right command\\n            carb.input.GamepadInput.LEFT_STICK_RIGHT: (0, 1, self.v_y_sensitivity),\\n            # left command\\n            carb.input.GamepadInput.LEFT_STICK_LEFT: (1, 1, self.v_y_sensitivity),\\n            # yaw command (positive)\\n            carb.input.GamepadInput.RIGHT_STICK_RIGHT: (0, 2, self.omega_z_sensitivity),\\n            # yaw command (negative)\\n            carb.input.GamepadInput.RIGHT_STICK_LEFT: (1, 2, self.omega_z_sensitivity),\\n        }\\n\\n    def _resolve_command_buffer(self, raw_command: np.ndarray) -> np.ndarray:\\n        \"\"\"Resolves the command buffer.\\n\\n        Args:\\n            raw_command: The raw command from the gamepad. Shape is (2, 3)\\n                This is a 2D array since gamepad dpad/stick returns two values corresponding to\\n                the positive and negative direction. The first index is the direction (0: positive, 1: negative)\\n                and the second index is value (absolute) of the command.\\n\\n        Returns:\\n            Resolved command. Shape is (3,)\\n        \"\"\"\\n        # compare the positive and negative value decide the sign of the value\\n        #   if the positive value is larger, the sign is positive (i.e. False, 0)\\n        #   if the negative value is larger, the sign is positive (i.e. True, 1)\\n        command_sign = raw_command[1, :] > raw_command[0, :]\\n        # extract the command value\\n        command = raw_command.max(axis=0)\\n        # apply the sign\\n        #  if the sign is positive, the value is already positive.\\n        #  if the sign is negative, the value is negative after applying the sign.\\n        command[command_sign] *= -1\\n\\n        return command'),\n",
       " Document(metadata={}, page_content='class Se3Gamepad(DeviceBase):\\n    \"\"\"A gamepad controller for sending SE(3) commands as delta poses and binary command (open/close).\\n\\n    This class is designed to provide a gamepad controller for a robotic arm with a gripper.\\n    It uses the gamepad interface to listen to gamepad events and map them to the robot\\'s\\n    task-space commands.\\n\\n    The command comprises of two parts:\\n\\n    * delta pose: a 6D vector of (x, y, z, roll, pitch, yaw) in meters and radians.\\n    * gripper: a binary command to open or close the gripper.\\n\\n    Stick and Button bindings:\\n        ============================ ========================= =========================\\n        Description                  Stick/Button (+ve axis)   Stick/Button (-ve axis)\\n        ============================ ========================= =========================\\n        Toggle gripper(open/close)   X Button                  X Button\\n        Move along x-axis            Left Stick Up             Left Stick Down\\n        Move along y-axis            Left Stick Left           Left Stick Right\\n        Move along z-axis            Right Stick Up            Right Stick Down\\n        Rotate along x-axis          D-Pad Left                D-Pad Right\\n        Rotate along y-axis          D-Pad Down                D-Pad Up\\n        Rotate along z-axis          Right Stick Left          Right Stick Right\\n        ============================ ========================= =========================\\n\\n    .. seealso::\\n\\n        The official documentation for the gamepad interface: `Carb Gamepad Interface <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/gamepad.html>`__.\\n\\n    \"\"\"\\n\\n    def __init__(self, pos_sensitivity: float = 1.0, rot_sensitivity: float = 1.6, dead_zone: float = 0.01):\\n        \"\"\"Initialize the gamepad layer.\\n\\n        Args:\\n            pos_sensitivity: Magnitude of input position command scaling. Defaults to 1.0.\\n            rot_sensitivity: Magnitude of scale input rotation commands scaling. Defaults to 1.6.\\n            dead_zone: Magnitude of dead zone for gamepad. An event value from the gamepad less than\\n                this value will be ignored. Defaults to 0.01.\\n        \"\"\"\\n        # turn off simulator gamepad control\\n        carb_settings_iface = carb.settings.get_settings()\\n        carb_settings_iface.set_bool(\"/persistent/app/omniverse/gamepadCameraControl\", False)\\n        # store inputs\\n        self.pos_sensitivity = pos_sensitivity\\n        self.rot_sensitivity = rot_sensitivity\\n        self.dead_zone = dead_zone\\n        # acquire omniverse interfaces\\n        self._appwindow = omni.appwindow.get_default_app_window()\\n        self._input = carb.input.acquire_input_interface()\\n        self._gamepad = self._appwindow.get_gamepad(0)\\n        # note: Use weakref on callbacks to ensure that this object can be deleted when its destructor is called\\n        self._gamepad_sub = self._input.subscribe_to_gamepad_events(\\n            self._gamepad,\\n            lambda event, *args, obj=weakref.proxy(self): obj._on_gamepad_event(event, *args),\\n        )\\n        # bindings for gamepad to command\\n        self._create_key_bindings()\\n        # command buffers\\n        self._close_gripper = False\\n        # When using the gamepad, two values are provided for each axis.\\n        # For example: when the left stick is moved down, there are two evens: `left_stick_down = 0.8`\\n        #   and `left_stick_up = 0.0`. If only the value of left_stick_up is used, the value will be 0.0,\\n        #   which is not the desired behavior. Therefore, we save both the values into the buffer and use\\n        #   the maximum value.\\n        # (positive, negative), (x, y, z, roll, pitch, yaw)\\n        self._delta_pose_raw = np.zeros([2, 6])\\n        # dictionary for additional callbacks\\n        self._additional_callbacks = dict()\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribe from gamepad events.\"\"\"\\n        self._input.unsubscribe_to_gamepad_events(self._gamepad, self._gamepad_sub)\\n        self._gamepad_sub = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing the information of joystick.\"\"\"\\n        msg = f\"Gamepad Controller for SE(3): {self.__class__.__name__}\\\\n\"\\n        msg += f\"\\\\tDevice name: {self._input.get_gamepad_name(self._gamepad)}\\\\n\"\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tToggle gripper (open/close): X\\\\n\"\\n        msg += \"\\\\tMove arm along x-axis: Left Stick Up/Down\\\\n\"\\n        msg += \"\\\\tMove arm along y-axis: Left Stick Left/Right\\\\n\"\\n        msg += \"\\\\tMove arm along z-axis: Right Stick Up/Down\\\\n\"\\n        msg += \"\\\\tRotate arm along x-axis: D-Pad Right/Left\\\\n\"\\n        msg += \"\\\\tRotate arm along y-axis: D-Pad Down/Up\\\\n\"\\n        msg += \"\\\\tRotate arm along z-axis: Right Stick Left/Right\\\\n\"\\n        return msg\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self):\\n        # default flags\\n        self._close_gripper = False\\n        self._delta_pose_raw.fill(0.0)\\n\\n    def add_callback(self, key: carb.input.GamepadInput, func: Callable):\\n        \"\"\"Add additional functions to bind gamepad.\\n\\n        A list of available gamepad keys are present in the\\n        `carb documentation <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/gamepad.html>`__.\\n\\n        Args:\\n            key: The gamepad button to check against.\\n            func: The function to call when key is pressed. The callback function should not\\n                take any arguments.\\n        \"\"\"\\n        self._additional_callbacks[key] = func\\n\\n    def advance(self) -> tuple[np.ndarray, bool]:\\n        \"\"\"Provides the result from gamepad event state.\\n\\n        Returns:\\n            A tuple containing the delta pose command and gripper commands.\\n        \"\"\"\\n        # -- resolve position command\\n        delta_pos = self._resolve_command_buffer(self._delta_pose_raw[:, :3])\\n        # -- resolve rotation command\\n        delta_rot = self._resolve_command_buffer(self._delta_pose_raw[:, 3:])\\n        # -- convert to rotation vector\\n        rot_vec = Rotation.from_euler(\"XYZ\", delta_rot).as_rotvec()\\n        # return the command and gripper state\\n        return np.concatenate([delta_pos, rot_vec]), self._close_gripper\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _on_gamepad_event(self, event, *args, **kwargs):\\n        \"\"\"Subscriber callback to when kit is updated.\\n\\n        Reference:\\n            https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/gamepad.html\\n        \"\"\"\\n        # check if the event is a button press\\n        cur_val = event.value\\n        if abs(cur_val) < self.dead_zone:\\n            cur_val = 0\\n        # -- button\\n        if event.input == carb.input.GamepadInput.X:\\n            # toggle gripper based on the button pressed\\n            if cur_val > 0.5:\\n                self._close_gripper = not self._close_gripper\\n        # -- left and right stick\\n        if event.input in self._INPUT_STICK_VALUE_MAPPING:\\n            direction, axis, value = self._INPUT_STICK_VALUE_MAPPING[event.input]\\n            # change the value only if the stick is moved (soft press)\\n            self._delta_pose_raw[direction, axis] = value * cur_val\\n        # -- dpad (4 arrow buttons on the console)\\n        if event.input in self._INPUT_DPAD_VALUE_MAPPING:\\n            direction, axis, value = self._INPUT_DPAD_VALUE_MAPPING[event.input]\\n            # change the value only if button is pressed on the DPAD\\n            if cur_val > 0.5:\\n                self._delta_pose_raw[direction, axis] = value\\n                self._delta_pose_raw[1 - direction, axis] = 0\\n            else:\\n                self._delta_pose_raw[:, axis] = 0\\n        # additional callbacks\\n        if event.input in self._additional_callbacks:\\n            self._additional_callbacks[event.input]()\\n\\n        # since no error, we are fine :)\\n        return True\\n\\n    def _create_key_bindings(self):\\n        \"\"\"Creates default key binding.\"\"\"\\n        # map gamepad input to the element in self._delta_pose_raw\\n        #   the first index is the direction (0: positive, 1: negative)\\n        #   the second index is the axis (0: x, 1: y, 2: z, 3: roll, 4: pitch, 5: yaw)\\n        #   the third index is the sensitivity of the command\\n        self._INPUT_STICK_VALUE_MAPPING = {\\n            # forward command\\n            carb.input.GamepadInput.LEFT_STICK_UP: (0, 0, self.pos_sensitivity),\\n            # backward command\\n            carb.input.GamepadInput.LEFT_STICK_DOWN: (1, 0, self.pos_sensitivity),\\n            # right command\\n            carb.input.GamepadInput.LEFT_STICK_RIGHT: (0, 1, self.pos_sensitivity),\\n            # left command\\n            carb.input.GamepadInput.LEFT_STICK_LEFT: (1, 1, self.pos_sensitivity),\\n            # upward command\\n            carb.input.GamepadInput.RIGHT_STICK_UP: (0, 2, self.pos_sensitivity),\\n            # downward command\\n            carb.input.GamepadInput.RIGHT_STICK_DOWN: (1, 2, self.pos_sensitivity),\\n            # yaw command (positive)\\n            carb.input.GamepadInput.RIGHT_STICK_RIGHT: (0, 5, self.rot_sensitivity),\\n            # yaw command (negative)\\n            carb.input.GamepadInput.RIGHT_STICK_LEFT: (1, 5, self.rot_sensitivity),\\n        }\\n\\n        self._INPUT_DPAD_VALUE_MAPPING = {\\n            # pitch command (positive)\\n            carb.input.GamepadInput.DPAD_UP: (1, 4, self.rot_sensitivity * 0.8),\\n            # pitch command (negative)\\n            carb.input.GamepadInput.DPAD_DOWN: (0, 4, self.rot_sensitivity * 0.8),\\n            # roll command (positive)\\n            carb.input.GamepadInput.DPAD_RIGHT: (1, 3, self.rot_sensitivity * 0.8),\\n            # roll command (negative)\\n            carb.input.GamepadInput.DPAD_LEFT: (0, 3, self.rot_sensitivity * 0.8),\\n        }\\n\\n    def _resolve_command_buffer(self, raw_command: np.ndarray) -> np.ndarray:\\n        \"\"\"Resolves the command buffer.\\n\\n        Args:\\n            raw_command: The raw command from the gamepad. Shape is (2, 3)\\n                This is a 2D array since gamepad dpad/stick returns two values corresponding to\\n                the positive and negative direction. The first index is the direction (0: positive, 1: negative)\\n                and the second index is value (absolute) of the command.\\n\\n        Returns:\\n            Resolved command. Shape is (3,)\\n        \"\"\"\\n        # compare the positive and negative value decide the sign of the value\\n        #   if the positive value is larger, the sign is positive (i.e. False, 0)\\n        #   if the negative value is larger, the sign is positive (i.e. True, 1)\\n        delta_command_sign = raw_command[1, :] > raw_command[0, :]\\n        # extract the command value\\n        delta_command = raw_command.max(axis=0)\\n        # apply the sign\\n        #  if the sign is positive, the value is already positive.\\n        #  if the sign is negative, the value is negative after applying the sign.\\n        delta_command[delta_command_sign] *= -1\\n\\n        return delta_command'),\n",
       " Document(metadata={}, page_content='class Se2Keyboard(DeviceBase):\\n    r\"\"\"A keyboard controller for sending SE(2) commands as velocity commands.\\n\\n    This class is designed to provide a keyboard controller for mobile base (such as quadrupeds).\\n    It uses the Omniverse keyboard interface to listen to keyboard events and map them to robot\\'s\\n    task-space commands.\\n\\n    The command comprises of the base linear and angular velocity: :math:`(v_x, v_y, \\\\omega_z)`.\\n\\n    Key bindings:\\n        ====================== ========================= ========================\\n        Command                Key (+ve axis)            Key (-ve axis)\\n        ====================== ========================= ========================\\n        Move along x-axis      Numpad 8 / Arrow Up       Numpad 2 / Arrow Down\\n        Move along y-axis      Numpad 4 / Arrow Right    Numpad 6 / Arrow Left\\n        Rotate along z-axis    Numpad 7 / Z              Numpad 9 / X\\n        ====================== ========================= ========================\\n\\n    .. seealso::\\n\\n        The official documentation for the keyboard interface: `Carb Keyboard Interface <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/keyboard.html>`__.\\n\\n    \"\"\"\\n\\n    def __init__(self, v_x_sensitivity: float = 0.8, v_y_sensitivity: float = 0.4, omega_z_sensitivity: float = 1.0):\\n        \"\"\"Initialize the keyboard layer.\\n\\n        Args:\\n            v_x_sensitivity: Magnitude of linear velocity along x-direction scaling. Defaults to 0.8.\\n            v_y_sensitivity: Magnitude of linear velocity along y-direction scaling. Defaults to 0.4.\\n            omega_z_sensitivity: Magnitude of angular velocity along z-direction scaling. Defaults to 1.0.\\n        \"\"\"\\n        # store inputs\\n        self.v_x_sensitivity = v_x_sensitivity\\n        self.v_y_sensitivity = v_y_sensitivity\\n        self.omega_z_sensitivity = omega_z_sensitivity\\n        # acquire omniverse interfaces\\n        self._appwindow = omni.appwindow.get_default_app_window()\\n        self._input = carb.input.acquire_input_interface()\\n        self._keyboard = self._appwindow.get_keyboard()\\n        # note: Use weakref on callbacks to ensure that this object can be deleted when its destructor is called\\n        self._keyboard_sub = self._input.subscribe_to_keyboard_events(\\n            self._keyboard,\\n            lambda event, *args, obj=weakref.proxy(self): obj._on_keyboard_event(event, *args),\\n        )\\n        # bindings for keyboard to command\\n        self._create_key_bindings()\\n        # command buffers\\n        self._base_command = np.zeros(3)\\n        # dictionary for additional callbacks\\n        self._additional_callbacks = dict()\\n\\n    def __del__(self):\\n        \"\"\"Release the keyboard interface.\"\"\"\\n        self._input.unsubscribe_from_keyboard_events(self._keyboard, self._keyboard_sub)\\n        self._keyboard_sub = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing the information of joystick.\"\"\"\\n        msg = f\"Keyboard Controller for SE(2): {self.__class__.__name__}\\\\n\"\\n        msg += f\"\\\\tKeyboard name: {self._input.get_keyboard_name(self._keyboard)}\\\\n\"\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tReset all commands: L\\\\n\"\\n        msg += \"\\\\tMove forward   (along x-axis): Numpad 8 / Arrow Up\\\\n\"\\n        msg += \"\\\\tMove backward  (along x-axis): Numpad 2 / Arrow Down\\\\n\"\\n        msg += \"\\\\tMove right     (along y-axis): Numpad 4 / Arrow Right\\\\n\"\\n        msg += \"\\\\tMove left      (along y-axis): Numpad 6 / Arrow Left\\\\n\"\\n        msg += \"\\\\tYaw positively (along z-axis): Numpad 7 / Z\\\\n\"\\n        msg += \"\\\\tYaw negatively (along z-axis): Numpad 9 / X\"\\n        return msg\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self):\\n        # default flags\\n        self._base_command.fill(0.0)\\n\\n    def add_callback(self, key: str, func: Callable):\\n        \"\"\"Add additional functions to bind keyboard.\\n\\n        A list of available keys are present in the\\n        `carb documentation <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/keyboard.html>`__.\\n\\n        Args:\\n            key: The keyboard button to check against.\\n            func: The function to call when key is pressed. The callback function should not\\n                take any arguments.\\n        \"\"\"\\n        self._additional_callbacks[key] = func\\n\\n    def advance(self) -> np.ndarray:\\n        \"\"\"Provides the result from keyboard event state.\\n\\n        Returns:\\n            3D array containing the linear (x,y) and angular velocity (z).\\n        \"\"\"\\n        return self._base_command\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _on_keyboard_event(self, event, *args, **kwargs):\\n        \"\"\"Subscriber callback to when kit is updated.\\n\\n        Reference:\\n            https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/keyboard.html\\n        \"\"\"\\n        # apply the command when pressed\\n        if event.type == carb.input.KeyboardEventType.KEY_PRESS:\\n            if event.input.name == \"L\":\\n                self.reset()\\n            elif event.input.name in self._INPUT_KEY_MAPPING:\\n                self._base_command += self._INPUT_KEY_MAPPING[event.input.name]\\n        # remove the command when un-pressed\\n        if event.type == carb.input.KeyboardEventType.KEY_RELEASE:\\n            if event.input.name in self._INPUT_KEY_MAPPING:\\n                self._base_command -= self._INPUT_KEY_MAPPING[event.input.name]\\n        # additional callbacks\\n        if event.type == carb.input.KeyboardEventType.KEY_PRESS:\\n            if event.input.name in self._additional_callbacks:\\n                self._additional_callbacks[event.input.name]()\\n\\n        # since no error, we are fine :)\\n        return True\\n\\n    def _create_key_bindings(self):\\n        \"\"\"Creates default key binding.\"\"\"\\n        self._INPUT_KEY_MAPPING = {\\n            # forward command\\n            \"NUMPAD_8\": np.asarray([1.0, 0.0, 0.0]) * self.v_x_sensitivity,\\n            \"UP\": np.asarray([1.0, 0.0, 0.0]) * self.v_x_sensitivity,\\n            # back command\\n            \"NUMPAD_2\": np.asarray([-1.0, 0.0, 0.0]) * self.v_x_sensitivity,\\n            \"DOWN\": np.asarray([-1.0, 0.0, 0.0]) * self.v_x_sensitivity,\\n            # right command\\n            \"NUMPAD_4\": np.asarray([0.0, 1.0, 0.0]) * self.v_y_sensitivity,\\n            \"LEFT\": np.asarray([0.0, 1.0, 0.0]) * self.v_y_sensitivity,\\n            # left command\\n            \"NUMPAD_6\": np.asarray([0.0, -1.0, 0.0]) * self.v_y_sensitivity,\\n            \"RIGHT\": np.asarray([0.0, -1.0, 0.0]) * self.v_y_sensitivity,\\n            # yaw command (positive)\\n            \"NUMPAD_7\": np.asarray([0.0, 0.0, 1.0]) * self.omega_z_sensitivity,\\n            \"Z\": np.asarray([0.0, 0.0, 1.0]) * self.omega_z_sensitivity,\\n            # yaw command (negative)\\n            \"NUMPAD_9\": np.asarray([0.0, 0.0, -1.0]) * self.omega_z_sensitivity,\\n            \"X\": np.asarray([0.0, 0.0, -1.0]) * self.omega_z_sensitivity,\\n        }'),\n",
       " Document(metadata={}, page_content='class Se3Keyboard(DeviceBase):\\n    \"\"\"A keyboard controller for sending SE(3) commands as delta poses and binary command (open/close).\\n\\n    This class is designed to provide a keyboard controller for a robotic arm with a gripper.\\n    It uses the Omniverse keyboard interface to listen to keyboard events and map them to robot\\'s\\n    task-space commands.\\n\\n    The command comprises of two parts:\\n\\n    * delta pose: a 6D vector of (x, y, z, roll, pitch, yaw) in meters and radians.\\n    * gripper: a binary command to open or close the gripper.\\n\\n    Key bindings:\\n        ============================== ================= =================\\n        Description                    Key (+ve axis)    Key (-ve axis)\\n        ============================== ================= =================\\n        Toggle gripper (open/close)    K\\n        Move along x-axis              W                 S\\n        Move along y-axis              A                 D\\n        Move along z-axis              Q                 E\\n        Rotate along x-axis            Z                 X\\n        Rotate along y-axis            T                 G\\n        Rotate along z-axis            C                 V\\n        ============================== ================= =================\\n\\n    .. seealso::\\n\\n        The official documentation for the keyboard interface: `Carb Keyboard Interface <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/keyboard.html>`__.\\n\\n    \"\"\"\\n\\n    def __init__(self, pos_sensitivity: float = 0.4, rot_sensitivity: float = 0.8):\\n        \"\"\"Initialize the keyboard layer.\\n\\n        Args:\\n            pos_sensitivity: Magnitude of input position command scaling. Defaults to 0.05.\\n            rot_sensitivity: Magnitude of scale input rotation commands scaling. Defaults to 0.5.\\n        \"\"\"\\n        # store inputs\\n        self.pos_sensitivity = pos_sensitivity\\n        self.rot_sensitivity = rot_sensitivity\\n        # acquire omniverse interfaces\\n        self._appwindow = omni.appwindow.get_default_app_window()\\n        self._input = carb.input.acquire_input_interface()\\n        self._keyboard = self._appwindow.get_keyboard()\\n        # note: Use weakref on callbacks to ensure that this object can be deleted when its destructor is called.\\n        self._keyboard_sub = self._input.subscribe_to_keyboard_events(\\n            self._keyboard,\\n            lambda event, *args, obj=weakref.proxy(self): obj._on_keyboard_event(event, *args),\\n        )\\n        # bindings for keyboard to command\\n        self._create_key_bindings()\\n        # command buffers\\n        self._close_gripper = False\\n        self._delta_pos = np.zeros(3)  # (x, y, z)\\n        self._delta_rot = np.zeros(3)  # (roll, pitch, yaw)\\n        # dictionary for additional callbacks\\n        self._additional_callbacks = dict()\\n\\n    def __del__(self):\\n        \"\"\"Release the keyboard interface.\"\"\"\\n        self._input.unsubscribe_from_keyboard_events(self._keyboard, self._keyboard_sub)\\n        self._keyboard_sub = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing the information of joystick.\"\"\"\\n        msg = f\"Keyboard Controller for SE(3): {self.__class__.__name__}\\\\n\"\\n        msg += f\"\\\\tKeyboard name: {self._input.get_keyboard_name(self._keyboard)}\\\\n\"\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tToggle gripper (open/close): K\\\\n\"\\n        msg += \"\\\\tMove arm along x-axis: W/S\\\\n\"\\n        msg += \"\\\\tMove arm along y-axis: A/D\\\\n\"\\n        msg += \"\\\\tMove arm along z-axis: Q/E\\\\n\"\\n        msg += \"\\\\tRotate arm along x-axis: Z/X\\\\n\"\\n        msg += \"\\\\tRotate arm along y-axis: T/G\\\\n\"\\n        msg += \"\\\\tRotate arm along z-axis: C/V\"\\n        return msg\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self):\\n        # default flags\\n        self._close_gripper = False\\n        self._delta_pos = np.zeros(3)  # (x, y, z)\\n        self._delta_rot = np.zeros(3)  # (roll, pitch, yaw)\\n\\n    def add_callback(self, key: str, func: Callable):\\n        \"\"\"Add additional functions to bind keyboard.\\n\\n        A list of available keys are present in the\\n        `carb documentation <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/keyboard.html>`__.\\n\\n        Args:\\n            key: The keyboard button to check against.\\n            func: The function to call when key is pressed. The callback function should not\\n                take any arguments.\\n        \"\"\"\\n        self._additional_callbacks[key] = func\\n\\n    def advance(self) -> tuple[np.ndarray, bool]:\\n        \"\"\"Provides the result from keyboard event state.\\n\\n        Returns:\\n            A tuple containing the delta pose command and gripper commands.\\n        \"\"\"\\n        # convert to rotation vector\\n        rot_vec = Rotation.from_euler(\"XYZ\", self._delta_rot).as_rotvec()\\n        # return the command and gripper state\\n        return np.concatenate([self._delta_pos, rot_vec]), self._close_gripper\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _on_keyboard_event(self, event, *args, **kwargs):\\n        \"\"\"Subscriber callback to when kit is updated.\\n\\n        Reference:\\n            https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/input-devices/keyboard.html\\n        \"\"\"\\n        # apply the command when pressed\\n        if event.type == carb.input.KeyboardEventType.KEY_PRESS:\\n            if event.input.name == \"L\":\\n                self.reset()\\n            if event.input.name == \"K\":\\n                self._close_gripper = not self._close_gripper\\n            elif event.input.name in [\"W\", \"S\", \"A\", \"D\", \"Q\", \"E\"]:\\n                self._delta_pos += self._INPUT_KEY_MAPPING[event.input.name]\\n            elif event.input.name in [\"Z\", \"X\", \"T\", \"G\", \"C\", \"V\"]:\\n                self._delta_rot += self._INPUT_KEY_MAPPING[event.input.name]\\n        # remove the command when un-pressed\\n        if event.type == carb.input.KeyboardEventType.KEY_RELEASE:\\n            if event.input.name in [\"W\", \"S\", \"A\", \"D\", \"Q\", \"E\"]:\\n                self._delta_pos -= self._INPUT_KEY_MAPPING[event.input.name]\\n            elif event.input.name in [\"Z\", \"X\", \"T\", \"G\", \"C\", \"V\"]:\\n                self._delta_rot -= self._INPUT_KEY_MAPPING[event.input.name]\\n        # additional callbacks\\n        if event.type == carb.input.KeyboardEventType.KEY_PRESS:\\n            if event.input.name in self._additional_callbacks:\\n                self._additional_callbacks[event.input.name]()\\n\\n        # since no error, we are fine :)\\n        return True\\n\\n    def _create_key_bindings(self):\\n        \"\"\"Creates default key binding.\"\"\"\\n        self._INPUT_KEY_MAPPING = {\\n            # toggle: gripper command\\n            \"K\": True,\\n            # x-axis (forward)\\n            \"W\": np.asarray([1.0, 0.0, 0.0]) * self.pos_sensitivity,\\n            \"S\": np.asarray([-1.0, 0.0, 0.0]) * self.pos_sensitivity,\\n            # y-axis (left-right)\\n            \"A\": np.asarray([0.0, 1.0, 0.0]) * self.pos_sensitivity,\\n            \"D\": np.asarray([0.0, -1.0, 0.0]) * self.pos_sensitivity,\\n            # z-axis (up-down)\\n            \"Q\": np.asarray([0.0, 0.0, 1.0]) * self.pos_sensitivity,\\n            \"E\": np.asarray([0.0, 0.0, -1.0]) * self.pos_sensitivity,\\n            # roll (around x-axis)\\n            \"Z\": np.asarray([1.0, 0.0, 0.0]) * self.rot_sensitivity,\\n            \"X\": np.asarray([-1.0, 0.0, 0.0]) * self.rot_sensitivity,\\n            # pitch (around y-axis)\\n            \"T\": np.asarray([0.0, 1.0, 0.0]) * self.rot_sensitivity,\\n            \"G\": np.asarray([0.0, -1.0, 0.0]) * self.rot_sensitivity,\\n            # yaw (around z-axis)\\n            \"C\": np.asarray([0.0, 0.0, 1.0]) * self.rot_sensitivity,\\n            \"V\": np.asarray([0.0, 0.0, -1.0]) * self.rot_sensitivity,\\n        }'),\n",
       " Document(metadata={}, page_content='class OpenXRDevice(DeviceBase):\\n    \"\"\"An OpenXR-powered device for teleoperation and interaction.\\n\\n    This device tracks hand joints using OpenXR and makes them available as:\\n    1. A dictionary of joint poses (when used directly)\\n    2. Retargeted commands for robot control (when a retargeter is provided)\\n\\n    Data format:\\n    * Joint poses: Each pose is a 7D vector (x, y, z, qw, qx, qy, qz) in meters and quaternion units\\n    * Dictionary keys: Joint names from HAND_JOINT_NAMES in isaaclab.devices.openxr.common\\n    * Supported joints include palm, wrist, and joints for thumb, index, middle, ring and little fingers\\n\\n    Teleop commands:\\n    The device responds to several teleop commands that can be subscribed to via add_callback():\\n    * \"START\": Resume hand tracking data flow\\n    * \"STOP\": Pause hand tracking data flow\\n    * \"RESET\": Reset the tracking and signal simulation reset\\n\\n    The device can track the left hand, right hand, head position, or any combination of these\\n    based on the TrackingTarget enum values. When retargeters are provided, the raw tracking\\n    data is transformed into robot control commands suitable for teleoperation.\\n    \"\"\"\\n\\n    class TrackingTarget(Enum):\\n        \"\"\"Enum class specifying what to track with OpenXR.\\n\\n        Attributes:\\n            HAND_LEFT: Track the left hand (index 0 in _get_raw_data output)\\n            HAND_RIGHT: Track the right hand (index 1 in _get_raw_data output)\\n            HEAD: Track the head/headset position (index 2 in _get_raw_data output)\\n        \"\"\"\\n\\n        HAND_LEFT = 0\\n        HAND_RIGHT = 1\\n        HEAD = 2\\n\\n    TELEOP_COMMAND_EVENT_TYPE = \"teleop_command\"\\n\\n    def __init__(\\n        self,\\n        xr_cfg: XrCfg | None,\\n        retargeters: list[RetargeterBase] | None = None,\\n    ):\\n        \"\"\"Initialize the OpenXR device.\\n\\n        Args:\\n            xr_cfg: Configuration object for OpenXR settings. If None, default settings are used.\\n            retargeters: List of retargeters to transform tracking data into robot commands.\\n                        If None or empty list, raw tracking data will be returned.\\n        \"\"\"\\n        super().__init__(retargeters)\\n        self._openxr = OpenXR()\\n        self._xr_cfg = xr_cfg or XrCfg()\\n        self._additional_callbacks = dict()\\n        self._vc_subscription = (\\n            XRCore.get_singleton()\\n            .get_message_bus()\\n            .create_subscription_to_pop_by_type(\\n                carb.events.type_from_string(self.TELEOP_COMMAND_EVENT_TYPE), self._on_teleop_command\\n            )\\n        )\\n        self._previous_joint_poses_left = np.full((26, 7), [0, 0, 0, 1, 0, 0, 0], dtype=np.float32)\\n        self._previous_joint_poses_right = np.full((26, 7), [0, 0, 0, 1, 0, 0, 0], dtype=np.float32)\\n        self._previous_headpose = np.array([0, 0, 0, 1, 0, 0, 0], dtype=np.float32)\\n\\n        # Specify the placement of the simulation when viewed in an XR device using a prim.\\n        xr_anchor = SingleXFormPrim(\"/XRAnchor\", position=self._xr_cfg.anchor_pos, orientation=self._xr_cfg.anchor_rot)\\n        carb.settings.get_settings().set_float(\"/persistent/xr/profile/ar/render/nearPlane\", self._xr_cfg.near_plane)\\n        carb.settings.get_settings().set_string(\"/persistent/xr/profile/ar/anchorMode\", \"custom anchor\")\\n        carb.settings.get_settings().set_string(\"/xrstage/profile/ar/customAnchor\", xr_anchor.prim_path)\\n\\n    def __del__(self):\\n        \"\"\"Clean up resources when the object is destroyed.\\n\\n        Properly unsubscribes from the XR message bus to prevent memory leaks\\n        and resource issues when the device is no longer needed.\\n        \"\"\"\\n        if hasattr(self, \"_vc_subscription\") and self._vc_subscription is not None:\\n            self._vc_subscription = None\\n\\n        # No need to explicitly clean up OpenXR instance as it\\'s managed by NVIDIA Isaac Sim\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns a string containing information about the OpenXR hand tracking device.\\n\\n        This provides details about the device configuration, tracking settings,\\n        and available gesture commands.\\n\\n        Returns:\\n            Formatted string with device information\\n        \"\"\"\\n\\n        msg = f\"OpenXR Hand Tracking Device: {self.__class__.__name__}\\\\n\"\\n        msg += f\"\\\\tAnchor Position: {self._xr_cfg.anchor_pos}\\\\n\"\\n        msg += f\"\\\\tAnchor Rotation: {self._xr_cfg.anchor_rot}\\\\n\"\\n\\n        # Add retargeter information\\n        if self._retargeters:\\n            msg += \"\\\\tRetargeters:\\\\n\"\\n            for i, retargeter in enumerate(self._retargeters):\\n                msg += f\"\\\\t\\\\t{i + 1}. {retargeter.__class__.__name__}\\\\n\"\\n        else:\\n            msg += \"\\\\tRetargeters: None (raw joint data output)\\\\n\"\\n\\n        # Add available gesture commands\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tAvailable Gesture Commands:\\\\n\"\\n\\n        # Check which callbacks are registered\\n        start_avail = \"START\" in self._additional_callbacks\\n        stop_avail = \"STOP\" in self._additional_callbacks\\n        reset_avail = \"RESET\" in self._additional_callbacks\\n\\n        msg += f\"\\\\t\\\\tStart Teleoperation: {\\'✓\\' if start_avail else \\'✗\\'}\\\\n\"\\n        msg += f\"\\\\t\\\\tStop Teleoperation: {\\'✓\\' if stop_avail else \\'✗\\'}\\\\n\"\\n        msg += f\"\\\\t\\\\tReset Environment: {\\'✓\\' if reset_avail else \\'✗\\'}\\\\n\"\\n\\n        # Add joint tracking information\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tTracked Joints: All 26 XR hand joints including:\\\\n\"\\n        msg += \"\\\\t\\\\t- Wrist, palm\\\\n\"\\n        msg += \"\\\\t\\\\t- Thumb (tip, intermediate joints)\\\\n\"\\n        msg += \"\\\\t\\\\t- Fingers (tip, distal, intermediate, proximal)\\\\n\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self):\\n        self._previous_joint_poses_left = np.full((26, 7), [0, 0, 0, 1, 0, 0, 0], dtype=np.float32)\\n        self._previous_joint_poses_right = np.full((26, 7), [0, 0, 0, 1, 0, 0, 0], dtype=np.float32)\\n        self._previous_headpose = np.array([0, 0, 0, 1, 0, 0, 0], dtype=np.float32)\\n\\n    def add_callback(self, key: str, func: Callable):\\n        \"\"\"Add additional functions to bind to client messages.\\n\\n        Args:\\n            key: The message type to bind to. Valid values are \"START\", \"STOP\", and \"RESET\".\\n            func: The function to call when the message is received. The callback function should not\\n                take any arguments.\\n        \"\"\"\\n        self._additional_callbacks[key] = func\\n\\n    def _get_raw_data(self) -> Any:\\n        \"\"\"Get the latest tracking data from the OpenXR runtime.\\n\\n        Returns:\\n            Dictionary containing tracking data for:\\n                - Left hand joint poses (26 joints with position and orientation)\\n                - Right hand joint poses (26 joints with position and orientation)\\n                - Head pose (position and orientation)\\n\\n        Each pose is represented as a 7-element array: [x, y, z, qw, qx, qy, qz]\\n        where the first 3 elements are position and the last 4 are quaternion orientation.\\n        \"\"\"\\n        return {\\n            self.TrackingTarget.HAND_LEFT: self._calculate_joint_poses(\\n                self._openxr.locate_hand_joints(OpenXRSpec.XrHandEXT.XR_HAND_LEFT_EXT),\\n                self._previous_joint_poses_left,\\n            ),\\n            self.TrackingTarget.HAND_RIGHT: self._calculate_joint_poses(\\n                self._openxr.locate_hand_joints(OpenXRSpec.XrHandEXT.XR_HAND_RIGHT_EXT),\\n                self._previous_joint_poses_right,\\n            ),\\n            self.TrackingTarget.HEAD: self._calculate_headpose(),\\n        }\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _calculate_joint_poses(self, hand_joints, previous_joint_poses) -> dict[str, np.ndarray]:\\n        if hand_joints is None:\\n            return self._joints_to_dict(previous_joint_poses)\\n\\n        hand_joints = np.array(hand_joints)\\n        positions = np.array([[j.pose.position.x, j.pose.position.y, j.pose.position.z] for j in hand_joints])\\n        orientations = np.array([\\n            [j.pose.orientation.w, j.pose.orientation.x, j.pose.orientation.y, j.pose.orientation.z]\\n            for j in hand_joints\\n        ])\\n        location_flags = np.array([j.locationFlags for j in hand_joints])\\n\\n        pos_mask = (location_flags & OpenXRSpec.XR_SPACE_LOCATION_POSITION_VALID_BIT) != 0\\n        ori_mask = (location_flags & OpenXRSpec.XR_SPACE_LOCATION_ORIENTATION_VALID_BIT) != 0\\n\\n        previous_joint_poses[pos_mask, 0:3] = positions[pos_mask]\\n        previous_joint_poses[ori_mask, 3:7] = orientations[ori_mask]\\n\\n        return self._joints_to_dict(previous_joint_poses)\\n\\n    def _calculate_headpose(self) -> np.ndarray:\\n        \"\"\"Calculate the head pose from OpenXR.\\n\\n        Returns:\\n            numpy.ndarray: 7-element array containing head position (xyz) and orientation (wxyz)\\n        \"\"\"\\n        head_device = XRCore.get_singleton().get_input_device(\"displayDevice\")\\n        if head_device:\\n            hmd = head_device.get_virtual_world_pose(\"\")\\n            position = hmd.ExtractTranslation()\\n            quat = hmd.ExtractRotationQuat()\\n            quati = quat.GetImaginary()\\n            quatw = quat.GetReal()\\n\\n            # Store in w, x, y, z order to match our convention\\n            self._previous_headpose = np.array([\\n                position[0],\\n                position[1],\\n                position[2],\\n                quatw,\\n                quati[0],\\n                quati[1],\\n                quati[2],\\n            ])\\n\\n        return self._previous_headpose\\n\\n    def _joints_to_dict(self, joint_data: np.ndarray) -> dict[str, np.ndarray]:\\n        \"\"\"Convert joint array to dictionary using standard joint names.\\n\\n        Args:\\n            joint_data: Array of joint data (Nx6 for N joints)\\n\\n        Returns:\\n            Dictionary mapping joint names to their data\\n        \"\"\"\\n        return {joint_name: joint_data[i] for i, joint_name in enumerate(HAND_JOINT_NAMES)}\\n\\n    def _on_teleop_command(self, event: carb.events.IEvent):\\n        msg = event.payload[\"message\"]\\n\\n        if \"start\" in msg:\\n            if \"START\" in self._additional_callbacks:\\n                self._additional_callbacks[\"START\"]()\\n        elif \"stop\" in msg:\\n            if \"STOP\" in self._additional_callbacks:\\n                self._additional_callbacks[\"STOP\"]()\\n        elif \"reset\" in msg:\\n            if \"RESET\" in self._additional_callbacks:\\n                self._additional_callbacks[\"RESET\"]()'),\n",
       " Document(metadata={}, page_content='class XrCfg:\\n    \"\"\"Configuration for viewing and interacting with the environment through an XR device.\"\"\"\\n\\n    anchor_pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n    \"\"\"Specifies the position (in m) of the simulation when viewed in an XR device.\\n\\n    Specifically: this position will appear at the origin of the XR device\\'s local coordinate frame.\\n    \"\"\"\\n\\n    anchor_rot: tuple[float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n    \"\"\"Specifies the rotation (as a quaternion) of the simulation when viewed in an XR device.\\n\\n    Specifically: this rotation will determine how the simulation is rotated with respect to the\\n    origin of the XR device\\'s local coordinate frame.\\n\\n    This quantity is only effective if :attr:`xr_anchor_pos` is set.\\n    \"\"\"\\n\\n    near_plane: float = 0.15\\n    \"\"\"Specifies the near plane distance for the XR device.\\n\\n    This value determines the closest distance at which objects will be rendered in the XR device.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class DexRetargeter(RetargeterBase):\\n    \"\"\"Retargets OpenXR hand joint data to DEX robot joint commands.\\n\\n    This class implements the RetargeterBase interface to convert hand tracking data\\n    into a format suitable for controlling DEX robot hands.\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\"Initialize the DEX retargeter.\"\"\"\\n        super().__init__()\\n        # TODO: Add any initialization parameters and state variables needed\\n        pass\\n\\n    def retarget(self, joint_data: dict[str, np.ndarray]) -> Any:\\n        \"\"\"Convert OpenXR hand joint poses to DEX robot commands.\\n\\n        Args:\\n            joint_data: Dictionary mapping OpenXR joint names to their pose data.\\n                       Each pose is a numpy array of shape (7,) containing\\n                       [x, y, z, qx, qy, qz, qw] for absolute mode or\\n                       [x, y, z, roll, pitch, yaw] for relative mode.\\n\\n        Returns:\\n            Retargeted data in the format expected by DEX robot control interface.\\n            TODO: Specify the exact return type and format\\n        \"\"\"\\n        # TODO: Implement the retargeting logic\\n        raise NotImplementedError(\"DexRetargeter.retarget() not implemented\")'),\n",
       " Document(metadata={}, page_content='class GR1T2Retargeter(RetargeterBase):\\n    \"\"\"Retargets OpenXR hand tracking data to GR1T2 hand end-effector commands.\\n\\n    This retargeter maps hand tracking data from OpenXR to joint commands for the GR1T2 robot\\'s hands.\\n    It handles both left and right hands, converting poses of the hands in OpenXR format joint angles for the GR1T2 robot\\'s hands.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        enable_visualization: bool = False,\\n        num_open_xr_hand_joints: int = 100,\\n        device: torch.device = torch.device(\"cuda:0\"),\\n        hand_joint_names: list[str] = [],\\n    ):\\n        \"\"\"Initialize the GR1T2 hand retargeter.\\n\\n        Args:\\n            enable_visualization: If True, visualize tracked hand joints\\n            num_open_xr_hand_joints: Number of joints tracked by OpenXR\\n            device: PyTorch device for computations\\n            hand_joint_names: List of robot hand joint names\\n        \"\"\"\\n\\n        self._hand_joint_names = hand_joint_names\\n        self._hands_controller = GR1TR2DexRetargeting(self._hand_joint_names)\\n\\n        # Initialize visualization if enabled\\n        self._enable_visualization = enable_visualization\\n        self._num_open_xr_hand_joints = num_open_xr_hand_joints\\n        self._device = device\\n        if self._enable_visualization:\\n            marker_cfg = VisualizationMarkersCfg(\\n                prim_path=\"/Visuals/markers\",\\n                markers={\\n                    \"joint\": sim_utils.SphereCfg(\\n                        radius=0.005,\\n                        visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0)),\\n                    ),\\n                },\\n            )\\n            self._markers = VisualizationMarkers(marker_cfg)\\n\\n    def retarget(self, data: dict) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\\n        \"\"\"Convert hand joint poses to robot end-effector commands.\\n\\n        Args:\\n            data: Dictionary mapping tracking targets to joint data dictionaries.\\n\\n        Returns:\\n            tuple containing:\\n                Left wrist pose\\n                Right wrist pose in USD frame\\n                Retargeted hand joint angles\\n        \"\"\"\\n\\n        # Access the left and right hand data using the enum key\\n        left_hand_poses = data[OpenXRDevice.TrackingTarget.HAND_LEFT]\\n        right_hand_poses = data[OpenXRDevice.TrackingTarget.HAND_RIGHT]\\n\\n        left_wrist = left_hand_poses.get(\"wrist\")\\n        right_wrist = right_hand_poses.get(\"wrist\")\\n\\n        if self._enable_visualization:\\n            joints_position = np.zeros((self._num_open_xr_hand_joints, 3))\\n\\n            joints_position[::2] = np.array([pose[:3] for pose in left_hand_poses.values()])\\n            joints_position[1::2] = np.array([pose[:3] for pose in right_hand_poses.values()])\\n\\n            self._markers.visualize(translations=torch.tensor(joints_position, device=self._device))\\n\\n        # Create array of zeros with length matching number of joint names\\n        left_hands_pos = self._hands_controller.compute_left(left_hand_poses)\\n        indexes = [self._hand_joint_names.index(name) for name in self._hands_controller.get_left_joint_names()]\\n        left_retargeted_hand_joints = np.zeros(len(self._hands_controller.get_joint_names()))\\n        left_retargeted_hand_joints[indexes] = left_hands_pos\\n        left_hand_joints = left_retargeted_hand_joints\\n\\n        right_hands_pos = self._hands_controller.compute_right(right_hand_poses)\\n        indexes = [self._hand_joint_names.index(name) for name in self._hands_controller.get_right_joint_names()]\\n        right_retargeted_hand_joints = np.zeros(len(self._hands_controller.get_joint_names()))\\n        right_retargeted_hand_joints[indexes] = right_hands_pos\\n        right_hand_joints = right_retargeted_hand_joints\\n        retargeted_hand_joints = left_hand_joints + right_hand_joints\\n\\n        return left_wrist, self._retarget_abs(right_wrist), retargeted_hand_joints\\n\\n    def _retarget_abs(self, wrist: np.ndarray) -> np.ndarray:\\n        \"\"\"Handle absolute pose retargeting.\\n\\n        Args:\\n            wrist: Wrist pose data from OpenXR\\n\\n        Returns:\\n            Retargeted wrist pose in USD control frame\\n        \"\"\"\\n\\n        # Convert wrist data in openxr frame to usd control frame\\n\\n        # Create pose object for openxr_right_wrist_in_world\\n        # Note: The pose utils require torch tensors\\n        wrist_pos = torch.tensor(wrist[:3], dtype=torch.float32)\\n        wrist_quat = torch.tensor(wrist[3:], dtype=torch.float32)\\n        openxr_right_wrist_in_world = PoseUtils.make_pose(wrist_pos, PoseUtils.matrix_from_quat(wrist_quat))\\n\\n        # The usd control frame is 180 degrees rotated around z axis wrt to the openxr frame\\n        # This was determined through trial and error\\n        zero_pos = torch.zeros(3, dtype=torch.float32)\\n        # 180 degree rotation around z axis\\n        z_axis_rot_quat = torch.tensor([0, 0, 0, 1], dtype=torch.float32)\\n        usd_right_roll_link_in_openxr_right_wrist = PoseUtils.make_pose(\\n            zero_pos, PoseUtils.matrix_from_quat(z_axis_rot_quat)\\n        )\\n\\n        # Convert wrist pose in openxr frame to usd control frame\\n        usd_right_roll_link_in_world = PoseUtils.pose_in_A_to_pose_in_B(\\n            usd_right_roll_link_in_openxr_right_wrist, openxr_right_wrist_in_world\\n        )\\n\\n        # extract position and rotation\\n        usd_right_roll_link_in_world_pos, usd_right_roll_link_in_world_mat = PoseUtils.unmake_pose(\\n            usd_right_roll_link_in_world\\n        )\\n        usd_right_roll_link_in_world_quat = PoseUtils.quat_from_matrix(usd_right_roll_link_in_world_mat)\\n\\n        return np.concatenate([usd_right_roll_link_in_world_pos, usd_right_roll_link_in_world_quat])'),\n",
       " Document(metadata={}, page_content='class GR1TR2DexRetargeting:\\n    \"\"\"A class for hand retargeting with GR1Fourier.\\n\\n    Handles retargeting of OpenXRhand tracking data to GR1T2 robot hand joint angles.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        hand_joint_names: list[str],\\n        right_hand_config_filename: str = \"fourier_hand_right_dexpilot.yml\",\\n        left_hand_config_filename: str = \"fourier_hand_left_dexpilot.yml\",\\n        left_hand_urdf_path: str = f\"{ISAACLAB_NUCLEUS_DIR}/Mimic/GR1T2_assets/GR1_T2_left_hand.urdf\",\\n        right_hand_urdf_path: str = f\"{ISAACLAB_NUCLEUS_DIR}/Mimic/GR1T2_assets/GR1_T2_right_hand.urdf\",\\n    ):\\n        \"\"\"Initialize the hand retargeting.\\n\\n        Args:\\n            hand_joint_names: Names of hand joints in the robot model\\n            right_hand_config_filename: Config file for right hand retargeting\\n            left_hand_config_filename: Config file for left hand retargeting\\n        \"\"\"\\n        data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"data/\"))\\n        config_dir = os.path.join(data_dir, \"configs/dex-retargeting\")\\n\\n        # Download urdf files from aws\\n        local_left_urdf_path = retrieve_file_path(left_hand_urdf_path, force_download=True)\\n        local_right_urdf_path = retrieve_file_path(right_hand_urdf_path, force_download=True)\\n\\n        left_config_path = os.path.join(config_dir, left_hand_config_filename)\\n        right_config_path = os.path.join(config_dir, right_hand_config_filename)\\n\\n        # Update the YAML files with the correct URDF paths\\n        self._update_yaml_with_urdf_path(left_config_path, local_left_urdf_path)\\n        self._update_yaml_with_urdf_path(right_config_path, local_right_urdf_path)\\n\\n        self._dex_left_hand = RetargetingConfig.load_from_file(left_config_path).build()\\n        self._dex_right_hand = RetargetingConfig.load_from_file(right_config_path).build()\\n\\n        self.left_dof_names = self._dex_left_hand.optimizer.robot.dof_joint_names\\n        self.right_dof_names = self._dex_right_hand.optimizer.robot.dof_joint_names\\n        self.dof_names = self.left_dof_names + self.right_dof_names\\n        self.isaac_lab_hand_joint_names = hand_joint_names\\n\\n        omni.log.info(\"[GR1T2DexRetargeter] init done.\")\\n\\n    def _update_yaml_with_urdf_path(self, yaml_path: str, urdf_path: str):\\n        \"\"\"Update YAML file with the correct URDF path.\\n\\n        Args:\\n            yaml_path: Path to the YAML configuration file\\n            urdf_path: Path to the URDF file to use\\n        \"\"\"\\n        try:\\n            # Read the YAML file\\n            with open(yaml_path) as file:\\n                config = yaml.safe_load(file)\\n\\n            # Update the URDF path in the configuration\\n            if \"retargeting\" in config:\\n                config[\"retargeting\"][\"urdf_path\"] = urdf_path\\n                omni.log.info(f\"Updated URDF path in {yaml_path} to {urdf_path}\")\\n            else:\\n                omni.log.warn(f\"Unable to find \\'retargeting\\' section in {yaml_path}\")\\n\\n            # Write the updated configuration back to the file\\n            with open(yaml_path, \"w\") as file:\\n                yaml.dump(config, file)\\n\\n        except Exception as e:\\n            omni.log.error(f\"Error updating YAML file {yaml_path}: {e}\")\\n\\n    def convert_hand_joints(self, hand_poses: dict[str, np.ndarray], operator2mano: np.ndarray) -> np.ndarray:\\n        \"\"\"Prepares the hand joints data for retargeting.\\n\\n        Args:\\n            hand_poses: Dictionary containing hand pose data with joint positions and rotations\\n            operator2mano: Transformation matrix to convert from operator to MANO frame\\n\\n        Returns:\\n            Joint positions with shape (21, 3)\\n        \"\"\"\\n        joint_position = np.zeros((21, 3))\\n        hand_joints = list(hand_poses.values())\\n        for i in range(len(_HAND_JOINTS_INDEX)):\\n            joint = hand_joints[_HAND_JOINTS_INDEX[i]]\\n            joint_position[i] = joint[:3]\\n\\n        # Convert hand pose to the canonical frame.\\n        joint_position = joint_position - joint_position[0:1, :]\\n        xr_wrist_quat = hand_poses.get(\"wrist\")[3:]\\n        # OpenXR hand uses w,x,y,z order for quaternions but scipy uses x,y,z,w order\\n        wrist_rot = R.from_quat([xr_wrist_quat[1], xr_wrist_quat[2], xr_wrist_quat[3], xr_wrist_quat[0]]).as_matrix()\\n\\n        return joint_position @ wrist_rot @ operator2mano\\n\\n    def compute_ref_value(self, joint_position: np.ndarray, indices: np.ndarray, retargeting_type: str) -> np.ndarray:\\n        \"\"\"Computes reference value for retargeting.\\n\\n        Args:\\n            joint_position: Joint positions array\\n            indices: Target link indices\\n            retargeting_type: Type of retargeting (\"POSITION\" or other)\\n\\n        Returns:\\n            Reference value in cartesian space\\n        \"\"\"\\n        if retargeting_type == \"POSITION\":\\n            return joint_position[indices, :]\\n        else:\\n            origin_indices = indices[0, :]\\n            task_indices = indices[1, :]\\n            ref_value = joint_position[task_indices, :] - joint_position[origin_indices, :]\\n            return ref_value\\n\\n    def compute_one_hand(\\n        self, hand_joints: dict[str, np.ndarray], retargeting: RetargetingConfig, operator2mano: np.ndarray\\n    ) -> np.ndarray:\\n        \"\"\"Computes retargeted joint angles for one hand.\\n\\n        Args:\\n            hand_joints: Dictionary containing hand joint data\\n            retargeting: Retargeting configuration object\\n            operator2mano: Transformation matrix from operator to MANO frame\\n\\n        Returns:\\n            Retargeted joint angles\\n        \"\"\"\\n        joint_pos = self.convert_hand_joints(hand_joints, operator2mano)\\n        ref_value = self.compute_ref_value(\\n            joint_pos,\\n            indices=retargeting.optimizer.target_link_human_indices,\\n            retargeting_type=retargeting.optimizer.retargeting_type,\\n        )\\n        # Enable gradient calculation and inference mode in case some other script has disabled it\\n        # This is necessary for the retargeting to work since it uses gradient features that\\n        # are not available in inference mode\\n        with torch.enable_grad():\\n            with torch.inference_mode(False):\\n                return retargeting.retarget(ref_value)\\n\\n    def get_joint_names(self) -> list[str]:\\n        \"\"\"Returns list of all joint names.\"\"\"\\n        return self.dof_names\\n\\n    def get_left_joint_names(self) -> list[str]:\\n        \"\"\"Returns list of left hand joint names.\"\"\"\\n        return self.left_dof_names\\n\\n    def get_right_joint_names(self) -> list[str]:\\n        \"\"\"Returns list of right hand joint names.\"\"\"\\n        return self.right_dof_names\\n\\n    def get_hand_indices(self, robot) -> np.ndarray:\\n        \"\"\"Gets indices of hand joints in robot\\'s DOF array.\\n\\n        Args:\\n            robot: Robot object containing DOF information\\n\\n        Returns:\\n            Array of joint indices\\n        \"\"\"\\n        return np.array([robot.dof_names.index(name) for name in self.dof_names], dtype=np.int64)\\n\\n    def compute_left(self, left_hand_poses: dict[str, np.ndarray]) -> np.ndarray:\\n        \"\"\"Computes retargeted joints for left hand.\\n\\n        Args:\\n            left_hand_poses: Dictionary of left hand joint poses\\n\\n        Returns:\\n            Retargeted joint angles for left hand\\n        \"\"\"\\n        if left_hand_poses is not None:\\n            left_hand_q = self.compute_one_hand(left_hand_poses, self._dex_left_hand, _OPERATOR2MANO_LEFT)\\n        else:\\n            left_hand_q = np.zeros(len(_LEFT_HAND_JOINT_NAMES))\\n        return left_hand_q\\n\\n    def compute_right(self, right_hand_poses: dict[str, np.ndarray]) -> np.ndarray:\\n        \"\"\"Computes retargeted joints for right hand.\\n\\n        Args:\\n            right_hand_poses: Dictionary of right hand joint poses\\n\\n        Returns:\\n            Retargeted joint angles for right hand\\n        \"\"\"\\n        if right_hand_poses is not None:\\n            right_hand_q = self.compute_one_hand(right_hand_poses, self._dex_right_hand, _OPERATOR2MANO_RIGHT)\\n        else:\\n            right_hand_q = np.zeros(len(_RIGHT_HAND_JOINT_NAMES))\\n        return right_hand_q'),\n",
       " Document(metadata={}, page_content='retargeting:\\n  finger_tip_link_names:\\n  - GR1T2_fourier_hand_6dof_L_thumb_distal_link\\n  - GR1T2_fourier_hand_6dof_L_index_intermediate_link\\n  - GR1T2_fourier_hand_6dof_L_middle_intermediate_link\\n  - GR1T2_fourier_hand_6dof_L_ring_intermediate_link\\n  - GR1T2_fourier_hand_6dof_L_pinky_intermediate_link\\n  low_pass_alpha: 0.2\\n  scaling_factor: 1.0\\n  target_joint_names:\\n  - L_index_proximal_joint\\n  - L_middle_proximal_joint\\n  - L_pinky_proximal_joint\\n  - L_ring_proximal_joint'),\n",
       " Document(metadata={}, page_content='- L_ring_proximal_joint\\n  - L_index_intermediate_joint\\n  - L_middle_intermediate_joint\\n  - L_pinky_intermediate_joint\\n  - L_ring_intermediate_joint\\n  - L_thumb_proximal_yaw_joint\\n  - L_thumb_proximal_pitch_joint\\n  - L_thumb_distal_joint\\n  - L_thumb_distal_joint\\n  type: DexPilot\\n  urdf_path: /tmp/GR1_T2_left_hand.urdf\\n  wrist_link_name: l_hand_base_link'),\n",
       " Document(metadata={}, page_content='retargeting:\\n  finger_tip_link_names:\\n  - GR1T2_fourier_hand_6dof_R_thumb_distal_link\\n  - GR1T2_fourier_hand_6dof_R_index_intermediate_link\\n  - GR1T2_fourier_hand_6dof_R_middle_intermediate_link\\n  - GR1T2_fourier_hand_6dof_R_ring_intermediate_link\\n  - GR1T2_fourier_hand_6dof_R_pinky_intermediate_link\\n  low_pass_alpha: 0.2\\n  scaling_factor: 1.0\\n  target_joint_names:\\n  - R_index_proximal_joint\\n  - R_middle_proximal_joint\\n  - R_pinky_proximal_joint\\n  - R_ring_proximal_joint'),\n",
       " Document(metadata={}, page_content='- R_ring_proximal_joint\\n  - R_index_intermediate_joint\\n  - R_middle_intermediate_joint\\n  - R_pinky_intermediate_joint\\n  - R_ring_intermediate_joint\\n  - R_thumb_proximal_yaw_joint\\n  - R_thumb_proximal_pitch_joint\\n  - R_thumb_distal_joint\\n  type: DexPilot\\n  urdf_path: /tmp/GR1_T2_right_hand.urdf\\n  wrist_link_name: r_hand_base_link'),\n",
       " Document(metadata={}, page_content='class GripperRetargeter(RetargeterBase):\\n    \"\"\"Retargeter specifically for gripper control based on hand tracking data.\\n\\n    This retargeter analyzes the distance between thumb and index finger tips to determine\\n    whether the gripper should be open or closed. It includes hysteresis to prevent rapid\\n    toggling between states when the finger distance is near the threshold.\\n\\n    Features:\\n    - Tracks thumb and index finger distance\\n    - Implements hysteresis for stable gripper control\\n    - Outputs boolean command (True = close gripper, False = open gripper)\\n    \"\"\"\\n\\n    GRIPPER_CLOSE_METERS: Final[float] = 0.03\\n    GRIPPER_OPEN_METERS: Final[float] = 0.05\\n\\n    def __init__(\\n        self,\\n        bound_hand: OpenXRDevice.TrackingTarget,\\n    ):\\n        \"\"\"Initialize the gripper retargeter.\"\"\"\\n        # Store the hand to track\\n        if bound_hand not in [OpenXRDevice.TrackingTarget.HAND_LEFT, OpenXRDevice.TrackingTarget.HAND_RIGHT]:\\n            raise ValueError(\\n                \"bound_hand must be either OpenXRDevice.TrackingTarget.HAND_LEFT or\"\\n                \" OpenXRDevice.TrackingTarget.HAND_RIGHT\"\\n            )\\n        self.bound_hand = bound_hand\\n        # Initialize gripper state\\n        self._previous_gripper_command = False\\n\\n    def retarget(self, data: dict) -> bool:\\n        \"\"\"Convert hand joint poses to gripper command.\\n\\n        Args:\\n            data: Dictionary mapping tracking targets to joint data dictionaries.\\n                The joint names are defined in isaaclab.devices.openxr.common.HAND_JOINT_NAMES\\n\\n        Returns:\\n            bool: Gripper command where True = close gripper, False = open gripper\\n        \"\"\"\\n        # Extract key joint poses\\n        hand_data = data[self.bound_hand]\\n        thumb_tip = hand_data[\"thumb_tip\"]\\n        index_tip = hand_data[\"index_tip\"]\\n\\n        # Calculate gripper command with hysteresis\\n        gripper_command = self._calculate_gripper_command(thumb_tip[:3], index_tip[:3])\\n        return gripper_command\\n\\n    def _calculate_gripper_command(self, thumb_pos: np.ndarray, index_pos: np.ndarray) -> bool:\\n        \"\"\"Calculate gripper command from finger positions with hysteresis.\\n\\n        Args:\\n            thumb_pos: 3D position of thumb tip\\n            index_pos: 3D position of index tip\\n\\n        Returns:\\n            bool: Gripper command (True = close, False = open)\\n        \"\"\"\\n        distance = np.linalg.norm(thumb_pos - index_pos)\\n\\n        # Apply hysteresis to prevent rapid switching\\n        if distance > self.GRIPPER_OPEN_METERS:\\n            self._previous_gripper_command = False\\n        elif distance < self.GRIPPER_CLOSE_METERS:\\n            self._previous_gripper_command = True\\n\\n        return self._previous_gripper_command'),\n",
       " Document(metadata={}, page_content='class Se3AbsRetargeter(RetargeterBase):\\n    \"\"\"Retargets OpenXR hand tracking data to end-effector commands using absolute positioning.\\n\\n    This retargeter maps hand joint poses directly to robot end-effector positions and orientations,\\n    rather than using relative movements. It can either:\\n    - Use the wrist position and orientation\\n    - Use the midpoint between thumb and index finger (pinch position)\\n\\n    Features:\\n    - Optional constraint to zero out X/Y rotations (keeping only Z-axis rotation)\\n    - Optional visualization of the target end-effector pose\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        bound_hand: OpenXRDevice.TrackingTarget,\\n        zero_out_xy_rotation: bool = False,\\n        use_wrist_rotation: bool = False,\\n        use_wrist_position: bool = False,\\n        enable_visualization: bool = False,\\n    ):\\n        \"\"\"Initialize the retargeter.\\n\\n        Args:\\n            bound_hand: The hand to track (OpenXRDevice.TrackingTarget.HAND_LEFT or OpenXRDevice.TrackingTarget.HAND_RIGHT)\\n            zero_out_xy_rotation: If True, zero out rotation around x and y axes\\n            use_wrist_rotation: If True, use wrist rotation instead of finger average\\n            use_wrist_position: If True, use wrist position instead of pinch position\\n            enable_visualization: If True, visualize the target pose in the scene\\n        \"\"\"\\n        if bound_hand not in [OpenXRDevice.TrackingTarget.HAND_LEFT, OpenXRDevice.TrackingTarget.HAND_RIGHT]:\\n            raise ValueError(\\n                \"bound_hand must be either OpenXRDevice.TrackingTarget.HAND_LEFT or\"\\n                \" OpenXRDevice.TrackingTarget.HAND_RIGHT\"\\n            )\\n        self.bound_hand = bound_hand\\n\\n        self._zero_out_xy_rotation = zero_out_xy_rotation\\n        self._use_wrist_rotation = use_wrist_rotation\\n        self._use_wrist_position = use_wrist_position\\n\\n        # Initialize visualization if enabled\\n        self._enable_visualization = enable_visualization\\n        if enable_visualization:\\n            frame_marker_cfg = FRAME_MARKER_CFG.copy()\\n            frame_marker_cfg.markers[\"frame\"].scale = (0.1, 0.1, 0.1)\\n            self._goal_marker = VisualizationMarkers(frame_marker_cfg.replace(prim_path=\"/Visuals/ee_goal\"))\\n            self._goal_marker.set_visibility(True)\\n            self._visualization_pos = np.zeros(3)\\n            self._visualization_rot = np.array([1.0, 0.0, 0.0, 0.0])\\n\\n    def retarget(self, data: dict) -> np.ndarray:\\n        \"\"\"Convert hand joint poses to robot end-effector command.\\n\\n        Args:\\n            data: Dictionary mapping tracking targets to joint data dictionaries.\\n                The joint names are defined in isaaclab.devices.openxr.common.HAND_JOINT_NAMES\\n\\n        Returns:\\n            np.ndarray: 7D array containing position (xyz) and orientation (quaternion)\\n                for the robot end-effector\\n        \"\"\"\\n        # Extract key joint poses from the bound hand\\n        hand_data = data[self.bound_hand]\\n        thumb_tip = hand_data.get(\"thumb_tip\")\\n        index_tip = hand_data.get(\"index_tip\")\\n        wrist = hand_data.get(\"wrist\")\\n\\n        ee_command = self._retarget_abs(thumb_tip, index_tip, wrist)\\n\\n        return ee_command\\n\\n    def _retarget_abs(self, thumb_tip: np.ndarray, index_tip: np.ndarray, wrist: np.ndarray) -> np.ndarray:\\n        \"\"\"Handle absolute pose retargeting.\\n\\n        Args:\\n            thumb_tip: 7D array containing position (xyz) and orientation (quaternion)\\n                for the thumb tip\\n            index_tip: 7D array containing position (xyz) and orientation (quaternion)\\n                for the index tip\\n            wrist: 7D array containing position (xyz) and orientation (quaternion)\\n                for the wrist\\n\\n        Returns:\\n            np.ndarray: 7D array containing position (xyz) and orientation (quaternion)\\n                for the robot end-effector\\n        \"\"\"\\n\\n        # Get position\\n        if self._use_wrist_position:\\n            position = wrist[:3]\\n        else:\\n            position = (thumb_tip[:3] + index_tip[:3]) / 2\\n\\n        # Get rotation\\n        if self._use_wrist_rotation:\\n            # wrist is w,x,y,z but scipy expects x,y,z,w\\n            base_rot = Rotation.from_quat([*wrist[4:], wrist[3]])\\n        else:\\n            # Average the orientations of thumb and index using SLERP\\n            # thumb_tip is w,x,y,z but scipy expects x,y,z,w\\n            r0 = Rotation.from_quat([*thumb_tip[4:], thumb_tip[3]])\\n            # index_tip is w,x,y,z but scipy expects x,y,z,w\\n            r1 = Rotation.from_quat([*index_tip[4:], index_tip[3]])\\n            key_times = [0, 1]\\n            slerp = Slerp(key_times, Rotation.concatenate([r0, r1]))\\n            base_rot = slerp([0.5])[0]\\n\\n        # Apply additional x-axis rotation to align with pinch gesture\\n        final_rot = base_rot * Rotation.from_euler(\"x\", 90, degrees=True)\\n\\n        if self._zero_out_xy_rotation:\\n            z, y, x = final_rot.as_euler(\"ZYX\")\\n            y = 0.0  # Zero out rotation around y-axis\\n            x = 0.0  # Zero out rotation around x-axis\\n            final_rot = Rotation.from_euler(\"ZYX\", [z, y, x]) * Rotation.from_euler(\"X\", np.pi, degrees=False)\\n\\n        # Convert back to w,x,y,z format\\n        quat = final_rot.as_quat()\\n        rotation = np.array([quat[3], quat[0], quat[1], quat[2]])  # Output remains w,x,y,z\\n\\n        # Update visualization if enabled\\n        if self._enable_visualization:\\n            self._visualization_pos = position\\n            self._visualization_rot = rotation\\n            self._update_visualization()\\n\\n        return np.concatenate([position, rotation])\\n\\n    def _update_visualization(self):\\n        \"\"\"Update visualization markers with current pose.\\n\\n        If visualization is enabled, the target end-effector pose is visualized in the scene.\\n        \"\"\"\\n        if self._enable_visualization:\\n            trans = np.array([self._visualization_pos])\\n            quat = Rotation.from_matrix(self._visualization_rot).as_quat()\\n            rot = np.array([np.array([quat[3], quat[0], quat[1], quat[2]])])\\n            self._goal_marker.visualize(translations=trans, orientations=rot)'),\n",
       " Document(metadata={}, page_content='class Se3RelRetargeter(RetargeterBase):\\n    \"\"\"Retargets OpenXR hand tracking data to end-effector commands using relative positioning.\\n\\n    This retargeter calculates delta poses between consecutive hand joint poses to generate incremental robot movements.\\n    It can either:\\n    - Use the wrist position and orientation\\n    - Use the midpoint between thumb and index finger (pinch position)\\n\\n    Features:\\n    - Optional constraint to zero out X/Y rotations (keeping only Z-axis rotation)\\n    - Motion smoothing with adjustable parameters\\n    - Optional visualization of the target end-effector pose\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        bound_hand: OpenXRDevice.TrackingTarget,\\n        zero_out_xy_rotation: bool = False,\\n        use_wrist_rotation: bool = False,\\n        use_wrist_position: bool = True,\\n        delta_pos_scale_factor: float = 10.0,\\n        delta_rot_scale_factor: float = 10.0,\\n        alpha_pos: float = 0.5,\\n        alpha_rot: float = 0.5,\\n        enable_visualization: bool = False,\\n    ):\\n        \"\"\"Initialize the relative motion retargeter.\\n\\n        Args:\\n            bound_hand: The hand to track (OpenXRDevice.TrackingTarget.HAND_LEFT or OpenXRDevice.TrackingTarget.HAND_RIGHT)\\n            zero_out_xy_rotation: If True, ignore rotations around x and y axes, allowing only z-axis rotation\\n            use_wrist_rotation: If True, use wrist rotation for control instead of averaging finger orientations\\n            use_wrist_position: If True, use wrist position instead of pinch position (midpoint between fingers)\\n            delta_pos_scale_factor: Amplification factor for position changes (higher = larger robot movements)\\n            delta_rot_scale_factor: Amplification factor for rotation changes (higher = larger robot rotations)\\n            alpha_pos: Position smoothing parameter (0-1); higher values track more closely to input, lower values smooth more\\n            alpha_rot: Rotation smoothing parameter (0-1); higher values track more closely to input, lower values smooth more\\n            enable_visualization: If True, show a visual marker representing the target end-effector pose\\n        \"\"\"\\n        # Store the hand to track\\n        if bound_hand not in [OpenXRDevice.TrackingTarget.HAND_LEFT, OpenXRDevice.TrackingTarget.HAND_RIGHT]:\\n            raise ValueError(\\n                \"bound_hand must be either OpenXRDevice.TrackingTarget.HAND_LEFT or\"\\n                \" OpenXRDevice.TrackingTarget.HAND_RIGHT\"\\n            )\\n        self.bound_hand = bound_hand\\n\\n        self._zero_out_xy_rotation = zero_out_xy_rotation\\n        self._use_wrist_rotation = use_wrist_rotation\\n        self._use_wrist_position = use_wrist_position\\n        self._delta_pos_scale_factor = delta_pos_scale_factor\\n        self._delta_rot_scale_factor = delta_rot_scale_factor\\n        self._alpha_pos = alpha_pos\\n        self._alpha_rot = alpha_rot\\n\\n        # Initialize smoothing state\\n        self._smoothed_delta_pos = np.zeros(3)\\n        self._smoothed_delta_rot = np.zeros(3)\\n\\n        # Define thresholds for small movements\\n        self._position_threshold = 0.001\\n        self._rotation_threshold = 0.01\\n\\n        # Initialize visualization if enabled\\n        self._enable_visualization = enable_visualization\\n        if enable_visualization:\\n            frame_marker_cfg = FRAME_MARKER_CFG.copy()\\n            frame_marker_cfg.markers[\"frame\"].scale = (0.1, 0.1, 0.1)\\n            self._goal_marker = VisualizationMarkers(frame_marker_cfg.replace(prim_path=\"/Visuals/ee_goal\"))\\n            self._goal_marker.set_visibility(True)\\n            self._visualization_pos = np.zeros(3)\\n            self._visualization_rot = np.array([1.0, 0.0, 0.0, 0.0])\\n\\n        self._previous_thumb_tip = np.array([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], dtype=np.float32)\\n        self._previous_index_tip = np.array([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], dtype=np.float32)\\n        self._previous_wrist = np.array([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], dtype=np.float32)\\n\\n    def retarget(self, data: dict) -> np.ndarray:\\n        \"\"\"Convert hand joint poses to robot end-effector command.\\n\\n        Args:\\n            data: Dictionary mapping tracking targets to joint data dictionaries.\\n                The joint names are defined in isaaclab.devices.openxr.common.HAND_JOINT_NAMES\\n\\n        Returns:\\n            np.ndarray: 6D array containing position (xyz) and rotation vector (rx,ry,rz)\\n                for the robot end-effector\\n        \"\"\"\\n        # Extract key joint poses from the bound hand\\n        hand_data = data[self.bound_hand]\\n        thumb_tip = hand_data.get(\"thumb_tip\")\\n        index_tip = hand_data.get(\"index_tip\")\\n        wrist = hand_data.get(\"wrist\")\\n\\n        delta_thumb_tip = self._calculate_delta_pose(thumb_tip, self._previous_thumb_tip)\\n        delta_index_tip = self._calculate_delta_pose(index_tip, self._previous_index_tip)\\n        delta_wrist = self._calculate_delta_pose(wrist, self._previous_wrist)\\n        ee_command = self._retarget_rel(delta_thumb_tip, delta_index_tip, delta_wrist)\\n\\n        self._previous_thumb_tip = thumb_tip.copy()\\n        self._previous_index_tip = index_tip.copy()\\n        self._previous_wrist = wrist.copy()\\n\\n        return ee_command\\n\\n    def _calculate_delta_pose(self, joint_pose: np.ndarray, previous_joint_pose: np.ndarray) -> np.ndarray:\\n        \"\"\"Calculate delta pose from previous joint pose.\\n\\n        Args:\\n            joint_pose: Current joint pose (position and orientation)\\n            previous_joint_pose: Previous joint pose for the same joint\\n\\n        Returns:\\n            np.ndarray: 6D array with position delta (xyz) and rotation delta as axis-angle (rx,ry,rz)\\n        \"\"\"\\n        delta_pos = joint_pose[:3] - previous_joint_pose[:3]\\n        abs_rotation = Rotation.from_quat([*joint_pose[4:7], joint_pose[3]])\\n        previous_rot = Rotation.from_quat([*previous_joint_pose[4:7], previous_joint_pose[3]])\\n        relative_rotation = abs_rotation * previous_rot.inv()\\n        return np.concatenate([delta_pos, relative_rotation.as_rotvec()])\\n\\n    def _retarget_rel(self, thumb_tip: np.ndarray, index_tip: np.ndarray, wrist: np.ndarray) -> np.ndarray:\\n        \"\"\"Handle relative (delta) pose retargeting.\\n\\n        Args:\\n            thumb_tip: Delta pose of thumb tip\\n            index_tip: Delta pose of index tip\\n            wrist: Delta pose of wrist\\n\\n        Returns:\\n            np.ndarray: 6D array with position delta (xyz) and rotation delta (rx,ry,rz)\\n        \"\"\"\\n        # Get position\\n        if self._use_wrist_position:\\n            position = wrist[:3]\\n        else:\\n            position = (thumb_tip[:3] + index_tip[:3]) / 2\\n\\n        # Get rotation\\n        if self._use_wrist_rotation:\\n            rotation = wrist[3:6]  # rx, ry, rz\\n        else:\\n            rotation = (thumb_tip[3:6] + index_tip[3:6]) / 2\\n\\n        # Apply zero_out_xy_rotation regardless of rotation source\\n        if self._zero_out_xy_rotation:\\n            rotation[0] = 0  # x-axis\\n            rotation[1] = 0  # y-axis\\n\\n        # Smooth and scale position\\n        self._smoothed_delta_pos = self._alpha_pos * position + (1 - self._alpha_pos) * self._smoothed_delta_pos\\n        if np.linalg.norm(self._smoothed_delta_pos) < self._position_threshold:\\n            self._smoothed_delta_pos = np.zeros(3)\\n        position = self._smoothed_delta_pos * self._delta_pos_scale_factor\\n\\n        # Smooth and scale rotation\\n        self._smoothed_delta_rot = self._alpha_rot * rotation + (1 - self._alpha_rot) * self._smoothed_delta_rot\\n        if np.linalg.norm(self._smoothed_delta_rot) < self._rotation_threshold:\\n            self._smoothed_delta_rot = np.zeros(3)\\n        rotation = self._smoothed_delta_rot * self._delta_rot_scale_factor\\n\\n        # Update visualization if enabled\\n        if self._enable_visualization:\\n            # Convert rotation vector to quaternion and combine with current rotation\\n            delta_quat = Rotation.from_rotvec(rotation).as_quat()  # x, y, z, w format\\n            current_rot = Rotation.from_quat([self._visualization_rot[1:], self._visualization_rot[0]])\\n            new_rot = Rotation.from_quat(delta_quat) * current_rot\\n            self._visualization_pos = self._visualization_pos + position\\n            # Convert back to w, x, y, z format\\n            self._visualization_rot = np.array([new_rot.as_quat()[3], *new_rot.as_quat()[:3]])\\n            self._update_visualization()\\n\\n        return np.concatenate([position, rotation])\\n\\n    def _update_visualization(self):\\n        \"\"\"Update visualization markers with current pose.\"\"\"\\n        if self._enable_visualization:\\n            trans = np.array([self._visualization_pos])\\n            quat = Rotation.from_matrix(self._visualization_rot).as_quat()\\n            rot = np.array([np.array([quat[3], quat[0], quat[1], quat[2]])])\\n            self._goal_marker.visualize(translations=trans, orientations=rot)'),\n",
       " Document(metadata={}, page_content='class Se2SpaceMouse(DeviceBase):\\n    r\"\"\"A space-mouse controller for sending SE(2) commands as delta poses.\\n\\n    This class implements a space-mouse controller to provide commands to mobile base.\\n    It uses the `HID-API`_ which interfaces with USD and Bluetooth HID-class devices across multiple platforms.\\n\\n    The command comprises of the base linear and angular velocity: :math:`(v_x, v_y, \\\\omega_z)`.\\n\\n    Note:\\n        The interface finds and uses the first supported device connected to the computer.\\n\\n    Currently tested for following devices:\\n\\n    - SpaceMouse Compact: https://3dconnexion.com/de/product/spacemouse-compact/\\n\\n    .. _HID-API: https://github.com/libusb/hidapi\\n\\n    \"\"\"\\n\\n    def __init__(self, v_x_sensitivity: float = 0.8, v_y_sensitivity: float = 0.4, omega_z_sensitivity: float = 1.0):\\n        \"\"\"Initialize the spacemouse layer.\\n\\n        Args:\\n            v_x_sensitivity: Magnitude of linear velocity along x-direction scaling. Defaults to 0.8.\\n            v_y_sensitivity: Magnitude of linear velocity along y-direction scaling. Defaults to 0.4.\\n            omega_z_sensitivity: Magnitude of angular velocity along z-direction scaling. Defaults to 1.0.\\n        \"\"\"\\n        # store inputs\\n        self.v_x_sensitivity = v_x_sensitivity\\n        self.v_y_sensitivity = v_y_sensitivity\\n        self.omega_z_sensitivity = omega_z_sensitivity\\n        # acquire device interface\\n        self._device = hid.device()\\n        self._find_device()\\n        # command buffers\\n        self._base_command = np.zeros(3)\\n        # dictionary for additional callbacks\\n        self._additional_callbacks = dict()\\n        # run a thread for listening to device updates\\n        self._thread = threading.Thread(target=self._run_device)\\n        self._thread.daemon = True\\n        self._thread.start()\\n\\n    def __del__(self):\\n        \"\"\"Destructor for the class.\"\"\"\\n        self._thread.join()\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing the information of joystick.\"\"\"\\n        msg = f\"Spacemouse Controller for SE(2): {self.__class__.__name__}\\\\n\"\\n        msg += f\"\\\\tManufacturer: {self._device.get_manufacturer_string()}\\\\n\"\\n        msg += f\"\\\\tProduct: {self._device.get_product_string()}\\\\n\"\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tRight button: reset command\\\\n\"\\n        msg += \"\\\\tMove mouse laterally: move base horizontally in x-y plane\\\\n\"\\n        msg += \"\\\\tTwist mouse about z-axis: yaw base about a corresponding axis\"\\n        return msg\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self):\\n        # default flags\\n        self._base_command.fill(0.0)\\n\\n    def add_callback(self, key: str, func: Callable):\\n        # check keys supported by callback\\n        if key not in [\"L\", \"R\"]:\\n            raise ValueError(f\"Only left (L) and right (R) buttons supported. Provided: {key}.\")\\n        # TODO: Improve this to allow multiple buttons on same key.\\n        self._additional_callbacks[key] = func\\n\\n    def advance(self) -> np.ndarray:\\n        \"\"\"Provides the result from spacemouse event state.\\n\\n        Returns:\\n            A 3D array containing the linear (x,y) and angular velocity (z).\\n        \"\"\"\\n        return self._base_command\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _find_device(self):\\n        \"\"\"Find the device connected to computer.\"\"\"\\n        found = False\\n        # implement a timeout for device search\\n        for _ in range(5):\\n            for device in hid.enumerate():\\n                if device[\"product_string\"] == \"SpaceMouse Compact\":\\n                    # set found flag\\n                    found = True\\n                    vendor_id = device[\"vendor_id\"]\\n                    product_id = device[\"product_id\"]\\n                    # connect to the device\\n                    self._device.open(vendor_id, product_id)\\n            # check if device found\\n            if not found:\\n                time.sleep(1.0)\\n            else:\\n                break\\n        # no device found: return false\\n        if not found:\\n            raise OSError(\"No device found by SpaceMouse. Is the device connected?\")\\n\\n    def _run_device(self):\\n        \"\"\"Listener thread that keeps pulling new messages.\"\"\"\\n        # keep running\\n        while True:\\n            # read the device data\\n            data = self._device.read(13)\\n            if data is not None:\\n                # readings from 6-DoF sensor\\n                if data[0] == 1:\\n                    # along y-axis\\n                    self._base_command[1] = self.v_y_sensitivity * convert_buffer(data[1], data[2])\\n                    # along x-axis\\n                    self._base_command[0] = self.v_x_sensitivity * convert_buffer(data[3], data[4])\\n                elif data[0] == 2:\\n                    # along z-axis\\n                    self._base_command[2] = self.omega_z_sensitivity * convert_buffer(data[3], data[4])\\n                # readings from the side buttons\\n                elif data[0] == 3:\\n                    # press left button\\n                    if data[1] == 1:\\n                        # additional callbacks\\n                        if \"L\" in self._additional_callbacks:\\n                            self._additional_callbacks[\"L\"]\\n                    # right button is for reset\\n                    if data[1] == 2:\\n                        # reset layer\\n                        self.reset()\\n                        # additional callbacks\\n                        if \"R\" in self._additional_callbacks:\\n                            self._additional_callbacks[\"R\"]'),\n",
       " Document(metadata={}, page_content='class Se3SpaceMouse(DeviceBase):\\n    \"\"\"A space-mouse controller for sending SE(3) commands as delta poses.\\n\\n    This class implements a space-mouse controller to provide commands to a robotic arm with a gripper.\\n    It uses the `HID-API`_ which interfaces with USD and Bluetooth HID-class devices across multiple platforms [1].\\n\\n    The command comprises of two parts:\\n\\n    * delta pose: a 6D vector of (x, y, z, roll, pitch, yaw) in meters and radians.\\n    * gripper: a binary command to open or close the gripper.\\n\\n    Note:\\n        The interface finds and uses the first supported device connected to the computer.\\n\\n    Currently tested for following devices:\\n\\n    - SpaceMouse Compact: https://3dconnexion.com/de/product/spacemouse-compact/\\n\\n    .. _HID-API: https://github.com/libusb/hidapi\\n\\n    \"\"\"\\n\\n    def __init__(self, pos_sensitivity: float = 0.4, rot_sensitivity: float = 0.8):\\n        \"\"\"Initialize the space-mouse layer.\\n\\n        Args:\\n            pos_sensitivity: Magnitude of input position command scaling. Defaults to 0.4.\\n            rot_sensitivity: Magnitude of scale input rotation commands scaling. Defaults to 0.8.\\n        \"\"\"\\n        # store inputs\\n        self.pos_sensitivity = pos_sensitivity\\n        self.rot_sensitivity = rot_sensitivity\\n        # acquire device interface\\n        self._device = hid.device()\\n        self._find_device()\\n        # read rotations\\n        self._read_rotation = False\\n\\n        # command buffers\\n        self._close_gripper = False\\n        self._delta_pos = np.zeros(3)  # (x, y, z)\\n        self._delta_rot = np.zeros(3)  # (roll, pitch, yaw)\\n        # dictionary for additional callbacks\\n        self._additional_callbacks = dict()\\n        # run a thread for listening to device updates\\n        self._thread = threading.Thread(target=self._run_device)\\n        self._thread.daemon = True\\n        self._thread.start()\\n\\n    def __del__(self):\\n        \"\"\"Destructor for the class.\"\"\"\\n        self._thread.join()\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing the information of joystick.\"\"\"\\n        msg = f\"Spacemouse Controller for SE(3): {self.__class__.__name__}\\\\n\"\\n        msg += f\"\\\\tManufacturer: {self._device.get_manufacturer_string()}\\\\n\"\\n        msg += f\"\\\\tProduct: {self._device.get_product_string()}\\\\n\"\\n        msg += \"\\\\t----------------------------------------------\\\\n\"\\n        msg += \"\\\\tRight button: reset command\\\\n\"\\n        msg += \"\\\\tLeft button: toggle gripper command (open/close)\\\\n\"\\n        msg += \"\\\\tMove mouse laterally: move arm horizontally in x-y plane\\\\n\"\\n        msg += \"\\\\tMove mouse vertically: move arm vertically\\\\n\"\\n        msg += \"\\\\tTwist mouse about an axis: rotate arm about a corresponding axis\"\\n        return msg\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self):\\n        # default flags\\n        self._close_gripper = False\\n        self._delta_pos = np.zeros(3)  # (x, y, z)\\n        self._delta_rot = np.zeros(3)  # (roll, pitch, yaw)\\n\\n    def add_callback(self, key: str, func: Callable):\\n        # check keys supported by callback\\n        if key not in [\"L\", \"R\"]:\\n            raise ValueError(f\"Only left (L) and right (R) buttons supported. Provided: {key}.\")\\n        # TODO: Improve this to allow multiple buttons on same key.\\n        self._additional_callbacks[key] = func\\n\\n    def advance(self) -> tuple[np.ndarray, bool]:\\n        \"\"\"Provides the result from spacemouse event state.\\n\\n        Returns:\\n            A tuple containing the delta pose command and gripper commands.\\n        \"\"\"\\n        rot_vec = Rotation.from_euler(\"XYZ\", self._delta_rot).as_rotvec()\\n        # if new command received, reset event flag to False until keyboard updated.\\n        return np.concatenate([self._delta_pos, rot_vec]), self._close_gripper\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _find_device(self):\\n        \"\"\"Find the device connected to computer.\"\"\"\\n        found = False\\n        # implement a timeout for device search\\n        for _ in range(5):\\n            for device in hid.enumerate():\\n                if (\\n                    device[\"product_string\"] == \"SpaceMouse Compact\"\\n                    or device[\"product_string\"] == \"SpaceMouse Wireless\"\\n                ):\\n                    # set found flag\\n                    found = True\\n                    vendor_id = device[\"vendor_id\"]\\n                    product_id = device[\"product_id\"]\\n                    # connect to the device\\n                    self._device.close()\\n                    self._device.open(vendor_id, product_id)\\n            # check if device found\\n            if not found:\\n                time.sleep(1.0)\\n            else:\\n                break\\n        # no device found: return false\\n        if not found:\\n            raise OSError(\"No device found by SpaceMouse. Is the device connected?\")\\n\\n    def _run_device(self):\\n        \"\"\"Listener thread that keeps pulling new messages.\"\"\"\\n        # keep running\\n        while True:\\n            # read the device data\\n            data = self._device.read(7)\\n            if data is not None:\\n                # readings from 6-DoF sensor\\n                if data[0] == 1:\\n                    self._delta_pos[1] = self.pos_sensitivity * convert_buffer(data[1], data[2])\\n                    self._delta_pos[0] = self.pos_sensitivity * convert_buffer(data[3], data[4])\\n                    self._delta_pos[2] = self.pos_sensitivity * convert_buffer(data[5], data[6]) * -1.0\\n                elif data[0] == 2 and not self._read_rotation:\\n                    self._delta_rot[1] = self.rot_sensitivity * convert_buffer(data[1], data[2])\\n                    self._delta_rot[0] = self.rot_sensitivity * convert_buffer(data[3], data[4])\\n                    self._delta_rot[2] = self.rot_sensitivity * convert_buffer(data[5], data[6]) * -1.0\\n                # readings from the side buttons\\n                elif data[0] == 3:\\n                    # press left button\\n                    if data[1] == 1:\\n                        # close gripper\\n                        self._close_gripper = not self._close_gripper\\n                        # additional callbacks\\n                        if \"L\" in self._additional_callbacks:\\n                            self._additional_callbacks[\"L\"]()\\n                    # right button is for reset\\n                    if data[1] == 2:\\n                        # reset layer\\n                        self.reset()\\n                        # additional callbacks\\n                        if \"R\" in self._additional_callbacks:\\n                            self._additional_callbacks[\"R\"]()\\n                    if data[1] == 3:\\n                        self._read_rotation = not self._read_rotation'),\n",
       " Document(metadata={}, page_content='def convert_buffer(b1, b2):\\n    \"\"\"Converts raw SpaceMouse readings to commands.\\n\\n    Args:\\n        b1: 8-bit byte\\n        b2: 8-bit byte\\n\\n    Returns:\\n        Scaled value from Space-mouse message\\n    \"\"\"\\n    return _scale_to_control(_to_int16(b1, b2))'),\n",
       " Document(metadata={}, page_content='def _to_int16(y1, y2):\\n    \"\"\"Convert two 8 bit bytes to a signed 16 bit integer.\\n\\n    Args:\\n        y1: 8-bit byte\\n        y2: 8-bit byte\\n\\n    Returns:\\n        16-bit integer\\n    \"\"\"\\n    x = (y1) | (y2 << 8)\\n    if x >= 32768:\\n        x = -(65536 - x)\\n    return x'),\n",
       " Document(metadata={}, page_content='def _scale_to_control(x, axis_scale=350.0, min_v=-1.0, max_v=1.0):\\n    \"\"\"Normalize raw HID readings to target range.\\n\\n    Args:\\n        x: Raw reading from HID\\n        axis_scale: (Inverted) scaling factor for mapping raw input value\\n        min_v: Minimum limit after scaling\\n        max_v: Maximum limit after scaling\\n\\n    Returns:\\n        Clipped, scaled input from HID\\n    \"\"\"\\n    x = x / axis_scale\\n    return min(max(x, min_v), max_v)'),\n",
       " Document(metadata={}, page_content='class ViewerCfg:\\n    \"\"\"Configuration of the scene viewport camera.\"\"\"\\n\\n    eye: tuple[float, float, float] = (7.5, 7.5, 7.5)\\n    \"\"\"Initial camera position (in m). Default is (7.5, 7.5, 7.5).\"\"\"\\n\\n    lookat: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n    \"\"\"Initial camera target position (in m). Default is (0.0, 0.0, 0.0).\"\"\"\\n\\n    cam_prim_path: str = \"/OmniverseKit_Persp\"\\n    \"\"\"The camera prim path to record images from. Default is \"/OmniverseKit_Persp\",\\n    which is the default camera in the viewport.\\n    \"\"\"\\n\\n    resolution: tuple[int, int] = (1280, 720)\\n    \"\"\"The resolution (width, height) of the camera specified using :attr:`cam_prim_path`.\\n    Default is (1280, 720).\\n    \"\"\"\\n\\n    origin_type: Literal[\"world\", \"env\", \"asset_root\", \"asset_body\"] = \"world\"\\n    \"\"\"The frame in which the camera position (eye) and target (lookat) are defined in. Default is \"world\".\\n\\n    Available options are:\\n\\n    * ``\"world\"``: The origin of the world.\\n    * ``\"env\"``: The origin of the environment defined by :attr:`env_index`.\\n    * ``\"asset_root\"``: The center of the asset defined by :attr:`asset_name` in environment :attr:`env_index`.\\n    * ``\"asset_body\"``: The center of the body defined by :attr:`body_name` in asset defined by :attr:`asset_name` in environment :attr:`env_index`.\\n    \"\"\"\\n\\n    env_index: int = 0\\n    \"\"\"The environment index for frame origin. Default is 0.\\n\\n    This quantity is only effective if :attr:`origin` is set to \"env\" or \"asset_root\".\\n    \"\"\"\\n\\n    asset_name: str | None = None\\n    \"\"\"The asset name in the interactive scene for the frame origin. Default is None.\\n\\n    This quantity is only effective if :attr:`origin` is set to \"asset_root\".\\n    \"\"\"\\n\\n    body_name: str | None = None\\n    \"\"\"The name of the body in :attr:`asset_name` in the interactive scene for the frame origin. Default is None.\\n\\n    This quantity is only effective if :attr:`origin` is set to \"asset_body\".\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class DirectMARLEnv(gym.Env):\\n    \"\"\"The superclass for the direct workflow to design multi-agent environments.\\n\\n    This class implements the core functionality for multi-agent reinforcement learning (MARL)\\n    environments. It is designed to be used with any RL library. The class is designed\\n    to be used with vectorized environments, i.e., the environment is expected to be run\\n    in parallel with multiple sub-environments.\\n\\n    The design of this class is based on the PettingZoo Parallel API.\\n    While the environment itself is implemented as a vectorized environment, we do not\\n    inherit from :class:`pettingzoo.ParallelEnv` or :class:`gym.vector.VectorEnv`. This is mainly\\n    because the class adds various attributes and methods that are inconsistent with them.\\n\\n    Note:\\n        For vectorized environments, it is recommended to **only** call the :meth:`reset`\\n        method once before the first call to :meth:`step`, i.e. after the environment is created.\\n        After that, the :meth:`step` function handles the reset of terminated sub-environments.\\n        This is because the simulator does not support resetting individual sub-environments\\n        in a vectorized environment.\\n\\n    \"\"\"\\n\\n    metadata: ClassVar[dict[str, Any]] = {\\n        \"render_modes\": [None, \"human\", \"rgb_array\"],\\n        \"isaac_sim_version\": get_version(),\\n    }\\n    \"\"\"Metadata for the environment.\"\"\"\\n\\n    def __init__(self, cfg: DirectMARLEnvCfg, render_mode: str | None = None, **kwargs):\\n        \"\"\"Initialize the environment.\\n\\n        Args:\\n            cfg: The configuration object for the environment.\\n            render_mode: The render mode for the environment. Defaults to None, which\\n                is similar to ``\"human\"``.\\n\\n        Raises:\\n            RuntimeError: If a simulation context already exists. The environment must always create one\\n                since it configures the simulation context and controls the simulation.\\n        \"\"\"\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs to class\\n        self.cfg = cfg\\n        # store the render mode\\n        self.render_mode = render_mode\\n        # initialize internal variables\\n        self._is_closed = False\\n\\n        # set the seed for the environment\\n        if self.cfg.seed is not None:\\n            self.cfg.seed = self.seed(self.cfg.seed)\\n        else:\\n            omni.log.warn(\"Seed not set for the environment. The environment creation may not be deterministic.\")\\n\\n        # create a simulation context to control the simulator\\n        if SimulationContext.instance() is None:\\n            self.sim: SimulationContext = SimulationContext(self.cfg.sim)\\n        else:\\n            raise RuntimeError(\"Simulation context already exists. Cannot create a new one.\")\\n\\n        # make sure torch is running on the correct device\\n        if \"cuda\" in self.device:\\n            torch.cuda.set_device(self.device)\\n\\n        # print useful information\\n        print(\"[INFO]: Base environment:\")\\n        print(f\"\\\\tEnvironment device    : {self.device}\")\\n        print(f\"\\\\tEnvironment seed      : {self.cfg.seed}\")\\n        print(f\"\\\\tPhysics step-size     : {self.physics_dt}\")\\n        print(f\"\\\\tRendering step-size   : {self.physics_dt * self.cfg.sim.render_interval}\")\\n        print(f\"\\\\tEnvironment step-size : {self.step_dt}\")\\n\\n        if self.cfg.sim.render_interval < self.cfg.decimation:\\n            msg = (\\n                f\"The render interval ({self.cfg.sim.render_interval}) is smaller than the decimation \"\\n                f\"({self.cfg.decimation}). Multiple render calls will happen for each environment step.\"\\n                \"If this is not intended, set the render interval to be equal to the decimation.\"\\n            )\\n            omni.log.warn(msg)\\n\\n        # generate scene\\n        with Timer(\"[INFO]: Time taken for scene creation\", \"scene_creation\"):\\n            self.scene = InteractiveScene(self.cfg.scene)\\n            self._setup_scene()\\n        print(\"[INFO]: Scene manager: \", self.scene)\\n\\n        # set up camera viewport controller\\n        # viewport is not available in other rendering modes so the function will throw a warning\\n        # FIXME: This needs to be fixed in the future when we unify the UI functionalities even for\\n        # non-rendering modes.\\n        if self.sim.render_mode >= self.sim.RenderMode.PARTIAL_RENDERING:\\n            self.viewport_camera_controller = ViewportCameraController(self, self.cfg.viewer)\\n        else:\\n            self.viewport_camera_controller = None\\n\\n        # create event manager\\n        # note: this is needed here (rather than after simulation play) to allow USD-related randomization events\\n        #   that must happen before the simulation starts. Example: randomizing mesh scale\\n        if self.cfg.events:\\n            self.event_manager = EventManager(self.cfg.events, self)\\n\\n            # apply USD-related randomization events\\n            if \"prestartup\" in self.event_manager.available_modes:\\n                self.event_manager.apply(mode=\"prestartup\")\\n\\n        # play the simulator to activate physics handles\\n        # note: this activates the physics simulation view that exposes TensorAPIs\\n        # note: when started in extension mode, first call sim.reset_async() and then initialize the managers\\n        if builtins.ISAAC_LAUNCHED_FROM_TERMINAL is False:\\n            print(\"[INFO]: Starting the simulation. This may take a few seconds. Please wait...\")\\n            with Timer(\"[INFO]: Time taken for simulation start\", \"simulation_start\"):\\n                self.sim.reset()\\n                # update scene to pre populate data buffers for assets and sensors.\\n                # this is needed for the observation manager to get valid tensors for initialization.\\n                # this shouldn\\'t cause an issue since later on, users do a reset over all the environments so the lazy buffers would be reset.\\n                self.scene.update(dt=self.physics_dt)\\n\\n        # check if debug visualization is has been implemented by the environment\\n        source_code = inspect.getsource(self._set_debug_vis_impl)\\n        self.has_debug_vis_implementation = \"NotImplementedError\" not in source_code\\n        self._debug_vis_handle = None\\n\\n        # extend UI elements\\n        # we need to do this here after all the managers are initialized\\n        # this is because they dictate the sensors and commands right now\\n        if self.sim.has_gui() and self.cfg.ui_window_class_type is not None:\\n            self._window = self.cfg.ui_window_class_type(self, window_name=\"IsaacLab\")\\n        else:\\n            # if no window, then we don\\'t need to store the window\\n            self._window = None\\n\\n        # allocate dictionary to store metrics\\n        self.extras = {agent: {} for agent in self.cfg.possible_agents}\\n\\n        # initialize data and constants\\n        # -- counter for simulation steps\\n        self._sim_step_counter = 0\\n        # -- counter for curriculum\\n        self.common_step_counter = 0\\n        # -- init buffers\\n        self.episode_length_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\\n        self.reset_buf = torch.zeros(self.num_envs, dtype=torch.bool, device=self.sim.device)\\n\\n        # setup the observation, state and action spaces\\n        self._configure_env_spaces()\\n\\n        # setup noise cfg for adding action and observation noise\\n        if self.cfg.action_noise_model:\\n            self._action_noise_model: dict[AgentID, NoiseModel] = {\\n                agent: noise_model.class_type(noise_model, num_envs=self.num_envs, device=self.device)\\n                for agent, noise_model in self.cfg.action_noise_model.items()\\n                if noise_model is not None\\n            }\\n        if self.cfg.observation_noise_model:\\n            self._observation_noise_model: dict[AgentID, NoiseModel] = {\\n                agent: noise_model.class_type(noise_model, num_envs=self.num_envs, device=self.device)\\n                for agent, noise_model in self.cfg.observation_noise_model.items()\\n                if noise_model is not None\\n            }\\n\\n        # perform events at the start of the simulation\\n        if self.cfg.events:\\n            # we print it here to make the logging consistent\\n            print(\"[INFO] Event Manager: \", self.event_manager)\\n\\n            if \"startup\" in self.event_manager.available_modes:\\n                self.event_manager.apply(mode=\"startup\")\\n\\n        # print the environment information\\n        print(\"[INFO]: Completed setting up the environment...\")\\n\\n    def __del__(self):\\n        \"\"\"Cleanup for the environment.\"\"\"\\n        self.close()\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_envs(self) -> int:\\n        \"\"\"The number of instances of the environment that are running.\"\"\"\\n        return self.scene.num_envs\\n\\n    @property\\n    def num_agents(self) -> int:\\n        \"\"\"Number of current agents.\\n\\n        The number of current agents may change as the environment progresses (e.g.: agents can be added or removed).\\n        \"\"\"\\n        return len(self.agents)\\n\\n    @property\\n    def max_num_agents(self) -> int:\\n        \"\"\"Number of all possible agents the environment can generate.\\n\\n        This value remains constant as the environment progresses.\\n        \"\"\"\\n        return len(self.possible_agents)\\n\\n    @property\\n    def unwrapped(self) -> DirectMARLEnv:\\n        \"\"\"Get the unwrapped environment underneath all the layers of wrappers.\"\"\"\\n        return self\\n\\n    @property\\n    def physics_dt(self) -> float:\\n        \"\"\"The physics time-step (in s).\\n\\n        This is the lowest time-decimation at which the simulation is happening.\\n        \"\"\"\\n        return self.cfg.sim.dt\\n\\n    @property\\n    def step_dt(self) -> float:\\n        \"\"\"The environment stepping time-step (in s).\\n\\n        This is the time-step at which the environment steps forward.\\n        \"\"\"\\n        return self.cfg.sim.dt * self.cfg.decimation\\n\\n    @property\\n    def device(self):\\n        \"\"\"The device on which the environment is running.\"\"\"\\n        return self.sim.device\\n\\n    @property\\n    def max_episode_length_s(self) -> float:\\n        \"\"\"Maximum episode length in seconds.\"\"\"\\n        return self.cfg.episode_length_s\\n\\n    @property\\n    def max_episode_length(self):\\n        \"\"\"The maximum episode length in steps adjusted from s.\"\"\"\\n        return math.ceil(self.max_episode_length_s / (self.cfg.sim.dt * self.cfg.decimation))\\n\\n    \"\"\"\\n    Space methods\\n    \"\"\"\\n\\n    def observation_space(self, agent: AgentID) -> gym.Space:\\n        \"\"\"Get the observation space for the specified agent.\\n\\n        Returns:\\n            The agent\\'s observation space.\\n        \"\"\"\\n        return self.observation_spaces[agent]\\n\\n    def action_space(self, agent: AgentID) -> gym.Space:\\n        \"\"\"Get the action space for the specified agent.\\n\\n        Returns:\\n            The agent\\'s action space.\\n        \"\"\"\\n        return self.action_spaces[agent]\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(\\n        self, seed: int | None = None, options: dict[str, Any] | None = None\\n    ) -> tuple[dict[AgentID, ObsType], dict[AgentID, dict]]:\\n        \"\"\"Resets all the environments and returns observations.\\n\\n        Args:\\n            seed: The seed to use for randomization. Defaults to None, in which case the seed is not set.\\n            options: Additional information to specify how the environment is reset. Defaults to None.\\n\\n                Note:\\n                    This argument is used for compatibility with Gymnasium environment definition.\\n\\n        Returns:\\n            A tuple containing the observations and extras (keyed by the agent ID).\\n        \"\"\"\\n        # set the seed\\n        if seed is not None:\\n            self.seed(seed)\\n\\n        # reset state of scene\\n        indices = torch.arange(self.num_envs, dtype=torch.int64, device=self.device)\\n        self._reset_idx(indices)\\n\\n        # update observations and the list of current agents (sorted as in possible_agents)\\n        self.obs_dict = self._get_observations()\\n        self.agents = [agent for agent in self.possible_agents if agent in self.obs_dict]\\n\\n        # return observations\\n        return self.obs_dict, self.extras\\n\\n    def step(self, actions: dict[AgentID, ActionType]) -> EnvStepReturn:\\n        \"\"\"Execute one time-step of the environment\\'s dynamics.\\n\\n        The environment steps forward at a fixed time-step, while the physics simulation is decimated at a\\n        lower time-step. This is to ensure that the simulation is stable. These two time-steps can be configured\\n        independently using the :attr:`DirectMARLEnvCfg.decimation` (number of simulation steps per environment step)\\n        and the :attr:`DirectMARLEnvCfg.sim.physics_dt` (physics time-step). Based on these parameters, the environment\\n        time-step is computed as the product of the two.\\n\\n        This function performs the following steps:\\n\\n        1. Pre-process the actions before stepping through the physics.\\n        2. Apply the actions to the simulator and step through the physics in a decimated manner.\\n        3. Compute the reward and done signals.\\n        4. Reset environments that have terminated or reached the maximum episode length.\\n        5. Apply interval events if they are enabled.\\n        6. Compute observations.\\n\\n        Args:\\n            actions: The actions to apply on the environment (keyed by the agent ID).\\n                Shape of individual tensors is (num_envs, action_dim).\\n\\n        Returns:\\n            A tuple containing the observations, rewards, resets (terminated and truncated) and extras (keyed by the agent ID).\\n        \"\"\"\\n        actions = {agent: action.to(self.device) for agent, action in actions.items()}\\n\\n        # add action noise\\n        if self.cfg.action_noise_model:\\n            for agent, action in actions.items():\\n                if agent in self._action_noise_model:\\n                    actions[agent] = self._action_noise_model[agent].apply(action)\\n        # process actions\\n        self._pre_physics_step(actions)\\n\\n        # check if we need to do rendering within the physics loop\\n        # note: checked here once to avoid multiple checks within the loop\\n        is_rendering = self.sim.has_gui() or self.sim.has_rtx_sensors()\\n\\n        # perform physics stepping\\n        for _ in range(self.cfg.decimation):\\n            self._sim_step_counter += 1\\n            # set actions into buffers\\n            self._apply_action()\\n            # set actions into simulator\\n            self.scene.write_data_to_sim()\\n            # simulate\\n            self.sim.step(render=False)\\n            # render between steps only if the GUI or an RTX sensor needs it\\n            # note: we assume the render interval to be the shortest accepted rendering interval.\\n            #    If a camera needs rendering at a faster frequency, this will lead to unexpected behavior.\\n            if self._sim_step_counter % self.cfg.sim.render_interval == 0 and is_rendering:\\n                self.sim.render()\\n            # update buffers at sim dt\\n            self.scene.update(dt=self.physics_dt)\\n\\n        # post-step:\\n        # -- update env counters (used for curriculum generation)\\n        self.episode_length_buf += 1  # step in current episode (per env)\\n        self.common_step_counter += 1  # total step (common for all envs)\\n\\n        self.terminated_dict, self.time_out_dict = self._get_dones()\\n        self.reset_buf[:] = math.prod(self.terminated_dict.values()) | math.prod(self.time_out_dict.values())\\n        self.reward_dict = self._get_rewards()\\n\\n        # -- reset envs that terminated/timed-out and log the episode information\\n        reset_env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)\\n        if len(reset_env_ids) > 0:\\n            self._reset_idx(reset_env_ids)\\n\\n        # post-step: step interval event\\n        if self.cfg.events:\\n            if \"interval\" in self.event_manager.available_modes:\\n                self.event_manager.apply(mode=\"interval\", dt=self.step_dt)\\n\\n        # update observations and the list of current agents (sorted as in possible_agents)\\n        self.obs_dict = self._get_observations()\\n        self.agents = [agent for agent in self.possible_agents if agent in self.obs_dict]\\n\\n        # add observation noise\\n        # note: we apply no noise to the state space (since it is used for centralized training or critic networks)\\n        if self.cfg.observation_noise_model:\\n            for agent, obs in self.obs_dict.items():\\n                if agent in self._observation_noise_model:\\n                    self.obs_dict[agent] = self._observation_noise_model[agent].apply(obs)\\n\\n        # return observations, rewards, resets and extras\\n        return self.obs_dict, self.reward_dict, self.terminated_dict, self.time_out_dict, self.extras\\n\\n    def state(self) -> StateType | None:\\n        \"\"\"Returns the state for the environment.\\n\\n        The state-space is used for centralized training or asymmetric actor-critic architectures. It is configured\\n        using the :attr:`DirectMARLEnvCfg.state_space` parameter.\\n\\n        Returns:\\n            The states for the environment, or None if :attr:`DirectMARLEnvCfg.state_space` parameter is zero.\\n        \"\"\"\\n        if not self.cfg.state_space:\\n            return None\\n        # concatenate and return the observations as state\\n        # FIXME: This implementation assumes the spaces are fundamental ones. Fix it to support composite spaces\\n        if isinstance(self.cfg.state_space, int) and self.cfg.state_space < 0:\\n            self.state_buf = torch.cat(\\n                [self.obs_dict[agent].reshape(self.num_envs, -1) for agent in self.cfg.possible_agents], dim=-1\\n            )\\n        # compute and return custom environment state\\n        else:\\n            self.state_buf = self._get_states()\\n        return self.state_buf\\n\\n    @staticmethod\\n    def seed(seed: int = -1) -> int:\\n        \"\"\"Set the seed for the environment.\\n\\n        Args:\\n            seed: The seed for random generator. Defaults to -1.\\n\\n        Returns:\\n            The seed used for random generator.\\n        \"\"\"\\n        # set seed for replicator\\n        try:\\n            import omni.replicator.core as rep\\n\\n            rep.set_global_seed(seed)\\n        except ModuleNotFoundError:\\n            pass\\n        # set seed for torch and other libraries\\n        return torch_utils.set_seed(seed)\\n\\n    def render(self, recompute: bool = False) -> np.ndarray | None:\\n        \"\"\"Run rendering without stepping through the physics.\\n\\n        By convention, if mode is:\\n\\n        - **human**: Render to the current display and return nothing. Usually for human consumption.\\n        - **rgb_array**: Return an numpy.ndarray with shape (x, y, 3), representing RGB values for an\\n          x-by-y pixel image, suitable for turning into a video.\\n\\n        Args:\\n            recompute: Whether to force a render even if the simulator has already rendered the scene.\\n                Defaults to False.\\n\\n        Returns:\\n            The rendered image as a numpy array if mode is \"rgb_array\". Otherwise, returns None.\\n\\n        Raises:\\n            RuntimeError: If mode is set to \"rgb_data\" and simulation render mode does not support it.\\n                In this case, the simulation render mode must be set to ``RenderMode.PARTIAL_RENDERING``\\n                or ``RenderMode.FULL_RENDERING``.\\n            NotImplementedError: If an unsupported rendering mode is specified.\\n        \"\"\"\\n        # run a rendering step of the simulator\\n        # if we have rtx sensors, we do not need to render again sin\\n        if not self.sim.has_rtx_sensors() and not recompute:\\n            self.sim.render()\\n        # decide the rendering mode\\n        if self.render_mode == \"human\" or self.render_mode is None:\\n            return None\\n        elif self.render_mode == \"rgb_array\":\\n            # check that if any render could have happened\\n            if self.sim.render_mode.value < self.sim.RenderMode.PARTIAL_RENDERING.value:\\n                raise RuntimeError(\\n                    f\"Cannot render \\'{self.render_mode}\\' when the simulation render mode is\"\\n                    f\" \\'{self.sim.render_mode.name}\\'. Please set the simulation render mode to:\"\\n                    f\"\\'{self.sim.RenderMode.PARTIAL_RENDERING.name}\\' or \\'{self.sim.RenderMode.FULL_RENDERING.name}\\'.\"\\n                    \" If running headless, make sure --enable_cameras is set.\"\\n                )\\n            # create the annotator if it does not exist\\n            if not hasattr(self, \"_rgb_annotator\"):\\n                import omni.replicator.core as rep\\n\\n                # create render product\\n                self._render_product = rep.create.render_product(\\n                    self.cfg.viewer.cam_prim_path, self.cfg.viewer.resolution\\n                )\\n                # create rgb annotator -- used to read data from the render product\\n                self._rgb_annotator = rep.AnnotatorRegistry.get_annotator(\"rgb\", device=\"cpu\")\\n                self._rgb_annotator.attach([self._render_product])\\n            # obtain the rgb data\\n            rgb_data = self._rgb_annotator.get_data()\\n            # convert to numpy array\\n            rgb_data = np.frombuffer(rgb_data, dtype=np.uint8).reshape(*rgb_data.shape)\\n            # return the rgb data\\n            # note: initially the renderer is warming up and returns empty data\\n            if rgb_data.size == 0:\\n                return np.zeros((self.cfg.viewer.resolution[1], self.cfg.viewer.resolution[0], 3), dtype=np.uint8)\\n            else:\\n                return rgb_data[:, :, :3]\\n        else:\\n            raise NotImplementedError(\\n                f\"Render mode \\'{self.render_mode}\\' is not supported. Please use: {self.metadata[\\'render_modes\\']}.\"\\n            )\\n\\n    def close(self):\\n        \"\"\"Cleanup for the environment.\"\"\"\\n        if not self._is_closed:\\n            # close entities related to the environment\\n            # note: this is order-sensitive to avoid any dangling references\\n            if self.cfg.events:\\n                del self.event_manager\\n            del self.scene\\n            if self.viewport_camera_controller is not None:\\n                del self.viewport_camera_controller\\n            # clear callbacks and instance\\n            self.sim.clear_all_callbacks()\\n            self.sim.clear_instance()\\n            # destroy the window\\n            if self._window is not None:\\n                self._window = None\\n            # update closing status\\n            self._is_closed = True\\n\\n    \"\"\"\\n    Operations - Debug Visualization.\\n    \"\"\"\\n\\n    def set_debug_vis(self, debug_vis: bool) -> bool:\\n        \"\"\"Toggles the environment debug visualization.\\n\\n        Args:\\n            debug_vis: Whether to visualize the environment debug visualization.\\n\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the environment\\n            does not support debug visualization.\\n        \"\"\"\\n        # check if debug visualization is supported\\n        if not self.has_debug_vis_implementation:\\n            return False\\n        # toggle debug visualization objects\\n        self._set_debug_vis_impl(debug_vis)\\n        # toggle debug visualization handles\\n        if debug_vis:\\n            # create a subscriber for the post update event if it doesn\\'t exist\\n            if self._debug_vis_handle is None:\\n                app_interface = omni.kit.app.get_app_interface()\\n                self._debug_vis_handle = app_interface.get_post_update_event_stream().create_subscription_to_pop(\\n                    lambda event, obj=weakref.proxy(self): obj._debug_vis_callback(event)\\n                )\\n        else:\\n            # remove the subscriber if it exists\\n            if self._debug_vis_handle is not None:\\n                self._debug_vis_handle.unsubscribe()\\n                self._debug_vis_handle = None\\n        # return success\\n        return True\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _configure_env_spaces(self):\\n        \"\"\"Configure the spaces for the environment.\"\"\"\\n        self.agents = self.cfg.possible_agents\\n        self.possible_agents = self.cfg.possible_agents\\n\\n        # show deprecation message and overwrite configuration\\n        if self.cfg.num_actions is not None:\\n            omni.log.warn(\"DirectMARLEnvCfg.num_actions is deprecated. Use DirectMARLEnvCfg.action_spaces instead.\")\\n            if isinstance(self.cfg.action_spaces, type(MISSING)):\\n                self.cfg.action_spaces = self.cfg.num_actions\\n        if self.cfg.num_observations is not None:\\n            omni.log.warn(\\n                \"DirectMARLEnvCfg.num_observations is deprecated. Use DirectMARLEnvCfg.observation_spaces instead.\"\\n            )\\n            if isinstance(self.cfg.observation_spaces, type(MISSING)):\\n                self.cfg.observation_spaces = self.cfg.num_observations\\n        if self.cfg.num_states is not None:\\n            omni.log.warn(\"DirectMARLEnvCfg.num_states is deprecated. Use DirectMARLEnvCfg.state_space instead.\")\\n            if isinstance(self.cfg.state_space, type(MISSING)):\\n                self.cfg.state_space = self.cfg.num_states\\n\\n        # set up observation and action spaces\\n        self.observation_spaces = {\\n            agent: spec_to_gym_space(self.cfg.observation_spaces[agent]) for agent in self.cfg.possible_agents\\n        }\\n        self.action_spaces = {\\n            agent: spec_to_gym_space(self.cfg.action_spaces[agent]) for agent in self.cfg.possible_agents\\n        }\\n\\n        # set up state space\\n        if not self.cfg.state_space:\\n            self.state_space = None\\n        if isinstance(self.cfg.state_space, int) and self.cfg.state_space < 0:\\n            self.state_space = gym.spaces.flatten_space(\\n                gym.spaces.Tuple([self.observation_spaces[agent] for agent in self.cfg.possible_agents])\\n            )\\n        else:\\n            self.state_space = spec_to_gym_space(self.cfg.state_space)\\n\\n        # instantiate actions (needed for tasks for which the observations computation is dependent on the actions)\\n        self.actions = {\\n            agent: sample_space(self.action_spaces[agent], self.sim.device, batch_size=self.num_envs, fill_value=0)\\n            for agent in self.cfg.possible_agents\\n        }\\n\\n    def _reset_idx(self, env_ids: Sequence[int]):\\n        \"\"\"Reset environments based on specified indices.\\n\\n        Args:\\n            env_ids: List of environment ids which must be reset\\n        \"\"\"\\n        self.scene.reset(env_ids)\\n\\n        # apply events such as randomization for environments that need a reset\\n        if self.cfg.events:\\n            if \"reset\" in self.event_manager.available_modes:\\n                env_step_count = self._sim_step_counter // self.cfg.decimation\\n                self.event_manager.apply(mode=\"reset\", env_ids=env_ids, global_env_step_count=env_step_count)\\n\\n        # reset noise models\\n        if self.cfg.action_noise_model:\\n            for noise_model in self._action_noise_model.values():\\n                noise_model.reset(env_ids)\\n        if self.cfg.observation_noise_model:\\n            for noise_model in self._observation_noise_model.values():\\n                noise_model.reset(env_ids)\\n\\n        # reset the episode length buffer\\n        self.episode_length_buf[env_ids] = 0\\n\\n    \"\"\"\\n    Implementation-specific functions.\\n    \"\"\"\\n\\n    def _setup_scene(self):\\n        \"\"\"Setup the scene for the environment.\\n\\n        This function is responsible for creating the scene objects and setting up the scene for the environment.\\n        The scene creation can happen through :class:`isaaclab.scene.InteractiveSceneCfg` or through\\n        directly creating the scene objects and registering them with the scene manager.\\n\\n        We leave the implementation of this function to the derived classes. If the environment does not require\\n        any explicit scene setup, the function can be left empty.\\n        \"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def _pre_physics_step(self, actions: dict[AgentID, ActionType]):\\n        \"\"\"Pre-process actions before stepping through the physics.\\n\\n        This function is responsible for pre-processing the actions before stepping through the physics.\\n        It is called before the physics stepping (which is decimated).\\n\\n        Args:\\n            actions: The actions to apply on the environment (keyed by the agent ID).\\n                Shape of individual tensors is (num_envs, action_dim).\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_pre_physics_step\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _apply_action(self):\\n        \"\"\"Apply actions to the simulator.\\n\\n        This function is responsible for applying the actions to the simulator. It is called at each\\n        physics time-step.\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_apply_action\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _get_observations(self) -> dict[AgentID, ObsType]:\\n        \"\"\"Compute and return the observations for the environment.\\n\\n        Returns:\\n            The observations for the environment (keyed by the agent ID).\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_get_observations\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _get_states(self) -> StateType:\\n        \"\"\"Compute and return the states for the environment.\\n\\n        This method is only called (and therefore has to be implemented) when the :attr:`DirectMARLEnvCfg.state_space`\\n        parameter is not a number less than or equal to zero.\\n\\n        Returns:\\n            The states for the environment.\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_get_states\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _get_rewards(self) -> dict[AgentID, torch.Tensor]:\\n        \"\"\"Compute and return the rewards for the environment.\\n\\n        Returns:\\n            The rewards for the environment (keyed by the agent ID).\\n            Shape of individual tensors is (num_envs,).\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_get_rewards\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _get_dones(self) -> tuple[dict[AgentID, torch.Tensor], dict[AgentID, torch.Tensor]]:\\n        \"\"\"Compute and return the done flags for the environment.\\n\\n        Returns:\\n            A tuple containing the done flags for termination and time-out (keyed by the agent ID).\\n            Shape of individual tensors is (num_envs,).\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_get_dones\\' method for {self.__class__.__name__}.\")\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set debug visualization into visualization objects.\\n\\n        This function is responsible for creating the visualization objects if they don\\'t exist\\n        and input ``debug_vis`` is True. If the visualization objects exist, the function should\\n        set their visibility into the stage.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")'),\n",
       " Document(metadata={}, page_content='class DirectMARLEnvCfg:\\n    \"\"\"Configuration for a MARL environment defined with the direct workflow.\\n\\n    Please refer to the :class:`isaaclab.envs.direct_marl_env.DirectMARLEnv` class for more details.\\n    \"\"\"\\n\\n    # simulation settings\\n    viewer: ViewerCfg = ViewerCfg()\\n    \"\"\"Viewer configuration. Default is ViewerCfg().\"\"\"\\n\\n    sim: SimulationCfg = SimulationCfg()\\n    \"\"\"Physics simulation configuration. Default is SimulationCfg().\"\"\"\\n\\n    # ui settings\\n    ui_window_class_type: type | None = BaseEnvWindow\\n    \"\"\"The class type of the UI window. Default is None.\\n\\n    If None, then no UI window is created.\\n\\n    Note:\\n        If you want to make your own UI window, you can create a class that inherits from\\n        from :class:`isaaclab.envs.ui.base_env_window.BaseEnvWindow`. Then, you can set\\n        this attribute to your class type.\\n    \"\"\"\\n\\n    # general settings\\n    seed: int | None = None\\n    \"\"\"The seed for the random number generator. Defaults to None, in which case the seed is not set.\\n\\n    Note:\\n      The seed is set at the beginning of the environment initialization. This ensures that the environment\\n      creation is deterministic and behaves similarly across different runs.\\n    \"\"\"\\n\\n    decimation: int = MISSING\\n    \"\"\"Number of control action updates @ sim dt per policy dt.\\n\\n    For instance, if the simulation dt is 0.01s and the policy dt is 0.1s, then the decimation is 10.\\n    This means that the control action is updated every 10 simulation steps.\\n    \"\"\"\\n\\n    is_finite_horizon: bool = False\\n    \"\"\"Whether the learning task is treated as a finite or infinite horizon problem for the agent.\\n    Defaults to False, which means the task is treated as an infinite horizon problem.\\n\\n    This flag handles the subtleties of finite and infinite horizon tasks:\\n\\n    * **Finite horizon**: no penalty or bootstrapping value is required by the the agent for\\n      running out of time. However, the environment still needs to terminate the episode after the\\n      time limit is reached.\\n    * **Infinite horizon**: the agent needs to bootstrap the value of the state at the end of the episode.\\n      This is done by sending a time-limit (or truncated) done signal to the agent, which triggers this\\n      bootstrapping calculation.\\n\\n    If True, then the environment is treated as a finite horizon problem and no time-out (or truncated) done signal\\n    is sent to the agent. If False, then the environment is treated as an infinite horizon problem and a time-out\\n    (or truncated) done signal is sent to the agent.\\n\\n    Note:\\n        The base :class:`ManagerBasedRLEnv` class does not use this flag directly. It is used by the environment\\n        wrappers to determine what type of done signal to send to the corresponding learning agent.\\n    \"\"\"\\n\\n    episode_length_s: float = MISSING\\n    \"\"\"Duration of an episode (in seconds).\\n\\n    Based on the decimation rate and physics time step, the episode length is calculated as:\\n\\n    .. code-block:: python\\n\\n        episode_length_steps = ceil(episode_length_s / (decimation_rate * physics_time_step))\\n\\n    For example, if the decimation rate is 10, the physics time step is 0.01, and the episode length is 10 seconds,\\n    then the episode length in steps is 100.\\n    \"\"\"\\n\\n    # environment settings\\n    scene: InteractiveSceneCfg = MISSING\\n    \"\"\"Scene settings.\\n\\n    Please refer to the :class:`isaaclab.scene.InteractiveSceneCfg` class for more details.\\n    \"\"\"\\n\\n    events: object = None\\n    \"\"\"Event settings. Defaults to None, in which case no events are applied through the event manager.\\n\\n    Please refer to the :class:`isaaclab.managers.EventManager` class for more details.\\n    \"\"\"\\n\\n    observation_spaces: dict[AgentID, SpaceType] = MISSING\\n    \"\"\"Observation space definition for each agent.\\n\\n    The space can be defined either using Gymnasium :py:mod:`~gymnasium.spaces` (when a more detailed\\n    specification of the space is desired) or basic Python data types (for simplicity).\\n\\n    .. list-table::\\n        :header-rows: 1\\n\\n        * - Gymnasium space\\n          - Python data type\\n        * - :class:`~gymnasium.spaces.Box`\\n          - Integer or list of integers (e.g.: ``7``, ``[64, 64, 3]``)\\n        * - :class:`~gymnasium.spaces.Discrete`\\n          - Single-element set (e.g.: ``{2}``)\\n        * - :class:`~gymnasium.spaces.MultiDiscrete`\\n          - List of single-element sets (e.g.: ``[{2}, {5}]``)\\n        * - :class:`~gymnasium.spaces.Dict`\\n          - Dictionary (e.g.: ``{\"joints\": 7, \"rgb\": [64, 64, 3], \"gripper\": {2}}``)\\n        * - :class:`~gymnasium.spaces.Tuple`\\n          - Tuple (e.g.: ``(7, [64, 64, 3], {2})``)\\n    \"\"\"\\n\\n    num_observations: dict[AgentID, int] | None = None\\n    \"\"\"The dimension of the observation space for each agent.\\n\\n    .. warning::\\n\\n        This attribute is deprecated. Use :attr:`~isaaclab.envs.DirectMARLEnvCfg.observation_spaces` instead.\\n    \"\"\"\\n\\n    state_space: SpaceType = MISSING\\n    \"\"\"State space definition.\\n\\n    The following values are supported:\\n\\n    * -1: All the observations from the different agents are automatically concatenated.\\n    * 0: No state-space will be constructed (`state_space` is None).\\n      This is useful to save computational resources when the algorithm to be trained does not need it.\\n    * greater than 0: Custom state-space dimension to be provided by the task implementation.\\n\\n    The space can be defined either using Gymnasium :py:mod:`~gymnasium.spaces` (when a more detailed\\n    specification of the space is desired) or basic Python data types (for simplicity).\\n\\n    .. list-table::\\n        :header-rows: 1\\n\\n        * - Gymnasium space\\n          - Python data type\\n        * - :class:`~gymnasium.spaces.Box`\\n          - Integer or list of integers (e.g.: ``7``, ``[64, 64, 3]``)\\n        * - :class:`~gymnasium.spaces.Discrete`\\n          - Single-element set (e.g.: ``{2}``)\\n        * - :class:`~gymnasium.spaces.MultiDiscrete`\\n          - List of single-element sets (e.g.: ``[{2}, {5}]``)\\n        * - :class:`~gymnasium.spaces.Dict`\\n          - Dictionary (e.g.: ``{\"joints\": 7, \"rgb\": [64, 64, 3], \"gripper\": {2}}``)\\n        * - :class:`~gymnasium.spaces.Tuple`\\n          - Tuple (e.g.: ``(7, [64, 64, 3], {2})``)\\n    \"\"\"\\n\\n    num_states: int | None = None\\n    \"\"\"The dimension of the state space from each environment instance.\\n\\n    .. warning::\\n\\n        This attribute is deprecated. Use :attr:`~isaaclab.envs.DirectMARLEnvCfg.state_space` instead.\\n    \"\"\"\\n\\n    observation_noise_model: dict[AgentID, NoiseModelCfg | None] | None = None\\n    \"\"\"The noise model to apply to the computed observations from the environment. Default is None, which means no noise is added.\\n\\n    Please refer to the :class:`isaaclab.utils.noise.NoiseModel` class for more details.\\n    \"\"\"\\n\\n    action_spaces: dict[AgentID, SpaceType] = MISSING\\n    \"\"\"Action space definition for each agent.\\n\\n    The space can be defined either using Gymnasium :py:mod:`~gymnasium.spaces` (when a more detailed\\n    specification of the space is desired) or basic Python data types (for simplicity).\\n\\n    .. list-table::\\n        :header-rows: 1\\n\\n        * - Gymnasium space\\n          - Python data type\\n        * - :class:`~gymnasium.spaces.Box`\\n          - Integer or list of integers (e.g.: ``7``, ``[64, 64, 3]``)\\n        * - :class:`~gymnasium.spaces.Discrete`\\n          - Single-element set (e.g.: ``{2}``)\\n        * - :class:`~gymnasium.spaces.MultiDiscrete`\\n          - List of single-element sets (e.g.: ``[{2}, {5}]``)\\n        * - :class:`~gymnasium.spaces.Dict`\\n          - Dictionary (e.g.: ``{\"joints\": 7, \"rgb\": [64, 64, 3], \"gripper\": {2}}``)\\n        * - :class:`~gymnasium.spaces.Tuple`\\n          - Tuple (e.g.: ``(7, [64, 64, 3], {2})``)\\n    \"\"\"\\n\\n    num_actions: dict[AgentID, int] | None = None\\n    \"\"\"The dimension of the action space for each agent.\\n\\n    .. warning::\\n\\n        This attribute is deprecated. Use :attr:`~isaaclab.envs.DirectMARLEnvCfg.action_spaces` instead.\\n    \"\"\"\\n\\n    action_noise_model: dict[AgentID, NoiseModelCfg | None] | None = None\\n    \"\"\"The noise model applied to the actions provided to the environment. Default is None, which means no noise is added.\\n\\n    Please refer to the :class:`isaaclab.utils.noise.NoiseModel` class for more details.\\n    \"\"\"\\n\\n    possible_agents: list[AgentID] = MISSING\\n    \"\"\"A list of all possible agents the environment could generate.\\n\\n    The contents of the list cannot be modified during the entire training process.\\n    \"\"\"\\n\\n    xr: XrCfg | None = None\\n    \"\"\"Configuration for viewing and interacting with the environment through an XR device.\"\"\"'),\n",
       " Document(metadata={}, page_content='class DirectRLEnv(gym.Env):\\n    \"\"\"The superclass for the direct workflow to design environments.\\n\\n    This class implements the core functionality for reinforcement learning (RL)\\n    environments. It is designed to be used with any RL library. The class is designed\\n    to be used with vectorized environments, i.e., the environment is expected to be run\\n    in parallel with multiple sub-environments.\\n\\n    While the environment itself is implemented as a vectorized environment, we do not\\n    inherit from :class:`gym.vector.VectorEnv`. This is mainly because the class adds\\n    various methods (for wait and asynchronous updates) which are not required.\\n    Additionally, each RL library typically has its own definition for a vectorized\\n    environment. Thus, to reduce complexity, we directly use the :class:`gym.Env` over\\n    here and leave it up to library-defined wrappers to take care of wrapping this\\n    environment for their agents.\\n\\n    Note:\\n        For vectorized environments, it is recommended to **only** call the :meth:`reset`\\n        method once before the first call to :meth:`step`, i.e. after the environment is created.\\n        After that, the :meth:`step` function handles the reset of terminated sub-environments.\\n        This is because the simulator does not support resetting individual sub-environments\\n        in a vectorized environment.\\n\\n    \"\"\"\\n\\n    is_vector_env: ClassVar[bool] = True\\n    \"\"\"Whether the environment is a vectorized environment.\"\"\"\\n    metadata: ClassVar[dict[str, Any]] = {\\n        \"render_modes\": [None, \"human\", \"rgb_array\"],\\n        \"isaac_sim_version\": get_version(),\\n    }\\n    \"\"\"Metadata for the environment.\"\"\"\\n\\n    def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):\\n        \"\"\"Initialize the environment.\\n\\n        Args:\\n            cfg: The configuration object for the environment.\\n            render_mode: The render mode for the environment. Defaults to None, which\\n                is similar to ``\"human\"``.\\n\\n        Raises:\\n            RuntimeError: If a simulation context already exists. The environment must always create one\\n                since it configures the simulation context and controls the simulation.\\n        \"\"\"\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs to class\\n        self.cfg = cfg\\n        # store the render mode\\n        self.render_mode = render_mode\\n        # initialize internal variables\\n        self._is_closed = False\\n\\n        # set the seed for the environment\\n        if self.cfg.seed is not None:\\n            self.cfg.seed = self.seed(self.cfg.seed)\\n        else:\\n            omni.log.warn(\"Seed not set for the environment. The environment creation may not be deterministic.\")\\n\\n        # create a simulation context to control the simulator\\n        if SimulationContext.instance() is None:\\n            self.sim: SimulationContext = SimulationContext(self.cfg.sim)\\n        else:\\n            raise RuntimeError(\"Simulation context already exists. Cannot create a new one.\")\\n\\n        # make sure torch is running on the correct device\\n        if \"cuda\" in self.device:\\n            torch.cuda.set_device(self.device)\\n\\n        # print useful information\\n        print(\"[INFO]: Base environment:\")\\n        print(f\"\\\\tEnvironment device    : {self.device}\")\\n        print(f\"\\\\tEnvironment seed      : {self.cfg.seed}\")\\n        print(f\"\\\\tPhysics step-size     : {self.physics_dt}\")\\n        print(f\"\\\\tRendering step-size   : {self.physics_dt * self.cfg.sim.render_interval}\")\\n        print(f\"\\\\tEnvironment step-size : {self.step_dt}\")\\n\\n        if self.cfg.sim.render_interval < self.cfg.decimation:\\n            msg = (\\n                f\"The render interval ({self.cfg.sim.render_interval}) is smaller than the decimation \"\\n                f\"({self.cfg.decimation}). Multiple render calls will happen for each environment step.\"\\n                \"If this is not intended, set the render interval to be equal to the decimation.\"\\n            )\\n            omni.log.warn(msg)\\n\\n        # generate scene\\n        with Timer(\"[INFO]: Time taken for scene creation\", \"scene_creation\"):\\n            self.scene = InteractiveScene(self.cfg.scene)\\n            self._setup_scene()\\n        print(\"[INFO]: Scene manager: \", self.scene)\\n\\n        # set up camera viewport controller\\n        # viewport is not available in other rendering modes so the function will throw a warning\\n        # FIXME: This needs to be fixed in the future when we unify the UI functionalities even for\\n        # non-rendering modes.\\n        if self.sim.render_mode >= self.sim.RenderMode.PARTIAL_RENDERING:\\n            self.viewport_camera_controller = ViewportCameraController(self, self.cfg.viewer)\\n        else:\\n            self.viewport_camera_controller = None\\n\\n        # create event manager\\n        # note: this is needed here (rather than after simulation play) to allow USD-related randomization events\\n        #   that must happen before the simulation starts. Example: randomizing mesh scale\\n        if self.cfg.events:\\n            self.event_manager = EventManager(self.cfg.events, self)\\n\\n            # apply USD-related randomization events\\n            if \"prestartup\" in self.event_manager.available_modes:\\n                self.event_manager.apply(mode=\"prestartup\")\\n\\n        # play the simulator to activate physics handles\\n        # note: this activates the physics simulation view that exposes TensorAPIs\\n        # note: when started in extension mode, first call sim.reset_async() and then initialize the managers\\n        if builtins.ISAAC_LAUNCHED_FROM_TERMINAL is False:\\n            print(\"[INFO]: Starting the simulation. This may take a few seconds. Please wait...\")\\n            with Timer(\"[INFO]: Time taken for simulation start\", \"simulation_start\"):\\n                self.sim.reset()\\n                # update scene to pre populate data buffers for assets and sensors.\\n                # this is needed for the observation manager to get valid tensors for initialization.\\n                # this shouldn\\'t cause an issue since later on, users do a reset over all the environments so the lazy buffers would be reset.\\n                self.scene.update(dt=self.physics_dt)\\n\\n        # check if debug visualization is has been implemented by the environment\\n        source_code = inspect.getsource(self._set_debug_vis_impl)\\n        self.has_debug_vis_implementation = \"NotImplementedError\" not in source_code\\n        self._debug_vis_handle = None\\n\\n        # extend UI elements\\n        # we need to do this here after all the managers are initialized\\n        # this is because they dictate the sensors and commands right now\\n        if self.sim.has_gui() and self.cfg.ui_window_class_type is not None:\\n            self._window = self.cfg.ui_window_class_type(self, window_name=\"IsaacLab\")\\n        else:\\n            # if no window, then we don\\'t need to store the window\\n            self._window = None\\n\\n        # allocate dictionary to store metrics\\n        self.extras = {}\\n\\n        # initialize data and constants\\n        # -- counter for simulation steps\\n        self._sim_step_counter = 0\\n        # -- counter for curriculum\\n        self.common_step_counter = 0\\n        # -- init buffers\\n        self.episode_length_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\\n        self.reset_terminated = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)\\n        self.reset_time_outs = torch.zeros_like(self.reset_terminated)\\n        self.reset_buf = torch.zeros(self.num_envs, dtype=torch.bool, device=self.sim.device)\\n\\n        # setup the action and observation spaces for Gym\\n        self._configure_gym_env_spaces()\\n\\n        # setup noise cfg for adding action and observation noise\\n        if self.cfg.action_noise_model:\\n            self._action_noise_model: NoiseModel = self.cfg.action_noise_model.class_type(\\n                self.cfg.action_noise_model, num_envs=self.num_envs, device=self.device\\n            )\\n        if self.cfg.observation_noise_model:\\n            self._observation_noise_model: NoiseModel = self.cfg.observation_noise_model.class_type(\\n                self.cfg.observation_noise_model, num_envs=self.num_envs, device=self.device\\n            )\\n\\n        # perform events at the start of the simulation\\n        if self.cfg.events:\\n            # we print it here to make the logging consistent\\n            print(\"[INFO] Event Manager: \", self.event_manager)\\n\\n            if \"startup\" in self.event_manager.available_modes:\\n                self.event_manager.apply(mode=\"startup\")\\n\\n        # set the framerate of the gym video recorder wrapper so that the playback speed of the produced\\n        # video matches the simulation\\n        self.metadata[\"render_fps\"] = 1 / self.step_dt\\n\\n        # print the environment information\\n        print(\"[INFO]: Completed setting up the environment...\")\\n\\n    def __del__(self):\\n        \"\"\"Cleanup for the environment.\"\"\"\\n        self.close()\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_envs(self) -> int:\\n        \"\"\"The number of instances of the environment that are running.\"\"\"\\n        return self.scene.num_envs\\n\\n    @property\\n    def physics_dt(self) -> float:\\n        \"\"\"The physics time-step (in s).\\n\\n        This is the lowest time-decimation at which the simulation is happening.\\n        \"\"\"\\n        return self.cfg.sim.dt\\n\\n    @property\\n    def step_dt(self) -> float:\\n        \"\"\"The environment stepping time-step (in s).\\n\\n        This is the time-step at which the environment steps forward.\\n        \"\"\"\\n        return self.cfg.sim.dt * self.cfg.decimation\\n\\n    @property\\n    def device(self):\\n        \"\"\"The device on which the environment is running.\"\"\"\\n        return self.sim.device\\n\\n    @property\\n    def max_episode_length_s(self) -> float:\\n        \"\"\"Maximum episode length in seconds.\"\"\"\\n        return self.cfg.episode_length_s\\n\\n    @property\\n    def max_episode_length(self):\\n        \"\"\"The maximum episode length in steps adjusted from s.\"\"\"\\n        return math.ceil(self.max_episode_length_s / (self.cfg.sim.dt * self.cfg.decimation))\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[VecEnvObs, dict]:\\n        \"\"\"Resets all the environments and returns observations.\\n\\n        This function calls the :meth:`_reset_idx` function to reset all the environments.\\n        However, certain operations, such as procedural terrain generation, that happened during initialization\\n        are not repeated.\\n\\n        Args:\\n            seed: The seed to use for randomization. Defaults to None, in which case the seed is not set.\\n            options: Additional information to specify how the environment is reset. Defaults to None.\\n\\n                Note:\\n                    This argument is used for compatibility with Gymnasium environment definition.\\n\\n        Returns:\\n            A tuple containing the observations and extras.\\n        \"\"\"\\n        # set the seed\\n        if seed is not None:\\n            self.seed(seed)\\n\\n        # reset state of scene\\n        indices = torch.arange(self.num_envs, dtype=torch.int64, device=self.device)\\n        self._reset_idx(indices)\\n\\n        # update articulation kinematics\\n        self.scene.write_data_to_sim()\\n        self.sim.forward()\\n\\n        # if sensors are added to the scene, make sure we render to reflect changes in reset\\n        if self.sim.has_rtx_sensors() and self.cfg.rerender_on_reset:\\n            self.sim.render()\\n\\n        if self.cfg.wait_for_textures and self.sim.has_rtx_sensors():\\n            while SimulationManager.assets_loading():\\n                self.sim.render()\\n\\n        # return observations\\n        return self._get_observations(), self.extras\\n\\n    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\\n        \"\"\"Execute one time-step of the environment\\'s dynamics.\\n\\n        The environment steps forward at a fixed time-step, while the physics simulation is decimated at a\\n        lower time-step. This is to ensure that the simulation is stable. These two time-steps can be configured\\n        independently using the :attr:`DirectRLEnvCfg.decimation` (number of simulation steps per environment step)\\n        and the :attr:`DirectRLEnvCfg.sim.physics_dt` (physics time-step). Based on these parameters, the environment\\n        time-step is computed as the product of the two.\\n\\n        This function performs the following steps:\\n\\n        1. Pre-process the actions before stepping through the physics.\\n        2. Apply the actions to the simulator and step through the physics in a decimated manner.\\n        3. Compute the reward and done signals.\\n        4. Reset environments that have terminated or reached the maximum episode length.\\n        5. Apply interval events if they are enabled.\\n        6. Compute observations.\\n\\n        Args:\\n            action: The actions to apply on the environment. Shape is (num_envs, action_dim).\\n\\n        Returns:\\n            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\\n        \"\"\"\\n        action = action.to(self.device)\\n        # add action noise\\n        if self.cfg.action_noise_model:\\n            action = self._action_noise_model.apply(action)\\n\\n        # process actions\\n        self._pre_physics_step(action)\\n\\n        # check if we need to do rendering within the physics loop\\n        # note: checked here once to avoid multiple checks within the loop\\n        is_rendering = self.sim.has_gui() or self.sim.has_rtx_sensors()\\n\\n        # perform physics stepping\\n        for _ in range(self.cfg.decimation):\\n            self._sim_step_counter += 1\\n            # set actions into buffers\\n            self._apply_action()\\n            # set actions into simulator\\n            self.scene.write_data_to_sim()\\n            # simulate\\n            self.sim.step(render=False)\\n            # render between steps only if the GUI or an RTX sensor needs it\\n            # note: we assume the render interval to be the shortest accepted rendering interval.\\n            #    If a camera needs rendering at a faster frequency, this will lead to unexpected behavior.\\n            if self._sim_step_counter % self.cfg.sim.render_interval == 0 and is_rendering:\\n                self.sim.render()\\n            # update buffers at sim dt\\n            self.scene.update(dt=self.physics_dt)\\n\\n        # post-step:\\n        # -- update env counters (used for curriculum generation)\\n        self.episode_length_buf += 1  # step in current episode (per env)\\n        self.common_step_counter += 1  # total step (common for all envs)\\n\\n        self.reset_terminated[:], self.reset_time_outs[:] = self._get_dones()\\n        self.reset_buf = self.reset_terminated | self.reset_time_outs\\n        self.reward_buf = self._get_rewards()\\n\\n        # -- reset envs that terminated/timed-out and log the episode information\\n        reset_env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)\\n        if len(reset_env_ids) > 0:\\n            self._reset_idx(reset_env_ids)\\n            # update articulation kinematics\\n            self.scene.write_data_to_sim()\\n            self.sim.forward()\\n            # if sensors are added to the scene, make sure we render to reflect changes in reset\\n            if self.sim.has_rtx_sensors() and self.cfg.rerender_on_reset:\\n                self.sim.render()\\n\\n        # post-step: step interval event\\n        if self.cfg.events:\\n            if \"interval\" in self.event_manager.available_modes:\\n                self.event_manager.apply(mode=\"interval\", dt=self.step_dt)\\n\\n        # update observations\\n        self.obs_buf = self._get_observations()\\n\\n        # add observation noise\\n        # note: we apply no noise to the state space (since it is used for critic networks)\\n        if self.cfg.observation_noise_model:\\n            self.obs_buf[\"policy\"] = self._observation_noise_model.apply(self.obs_buf[\"policy\"])\\n\\n        # return observations, rewards, resets and extras\\n        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\\n\\n    @staticmethod\\n    def seed(seed: int = -1) -> int:\\n        \"\"\"Set the seed for the environment.\\n\\n        Args:\\n            seed: The seed for random generator. Defaults to -1.\\n\\n        Returns:\\n            The seed used for random generator.\\n        \"\"\"\\n        # set seed for replicator\\n        try:\\n            import omni.replicator.core as rep\\n\\n            rep.set_global_seed(seed)\\n        except ModuleNotFoundError:\\n            pass\\n        # set seed for torch and other libraries\\n        return torch_utils.set_seed(seed)\\n\\n    def render(self, recompute: bool = False) -> np.ndarray | None:\\n        \"\"\"Run rendering without stepping through the physics.\\n\\n        By convention, if mode is:\\n\\n        - **human**: Render to the current display and return nothing. Usually for human consumption.\\n        - **rgb_array**: Return an numpy.ndarray with shape (x, y, 3), representing RGB values for an\\n          x-by-y pixel image, suitable for turning into a video.\\n\\n        Args:\\n            recompute: Whether to force a render even if the simulator has already rendered the scene.\\n                Defaults to False.\\n\\n        Returns:\\n            The rendered image as a numpy array if mode is \"rgb_array\". Otherwise, returns None.\\n\\n        Raises:\\n            RuntimeError: If mode is set to \"rgb_data\" and simulation render mode does not support it.\\n                In this case, the simulation render mode must be set to ``RenderMode.PARTIAL_RENDERING``\\n                or ``RenderMode.FULL_RENDERING``.\\n            NotImplementedError: If an unsupported rendering mode is specified.\\n        \"\"\"\\n        # run a rendering step of the simulator\\n        # if we have rtx sensors, we do not need to render again sin\\n        if not self.sim.has_rtx_sensors() and not recompute:\\n            self.sim.render()\\n        # decide the rendering mode\\n        if self.render_mode == \"human\" or self.render_mode is None:\\n            return None\\n        elif self.render_mode == \"rgb_array\":\\n            # check that if any render could have happened\\n            if self.sim.render_mode.value < self.sim.RenderMode.PARTIAL_RENDERING.value:\\n                raise RuntimeError(\\n                    f\"Cannot render \\'{self.render_mode}\\' when the simulation render mode is\"\\n                    f\" \\'{self.sim.render_mode.name}\\'. Please set the simulation render mode to:\"\\n                    f\"\\'{self.sim.RenderMode.PARTIAL_RENDERING.name}\\' or \\'{self.sim.RenderMode.FULL_RENDERING.name}\\'.\"\\n                    \" If running headless, make sure --enable_cameras is set.\"\\n                )\\n            # create the annotator if it does not exist\\n            if not hasattr(self, \"_rgb_annotator\"):\\n                import omni.replicator.core as rep\\n\\n                # create render product\\n                self._render_product = rep.create.render_product(\\n                    self.cfg.viewer.cam_prim_path, self.cfg.viewer.resolution\\n                )\\n                # create rgb annotator -- used to read data from the render product\\n                self._rgb_annotator = rep.AnnotatorRegistry.get_annotator(\"rgb\", device=\"cpu\")\\n                self._rgb_annotator.attach([self._render_product])\\n            # obtain the rgb data\\n            rgb_data = self._rgb_annotator.get_data()\\n            # convert to numpy array\\n            rgb_data = np.frombuffer(rgb_data, dtype=np.uint8).reshape(*rgb_data.shape)\\n            # return the rgb data\\n            # note: initially the renerer is warming up and returns empty data\\n            if rgb_data.size == 0:\\n                return np.zeros((self.cfg.viewer.resolution[1], self.cfg.viewer.resolution[0], 3), dtype=np.uint8)\\n            else:\\n                return rgb_data[:, :, :3]\\n        else:\\n            raise NotImplementedError(\\n                f\"Render mode \\'{self.render_mode}\\' is not supported. Please use: {self.metadata[\\'render_modes\\']}.\"\\n            )\\n\\n    def close(self):\\n        \"\"\"Cleanup for the environment.\"\"\"\\n        if not self._is_closed:\\n            # close entities related to the environment\\n            # note: this is order-sensitive to avoid any dangling references\\n            if self.cfg.events:\\n                del self.event_manager\\n            del self.scene\\n            if self.viewport_camera_controller is not None:\\n                del self.viewport_camera_controller\\n            # clear callbacks and instance\\n            self.sim.clear_all_callbacks()\\n            self.sim.clear_instance()\\n            # destroy the window\\n            if self._window is not None:\\n                self._window = None\\n            # update closing status\\n            self._is_closed = True\\n\\n    \"\"\"\\n    Operations - Debug Visualization.\\n    \"\"\"\\n\\n    def set_debug_vis(self, debug_vis: bool) -> bool:\\n        \"\"\"Toggles the environment debug visualization.\\n\\n        Args:\\n            debug_vis: Whether to visualize the environment debug visualization.\\n\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the environment\\n            does not support debug visualization.\\n        \"\"\"\\n        # check if debug visualization is supported\\n        if not self.has_debug_vis_implementation:\\n            return False\\n        # toggle debug visualization objects\\n        self._set_debug_vis_impl(debug_vis)\\n        # toggle debug visualization handles\\n        if debug_vis:\\n            # create a subscriber for the post update event if it doesn\\'t exist\\n            if self._debug_vis_handle is None:\\n                app_interface = omni.kit.app.get_app_interface()\\n                self._debug_vis_handle = app_interface.get_post_update_event_stream().create_subscription_to_pop(\\n                    lambda event, obj=weakref.proxy(self): obj._debug_vis_callback(event)\\n                )\\n        else:\\n            # remove the subscriber if it exists\\n            if self._debug_vis_handle is not None:\\n                self._debug_vis_handle.unsubscribe()\\n                self._debug_vis_handle = None\\n        # return success\\n        return True\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _configure_gym_env_spaces(self):\\n        \"\"\"Configure the action and observation spaces for the Gym environment.\"\"\"\\n        # show deprecation message and overwrite configuration\\n        if self.cfg.num_actions is not None:\\n            omni.log.warn(\"DirectRLEnvCfg.num_actions is deprecated. Use DirectRLEnvCfg.action_space instead.\")\\n            if isinstance(self.cfg.action_space, type(MISSING)):\\n                self.cfg.action_space = self.cfg.num_actions\\n        if self.cfg.num_observations is not None:\\n            omni.log.warn(\\n                \"DirectRLEnvCfg.num_observations is deprecated. Use DirectRLEnvCfg.observation_space instead.\"\\n            )\\n            if isinstance(self.cfg.observation_space, type(MISSING)):\\n                self.cfg.observation_space = self.cfg.num_observations\\n        if self.cfg.num_states is not None:\\n            omni.log.warn(\"DirectRLEnvCfg.num_states is deprecated. Use DirectRLEnvCfg.state_space instead.\")\\n            if isinstance(self.cfg.state_space, type(MISSING)):\\n                self.cfg.state_space = self.cfg.num_states\\n\\n        # set up spaces\\n        self.single_observation_space = gym.spaces.Dict()\\n        self.single_observation_space[\"policy\"] = spec_to_gym_space(self.cfg.observation_space)\\n        self.single_action_space = spec_to_gym_space(self.cfg.action_space)\\n\\n        # batch the spaces for vectorized environments\\n        self.observation_space = gym.vector.utils.batch_space(self.single_observation_space[\"policy\"], self.num_envs)\\n        self.action_space = gym.vector.utils.batch_space(self.single_action_space, self.num_envs)\\n\\n        # optional state space for asymmetric actor-critic architectures\\n        self.state_space = None\\n        if self.cfg.state_space:\\n            self.single_observation_space[\"critic\"] = spec_to_gym_space(self.cfg.state_space)\\n            self.state_space = gym.vector.utils.batch_space(self.single_observation_space[\"critic\"], self.num_envs)\\n\\n        # instantiate actions (needed for tasks for which the observations computation is dependent on the actions)\\n        self.actions = sample_space(self.single_action_space, self.sim.device, batch_size=self.num_envs, fill_value=0)\\n\\n    def _reset_idx(self, env_ids: Sequence[int]):\\n        \"\"\"Reset environments based on specified indices.\\n\\n        Args:\\n            env_ids: List of environment ids which must be reset\\n        \"\"\"\\n        self.scene.reset(env_ids)\\n\\n        # apply events such as randomization for environments that need a reset\\n        if self.cfg.events:\\n            if \"reset\" in self.event_manager.available_modes:\\n                env_step_count = self._sim_step_counter // self.cfg.decimation\\n                self.event_manager.apply(mode=\"reset\", env_ids=env_ids, global_env_step_count=env_step_count)\\n\\n        # reset noise models\\n        if self.cfg.action_noise_model:\\n            self._action_noise_model.reset(env_ids)\\n        if self.cfg.observation_noise_model:\\n            self._observation_noise_model.reset(env_ids)\\n\\n        # reset the episode length buffer\\n        self.episode_length_buf[env_ids] = 0\\n\\n    \"\"\"\\n    Implementation-specific functions.\\n    \"\"\"\\n\\n    def _setup_scene(self):\\n        \"\"\"Setup the scene for the environment.\\n\\n        This function is responsible for creating the scene objects and setting up the scene for the environment.\\n        The scene creation can happen through :class:`isaaclab.scene.InteractiveSceneCfg` or through\\n        directly creating the scene objects and registering them with the scene manager.\\n\\n        We leave the implementation of this function to the derived classes. If the environment does not require\\n        any explicit scene setup, the function can be left empty.\\n        \"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def _pre_physics_step(self, actions: torch.Tensor):\\n        \"\"\"Pre-process actions before stepping through the physics.\\n\\n        This function is responsible for pre-processing the actions before stepping through the physics.\\n        It is called before the physics stepping (which is decimated).\\n\\n        Args:\\n            actions: The actions to apply on the environment. Shape is (num_envs, action_dim).\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_pre_physics_step\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _apply_action(self):\\n        \"\"\"Apply actions to the simulator.\\n\\n        This function is responsible for applying the actions to the simulator. It is called at each\\n        physics time-step.\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_apply_action\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _get_observations(self) -> VecEnvObs:\\n        \"\"\"Compute and return the observations for the environment.\\n\\n        Returns:\\n            The observations for the environment.\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_get_observations\\' method for {self.__class__.__name__}.\")\\n\\n    def _get_states(self) -> VecEnvObs | None:\\n        \"\"\"Compute and return the states for the environment.\\n\\n        The state-space is used for asymmetric actor-critic architectures. It is configured\\n        using the :attr:`DirectRLEnvCfg.state_space` parameter.\\n\\n        Returns:\\n            The states for the environment. If the environment does not have a state-space, the function\\n            returns a None.\\n        \"\"\"\\n        return None  # noqa: R501\\n\\n    @abstractmethod\\n    def _get_rewards(self) -> torch.Tensor:\\n        \"\"\"Compute and return the rewards for the environment.\\n\\n        Returns:\\n            The rewards for the environment. Shape is (num_envs,).\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_get_rewards\\' method for {self.__class__.__name__}.\")\\n\\n    @abstractmethod\\n    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"Compute and return the done flags for the environment.\\n\\n        Returns:\\n            A tuple containing the done flags for termination and time-out.\\n            Shape of individual tensors is (num_envs,).\\n        \"\"\"\\n        raise NotImplementedError(f\"Please implement the \\'_get_dones\\' method for {self.__class__.__name__}.\")\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set debug visualization into visualization objects.\\n\\n        This function is responsible for creating the visualization objects if they don\\'t exist\\n        and input ``debug_vis`` is True. If the visualization objects exist, the function should\\n        set their visibility into the stage.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")'),\n",
       " Document(metadata={}, page_content='class DirectRLEnvCfg:\\n    \"\"\"Configuration for an RL environment defined with the direct workflow.\\n\\n    Please refer to the :class:`isaaclab.envs.direct_rl_env.DirectRLEnv` class for more details.\\n    \"\"\"\\n\\n    # simulation settings\\n    viewer: ViewerCfg = ViewerCfg()\\n    \"\"\"Viewer configuration. Default is ViewerCfg().\"\"\"\\n\\n    sim: SimulationCfg = SimulationCfg()\\n    \"\"\"Physics simulation configuration. Default is SimulationCfg().\"\"\"\\n\\n    # ui settings\\n    ui_window_class_type: type | None = BaseEnvWindow\\n    \"\"\"The class type of the UI window. Default is None.\\n\\n    If None, then no UI window is created.\\n\\n    Note:\\n        If you want to make your own UI window, you can create a class that inherits from\\n        from :class:`isaaclab.envs.ui.base_env_window.BaseEnvWindow`. Then, you can set\\n        this attribute to your class type.\\n    \"\"\"\\n\\n    # general settings\\n    seed: int | None = None\\n    \"\"\"The seed for the random number generator. Defaults to None, in which case the seed is not set.\\n\\n    Note:\\n      The seed is set at the beginning of the environment initialization. This ensures that the environment\\n      creation is deterministic and behaves similarly across different runs.\\n    \"\"\"\\n\\n    decimation: int = MISSING\\n    \"\"\"Number of control action updates @ sim dt per policy dt.\\n\\n    For instance, if the simulation dt is 0.01s and the policy dt is 0.1s, then the decimation is 10.\\n    This means that the control action is updated every 10 simulation steps.\\n    \"\"\"\\n\\n    is_finite_horizon: bool = False\\n    \"\"\"Whether the learning task is treated as a finite or infinite horizon problem for the agent.\\n    Defaults to False, which means the task is treated as an infinite horizon problem.\\n\\n    This flag handles the subtleties of finite and infinite horizon tasks:\\n\\n    * **Finite horizon**: no penalty or bootstrapping value is required by the the agent for\\n      running out of time. However, the environment still needs to terminate the episode after the\\n      time limit is reached.\\n    * **Infinite horizon**: the agent needs to bootstrap the value of the state at the end of the episode.\\n      This is done by sending a time-limit (or truncated) done signal to the agent, which triggers this\\n      bootstrapping calculation.\\n\\n    If True, then the environment is treated as a finite horizon problem and no time-out (or truncated) done signal\\n    is sent to the agent. If False, then the environment is treated as an infinite horizon problem and a time-out\\n    (or truncated) done signal is sent to the agent.\\n\\n    Note:\\n        The base :class:`ManagerBasedRLEnv` class does not use this flag directly. It is used by the environment\\n        wrappers to determine what type of done signal to send to the corresponding learning agent.\\n    \"\"\"\\n\\n    episode_length_s: float = MISSING\\n    \"\"\"Duration of an episode (in seconds).\\n\\n    Based on the decimation rate and physics time step, the episode length is calculated as:\\n\\n    .. code-block:: python\\n\\n        episode_length_steps = ceil(episode_length_s / (decimation_rate * physics_time_step))\\n\\n    For example, if the decimation rate is 10, the physics time step is 0.01, and the episode length is 10 seconds,\\n    then the episode length in steps is 100.\\n    \"\"\"\\n\\n    # environment settings\\n    scene: InteractiveSceneCfg = MISSING\\n    \"\"\"Scene settings.\\n\\n    Please refer to the :class:`isaaclab.scene.InteractiveSceneCfg` class for more details.\\n    \"\"\"\\n\\n    events: object | None = None\\n    \"\"\"Event settings. Defaults to None, in which case no events are applied through the event manager.\\n\\n    Please refer to the :class:`isaaclab.managers.EventManager` class for more details.\\n    \"\"\"\\n\\n    observation_space: SpaceType = MISSING\\n    \"\"\"Observation space definition.\\n\\n    The space can be defined either using Gymnasium :py:mod:`~gymnasium.spaces` (when a more detailed\\n    specification of the space is desired) or basic Python data types (for simplicity).\\n\\n    .. list-table::\\n        :header-rows: 1\\n\\n        * - Gymnasium space\\n          - Python data type\\n        * - :class:`~gymnasium.spaces.Box`\\n          - Integer or list of integers (e.g.: ``7``, ``[64, 64, 3]``)\\n        * - :class:`~gymnasium.spaces.Discrete`\\n          - Single-element set (e.g.: ``{2}``)\\n        * - :class:`~gymnasium.spaces.MultiDiscrete`\\n          - List of single-element sets (e.g.: ``[{2}, {5}]``)\\n        * - :class:`~gymnasium.spaces.Dict`\\n          - Dictionary (e.g.: ``{\"joints\": 7, \"rgb\": [64, 64, 3], \"gripper\": {2}}``)\\n        * - :class:`~gymnasium.spaces.Tuple`\\n          - Tuple (e.g.: ``(7, [64, 64, 3], {2})``)\\n    \"\"\"\\n\\n    num_observations: int | None = None\\n    \"\"\"The dimension of the observation space from each environment instance.\\n\\n    .. warning::\\n\\n        This attribute is deprecated. Use :attr:`~isaaclab.envs.DirectRLEnvCfg.observation_space` instead.\\n    \"\"\"\\n\\n    state_space: SpaceType | None = None\\n    \"\"\"State space definition.\\n\\n    This is useful for asymmetric actor-critic and defines the observation space for the critic.\\n\\n    The space can be defined either using Gymnasium :py:mod:`~gymnasium.spaces` (when a more detailed\\n    specification of the space is desired) or basic Python data types (for simplicity).\\n\\n    .. list-table::\\n        :header-rows: 1\\n\\n        * - Gymnasium space\\n          - Python data type\\n        * - :class:`~gymnasium.spaces.Box`\\n          - Integer or list of integers (e.g.: ``7``, ``[64, 64, 3]``)\\n        * - :class:`~gymnasium.spaces.Discrete`\\n          - Single-element set (e.g.: ``{2}``)\\n        * - :class:`~gymnasium.spaces.MultiDiscrete`\\n          - List of single-element sets (e.g.: ``[{2}, {5}]``)\\n        * - :class:`~gymnasium.spaces.Dict`\\n          - Dictionary (e.g.: ``{\"joints\": 7, \"rgb\": [64, 64, 3], \"gripper\": {2}}``)\\n        * - :class:`~gymnasium.spaces.Tuple`\\n          - Tuple (e.g.: ``(7, [64, 64, 3], {2})``)\\n    \"\"\"\\n\\n    num_states: int | None = None\\n    \"\"\"The dimension of the state-space from each environment instance.\\n\\n    .. warning::\\n\\n        This attribute is deprecated. Use :attr:`~isaaclab.envs.DirectRLEnvCfg.state_space` instead.\\n    \"\"\"\\n\\n    observation_noise_model: NoiseModelCfg | None = None\\n    \"\"\"The noise model to apply to the computed observations from the environment. Default is None, which means no noise is added.\\n\\n    Please refer to the :class:`isaaclab.utils.noise.NoiseModel` class for more details.\\n    \"\"\"\\n\\n    action_space: SpaceType = MISSING\\n    \"\"\"Action space definition.\\n\\n    The space can be defined either using Gymnasium :py:mod:`~gymnasium.spaces` (when a more detailed\\n    specification of the space is desired) or basic Python data types (for simplicity).\\n\\n    .. list-table::\\n        :header-rows: 1\\n\\n        * - Gymnasium space\\n          - Python data type\\n        * - :class:`~gymnasium.spaces.Box`\\n          - Integer or list of integers (e.g.: ``7``, ``[64, 64, 3]``)\\n        * - :class:`~gymnasium.spaces.Discrete`\\n          - Single-element set (e.g.: ``{2}``)\\n        * - :class:`~gymnasium.spaces.MultiDiscrete`\\n          - List of single-element sets (e.g.: ``[{2}, {5}]``)\\n        * - :class:`~gymnasium.spaces.Dict`\\n          - Dictionary (e.g.: ``{\"joints\": 7, \"rgb\": [64, 64, 3], \"gripper\": {2}}``)\\n        * - :class:`~gymnasium.spaces.Tuple`\\n          - Tuple (e.g.: ``(7, [64, 64, 3], {2})``)\\n    \"\"\"\\n\\n    num_actions: int | None = None\\n    \"\"\"The dimension of the action space for each environment.\\n\\n    .. warning::\\n\\n        This attribute is deprecated. Use :attr:`~isaaclab.envs.DirectRLEnvCfg.action_space` instead.\\n    \"\"\"\\n\\n    action_noise_model: NoiseModelCfg | None = None\\n    \"\"\"The noise model applied to the actions provided to the environment. Default is None, which means no noise is added.\\n\\n    Please refer to the :class:`isaaclab.utils.noise.NoiseModel` class for more details.\\n    \"\"\"\\n\\n    rerender_on_reset: bool = False\\n    \"\"\"Whether a render step is performed again after at least one environment has been reset.\\n    Defaults to False, which means no render step will be performed after reset.\\n\\n    * When this is False, data collected from sensors after performing reset will be stale and will not reflect the\\n      latest states in simulation caused by the reset.\\n    * When this is True, an extra render step will be performed to update the sensor data\\n      to reflect the latest states from the reset. This comes at a cost of performance as an additional render\\n      step will be performed after each time an environment is reset.\\n\\n    \"\"\"\\n\\n    wait_for_textures: bool = True\\n    \"\"\"True to wait for assets to be loaded completely, False otherwise. Defaults to True.\"\"\"\\n\\n    xr: XrCfg | None = None\\n    \"\"\"Configuration for viewing and interacting with the environment through an XR device.\"\"\"'),\n",
       " Document(metadata={}, page_content='class ManagerBasedEnv:\\n    \"\"\"The base environment encapsulates the simulation scene and the environment managers for the manager-based workflow.\\n\\n    While a simulation scene or world comprises of different components such as the robots, objects,\\n    and sensors (cameras, lidars, etc.), the environment is a higher level abstraction\\n    that provides an interface for interacting with the simulation. The environment is comprised of\\n    the following components:\\n\\n    * **Scene**: The scene manager that creates and manages the virtual world in which the robot operates.\\n      This includes defining the robot, static and dynamic objects, sensors, etc.\\n    * **Observation Manager**: The observation manager that generates observations from the current simulation\\n      state and the data gathered from the sensors. These observations may include privileged information\\n      that is not available to the robot in the real world. Additionally, user-defined terms can be added\\n      to process the observations and generate custom observations. For example, using a network to embed\\n      high-dimensional observations into a lower-dimensional space.\\n    * **Action Manager**: The action manager that processes the raw actions sent to the environment and\\n      converts them to low-level commands that are sent to the simulation. It can be configured to accept\\n      raw actions at different levels of abstraction. For example, in case of a robotic arm, the raw actions\\n      can be joint torques, joint positions, or end-effector poses. Similarly for a mobile base, it can be\\n      the joint torques, or the desired velocity of the floating base.\\n    * **Event Manager**: The event manager orchestrates operations triggered based on simulation events.\\n      This includes resetting the scene to a default state, applying random pushes to the robot at different intervals\\n      of time, or randomizing properties such as mass and friction coefficients. This is useful for training\\n      and evaluating the robot in a variety of scenarios.\\n    * **Recorder Manager**: The recorder manager that handles recording data produced during different steps\\n      in the simulation. This includes recording in the beginning and end of a reset and a step. The recorded data\\n      is distinguished per episode, per environment and can be exported through a dataset file handler to a file.\\n\\n    The environment provides a unified interface for interacting with the simulation. However, it does not\\n    include task-specific quantities such as the reward function, or the termination conditions. These\\n    quantities are often specific to defining Markov Decision Processes (MDPs) while the base environment\\n    is agnostic to the MDP definition.\\n\\n    The environment steps forward in time at a fixed time-step. The physics simulation is decimated at a\\n    lower time-step. This is to ensure that the simulation is stable. These two time-steps can be configured\\n    independently using the :attr:`ManagerBasedEnvCfg.decimation` (number of simulation steps per environment step)\\n    and the :attr:`ManagerBasedEnvCfg.sim.dt` (physics time-step) parameters. Based on these parameters, the\\n    environment time-step is computed as the product of the two. The two time-steps can be obtained by\\n    querying the :attr:`physics_dt` and the :attr:`step_dt` properties respectively.\\n    \"\"\"\\n\\n    def __init__(self, cfg: ManagerBasedEnvCfg):\\n        \"\"\"Initialize the environment.\\n\\n        Args:\\n            cfg: The configuration object for the environment.\\n\\n        Raises:\\n            RuntimeError: If a simulation context already exists. The environment must always create one\\n                since it configures the simulation context and controls the simulation.\\n        \"\"\"\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs to class\\n        self.cfg = cfg\\n        # initialize internal variables\\n        self._is_closed = False\\n\\n        # set the seed for the environment\\n        if self.cfg.seed is not None:\\n            self.cfg.seed = self.seed(self.cfg.seed)\\n        else:\\n            omni.log.warn(\"Seed not set for the environment. The environment creation may not be deterministic.\")\\n\\n        # create a simulation context to control the simulator\\n        if SimulationContext.instance() is None:\\n            # the type-annotation is required to avoid a type-checking error\\n            # since it gets confused with Isaac Sim\\'s SimulationContext class\\n            self.sim: SimulationContext = SimulationContext(self.cfg.sim)\\n        else:\\n            # simulation context should only be created before the environment\\n            # when in extension mode\\n            if not builtins.ISAAC_LAUNCHED_FROM_TERMINAL:\\n                raise RuntimeError(\"Simulation context already exists. Cannot create a new one.\")\\n            self.sim: SimulationContext = SimulationContext.instance()\\n\\n        # make sure torch is running on the correct device\\n        if \"cuda\" in self.device:\\n            torch.cuda.set_device(self.device)\\n\\n        # print useful information\\n        print(\"[INFO]: Base environment:\")\\n        print(f\"\\\\tEnvironment device    : {self.device}\")\\n        print(f\"\\\\tEnvironment seed      : {self.cfg.seed}\")\\n        print(f\"\\\\tPhysics step-size     : {self.physics_dt}\")\\n        print(f\"\\\\tRendering step-size   : {self.physics_dt * self.cfg.sim.render_interval}\")\\n        print(f\"\\\\tEnvironment step-size : {self.step_dt}\")\\n\\n        if self.cfg.sim.render_interval < self.cfg.decimation:\\n            msg = (\\n                f\"The render interval ({self.cfg.sim.render_interval}) is smaller than the decimation \"\\n                f\"({self.cfg.decimation}). Multiple render calls will happen for each environment step. \"\\n                \"If this is not intended, set the render interval to be equal to the decimation.\"\\n            )\\n            omni.log.warn(msg)\\n\\n        # counter for simulation steps\\n        self._sim_step_counter = 0\\n\\n        # allocate dictionary to store metrics\\n        self.extras = {}\\n\\n        # generate scene\\n        with Timer(\"[INFO]: Time taken for scene creation\", \"scene_creation\"):\\n            self.scene = InteractiveScene(self.cfg.scene)\\n        print(\"[INFO]: Scene manager: \", self.scene)\\n\\n        # set up camera viewport controller\\n        # viewport is not available in other rendering modes so the function will throw a warning\\n        # FIXME: This needs to be fixed in the future when we unify the UI functionalities even for\\n        # non-rendering modes.\\n        if self.sim.render_mode >= self.sim.RenderMode.PARTIAL_RENDERING:\\n            self.viewport_camera_controller = ViewportCameraController(self, self.cfg.viewer)\\n        else:\\n            self.viewport_camera_controller = None\\n\\n        # create event manager\\n        # note: this is needed here (rather than after simulation play) to allow USD-related randomization events\\n        #   that must happen before the simulation starts. Example: randomizing mesh scale\\n        self.event_manager = EventManager(self.cfg.events, self)\\n\\n        # apply USD-related randomization events\\n        if \"prestartup\" in self.event_manager.available_modes:\\n            self.event_manager.apply(mode=\"prestartup\")\\n\\n        # play the simulator to activate physics handles\\n        # note: this activates the physics simulation view that exposes TensorAPIs\\n        # note: when started in extension mode, first call sim.reset_async() and then initialize the managers\\n        if builtins.ISAAC_LAUNCHED_FROM_TERMINAL is False:\\n            print(\"[INFO]: Starting the simulation. This may take a few seconds. Please wait...\")\\n            with Timer(\"[INFO]: Time taken for simulation start\", \"simulation_start\"):\\n                self.sim.reset()\\n                # update scene to pre populate data buffers for assets and sensors.\\n                # this is needed for the observation manager to get valid tensors for initialization.\\n                # this shouldn\\'t cause an issue since later on, users do a reset over all the environments so the lazy buffers would be reset.\\n                self.scene.update(dt=self.physics_dt)\\n            # add timeline event to load managers\\n            self.load_managers()\\n\\n        # extend UI elements\\n        # we need to do this here after all the managers are initialized\\n        # this is because they dictate the sensors and commands right now\\n        if self.sim.has_gui() and self.cfg.ui_window_class_type is not None:\\n            # setup live visualizers\\n            self.setup_manager_visualizers()\\n            self._window = self.cfg.ui_window_class_type(self, window_name=\"IsaacLab\")\\n        else:\\n            # if no window, then we don\\'t need to store the window\\n            self._window = None\\n\\n        # initialize observation buffers\\n        self.obs_buf = {}\\n\\n    def __del__(self):\\n        \"\"\"Cleanup for the environment.\"\"\"\\n        self.close()\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_envs(self) -> int:\\n        \"\"\"The number of instances of the environment that are running.\"\"\"\\n        return self.scene.num_envs\\n\\n    @property\\n    def physics_dt(self) -> float:\\n        \"\"\"The physics time-step (in s).\\n\\n        This is the lowest time-decimation at which the simulation is happening.\\n        \"\"\"\\n        return self.cfg.sim.dt\\n\\n    @property\\n    def step_dt(self) -> float:\\n        \"\"\"The environment stepping time-step (in s).\\n\\n        This is the time-step at which the environment steps forward.\\n        \"\"\"\\n        return self.cfg.sim.dt * self.cfg.decimation\\n\\n    @property\\n    def device(self):\\n        \"\"\"The device on which the environment is running.\"\"\"\\n        return self.sim.device\\n\\n    \"\"\"\\n    Operations - Setup.\\n    \"\"\"\\n\\n    def load_managers(self):\\n        \"\"\"Load the managers for the environment.\\n\\n        This function is responsible for creating the various managers (action, observation,\\n        events, etc.) for the environment. Since the managers require access to physics handles,\\n        they can only be created after the simulator is reset (i.e. played for the first time).\\n\\n        .. note::\\n            In case of standalone application (when running simulator from Python), the function is called\\n            automatically when the class is initialized.\\n\\n            However, in case of extension mode, the user must call this function manually after the simulator\\n            is reset. This is because the simulator is only reset when the user calls\\n            :meth:`SimulationContext.reset_async` and it isn\\'t possible to call async functions in the constructor.\\n\\n        \"\"\"\\n        # prepare the managers\\n        # -- event manager (we print it here to make the logging consistent)\\n        print(\"[INFO] Event Manager: \", self.event_manager)\\n        # -- recorder manager\\n        self.recorder_manager = RecorderManager(self.cfg.recorders, self)\\n        print(\"[INFO] Recorder Manager: \", self.recorder_manager)\\n        # -- action manager\\n        self.action_manager = ActionManager(self.cfg.actions, self)\\n        print(\"[INFO] Action Manager: \", self.action_manager)\\n        # -- observation manager\\n        self.observation_manager = ObservationManager(self.cfg.observations, self)\\n        print(\"[INFO] Observation Manager:\", self.observation_manager)\\n\\n        # perform events at the start of the simulation\\n        # in-case a child implementation creates other managers, the randomization should happen\\n        # when all the other managers are created\\n        if self.__class__ == ManagerBasedEnv and \"startup\" in self.event_manager.available_modes:\\n            self.event_manager.apply(mode=\"startup\")\\n\\n    def setup_manager_visualizers(self):\\n        \"\"\"Creates live visualizers for manager terms.\"\"\"\\n\\n        self.manager_visualizers = {\\n            \"action_manager\": ManagerLiveVisualizer(manager=self.action_manager),\\n            \"observation_manager\": ManagerLiveVisualizer(manager=self.observation_manager),\\n        }\\n\\n    \"\"\"\\n    Operations - MDP.\\n    \"\"\"\\n\\n    def reset(\\n        self, seed: int | None = None, env_ids: Sequence[int] | None = None, options: dict[str, Any] | None = None\\n    ) -> tuple[VecEnvObs, dict]:\\n        \"\"\"Resets the specified environments and returns observations.\\n\\n        This function calls the :meth:`_reset_idx` function to reset the specified environments.\\n        However, certain operations, such as procedural terrain generation, that happened during initialization\\n        are not repeated.\\n\\n        Args:\\n            seed: The seed to use for randomization. Defaults to None, in which case the seed is not set.\\n            env_ids: The environment ids to reset. Defaults to None, in which case all environments are reset.\\n            options: Additional information to specify how the environment is reset. Defaults to None.\\n\\n                Note:\\n                    This argument is used for compatibility with Gymnasium environment definition.\\n\\n        Returns:\\n            A tuple containing the observations and extras.\\n        \"\"\"\\n        if env_ids is None:\\n            env_ids = torch.arange(self.num_envs, dtype=torch.int64, device=self.device)\\n\\n        # trigger recorder terms for pre-reset calls\\n        self.recorder_manager.record_pre_reset(env_ids)\\n\\n        # set the seed\\n        if seed is not None:\\n            self.seed(seed)\\n\\n        # reset state of scene\\n        self._reset_idx(env_ids)\\n\\n        # update articulation kinematics\\n        self.scene.write_data_to_sim()\\n        self.sim.forward()\\n        # if sensors are added to the scene, make sure we render to reflect changes in reset\\n        if self.sim.has_rtx_sensors() and self.cfg.rerender_on_reset:\\n            self.sim.render()\\n\\n        # trigger recorder terms for post-reset calls\\n        self.recorder_manager.record_post_reset(env_ids)\\n\\n        # compute observations\\n        self.obs_buf = self.observation_manager.compute()\\n\\n        if self.cfg.wait_for_textures and self.sim.has_rtx_sensors():\\n            while SimulationManager.assets_loading():\\n                self.sim.render()\\n\\n        # return observations\\n        return self.obs_buf, self.extras\\n\\n    def reset_to(\\n        self,\\n        state: dict[str, dict[str, dict[str, torch.Tensor]]],\\n        env_ids: Sequence[int] | None,\\n        seed: int | None = None,\\n        is_relative: bool = False,\\n    ):\\n        \"\"\"Resets specified environments to provided states.\\n\\n        This function resets the environments to the provided states. The state is a dictionary\\n        containing the state of the scene entities. Please refer to :meth:`InteractiveScene.get_state`\\n        for the format.\\n\\n        The function is different from the :meth:`reset` function as it resets the environments to specific states,\\n        instead of using the randomization events for resetting the environments.\\n\\n        Args:\\n            state: The state to reset the specified environments to. Please refer to\\n                :meth:`InteractiveScene.get_state` for the format.\\n            env_ids: The environment ids to reset. Defaults to None, in which case all environments are reset.\\n            seed: The seed to use for randomization. Defaults to None, in which case the seed is not set.\\n            is_relative: If set to True, the state is considered relative to the environment origins.\\n                Defaults to False.\\n        \"\"\"\\n        # reset all envs in the scene if env_ids is None\\n        if env_ids is None:\\n            env_ids = torch.arange(self.num_envs, dtype=torch.int64, device=self.device)\\n\\n        # trigger recorder terms for pre-reset calls\\n        self.recorder_manager.record_pre_reset(env_ids)\\n\\n        # set the seed\\n        if seed is not None:\\n            self.seed(seed)\\n\\n        self._reset_idx(env_ids)\\n\\n        # set the state\\n        self.scene.reset_to(state, env_ids, is_relative=is_relative)\\n\\n        # update articulation kinematics\\n        self.sim.forward()\\n\\n        # if sensors are added to the scene, make sure we render to reflect changes in reset\\n        if self.sim.has_rtx_sensors() and self.cfg.rerender_on_reset:\\n            self.sim.render()\\n\\n        # trigger recorder terms for post-reset calls\\n        self.recorder_manager.record_post_reset(env_ids)\\n\\n        # compute observations\\n        self.obs_buf = self.observation_manager.compute()\\n\\n        # return observations\\n        return self.obs_buf, self.extras\\n\\n    def step(self, action: torch.Tensor) -> tuple[VecEnvObs, dict]:\\n        \"\"\"Execute one time-step of the environment\\'s dynamics.\\n\\n        The environment steps forward at a fixed time-step, while the physics simulation is\\n        decimated at a lower time-step. This is to ensure that the simulation is stable. These two\\n        time-steps can be configured independently using the :attr:`ManagerBasedEnvCfg.decimation` (number of\\n        simulation steps per environment step) and the :attr:`ManagerBasedEnvCfg.sim.dt` (physics time-step).\\n        Based on these parameters, the environment time-step is computed as the product of the two.\\n\\n        Args:\\n            action: The actions to apply on the environment. Shape is (num_envs, action_dim).\\n\\n        Returns:\\n            A tuple containing the observations and extras.\\n        \"\"\"\\n        # process actions\\n        self.action_manager.process_action(action.to(self.device))\\n\\n        self.recorder_manager.record_pre_step()\\n\\n        # check if we need to do rendering within the physics loop\\n        # note: checked here once to avoid multiple checks within the loop\\n        is_rendering = self.sim.has_gui() or self.sim.has_rtx_sensors()\\n\\n        # perform physics stepping\\n        for _ in range(self.cfg.decimation):\\n            self._sim_step_counter += 1\\n            # set actions into buffers\\n            self.action_manager.apply_action()\\n            # set actions into simulator\\n            self.scene.write_data_to_sim()\\n            # simulate\\n            self.sim.step(render=False)\\n            # render between steps only if the GUI or an RTX sensor needs it\\n            # note: we assume the render interval to be the shortest accepted rendering interval.\\n            #    If a camera needs rendering at a faster frequency, this will lead to unexpected behavior.\\n            if self._sim_step_counter % self.cfg.sim.render_interval == 0 and is_rendering:\\n                self.sim.render()\\n            # update buffers at sim dt\\n            self.scene.update(dt=self.physics_dt)\\n\\n        # post-step: step interval event\\n        if \"interval\" in self.event_manager.available_modes:\\n            self.event_manager.apply(mode=\"interval\", dt=self.step_dt)\\n\\n        # -- compute observations\\n        self.obs_buf = self.observation_manager.compute()\\n        self.recorder_manager.record_post_step()\\n\\n        # return observations and extras\\n        return self.obs_buf, self.extras\\n\\n    @staticmethod\\n    def seed(seed: int = -1) -> int:\\n        \"\"\"Set the seed for the environment.\\n\\n        Args:\\n            seed: The seed for random generator. Defaults to -1.\\n\\n        Returns:\\n            The seed used for random generator.\\n        \"\"\"\\n        # set seed for replicator\\n        try:\\n            import omni.replicator.core as rep\\n\\n            rep.set_global_seed(seed)\\n        except ModuleNotFoundError:\\n            pass\\n        # set seed for torch and other libraries\\n        return torch_utils.set_seed(seed)\\n\\n    def close(self):\\n        \"\"\"Cleanup for the environment.\"\"\"\\n        if not self._is_closed:\\n            # destructor is order-sensitive\\n            del self.viewport_camera_controller\\n            del self.action_manager\\n            del self.observation_manager\\n            del self.event_manager\\n            del self.recorder_manager\\n            del self.scene\\n            # clear callbacks and instance\\n            self.sim.clear_all_callbacks()\\n            self.sim.clear_instance()\\n            # destroy the window\\n            if self._window is not None:\\n                self._window = None\\n            # update closing status\\n            self._is_closed = True\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _reset_idx(self, env_ids: Sequence[int]):\\n        \"\"\"Reset environments based on specified indices.\\n\\n        Args:\\n            env_ids: List of environment ids which must be reset\\n        \"\"\"\\n        # reset the internal buffers of the scene elements\\n        self.scene.reset(env_ids)\\n\\n        # apply events such as randomization for environments that need a reset\\n        if \"reset\" in self.event_manager.available_modes:\\n            env_step_count = self._sim_step_counter // self.cfg.decimation\\n            self.event_manager.apply(mode=\"reset\", env_ids=env_ids, global_env_step_count=env_step_count)\\n\\n        # iterate over all managers and reset them\\n        # this returns a dictionary of information which is stored in the extras\\n        # note: This is order-sensitive! Certain things need be reset before others.\\n        self.extras[\"log\"] = dict()\\n        # -- observation manager\\n        info = self.observation_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- action manager\\n        info = self.action_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- event manager\\n        info = self.event_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- recorder manager\\n        info = self.recorder_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)'),\n",
       " Document(metadata={}, page_content='class DefaultEventManagerCfg:\\n    \"\"\"Configuration of the default event manager.\\n\\n    This manager is used to reset the scene to a default state. The default state is specified\\n    by the scene configuration.\\n    \"\"\"\\n\\n    reset_scene_to_default = EventTerm(func=mdp.reset_scene_to_default, mode=\"reset\")'),\n",
       " Document(metadata={}, page_content='class ManagerBasedEnvCfg:\\n    \"\"\"Base configuration of the environment.\"\"\"\\n\\n    # simulation settings\\n    viewer: ViewerCfg = ViewerCfg()\\n    \"\"\"Viewer configuration. Default is ViewerCfg().\"\"\"\\n\\n    sim: SimulationCfg = SimulationCfg()\\n    \"\"\"Physics simulation configuration. Default is SimulationCfg().\"\"\"\\n\\n    # ui settings\\n    ui_window_class_type: type | None = BaseEnvWindow\\n    \"\"\"The class type of the UI window. Default is None.\\n\\n    If None, then no UI window is created.\\n\\n    Note:\\n        If you want to make your own UI window, you can create a class that inherits from\\n        from :class:`isaaclab.envs.ui.base_env_window.BaseEnvWindow`. Then, you can set\\n        this attribute to your class type.\\n    \"\"\"\\n\\n    # general settings\\n    seed: int | None = None\\n    \"\"\"The seed for the random number generator. Defaults to None, in which case the seed is not set.\\n\\n    Note:\\n      The seed is set at the beginning of the environment initialization. This ensures that the environment\\n      creation is deterministic and behaves similarly across different runs.\\n    \"\"\"\\n\\n    decimation: int = MISSING\\n    \"\"\"Number of control action updates @ sim dt per policy dt.\\n\\n    For instance, if the simulation dt is 0.01s and the policy dt is 0.1s, then the decimation is 10.\\n    This means that the control action is updated every 10 simulation steps.\\n    \"\"\"\\n\\n    # environment settings\\n    scene: InteractiveSceneCfg = MISSING\\n    \"\"\"Scene settings.\\n\\n    Please refer to the :class:`isaaclab.scene.InteractiveSceneCfg` class for more details.\\n    \"\"\"\\n\\n    recorders: object = DefaultEmptyRecorderManagerCfg()\\n    \"\"\"Recorder settings. Defaults to recording nothing.\\n\\n    Please refer to the :class:`isaaclab.managers.RecorderManager` class for more details.\\n    \"\"\"\\n\\n    observations: object = MISSING\\n    \"\"\"Observation space settings.\\n\\n    Please refer to the :class:`isaaclab.managers.ObservationManager` class for more details.\\n    \"\"\"\\n\\n    actions: object = MISSING\\n    \"\"\"Action space settings.\\n\\n    Please refer to the :class:`isaaclab.managers.ActionManager` class for more details.\\n    \"\"\"\\n\\n    events: object = DefaultEventManagerCfg()\\n    \"\"\"Event settings. Defaults to the basic configuration that resets the scene to its default state.\\n\\n    Please refer to the :class:`isaaclab.managers.EventManager` class for more details.\\n    \"\"\"\\n\\n    rerender_on_reset: bool = False\\n    \"\"\"Whether a render step is performed again after at least one environment has been reset.\\n    Defaults to False, which means no render step will be performed after reset.\\n\\n    * When this is False, data collected from sensors after performing reset will be stale and will not reflect the\\n      latest states in simulation caused by the reset.\\n    * When this is True, an extra render step will be performed to update the sensor data\\n      to reflect the latest states from the reset. This comes at a cost of performance as an additional render\\n      step will be performed after each time an environment is reset.\\n\\n    \"\"\"\\n\\n    wait_for_textures: bool = True\\n    \"\"\"True to wait for assets to be loaded completely, False otherwise. Defaults to True.\"\"\"\\n\\n    xr: XrCfg | None = None\\n    \"\"\"Configuration for viewing and interacting with the environment through an XR device.\"\"\"'),\n",
       " Document(metadata={}, page_content='class ManagerBasedRLEnv(ManagerBasedEnv, gym.Env):\\n    \"\"\"The superclass for the manager-based workflow reinforcement learning-based environments.\\n\\n    This class inherits from :class:`ManagerBasedEnv` and implements the core functionality for\\n    reinforcement learning-based environments. It is designed to be used with any RL\\n    library. The class is designed to be used with vectorized environments, i.e., the\\n    environment is expected to be run in parallel with multiple sub-environments. The\\n    number of sub-environments is specified using the ``num_envs``.\\n\\n    Each observation from the environment is a batch of observations for each sub-\\n    environments. The method :meth:`step` is also expected to receive a batch of actions\\n    for each sub-environment.\\n\\n    While the environment itself is implemented as a vectorized environment, we do not\\n    inherit from :class:`gym.vector.VectorEnv`. This is mainly because the class adds\\n    various methods (for wait and asynchronous updates) which are not required.\\n    Additionally, each RL library typically has its own definition for a vectorized\\n    environment. Thus, to reduce complexity, we directly use the :class:`gym.Env` over\\n    here and leave it up to library-defined wrappers to take care of wrapping this\\n    environment for their agents.\\n\\n    Note:\\n        For vectorized environments, it is recommended to **only** call the :meth:`reset`\\n        method once before the first call to :meth:`step`, i.e. after the environment is created.\\n        After that, the :meth:`step` function handles the reset of terminated sub-environments.\\n        This is because the simulator does not support resetting individual sub-environments\\n        in a vectorized environment.\\n\\n    \"\"\"\\n\\n    is_vector_env: ClassVar[bool] = True\\n    \"\"\"Whether the environment is a vectorized environment.\"\"\"\\n    metadata: ClassVar[dict[str, Any]] = {\\n        \"render_modes\": [None, \"human\", \"rgb_array\"],\\n        \"isaac_sim_version\": get_version(),\\n    }\\n    \"\"\"Metadata for the environment.\"\"\"\\n\\n    cfg: ManagerBasedRLEnvCfg\\n    \"\"\"Configuration for the environment.\"\"\"\\n\\n    def __init__(self, cfg: ManagerBasedRLEnvCfg, render_mode: str | None = None, **kwargs):\\n        \"\"\"Initialize the environment.\\n\\n        Args:\\n            cfg: The configuration for the environment.\\n            render_mode: The render mode for the environment. Defaults to None, which\\n                is similar to ``\"human\"``.\\n        \"\"\"\\n        # -- counter for curriculum\\n        self.common_step_counter = 0\\n\\n        # initialize the episode length buffer BEFORE loading the managers to use it in mdp functions.\\n        self.episode_length_buf = torch.zeros(cfg.scene.num_envs, device=cfg.sim.device, dtype=torch.long)\\n\\n        # initialize the base class to setup the scene.\\n        super().__init__(cfg=cfg)\\n        # store the render mode\\n        self.render_mode = render_mode\\n\\n        # initialize data and constants\\n        # -- set the framerate of the gym video recorder wrapper so that the playback speed of the produced video matches the simulation\\n        self.metadata[\"render_fps\"] = 1 / self.step_dt\\n\\n        print(\"[INFO]: Completed setting up the environment...\")\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def max_episode_length_s(self) -> float:\\n        \"\"\"Maximum episode length in seconds.\"\"\"\\n        return self.cfg.episode_length_s\\n\\n    @property\\n    def max_episode_length(self) -> int:\\n        \"\"\"Maximum episode length in environment steps.\"\"\"\\n        return math.ceil(self.max_episode_length_s / self.step_dt)\\n\\n    \"\"\"\\n    Operations - Setup.\\n    \"\"\"\\n\\n    def load_managers(self):\\n        # note: this order is important since observation manager needs to know the command and action managers\\n        # and the reward manager needs to know the termination manager\\n        # -- command manager\\n        self.command_manager: CommandManager = CommandManager(self.cfg.commands, self)\\n        print(\"[INFO] Command Manager: \", self.command_manager)\\n\\n        # call the parent class to load the managers for observations and actions.\\n        super().load_managers()\\n\\n        # prepare the managers\\n        # -- termination manager\\n        self.termination_manager = TerminationManager(self.cfg.terminations, self)\\n        print(\"[INFO] Termination Manager: \", self.termination_manager)\\n        # -- reward manager\\n        self.reward_manager = RewardManager(self.cfg.rewards, self)\\n        print(\"[INFO] Reward Manager: \", self.reward_manager)\\n        # -- curriculum manager\\n        self.curriculum_manager = CurriculumManager(self.cfg.curriculum, self)\\n        print(\"[INFO] Curriculum Manager: \", self.curriculum_manager)\\n\\n        # setup the action and observation spaces for Gym\\n        self._configure_gym_env_spaces()\\n\\n        # perform events at the start of the simulation\\n        if \"startup\" in self.event_manager.available_modes:\\n            self.event_manager.apply(mode=\"startup\")\\n\\n    def setup_manager_visualizers(self):\\n        \"\"\"Creates live visualizers for manager terms.\"\"\"\\n\\n        self.manager_visualizers = {\\n            \"action_manager\": ManagerLiveVisualizer(manager=self.action_manager),\\n            \"observation_manager\": ManagerLiveVisualizer(manager=self.observation_manager),\\n            \"command_manager\": ManagerLiveVisualizer(manager=self.command_manager),\\n            \"termination_manager\": ManagerLiveVisualizer(manager=self.termination_manager),\\n            \"reward_manager\": ManagerLiveVisualizer(manager=self.reward_manager),\\n            \"curriculum_manager\": ManagerLiveVisualizer(manager=self.curriculum_manager),\\n        }\\n\\n    \"\"\"\\n    Operations - MDP\\n    \"\"\"\\n\\n    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\\n        \"\"\"Execute one time-step of the environment\\'s dynamics and reset terminated environments.\\n\\n        Unlike the :class:`ManagerBasedEnv.step` class, the function performs the following operations:\\n\\n        1. Process the actions.\\n        2. Perform physics stepping.\\n        3. Perform rendering if gui is enabled.\\n        4. Update the environment counters and compute the rewards and terminations.\\n        5. Reset the environments that terminated.\\n        6. Compute the observations.\\n        7. Return the observations, rewards, resets and extras.\\n\\n        Args:\\n            action: The actions to apply on the environment. Shape is (num_envs, action_dim).\\n\\n        Returns:\\n            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\\n        \"\"\"\\n        # process actions\\n        self.action_manager.process_action(action.to(self.device))\\n\\n        self.recorder_manager.record_pre_step()\\n\\n        # check if we need to do rendering within the physics loop\\n        # note: checked here once to avoid multiple checks within the loop\\n        is_rendering = self.sim.has_gui() or self.sim.has_rtx_sensors()\\n\\n        # perform physics stepping\\n        for _ in range(self.cfg.decimation):\\n            self._sim_step_counter += 1\\n            # set actions into buffers\\n            self.action_manager.apply_action()\\n            # set actions into simulator\\n            self.scene.write_data_to_sim()\\n            # simulate\\n            self.sim.step(render=False)\\n            # render between steps only if the GUI or an RTX sensor needs it\\n            # note: we assume the render interval to be the shortest accepted rendering interval.\\n            #    If a camera needs rendering at a faster frequency, this will lead to unexpected behavior.\\n            if self._sim_step_counter % self.cfg.sim.render_interval == 0 and is_rendering:\\n                self.sim.render()\\n            # update buffers at sim dt\\n            self.scene.update(dt=self.physics_dt)\\n\\n        # post-step:\\n        # -- update env counters (used for curriculum generation)\\n        self.episode_length_buf += 1  # step in current episode (per env)\\n        self.common_step_counter += 1  # total step (common for all envs)\\n        # -- check terminations\\n        self.reset_buf = self.termination_manager.compute()\\n        self.reset_terminated = self.termination_manager.terminated\\n        self.reset_time_outs = self.termination_manager.time_outs\\n        # -- reward computation\\n        self.reward_buf = self.reward_manager.compute(dt=self.step_dt)\\n\\n        if len(self.recorder_manager.active_terms) > 0:\\n            # update observations for recording if needed\\n            self.obs_buf = self.observation_manager.compute()\\n            self.recorder_manager.record_post_step()\\n\\n        # -- reset envs that terminated/timed-out and log the episode information\\n        reset_env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)\\n        if len(reset_env_ids) > 0:\\n            # trigger recorder terms for pre-reset calls\\n            self.recorder_manager.record_pre_reset(reset_env_ids)\\n\\n            self._reset_idx(reset_env_ids)\\n            # update articulation kinematics\\n            self.scene.write_data_to_sim()\\n            self.sim.forward()\\n\\n            # if sensors are added to the scene, make sure we render to reflect changes in reset\\n            if self.sim.has_rtx_sensors() and self.cfg.rerender_on_reset:\\n                self.sim.render()\\n\\n            # trigger recorder terms for post-reset calls\\n            self.recorder_manager.record_post_reset(reset_env_ids)\\n\\n        # -- update command\\n        self.command_manager.compute(dt=self.step_dt)\\n        # -- step interval events\\n        if \"interval\" in self.event_manager.available_modes:\\n            self.event_manager.apply(mode=\"interval\", dt=self.step_dt)\\n        # -- compute observations\\n        # note: done after reset to get the correct observations for reset envs\\n        self.obs_buf = self.observation_manager.compute()\\n\\n        # return observations, rewards, resets and extras\\n        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\\n\\n    def render(self, recompute: bool = False) -> np.ndarray | None:\\n        \"\"\"Run rendering without stepping through the physics.\\n\\n        By convention, if mode is:\\n\\n        - **human**: Render to the current display and return nothing. Usually for human consumption.\\n        - **rgb_array**: Return an numpy.ndarray with shape (x, y, 3), representing RGB values for an\\n          x-by-y pixel image, suitable for turning into a video.\\n\\n        Args:\\n            recompute: Whether to force a render even if the simulator has already rendered the scene.\\n                Defaults to False.\\n\\n        Returns:\\n            The rendered image as a numpy array if mode is \"rgb_array\". Otherwise, returns None.\\n\\n        Raises:\\n            RuntimeError: If mode is set to \"rgb_data\" and simulation render mode does not support it.\\n                In this case, the simulation render mode must be set to ``RenderMode.PARTIAL_RENDERING``\\n                or ``RenderMode.FULL_RENDERING``.\\n            NotImplementedError: If an unsupported rendering mode is specified.\\n        \"\"\"\\n        # run a rendering step of the simulator\\n        # if we have rtx sensors, we do not need to render again sin\\n        if not self.sim.has_rtx_sensors() and not recompute:\\n            self.sim.render()\\n        # decide the rendering mode\\n        if self.render_mode == \"human\" or self.render_mode is None:\\n            return None\\n        elif self.render_mode == \"rgb_array\":\\n            # check that if any render could have happened\\n            if self.sim.render_mode.value < self.sim.RenderMode.PARTIAL_RENDERING.value:\\n                raise RuntimeError(\\n                    f\"Cannot render \\'{self.render_mode}\\' when the simulation render mode is\"\\n                    f\" \\'{self.sim.render_mode.name}\\'. Please set the simulation render mode to:\"\\n                    f\"\\'{self.sim.RenderMode.PARTIAL_RENDERING.name}\\' or \\'{self.sim.RenderMode.FULL_RENDERING.name}\\'.\"\\n                    \" If running headless, make sure --enable_cameras is set.\"\\n                )\\n            # create the annotator if it does not exist\\n            if not hasattr(self, \"_rgb_annotator\"):\\n                import omni.replicator.core as rep\\n\\n                # create render product\\n                self._render_product = rep.create.render_product(\\n                    self.cfg.viewer.cam_prim_path, self.cfg.viewer.resolution\\n                )\\n                # create rgb annotator -- used to read data from the render product\\n                self._rgb_annotator = rep.AnnotatorRegistry.get_annotator(\"rgb\", device=\"cpu\")\\n                self._rgb_annotator.attach([self._render_product])\\n            # obtain the rgb data\\n            rgb_data = self._rgb_annotator.get_data()\\n            # convert to numpy array\\n            rgb_data = np.frombuffer(rgb_data, dtype=np.uint8).reshape(*rgb_data.shape)\\n            # return the rgb data\\n            # note: initially the renerer is warming up and returns empty data\\n            if rgb_data.size == 0:\\n                return np.zeros((self.cfg.viewer.resolution[1], self.cfg.viewer.resolution[0], 3), dtype=np.uint8)\\n            else:\\n                return rgb_data[:, :, :3]\\n        else:\\n            raise NotImplementedError(\\n                f\"Render mode \\'{self.render_mode}\\' is not supported. Please use: {self.metadata[\\'render_modes\\']}.\"\\n            )\\n\\n    def close(self):\\n        if not self._is_closed:\\n            # destructor is order-sensitive\\n            del self.command_manager\\n            del self.reward_manager\\n            del self.termination_manager\\n            del self.curriculum_manager\\n            # call the parent class to close the environment\\n            super().close()\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _configure_gym_env_spaces(self):\\n        \"\"\"Configure the action and observation spaces for the Gym environment.\"\"\"\\n        # observation space (unbounded since we don\\'t impose any limits)\\n        self.single_observation_space = gym.spaces.Dict()\\n        for group_name, group_term_names in self.observation_manager.active_terms.items():\\n            # extract quantities about the group\\n            has_concatenated_obs = self.observation_manager.group_obs_concatenate[group_name]\\n            group_dim = self.observation_manager.group_obs_dim[group_name]\\n            # check if group is concatenated or not\\n            # if not concatenated, then we need to add each term separately as a dictionary\\n            if has_concatenated_obs:\\n                self.single_observation_space[group_name] = gym.spaces.Box(low=-np.inf, high=np.inf, shape=group_dim)\\n            else:\\n                self.single_observation_space[group_name] = gym.spaces.Dict({\\n                    term_name: gym.spaces.Box(low=-np.inf, high=np.inf, shape=term_dim)\\n                    for term_name, term_dim in zip(group_term_names, group_dim)\\n                })\\n        # action space (unbounded since we don\\'t impose any limits)\\n        action_dim = sum(self.action_manager.action_term_dim)\\n        self.single_action_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(action_dim,))\\n\\n        # batch the spaces for vectorized environments\\n        self.observation_space = gym.vector.utils.batch_space(self.single_observation_space, self.num_envs)\\n        self.action_space = gym.vector.utils.batch_space(self.single_action_space, self.num_envs)\\n\\n    def _reset_idx(self, env_ids: Sequence[int]):\\n        \"\"\"Reset environments based on specified indices.\\n\\n        Args:\\n            env_ids: List of environment ids which must be reset\\n        \"\"\"\\n        # update the curriculum for environments that need a reset\\n        self.curriculum_manager.compute(env_ids=env_ids)\\n        # reset the internal buffers of the scene elements\\n        self.scene.reset(env_ids)\\n        # apply events such as randomizations for environments that need a reset\\n        if \"reset\" in self.event_manager.available_modes:\\n            env_step_count = self._sim_step_counter // self.cfg.decimation\\n            self.event_manager.apply(mode=\"reset\", env_ids=env_ids, global_env_step_count=env_step_count)\\n\\n        # iterate over all managers and reset them\\n        # this returns a dictionary of information which is stored in the extras\\n        # note: This is order-sensitive! Certain things need be reset before others.\\n        self.extras[\"log\"] = dict()\\n        # -- observation manager\\n        info = self.observation_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- action manager\\n        info = self.action_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- rewards manager\\n        info = self.reward_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- curriculum manager\\n        info = self.curriculum_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- command manager\\n        info = self.command_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- event manager\\n        info = self.event_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- termination manager\\n        info = self.termination_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n        # -- recorder manager\\n        info = self.recorder_manager.reset(env_ids)\\n        self.extras[\"log\"].update(info)\\n\\n        # reset the episode length buffer\\n        self.episode_length_buf[env_ids] = 0'),\n",
       " Document(metadata={}, page_content='class ManagerBasedRLEnvCfg(ManagerBasedEnvCfg):\\n    \"\"\"Configuration for a reinforcement learning environment with the manager-based workflow.\"\"\"\\n\\n    # ui settings\\n    ui_window_class_type: type | None = ManagerBasedRLEnvWindow\\n\\n    # general settings\\n    is_finite_horizon: bool = False\\n    \"\"\"Whether the learning task is treated as a finite or infinite horizon problem for the agent.\\n    Defaults to False, which means the task is treated as an infinite horizon problem.\\n\\n    This flag handles the subtleties of finite and infinite horizon tasks:\\n\\n    * **Finite horizon**: no penalty or bootstrapping value is required by the the agent for\\n      running out of time. However, the environment still needs to terminate the episode after the\\n      time limit is reached.\\n    * **Infinite horizon**: the agent needs to bootstrap the value of the state at the end of the episode.\\n      This is done by sending a time-limit (or truncated) done signal to the agent, which triggers this\\n      bootstrapping calculation.\\n\\n    If True, then the environment is treated as a finite horizon problem and no time-out (or truncated) done signal\\n    is sent to the agent. If False, then the environment is treated as an infinite horizon problem and a time-out\\n    (or truncated) done signal is sent to the agent.\\n\\n    Note:\\n        The base :class:`ManagerBasedRLEnv` class does not use this flag directly. It is used by the environment\\n        wrappers to determine what type of done signal to send to the corresponding learning agent.\\n    \"\"\"\\n\\n    episode_length_s: float = MISSING\\n    \"\"\"Duration of an episode (in seconds).\\n\\n    Based on the decimation rate and physics time step, the episode length is calculated as:\\n\\n    .. code-block:: python\\n\\n        episode_length_steps = ceil(episode_length_s / (decimation_rate * physics_time_step))\\n\\n    For example, if the decimation rate is 10, the physics time step is 0.01, and the episode length is 10 seconds,\\n    then the episode length in steps is 100.\\n    \"\"\"\\n\\n    # environment settings\\n    rewards: object = MISSING\\n    \"\"\"Reward settings.\\n\\n    Please refer to the :class:`isaaclab.managers.RewardManager` class for more details.\\n    \"\"\"\\n\\n    terminations: object = MISSING\\n    \"\"\"Termination settings.\\n\\n    Please refer to the :class:`isaaclab.managers.TerminationManager` class for more details.\\n    \"\"\"\\n\\n    curriculum: object | None = None\\n    \"\"\"Curriculum settings. Defaults to None, in which case no curriculum is applied.\\n\\n    Please refer to the :class:`isaaclab.managers.CurriculumManager` class for more details.\\n    \"\"\"\\n\\n    commands: object | None = None\\n    \"\"\"Command settings. Defaults to None, in which case no commands are generated.\\n\\n    Please refer to the :class:`isaaclab.managers.CommandManager` class for more details.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ManagerBasedRLMimicEnv(ManagerBasedRLEnv):\\n    \"\"\"The superclass for the Isaac Lab Mimic environments.\\n\\n    This class inherits from :class:`ManagerBasedRLEnv` and provides a template for the functions that\\n    need to be defined to run the Isaac Lab Mimic data generation workflow. The Isaac Lab data generation\\n    pipeline, inspired by the MimicGen system, enables the generation of new datasets based on a few human\\n    collected demonstrations. MimicGen is a novel approach designed to automatically synthesize large-scale,\\n    rich datasets from a sparse set of human demonstrations by adapting them to new contexts. It manages to\\n    replicate the benefits of large datasets while reducing the immense time and effort usually required to\\n    gather extensive human demonstrations.\\n\\n    The MimicGen system works by parsing demonstrations into object-centric segments. It then adapts\\n    these segments to new scenes by transforming each segment according to the new scene’s context, stitching\\n    them into a coherent trajectory for a robotic end-effector to execute. This approach allows learners to train\\n    proficient agents through imitation learning on diverse configurations of scenes, object instances, etc.\\n\\n    Key Features:\\n        - Efficient Dataset Generation: Utilizes a small set of human demos to produce large scale demonstrations.\\n        - Broad Applicability: Capable of supporting tasks that require a range of manipulation skills, such as\\n          pick-and-place and interacting with articulated objects.\\n        - Dataset Versatility: The synthetic data retains a quality that compares favorably with additional human demos.\\n    \"\"\"\\n\\n    def get_robot_eef_pose(self, eef_name: str, env_ids: Sequence[int] | None = None) -> torch.Tensor:\\n        \"\"\"\\n        Get current robot end effector pose. Should be the same frame as used by the robot end-effector controller.\\n\\n        Args:\\n            eef_name: Name of the end effector.\\n            env_ids: Environment indices to get the pose for. If None, all envs are considered.\\n\\n        Returns:\\n            A torch.Tensor eef pose matrix. Shape is (len(env_ids), 4, 4)\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def target_eef_pose_to_action(\\n        self,\\n        target_eef_pose_dict: dict,\\n        gripper_action_dict: dict,\\n        action_noise_dict: dict | None = None,\\n        env_id: int = 0,\\n    ) -> torch.Tensor:\\n        \"\"\"\\n        Takes a target pose and gripper action for the end effector controller and returns an action\\n        (usually a normalized delta pose action) to try and achieve that target pose.\\n        Noise is added to the target pose action if specified.\\n\\n        Args:\\n            target_eef_pose_dict: Dictionary of 4x4 target eef pose for each end-effector.\\n            gripper_action_dict: Dictionary of gripper actions for each end-effector.\\n            action_noise_dict: Noise to add to the action. If None, no noise is added.\\n            env_id: Environment index to compute the action for.\\n\\n        Returns:\\n            An action torch.Tensor that\\'s compatible with env.step().\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def action_to_target_eef_pose(self, action: torch.Tensor) -> dict[str, torch.Tensor]:\\n        \"\"\"\\n        Converts action (compatible with env.step) to a target pose for the end effector controller.\\n        Inverse of @target_eef_pose_to_action. Usually used to infer a sequence of target controller poses\\n        from a demonstration trajectory using the recorded actions.\\n\\n        Args:\\n            action: Environment action. Shape is (num_envs, action_dim).\\n\\n        Returns:\\n            A dictionary of eef pose torch.Tensor that @action corresponds to.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def actions_to_gripper_actions(self, actions: torch.Tensor) -> dict[str, torch.Tensor]:\\n        \"\"\"\\n        Extracts the gripper actuation part from a sequence of env actions (compatible with env.step).\\n\\n        Args:\\n            actions: environment actions. The shape is (num_envs, num steps in a demo, action_dim).\\n\\n        Returns:\\n            A dictionary of torch.Tensor gripper actions. Key to each dict is an eef_name.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def get_object_poses(self, env_ids: Sequence[int] | None = None):\\n        \"\"\"\\n        Gets the pose of each object relevant to Isaac Lab Mimic data generation in the current scene.\\n\\n        Args:\\n            env_ids: Environment indices to get the pose for. If None, all envs are considered.\\n\\n        Returns:\\n            A dictionary that maps object names to object pose matrix (4x4 torch.Tensor)\\n        \"\"\"\\n        if env_ids is None:\\n            env_ids = slice(None)\\n\\n        rigid_object_states = self.scene.get_state(is_relative=True)[\"rigid_object\"]\\n        object_pose_matrix = dict()\\n        for obj_name, obj_state in rigid_object_states.items():\\n            object_pose_matrix[obj_name] = PoseUtils.make_pose(\\n                obj_state[\"root_pose\"][env_ids, :3], PoseUtils.matrix_from_quat(obj_state[\"root_pose\"][env_ids, 3:7])\\n            )\\n        return object_pose_matrix\\n\\n    def get_subtask_term_signals(self, env_ids: Sequence[int] | None = None) -> dict[str, torch.Tensor]:\\n        \"\"\"\\n        Gets a dictionary of termination signal flags for each subtask in a task. The flag is 1\\n        when the subtask has been completed and 0 otherwise. The implementation of this method is\\n        required if intending to enable automatic subtask term signal annotation when running the\\n        dataset annotation tool. This method can be kept unimplemented if intending to use manual\\n        subtask term signal annotation.\\n\\n        Args:\\n            env_ids: Environment indices to get the termination signals for. If None, all envs are considered.\\n\\n        Returns:\\n            A dictionary termination signal flags (False or True) for each subtask.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def serialize(self):\\n        \"\"\"\\n        Save all information needed to re-instantiate this environment in a dictionary.\\n        This is the same as @env_meta - environment metadata stored in hdf5 datasets,\\n        and used in utils/env_utils.py.\\n        \"\"\"\\n        return dict(env_name=self.spec.id, type=2, env_kwargs=dict())'),\n",
       " Document(metadata={}, page_content='class DataGenConfig:\\n    \"\"\"Configuration settings for data generation processes within the Isaac Lab Mimic environment.\"\"\"\\n\\n    name: str = \"demo\"\\n    \"\"\"The name of the data generation process. Defaults to \"demo\".\"\"\"\\n\\n    generation_guarantee: bool = True\\n    \"\"\"Whether to retry generation until generation_num_trials successful demos have been generated.\\n\\n    If True, generation will be retried until generation_num_trials successful demos are created.\\n    If False, generation will stop after generation_num_trails, regardless of success.\\n    \"\"\"\\n\\n    generation_keep_failed: bool = False\\n    \"\"\"Whether to keep failed generation trials.\\n\\n    Keeping failed demonstrations is useful for visualizing and debugging low success rates.\\n    \"\"\"\\n\\n    max_num_failures: int = 50\\n    \"\"\"Maximum number of failures allowed before stopping generation.\"\"\"\\n\\n    seed: int = 1\\n    \"\"\"Seed for randomization to ensure reproducibility.\"\"\"\\n\\n    \"\"\"The following configuration values can be changed on the command line, and only serve as defaults.\"\"\"\\n\\n    source_dataset_path: str = None\\n    \"\"\"Path to the source dataset for mimic generation.\"\"\"\\n\\n    generation_path: str = None\\n    \"\"\"Path where the generated data will be saved.\"\"\"\\n\\n    generation_num_trials: int = 10\\n    \"\"\"Number of trials to be generated.\"\"\"\\n\\n    task_name: str = None\\n    \"\"\"Name of the task being configured.\"\"\"\\n\\n    \"\"\"The following configurations are advanced and do not usually need to be changed.\"\"\"\\n\\n    generation_select_src_per_subtask: bool = False\\n    \"\"\"Whether to select source data per subtask.\\n\\n    Note:\\n        This requires subtasks to be properly temporally constrained, and may require\\n        additional subtasks to allow for time synchronization.\\n    \"\"\"\\n\\n    generation_select_src_per_arm: bool = False\\n    \"\"\"Whether to select source data per arm.\"\"\"\\n\\n    generation_transform_first_robot_pose: bool = False\\n    \"\"\"Whether to transform the first robot pose during generation.\"\"\"\\n\\n    generation_interpolate_from_last_target_pose: bool = True\\n    \"\"\"Whether to interpolate from last target pose.\"\"\"'),\n",
       " Document(metadata={}, page_content='class SubTaskConfig:\\n    \"\"\"\\n    Configuration settings for specifying subtasks used in Mimic environments.\\n    \"\"\"\\n\\n    \"\"\"Mandatory options that should be defined for every subtask.\"\"\"\\n\\n    object_ref: str = None\\n    \"\"\"Reference to the object involved in this subtask.\\n\\n    Set to None if no object is involved (this is rarely the case).\\n    \"\"\"\\n\\n    subtask_term_signal: str = None\\n    \"\"\"Subtask termination signal name.\"\"\"\\n\\n    \"\"\"Advanced options for tuning the generation results.\"\"\"\\n\\n    selection_strategy: str = \"random\"\\n    \"\"\"Strategy for selecting a subtask segment.\\n\\n    Can be one of:\\n        * \\'random\\'\\n        * \\'nearest_neighbor_object\\'\\n        * \\'nearest_neighbor_robot_distance\\'\\n\\n    Note:\\n        For \\'nearest_neighbor_object\\' and \\'nearest_neighbor_robot_distance\\', the subtask needs\\n        to have \\'object_ref\\' set to a value other than \\'None\\'. These strategies typically yield\\n        higher success rates than the default \\'random\\' strategy when object_ref is set.\\n    \"\"\"\\n\\n    selection_strategy_kwargs: dict = {}\\n    \"\"\"Additional arguments to the selected strategy. See details on each strategy in\\n    source/isaaclab_mimic/isaaclab_mimic/datagen/selection_strategy.py\\n    Arguments will be passed through to the `select_source_demo` method.\"\"\"\\n\\n    first_subtask_start_offset_range: tuple = (0, 0)\\n    \"\"\"Range for start offset of the first subtask.\"\"\"\\n\\n    subtask_term_offset_range: tuple = (0, 0)\\n    \"\"\"Range for offsetting subtask termination.\"\"\"\\n\\n    action_noise: float = 0.03\\n    \"\"\"Amplitude of action noise applied.\"\"\"\\n\\n    num_interpolation_steps: int = 5\\n    \"\"\"Number of steps for interpolation between waypoints.\"\"\"\\n\\n    num_fixed_steps: int = 0\\n    \"\"\"Number of fixed steps for the subtask.\"\"\"\\n\\n    apply_noise_during_interpolation: bool = False\\n    \"\"\"Whether to apply noise during interpolation.\"\"\"\\n\\n    description: str = \"\"\\n    \"\"\"Description of the subtask\"\"\"\\n\\n    next_subtask_description: str = \"\"\\n    \"\"\"Instructions for the next subtask\"\"\"'),\n",
       " Document(metadata={}, page_content='class SubTaskConstraintType(enum.IntEnum):\\n    \"\"\"Enum for subtask constraint types.\"\"\"\\n\\n    SEQUENTIAL = 0\\n    COORDINATION = 1\\n\\n    _SEQUENTIAL_FORMER = 2\\n    _SEQUENTIAL_LATTER = 3'),\n",
       " Document(metadata={}, page_content='class SubTaskConstraintCoordinationScheme(enum.IntEnum):\\n    \"\"\"Enum for coordination schemes.\"\"\"\\n\\n    REPLAY = 0\\n    TRANSFORM = 1\\n    TRANSLATE = 2'),\n",
       " Document(metadata={}, page_content='class SubTaskConstraintConfig:\\n    \"\"\"\\n    Configuration settings for specifying subtask constraints used in multi-eef Mimic environments.\\n    \"\"\"\\n\\n    eef_subtask_constraint_tuple: list[tuple[str, int]] = ((\"\", 0), (\"\", 0))\\n    \"\"\"List of associated subtasks tuples in order.\\n\\n    The first element of the tuple refers to the eef name.\\n    The second element of the tuple refers to the subtask index of the eef.\\n    \"\"\"\\n\\n    constraint_type: SubTaskConstraintType = None\\n    \"\"\"Type of constraint to apply between subtasks.\"\"\"\\n\\n    sequential_min_time_diff: int = -1\\n    \"\"\"Minimum time difference between two sequential subtasks finishing.\\n\\n    The second subtask will execute until sequential_min_time_diff steps left in its subtask trajectory\\n    and wait until the first (preconditioned) subtask is finished to continue executing the rest.\\n    If set to -1, the second subtask will start only after the first subtask is finished.\\n    \"\"\"\\n\\n    coordination_scheme: SubTaskConstraintCoordinationScheme = SubTaskConstraintCoordinationScheme.REPLAY\\n    \"\"\"Scheme to use for coordinating subtasks.\"\"\"\\n\\n    coordination_scheme_pos_noise_scale: float = 0.0\\n    \"\"\"Scale of position noise to apply during coordination.\"\"\"\\n\\n    coordination_scheme_rot_noise_scale: float = 0.0\\n    \"\"\"Scale of rotation noise to apply during coordination.\"\"\"\\n\\n    coordination_synchronize_start: bool = False\\n    \"\"\"Whether subtasks should start at the same time.\"\"\"\\n\\n    def generate_runtime_subtask_constraints(self):\\n        \"\"\"\\n        Populate expanded task constraints dictionary based on the task constraint config.\\n        The task constraint config contains the configurations set by the user. While the\\n        task_constraints_dict contains flags used to implement the constraint logic in this class.\\n\\n        The task_constraint_configs may include the following types:\\n        - \"sequential\"\\n        - \"coordination\"\\n\\n        For a \"sequential\" constraint:\\n            - Data from task_constraint_configs is added to task_constraints_dict as \"sequential former\" task constraint.\\n            - The opposite constraint, of type \"sequential latter\", is also added to task_constraints_dict.\\n            - Additionally, a (\"fulfilled\", Bool) key-value pair is added to task_constraints_dict.\\n            - This is used to check if the precondition (i.e., the sequential former task) has been met.\\n            - Until the \"fulfilled\" flag in \"sequential latter\" is set by \"sequential former\",\\n                the \"sequential latter\" subtask will remain paused.\\n\\n        For a \"coordination\" constraint:\\n            - Data from task_constraint_configs is added to task_constraints_dict.\\n            - The opposite constraint, of type \"coordination\", is also added to task_constraints_dict.\\n            - The number of synchronous steps is set to the minimum of subtask_len and concurrent_subtask_len.\\n            - This ensures both concurrent tasks end at the same time step.\\n            - A \"selected_src_demo_ind\" and \"transform\" field are used to ensure the transforms used by both subtasks are the same.\\n        \"\"\"\\n        task_constraints_dict = dict()\\n        if self.constraint_type == SubTaskConstraintType.SEQUENTIAL:\\n            constrained_task_spec_key, constrained_subtask_ind = self.eef_subtask_constraint_tuple[1]\\n            assert isinstance(constrained_subtask_ind, int)\\n            pre_condition_task_spec_key, pre_condition_subtask_ind = self.eef_subtask_constraint_tuple[0]\\n            assert isinstance(pre_condition_subtask_ind, int)\\n            assert (\\n                constrained_task_spec_key,\\n                constrained_subtask_ind,\\n            ) not in task_constraints_dict, \"only one constraint per subtask allowed\"\\n            task_constraints_dict[(constrained_task_spec_key, constrained_subtask_ind)] = dict(\\n                type=SubTaskConstraintType._SEQUENTIAL_LATTER,\\n                pre_condition_task_spec_key=pre_condition_task_spec_key,\\n                pre_condition_subtask_ind=pre_condition_subtask_ind,\\n                min_time_diff=self.sequential_min_time_diff,\\n                fulfilled=False,\\n            )\\n            task_constraints_dict[(pre_condition_task_spec_key, pre_condition_subtask_ind)] = dict(\\n                type=SubTaskConstraintType._SEQUENTIAL_FORMER,\\n                constrained_task_spec_key=constrained_task_spec_key,\\n                constrained_subtask_ind=constrained_subtask_ind,\\n            )\\n        elif self.constraint_type == SubTaskConstraintType.COORDINATION:\\n            constrained_task_spec_key, constrained_subtask_ind = self.eef_subtask_constraint_tuple[0]\\n            assert isinstance(constrained_subtask_ind, int)\\n            concurrent_task_spec_key, concurrent_subtask_ind = self.eef_subtask_constraint_tuple[1]\\n            assert isinstance(concurrent_subtask_ind, int)\\n            if self.coordination_scheme is None:\\n                raise ValueError(\"Coordination scheme must be specified.\")\\n            assert (\\n                constrained_task_spec_key,\\n                constrained_subtask_ind,\\n            ) not in task_constraints_dict, \"only one constraint per subtask allowed\"\\n            task_constraints_dict[(constrained_task_spec_key, constrained_subtask_ind)] = dict(\\n                concurrent_task_spec_key=concurrent_task_spec_key,\\n                concurrent_subtask_ind=concurrent_subtask_ind,\\n                type=SubTaskConstraintType.COORDINATION,\\n                fulfilled=False,\\n                finished=False,\\n                selected_src_demo_ind=None,\\n                coordination_scheme=self.coordination_scheme,\\n                coordination_scheme_pos_noise_scale=self.coordination_scheme_pos_noise_scale,\\n                coordination_scheme_rot_noise_scale=self.coordination_scheme_rot_noise_scale,\\n                coordination_synchronize_start=self.coordination_synchronize_start,\\n                synchronous_steps=None,  # to be calculated at runtime\\n            )\\n            task_constraints_dict[(concurrent_task_spec_key, concurrent_subtask_ind)] = dict(\\n                concurrent_task_spec_key=constrained_task_spec_key,\\n                concurrent_subtask_ind=constrained_subtask_ind,\\n                type=SubTaskConstraintType.COORDINATION,\\n                fulfilled=False,\\n                finished=False,\\n                selected_src_demo_ind=None,\\n                coordination_scheme=self.coordination_scheme,\\n                coordination_scheme_pos_noise_scale=self.coordination_scheme_pos_noise_scale,\\n                coordination_scheme_rot_noise_scale=self.coordination_scheme_rot_noise_scale,\\n                coordination_synchronize_start=self.coordination_synchronize_start,\\n                synchronous_steps=None,  # to be calculated at runtime\\n            )\\n        else:\\n            raise ValueError(\"Constraint type not supported.\")\\n\\n        return task_constraints_dict'),\n",
       " Document(metadata={}, page_content='class MimicEnvCfg:\\n    \"\"\"\\n    Configuration class for the Mimic environment integration.\\n\\n    This class consolidates various configuration aspects for the\\n    Isaac Lab Mimic data generation pipeline.\\n    \"\"\"\\n\\n    # Overall configuration for the data generation\\n    datagen_config: DataGenConfig = DataGenConfig()\\n\\n    # Dictionary of list of subtask configurations for each end-effector.\\n    # Keys are end-effector names.\\n    subtask_configs: dict[str, list[SubTaskConfig]] = {}\\n\\n    # List of configurations for subtask constraints\\n    task_constraint_configs: list[SubTaskConstraintConfig] = []'),\n",
       " Document(metadata={}, page_content='def modify_reward_weight(env: ManagerBasedRLEnv, env_ids: Sequence[int], term_name: str, weight: float, num_steps: int):\\n    \"\"\"Curriculum that modifies a reward weight a given number of steps.\\n\\n    Args:\\n        env: The learning environment.\\n        env_ids: Not used since all environments are affected.\\n        term_name: The name of the reward term.\\n        weight: The weight of the reward term.\\n        num_steps: The number of steps after which the change should be applied.\\n    \"\"\"\\n    if env.common_step_counter > num_steps:\\n        # obtain term settings\\n        term_cfg = env.reward_manager.get_term_cfg(term_name)\\n        # update term settings\\n        term_cfg.weight = weight\\n        env.reward_manager.set_term_cfg(term_name, term_cfg)'),\n",
       " Document(metadata={}, page_content='def randomize_rigid_body_scale(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    scale_range: tuple[float, float] | dict[str, tuple[float, float]],\\n    asset_cfg: SceneEntityCfg,\\n    relative_child_path: str | None = None,\\n):\\n    \"\"\"Randomize the scale of a rigid body asset in the USD stage.\\n\\n    This function modifies the \"xformOp:scale\" property of all the prims corresponding to the asset.\\n\\n    It takes a tuple or dictionary for the scale ranges. If it is a tuple, then the scaling along\\n    individual axis is performed equally. If it is a dictionary, the scaling is independent across each dimension.\\n    The keys of the dictionary are ``x``, ``y``, and ``z``. The values are tuples of the form ``(min, max)``.\\n\\n    If the dictionary does not contain a key, the range is set to one for that axis.\\n\\n    Relative child path can be used to randomize the scale of a specific child prim of the asset.\\n    For example, if the asset at prim path expression \"/World/envs/env_.*/Object\" has a child\\n    with the path \"/World/envs/env_.*/Object/mesh\", then the relative child path should be \"mesh\" or\\n    \"/mesh\".\\n\\n    .. attention::\\n        Since this function modifies USD properties that are parsed by the physics engine once the simulation\\n        starts, the term should only be used before the simulation starts playing. This corresponds to the\\n        event mode named \"usd\". Using it at simulation time, may lead to unpredictable behaviors.\\n\\n    .. note::\\n        When randomizing the scale of individual assets, please make sure to set\\n        :attr:`isaaclab.scene.InteractiveSceneCfg.replicate_physics` to False. This ensures that physics\\n        parser will parse the individual asset properties separately.\\n    \"\"\"\\n    # check if sim is running\\n    if env.sim.is_playing():\\n        raise RuntimeError(\\n            \"Randomizing scale while simulation is running leads to unpredictable behaviors.\"\\n            \" Please ensure that the event term is called before the simulation starts by using the \\'usd\\' mode.\"\\n        )\\n\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n\\n    if isinstance(asset, Articulation):\\n        raise ValueError(\\n            \"Scaling an articulation randomly is not supported, as it affects joint attributes and can cause\"\\n            \" unexpected behavior. To achieve different scales, we recommend generating separate USD files for\"\\n            \" each version of the articulation and using multi-asset spawning. For more details, refer to:\"\\n            \" https://isaac-sim.github.io/IsaacLab/main/source/how-to/multi_asset_spawning.html\"\\n        )\\n\\n    # resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=\"cpu\")\\n    else:\\n        env_ids = env_ids.cpu()\\n\\n    # acquire stage\\n    stage = omni.usd.get_context().get_stage()\\n    # resolve prim paths for spawning and cloning\\n    prim_paths = sim_utils.find_matching_prim_paths(asset.cfg.prim_path)\\n\\n    # sample scale values\\n    if isinstance(scale_range, dict):\\n        range_list = [scale_range.get(key, (1.0, 1.0)) for key in [\"x\", \"y\", \"z\"]]\\n        ranges = torch.tensor(range_list, device=\"cpu\")\\n        rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 3), device=\"cpu\")\\n    else:\\n        rand_samples = math_utils.sample_uniform(*scale_range, (len(env_ids), 1), device=\"cpu\")\\n        rand_samples = rand_samples.repeat(1, 3)\\n    # convert to list for the for loop\\n    rand_samples = rand_samples.tolist()\\n\\n    # apply the randomization to the parent if no relative child path is provided\\n    # this might be useful if user wants to randomize a particular mesh in the prim hierarchy\\n    if relative_child_path is None:\\n        relative_child_path = \"\"\\n    elif not relative_child_path.startswith(\"/\"):\\n        relative_child_path = \"/\" + relative_child_path\\n\\n    # use sdf changeblock for faster processing of USD properties\\n    with Sdf.ChangeBlock():\\n        for i, env_id in enumerate(env_ids):\\n            # path to prim to randomize\\n            prim_path = prim_paths[env_id] + relative_child_path\\n            # spawn single instance\\n            prim_spec = Sdf.CreatePrimInLayer(stage.GetRootLayer(), prim_path)\\n\\n            # get the attribute to randomize\\n            scale_spec = prim_spec.GetAttributeAtPath(prim_path + \".xformOp:scale\")\\n            # if the scale attribute does not exist, create it\\n            has_scale_attr = scale_spec is not None\\n            if not has_scale_attr:\\n                scale_spec = Sdf.AttributeSpec(prim_spec, prim_path + \".xformOp:scale\", Sdf.ValueTypeNames.Double3)\\n\\n            # set the new scale\\n            scale_spec.default = Gf.Vec3f(*rand_samples[i])\\n\\n            # ensure the operation is done in the right ordering if we created the scale attribute.\\n            # otherwise, we assume the scale attribute is already in the right order.\\n            # note: by default isaac sim follows this ordering for the transform stack so any asset\\n            #   created through it will have the correct ordering\\n            if not has_scale_attr:\\n                op_order_spec = prim_spec.GetAttributeAtPath(prim_path + \".xformOpOrder\")\\n                if op_order_spec is None:\\n                    op_order_spec = Sdf.AttributeSpec(\\n                        prim_spec, UsdGeom.Tokens.xformOpOrder, Sdf.ValueTypeNames.TokenArray\\n                    )\\n                op_order_spec.default = Vt.TokenArray([\"xformOp:translate\", \"xformOp:orient\", \"xformOp:scale\"])'),\n",
       " Document(metadata={}, page_content='class randomize_rigid_body_material(ManagerTermBase):\\n    \"\"\"Randomize the physics materials on all geometries of the asset.\\n\\n    This function creates a set of physics materials with random static friction, dynamic friction, and restitution\\n    values. The number of materials is specified by ``num_buckets``. The materials are generated by sampling\\n    uniform random values from the given ranges.\\n\\n    The material properties are then assigned to the geometries of the asset. The assignment is done by\\n    creating a random integer tensor of shape  (num_instances, max_num_shapes) where ``num_instances``\\n    is the number of assets spawned and ``max_num_shapes`` is the maximum number of shapes in the asset (over\\n    all bodies). The integer values are used as indices to select the material properties from the\\n    material buckets.\\n\\n    If the flag ``make_consistent`` is set to ``True``, the dynamic friction is set to be less than or equal to\\n    the static friction. This obeys the physics constraint on friction values. However, it may not always be\\n    essential for the application. Thus, the flag is set to ``False`` by default.\\n\\n    .. attention::\\n        This function uses CPU tensors to assign the material properties. It is recommended to use this function\\n        only during the initialization of the environment. Otherwise, it may lead to a significant performance\\n        overhead.\\n\\n    .. note::\\n        PhysX only allows 64000 unique physics materials in the scene. If the number of materials exceeds this\\n        limit, the simulation will crash. Due to this reason, we sample the materials only once during initialization.\\n        Afterwards, these materials are randomly assigned to the geometries of the asset.\\n    \"\"\"\\n\\n    def __init__(self, cfg: EventTermCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the term.\\n\\n        Args:\\n            cfg: The configuration of the event term.\\n            env: The environment instance.\\n\\n        Raises:\\n            ValueError: If the asset is not a RigidObject or an Articulation.\\n        \"\"\"\\n        super().__init__(cfg, env)\\n\\n        # extract the used quantities (to enable type-hinting)\\n        self.asset_cfg: SceneEntityCfg = cfg.params[\"asset_cfg\"]\\n        self.asset: RigidObject | Articulation = env.scene[self.asset_cfg.name]\\n\\n        if not isinstance(self.asset, (RigidObject, Articulation)):\\n            raise ValueError(\\n                f\"Randomization term \\'randomize_rigid_body_material\\' not supported for asset: \\'{self.asset_cfg.name}\\'\"\\n                f\" with type: \\'{type(self.asset)}\\'.\"\\n            )\\n\\n        # obtain number of shapes per body (needed for indexing the material properties correctly)\\n        # note: this is a workaround since the Articulation does not provide a direct way to obtain the number of shapes\\n        #  per body. We use the physics simulation view to obtain the number of shapes per body.\\n        if isinstance(self.asset, Articulation) and self.asset_cfg.body_ids != slice(None):\\n            self.num_shapes_per_body = []\\n            for link_path in self.asset.root_physx_view.link_paths[0]:\\n                link_physx_view = self.asset._physics_sim_view.create_rigid_body_view(link_path)  # type: ignore\\n                self.num_shapes_per_body.append(link_physx_view.max_shapes)\\n            # ensure the parsing is correct\\n            num_shapes = sum(self.num_shapes_per_body)\\n            expected_shapes = self.asset.root_physx_view.max_shapes\\n            if num_shapes != expected_shapes:\\n                raise ValueError(\\n                    \"Randomization term \\'randomize_rigid_body_material\\' failed to parse the number of shapes per body.\"\\n                    f\" Expected total shapes: {expected_shapes}, but got: {num_shapes}.\"\\n                )\\n        else:\\n            # in this case, we don\\'t need to do special indexing\\n            self.num_shapes_per_body = None\\n\\n        # obtain parameters for sampling friction and restitution values\\n        static_friction_range = cfg.params.get(\"static_friction_range\", (1.0, 1.0))\\n        dynamic_friction_range = cfg.params.get(\"dynamic_friction_range\", (1.0, 1.0))\\n        restitution_range = cfg.params.get(\"restitution_range\", (0.0, 0.0))\\n        num_buckets = int(cfg.params.get(\"num_buckets\", 1))\\n\\n        # sample material properties from the given ranges\\n        # note: we only sample the materials once during initialization\\n        #   afterwards these are randomly assigned to the geometries of the asset\\n        range_list = [static_friction_range, dynamic_friction_range, restitution_range]\\n        ranges = torch.tensor(range_list, device=\"cpu\")\\n        self.material_buckets = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (num_buckets, 3), device=\"cpu\")\\n\\n        # ensure dynamic friction is always less than static friction\\n        make_consistent = cfg.params.get(\"make_consistent\", False)\\n        if make_consistent:\\n            self.material_buckets[:, 1] = torch.min(self.material_buckets[:, 0], self.material_buckets[:, 1])\\n\\n    def __call__(\\n        self,\\n        env: ManagerBasedEnv,\\n        env_ids: torch.Tensor | None,\\n        static_friction_range: tuple[float, float],\\n        dynamic_friction_range: tuple[float, float],\\n        restitution_range: tuple[float, float],\\n        num_buckets: int,\\n        asset_cfg: SceneEntityCfg,\\n        make_consistent: bool = False,\\n    ):\\n        # resolve environment ids\\n        if env_ids is None:\\n            env_ids = torch.arange(env.scene.num_envs, device=\"cpu\")\\n        else:\\n            env_ids = env_ids.cpu()\\n\\n        # randomly assign material IDs to the geometries\\n        total_num_shapes = self.asset.root_physx_view.max_shapes\\n        bucket_ids = torch.randint(0, num_buckets, (len(env_ids), total_num_shapes), device=\"cpu\")\\n        material_samples = self.material_buckets[bucket_ids]\\n\\n        # retrieve material buffer from the physics simulation\\n        materials = self.asset.root_physx_view.get_material_properties()\\n\\n        # update material buffer with new samples\\n        if self.num_shapes_per_body is not None:\\n            # sample material properties from the given ranges\\n            for body_id in self.asset_cfg.body_ids:\\n                # obtain indices of shapes for the body\\n                start_idx = sum(self.num_shapes_per_body[:body_id])\\n                end_idx = start_idx + self.num_shapes_per_body[body_id]\\n                # assign the new materials\\n                # material samples are of shape: num_env_ids x total_num_shapes x 3\\n                materials[env_ids, start_idx:end_idx] = material_samples[:, start_idx:end_idx]\\n        else:\\n            # assign all the materials\\n            materials[env_ids] = material_samples[:]\\n\\n        # apply to simulation\\n        self.asset.root_physx_view.set_material_properties(materials, env_ids)'),\n",
       " Document(metadata={}, page_content='def randomize_rigid_body_mass(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    asset_cfg: SceneEntityCfg,\\n    mass_distribution_params: tuple[float, float],\\n    operation: Literal[\"add\", \"scale\", \"abs\"],\\n    distribution: Literal[\"uniform\", \"log_uniform\", \"gaussian\"] = \"uniform\",\\n    recompute_inertia: bool = True,\\n):\\n    \"\"\"Randomize the mass of the bodies by adding, scaling, or setting random values.\\n\\n    This function allows randomizing the mass of the bodies of the asset. The function samples random values from the\\n    given distribution parameters and adds, scales, or sets the values into the physics simulation based on the operation.\\n\\n    If the ``recompute_inertia`` flag is set to ``True``, the function recomputes the inertia tensor of the bodies\\n    after setting the mass. This is useful when the mass is changed significantly, as the inertia tensor depends\\n    on the mass. It assumes the body is a uniform density object. If the body is not a uniform density object,\\n    the inertia tensor may not be accurate.\\n\\n    .. tip::\\n        This function uses CPU tensors to assign the body masses. It is recommended to use this function\\n        only during the initialization of the environment.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject | Articulation = env.scene[asset_cfg.name]\\n\\n    # resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=\"cpu\")\\n    else:\\n        env_ids = env_ids.cpu()\\n\\n    # resolve body indices\\n    if asset_cfg.body_ids == slice(None):\\n        body_ids = torch.arange(asset.num_bodies, dtype=torch.int, device=\"cpu\")\\n    else:\\n        body_ids = torch.tensor(asset_cfg.body_ids, dtype=torch.int, device=\"cpu\")\\n\\n    # get the current masses of the bodies (num_assets, num_bodies)\\n    masses = asset.root_physx_view.get_masses()\\n\\n    # apply randomization on default values\\n    # this is to make sure when calling the function multiple times, the randomization is applied on the\\n    # default values and not the previously randomized values\\n    masses[env_ids[:, None], body_ids] = asset.data.default_mass[env_ids[:, None], body_ids].clone()\\n\\n    # sample from the given range\\n    # note: we modify the masses in-place for all environments\\n    #   however, the setter takes care that only the masses of the specified environments are modified\\n    masses = _randomize_prop_by_op(\\n        masses, mass_distribution_params, env_ids, body_ids, operation=operation, distribution=distribution\\n    )\\n\\n    # set the mass into the physics simulation\\n    asset.root_physx_view.set_masses(masses, env_ids)\\n\\n    # recompute inertia tensors if needed\\n    if recompute_inertia:\\n        # compute the ratios of the new masses to the initial masses\\n        ratios = masses[env_ids[:, None], body_ids] / asset.data.default_mass[env_ids[:, None], body_ids]\\n        # scale the inertia tensors by the the ratios\\n        # since mass randomization is done on default values, we can use the default inertia tensors\\n        inertias = asset.root_physx_view.get_inertias()\\n        if isinstance(asset, Articulation):\\n            # inertia has shape: (num_envs, num_bodies, 9) for articulation\\n            inertias[env_ids[:, None], body_ids] = (\\n                asset.data.default_inertia[env_ids[:, None], body_ids] * ratios[..., None]\\n            )\\n        else:\\n            # inertia has shape: (num_envs, 9) for rigid object\\n            inertias[env_ids] = asset.data.default_inertia[env_ids] * ratios\\n        # set the inertia tensors into the physics simulation\\n        asset.root_physx_view.set_inertias(inertias, env_ids)'),\n",
       " Document(metadata={}, page_content='def randomize_rigid_body_com(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    com_range: dict[str, tuple[float, float]],\\n    asset_cfg: SceneEntityCfg,\\n):\\n    \"\"\"Randomize the center of mass (CoM) of rigid bodies by adding a random value sampled from the given ranges.\\n\\n    .. note::\\n        This function uses CPU tensors to assign the CoM. It is recommended to use this function\\n        only during the initialization of the environment.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=\"cpu\")\\n    else:\\n        env_ids = env_ids.cpu()\\n\\n    # resolve body indices\\n    if asset_cfg.body_ids == slice(None):\\n        body_ids = torch.arange(asset.num_bodies, dtype=torch.int, device=\"cpu\")\\n    else:\\n        body_ids = torch.tensor(asset_cfg.body_ids, dtype=torch.int, device=\"cpu\")\\n\\n    # sample random CoM values\\n    range_list = [com_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\"]]\\n    ranges = torch.tensor(range_list, device=\"cpu\")\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 3), device=\"cpu\").unsqueeze(1)\\n\\n    # get the current com of the bodies (num_assets, num_bodies)\\n    coms = asset.root_physx_view.get_coms().clone()\\n\\n    # Randomize the com in range\\n    coms[:, body_ids, :3] += rand_samples\\n\\n    # Set the new coms\\n    asset.root_physx_view.set_coms(coms, env_ids)'),\n",
       " Document(metadata={}, page_content='def randomize_rigid_body_collider_offsets(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    asset_cfg: SceneEntityCfg,\\n    rest_offset_distribution_params: tuple[float, float] | None = None,\\n    contact_offset_distribution_params: tuple[float, float] | None = None,\\n    distribution: Literal[\"uniform\", \"log_uniform\", \"gaussian\"] = \"uniform\",\\n):\\n    \"\"\"Randomize the collider parameters of rigid bodies in an asset by adding, scaling, or setting random values.\\n\\n    This function allows randomizing the collider parameters of the asset, such as rest and contact offsets.\\n    These correspond to the physics engine collider properties that affect the collision checking.\\n\\n    The function samples random values from the given distribution parameters and applies the operation to\\n    the collider properties. It then sets the values into the physics simulation. If the distribution parameters\\n    are not provided for a particular property, the function does not modify the property.\\n\\n    Currently, the distribution parameters are applied as absolute values.\\n\\n    .. tip::\\n        This function uses CPU tensors to assign the collision properties. It is recommended to use this function\\n        only during the initialization of the environment.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject | Articulation = env.scene[asset_cfg.name]\\n\\n    # resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=\"cpu\")\\n\\n    # sample collider properties from the given ranges and set into the physics simulation\\n    # -- rest offsets\\n    if rest_offset_distribution_params is not None:\\n        rest_offset = asset.root_physx_view.get_rest_offsets().clone()\\n        rest_offset = _randomize_prop_by_op(\\n            rest_offset,\\n            rest_offset_distribution_params,\\n            None,\\n            slice(None),\\n            operation=\"abs\",\\n            distribution=distribution,\\n        )\\n        asset.root_physx_view.set_rest_offsets(rest_offset, env_ids.cpu())\\n    # -- contact offsets\\n    if contact_offset_distribution_params is not None:\\n        contact_offset = asset.root_physx_view.get_contact_offsets().clone()\\n        contact_offset = _randomize_prop_by_op(\\n            contact_offset,\\n            contact_offset_distribution_params,\\n            None,\\n            slice(None),\\n            operation=\"abs\",\\n            distribution=distribution,\\n        )\\n        asset.root_physx_view.set_contact_offsets(contact_offset, env_ids.cpu())'),\n",
       " Document(metadata={}, page_content='def randomize_physics_scene_gravity(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    gravity_distribution_params: tuple[list[float], list[float]],\\n    operation: Literal[\"add\", \"scale\", \"abs\"],\\n    distribution: Literal[\"uniform\", \"log_uniform\", \"gaussian\"] = \"uniform\",\\n):\\n    \"\"\"Randomize gravity by adding, scaling, or setting random values.\\n\\n    This function allows randomizing gravity of the physics scene. The function samples random values from the\\n    given distribution parameters and adds, scales, or sets the values into the physics simulation based on the\\n    operation.\\n\\n    The distribution parameters are lists of two elements each, representing the lower and upper bounds of the\\n    distribution for the x, y, and z components of the gravity vector. The function samples random values for each\\n    component independently.\\n\\n    .. attention::\\n        This function applied the same gravity for all the environments.\\n\\n    .. tip::\\n        This function uses CPU tensors to assign gravity.\\n    \"\"\"\\n    # get the current gravity\\n    gravity = torch.tensor(env.sim.cfg.gravity, device=\"cpu\").unsqueeze(0)\\n    dist_param_0 = torch.tensor(gravity_distribution_params[0], device=\"cpu\")\\n    dist_param_1 = torch.tensor(gravity_distribution_params[1], device=\"cpu\")\\n    gravity = _randomize_prop_by_op(\\n        gravity,\\n        (dist_param_0, dist_param_1),\\n        None,\\n        slice(None),\\n        operation=operation,\\n        distribution=distribution,\\n    )\\n    # unbatch the gravity tensor into a list\\n    gravity = gravity[0].tolist()\\n\\n    # set the gravity into the physics simulation\\n    physics_sim_view: physx.SimulationView = sim_utils.SimulationContext.instance().physics_sim_view\\n    physics_sim_view.set_gravity(carb.Float3(*gravity))'),\n",
       " Document(metadata={}, page_content='def randomize_actuator_gains(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    asset_cfg: SceneEntityCfg,\\n    stiffness_distribution_params: tuple[float, float] | None = None,\\n    damping_distribution_params: tuple[float, float] | None = None,\\n    operation: Literal[\"add\", \"scale\", \"abs\"] = \"abs\",\\n    distribution: Literal[\"uniform\", \"log_uniform\", \"gaussian\"] = \"uniform\",\\n):\\n    \"\"\"Randomize the actuator gains in an articulation by adding, scaling, or setting random values.\\n\\n    This function allows randomizing the actuator stiffness and damping gains.\\n\\n    The function samples random values from the given distribution parameters and applies the operation to the joint properties.\\n    It then sets the values into the actuator models. If the distribution parameters are not provided for a particular property,\\n    the function does not modify the property.\\n\\n    .. tip::\\n        For implicit actuators, this function uses CPU tensors to assign the actuator gains into the simulation.\\n        In such cases, it is recommended to use this function only during the initialization of the environment.\\n    \"\"\"\\n    # Extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n\\n    # Resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=asset.device)\\n\\n    def randomize(data: torch.Tensor, params: tuple[float, float]) -> torch.Tensor:\\n        return _randomize_prop_by_op(\\n            data, params, dim_0_ids=None, dim_1_ids=actuator_indices, operation=operation, distribution=distribution\\n        )\\n\\n    # Loop through actuators and randomize gains\\n    for actuator in asset.actuators.values():\\n        if isinstance(asset_cfg.joint_ids, slice):\\n            # we take all the joints of the actuator\\n            actuator_indices = slice(None)\\n            if isinstance(actuator.joint_indices, slice):\\n                global_indices = slice(None)\\n            else:\\n                global_indices = torch.tensor(actuator.joint_indices, device=asset.device)\\n        elif isinstance(actuator.joint_indices, slice):\\n            # we take the joints defined in the asset config\\n            global_indices = actuator_indices = torch.tensor(asset_cfg.joint_ids, device=asset.device)\\n        else:\\n            # we take the intersection of the actuator joints and the asset config joints\\n            actuator_joint_indices = torch.tensor(actuator.joint_indices, device=asset.device)\\n            asset_joint_ids = torch.tensor(asset_cfg.joint_ids, device=asset.device)\\n            # the indices of the joints in the actuator that have to be randomized\\n            actuator_indices = torch.nonzero(torch.isin(actuator_joint_indices, asset_joint_ids)).view(-1)\\n            if len(actuator_indices) == 0:\\n                continue\\n            # maps actuator indices that have to be randomized to global joint indices\\n            global_indices = actuator_joint_indices[actuator_indices]\\n        # Randomize stiffness\\n        if stiffness_distribution_params is not None:\\n            stiffness = actuator.stiffness[env_ids].clone()\\n            stiffness[:, actuator_indices] = asset.data.default_joint_stiffness[env_ids][:, global_indices].clone()\\n            randomize(stiffness, stiffness_distribution_params)\\n            actuator.stiffness[env_ids] = stiffness\\n            if isinstance(actuator, ImplicitActuator):\\n                asset.write_joint_stiffness_to_sim(stiffness, joint_ids=actuator.joint_indices, env_ids=env_ids)\\n        # Randomize damping\\n        if damping_distribution_params is not None:\\n            damping = actuator.damping[env_ids].clone()\\n            damping[:, actuator_indices] = asset.data.default_joint_damping[env_ids][:, global_indices].clone()\\n            randomize(damping, damping_distribution_params)\\n            actuator.damping[env_ids] = damping\\n            if isinstance(actuator, ImplicitActuator):\\n                asset.write_joint_damping_to_sim(damping, joint_ids=actuator.joint_indices, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def randomize_joint_parameters(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    asset_cfg: SceneEntityCfg,\\n    friction_distribution_params: tuple[float, float] | None = None,\\n    armature_distribution_params: tuple[float, float] | None = None,\\n    lower_limit_distribution_params: tuple[float, float] | None = None,\\n    upper_limit_distribution_params: tuple[float, float] | None = None,\\n    operation: Literal[\"add\", \"scale\", \"abs\"] = \"abs\",\\n    distribution: Literal[\"uniform\", \"log_uniform\", \"gaussian\"] = \"uniform\",\\n):\\n    \"\"\"Randomize the simulated joint parameters of an articulation by adding, scaling, or setting random values.\\n\\n    This function allows randomizing the joint parameters of the asset. These correspond to the physics engine\\n    joint properties that affect the joint behavior. The properties include the joint friction coefficient, armature,\\n    and joint position limits.\\n\\n    The function samples random values from the given distribution parameters and applies the operation to the\\n    joint properties. It then sets the values into the physics simulation. If the distribution parameters are\\n    not provided for a particular property, the function does not modify the property.\\n\\n    .. tip::\\n        This function uses CPU tensors to assign the joint properties. It is recommended to use this function\\n        only during the initialization of the environment.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n\\n    # resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=asset.device)\\n\\n    # resolve joint indices\\n    if asset_cfg.joint_ids == slice(None):\\n        joint_ids = slice(None)  # for optimization purposes\\n    else:\\n        joint_ids = torch.tensor(asset_cfg.joint_ids, dtype=torch.int, device=asset.device)\\n\\n    # sample joint properties from the given ranges and set into the physics simulation\\n    # joint friction coefficient\\n    if friction_distribution_params is not None:\\n        friction_coeff = _randomize_prop_by_op(\\n            asset.data.default_joint_friction_coeff.clone(),\\n            friction_distribution_params,\\n            env_ids,\\n            joint_ids,\\n            operation=operation,\\n            distribution=distribution,\\n        )\\n        asset.write_joint_friction_coefficient_to_sim(\\n            friction_coeff[env_ids[:, None], joint_ids], joint_ids=joint_ids, env_ids=env_ids\\n        )\\n\\n    # joint armature\\n    if armature_distribution_params is not None:\\n        armature = _randomize_prop_by_op(\\n            asset.data.default_joint_armature.clone(),\\n            armature_distribution_params,\\n            env_ids,\\n            joint_ids,\\n            operation=operation,\\n            distribution=distribution,\\n        )\\n        asset.write_joint_armature_to_sim(armature[env_ids[:, None], joint_ids], joint_ids=joint_ids, env_ids=env_ids)\\n\\n    # joint position limits\\n    if lower_limit_distribution_params is not None or upper_limit_distribution_params is not None:\\n        joint_pos_limits = asset.data.default_joint_pos_limits.clone()\\n        # -- randomize the lower limits\\n        if lower_limit_distribution_params is not None:\\n            joint_pos_limits[..., 0] = _randomize_prop_by_op(\\n                joint_pos_limits[..., 0],\\n                lower_limit_distribution_params,\\n                env_ids,\\n                joint_ids,\\n                operation=operation,\\n                distribution=distribution,\\n            )\\n        # -- randomize the upper limits\\n        if upper_limit_distribution_params is not None:\\n            joint_pos_limits[..., 1] = _randomize_prop_by_op(\\n                joint_pos_limits[..., 1],\\n                upper_limit_distribution_params,\\n                env_ids,\\n                joint_ids,\\n                operation=operation,\\n                distribution=distribution,\\n            )\\n\\n        # extract the position limits for the concerned joints\\n        joint_pos_limits = joint_pos_limits[env_ids[:, None], joint_ids]\\n        if (joint_pos_limits[..., 0] > joint_pos_limits[..., 1]).any():\\n            raise ValueError(\\n                \"Randomization term \\'randomize_joint_parameters\\' is setting lower joint limits that are greater than\"\\n                \" upper joint limits. Please check the distribution parameters for the joint position limits.\"\\n            )\\n        # set the position limits into the physics simulation\\n        asset.write_joint_position_limit_to_sim(\\n            joint_pos_limits, joint_ids=joint_ids, env_ids=env_ids, warn_limit_violation=False\\n        )'),\n",
       " Document(metadata={}, page_content='def randomize_fixed_tendon_parameters(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor | None,\\n    asset_cfg: SceneEntityCfg,\\n    stiffness_distribution_params: tuple[float, float] | None = None,\\n    damping_distribution_params: tuple[float, float] | None = None,\\n    limit_stiffness_distribution_params: tuple[float, float] | None = None,\\n    lower_limit_distribution_params: tuple[float, float] | None = None,\\n    upper_limit_distribution_params: tuple[float, float] | None = None,\\n    rest_length_distribution_params: tuple[float, float] | None = None,\\n    offset_distribution_params: tuple[float, float] | None = None,\\n    operation: Literal[\"add\", \"scale\", \"abs\"] = \"abs\",\\n    distribution: Literal[\"uniform\", \"log_uniform\", \"gaussian\"] = \"uniform\",\\n):\\n    \"\"\"Randomize the simulated fixed tendon parameters of an articulation by adding, scaling, or setting random values.\\n\\n    This function allows randomizing the fixed tendon parameters of the asset.\\n    These correspond to the physics engine tendon properties that affect the joint behavior.\\n\\n    The function samples random values from the given distribution parameters and applies the operation to the tendon properties.\\n    It then sets the values into the physics simulation. If the distribution parameters are not provided for a\\n    particular property, the function does not modify the property.\\n\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n\\n    # resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=asset.device)\\n\\n    # resolve joint indices\\n    if asset_cfg.fixed_tendon_ids == slice(None):\\n        tendon_ids = slice(None)  # for optimization purposes\\n    else:\\n        tendon_ids = torch.tensor(asset_cfg.fixed_tendon_ids, dtype=torch.int, device=asset.device)\\n\\n    # sample tendon properties from the given ranges and set into the physics simulation\\n    # stiffness\\n    if stiffness_distribution_params is not None:\\n        stiffness = _randomize_prop_by_op(\\n            asset.data.default_fixed_tendon_stiffness.clone(),\\n            stiffness_distribution_params,\\n            env_ids,\\n            tendon_ids,\\n            operation=operation,\\n            distribution=distribution,\\n        )\\n        asset.set_fixed_tendon_stiffness(stiffness[env_ids[:, None], tendon_ids], tendon_ids, env_ids)\\n\\n    # damping\\n    if damping_distribution_params is not None:\\n        damping = _randomize_prop_by_op(\\n            asset.data.default_fixed_tendon_damping.clone(),\\n            damping_distribution_params,\\n            env_ids,\\n            tendon_ids,\\n            operation=operation,\\n            distribution=distribution,\\n        )\\n        asset.set_fixed_tendon_damping(damping[env_ids[:, None], tendon_ids], tendon_ids, env_ids)\\n\\n    # limit stiffness\\n    if limit_stiffness_distribution_params is not None:\\n        limit_stiffness = _randomize_prop_by_op(\\n            asset.data.default_fixed_tendon_limit_stiffness.clone(),\\n            limit_stiffness_distribution_params,\\n            env_ids,\\n            tendon_ids,\\n            operation=operation,\\n            distribution=distribution,\\n        )\\n        asset.set_fixed_tendon_limit_stiffness(limit_stiffness[env_ids[:, None], tendon_ids], tendon_ids, env_ids)\\n\\n    # position limits\\n    if lower_limit_distribution_params is not None or upper_limit_distribution_params is not None:\\n        limit = asset.data.default_fixed_tendon_pos_limits.clone()\\n        # -- lower limit\\n        if lower_limit_distribution_params is not None:\\n            limit[..., 0] = _randomize_prop_by_op(\\n                limit[..., 0],\\n                lower_limit_distribution_params,\\n                env_ids,\\n                tendon_ids,\\n                operation=operation,\\n                distribution=distribution,\\n            )\\n        # -- upper limit\\n        if upper_limit_distribution_params is not None:\\n            limit[..., 1] = _randomize_prop_by_op(\\n                limit[..., 1],\\n                upper_limit_distribution_params,\\n                env_ids,\\n                tendon_ids,\\n                operation=operation,\\n                distribution=distribution,\\n            )\\n\\n        # check if the limits are valid\\n        tendon_limits = limit[env_ids[:, None], tendon_ids]\\n        if (tendon_limits[..., 0] > tendon_limits[..., 1]).any():\\n            raise ValueError(\\n                \"Randomization term \\'randomize_fixed_tendon_parameters\\' is setting lower tendon limits that are greater\"\\n                \" than upper tendon limits.\"\\n            )\\n        asset.set_fixed_tendon_position_limit(tendon_limits, tendon_ids, env_ids)\\n\\n    # rest length\\n    if rest_length_distribution_params is not None:\\n        rest_length = _randomize_prop_by_op(\\n            asset.data.default_fixed_tendon_rest_length.clone(),\\n            rest_length_distribution_params,\\n            env_ids,\\n            tendon_ids,\\n            operation=operation,\\n            distribution=distribution,\\n        )\\n        asset.set_fixed_tendon_rest_length(rest_length[env_ids[:, None], tendon_ids], tendon_ids, env_ids)\\n\\n    # offset\\n    if offset_distribution_params is not None:\\n        offset = _randomize_prop_by_op(\\n            asset.data.default_fixed_tendon_offset.clone(),\\n            offset_distribution_params,\\n            env_ids,\\n            tendon_ids,\\n            operation=operation,\\n            distribution=distribution,\\n        )\\n        asset.set_fixed_tendon_offset(offset[env_ids[:, None], tendon_ids], tendon_ids, env_ids)\\n\\n    # write the fixed tendon properties into the simulation\\n    asset.write_fixed_tendon_properties_to_sim(tendon_ids, env_ids)'),\n",
       " Document(metadata={}, page_content='def apply_external_force_torque(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    force_range: tuple[float, float],\\n    torque_range: tuple[float, float],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Randomize the external forces and torques applied to the bodies.\\n\\n    This function creates a set of random forces and torques sampled from the given ranges. The number of forces\\n    and torques is equal to the number of bodies times the number of environments. The forces and torques are\\n    applied to the bodies by calling ``asset.set_external_force_and_torque``. The forces and torques are only\\n    applied when ``asset.write_data_to_sim()`` is called in the environment.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject | Articulation = env.scene[asset_cfg.name]\\n    # resolve environment ids\\n    if env_ids is None:\\n        env_ids = torch.arange(env.scene.num_envs, device=asset.device)\\n    # resolve number of bodies\\n    num_bodies = len(asset_cfg.body_ids) if isinstance(asset_cfg.body_ids, list) else asset.num_bodies\\n\\n    # sample random forces and torques\\n    size = (len(env_ids), num_bodies, 3)\\n    forces = math_utils.sample_uniform(*force_range, size, asset.device)\\n    torques = math_utils.sample_uniform(*torque_range, size, asset.device)\\n    # set the forces and torques into the buffers\\n    # note: these are only applied when you call: `asset.write_data_to_sim()`\\n    asset.set_external_force_and_torque(forces, torques, env_ids=env_ids, body_ids=asset_cfg.body_ids)'),\n",
       " Document(metadata={}, page_content='def push_by_setting_velocity(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    velocity_range: dict[str, tuple[float, float]],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Push the asset by setting the root velocity to a random value within the given ranges.\\n\\n    This creates an effect similar to pushing the asset with a random impulse that changes the asset\\'s velocity.\\n    It samples the root velocity from the given ranges and sets the velocity into the physics simulation.\\n\\n    The function takes a dictionary of velocity ranges for each axis and rotation. The keys of the dictionary\\n    are ``x``, ``y``, ``z``, ``roll``, ``pitch``, and ``yaw``. The values are tuples of the form ``(min, max)``.\\n    If the dictionary does not contain a key, the velocity is set to zero for that axis.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject | Articulation = env.scene[asset_cfg.name]\\n\\n    # velocities\\n    vel_w = asset.data.root_vel_w[env_ids]\\n    # sample random velocities\\n    range_list = [velocity_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\", \"roll\", \"pitch\", \"yaw\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    vel_w += math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], vel_w.shape, device=asset.device)\\n    # set the velocities into the physics simulation\\n    asset.write_root_velocity_to_sim(vel_w, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def reset_root_state_uniform(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    pose_range: dict[str, tuple[float, float]],\\n    velocity_range: dict[str, tuple[float, float]],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Reset the asset root state to a random position and velocity uniformly within the given ranges.\\n\\n    This function randomizes the root position and velocity of the asset.\\n\\n    * It samples the root position from the given ranges and adds them to the default root position, before setting\\n      them into the physics simulation.\\n    * It samples the root orientation from the given ranges and sets them into the physics simulation.\\n    * It samples the root velocity from the given ranges and sets them into the physics simulation.\\n\\n    The function takes a dictionary of pose and velocity ranges for each axis and rotation. The keys of the\\n    dictionary are ``x``, ``y``, ``z``, ``roll``, ``pitch``, and ``yaw``. The values are tuples of the form\\n    ``(min, max)``. If the dictionary does not contain a key, the position or velocity is set to zero for that axis.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject | Articulation = env.scene[asset_cfg.name]\\n    # get default root state\\n    root_states = asset.data.default_root_state[env_ids].clone()\\n\\n    # poses\\n    range_list = [pose_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\", \"roll\", \"pitch\", \"yaw\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 6), device=asset.device)\\n\\n    positions = root_states[:, 0:3] + env.scene.env_origins[env_ids] + rand_samples[:, 0:3]\\n    orientations_delta = math_utils.quat_from_euler_xyz(rand_samples[:, 3], rand_samples[:, 4], rand_samples[:, 5])\\n    orientations = math_utils.quat_mul(root_states[:, 3:7], orientations_delta)\\n    # velocities\\n    range_list = [velocity_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\", \"roll\", \"pitch\", \"yaw\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 6), device=asset.device)\\n\\n    velocities = root_states[:, 7:13] + rand_samples\\n\\n    # set into the physics simulation\\n    asset.write_root_pose_to_sim(torch.cat([positions, orientations], dim=-1), env_ids=env_ids)\\n    asset.write_root_velocity_to_sim(velocities, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def reset_root_state_with_random_orientation(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    pose_range: dict[str, tuple[float, float]],\\n    velocity_range: dict[str, tuple[float, float]],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Reset the asset root position and velocities sampled randomly within the given ranges\\n    and the asset root orientation sampled randomly from the SO(3).\\n\\n    This function randomizes the root position and velocity of the asset.\\n\\n    * It samples the root position from the given ranges and adds them to the default root position, before setting\\n      them into the physics simulation.\\n    * It samples the root orientation uniformly from the SO(3) and sets them into the physics simulation.\\n    * It samples the root velocity from the given ranges and sets them into the physics simulation.\\n\\n    The function takes a dictionary of position and velocity ranges for each axis and rotation:\\n\\n    * :attr:`pose_range` - a dictionary of position ranges for each axis. The keys of the dictionary are ``x``,\\n      ``y``, and ``z``. The orientation is sampled uniformly from the SO(3).\\n    * :attr:`velocity_range` - a dictionary of velocity ranges for each axis and rotation. The keys of the dictionary\\n      are ``x``, ``y``, ``z``, ``roll``, ``pitch``, and ``yaw``.\\n\\n    The values are tuples of the form ``(min, max)``. If the dictionary does not contain a particular key,\\n    the position is set to zero for that axis.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject | Articulation = env.scene[asset_cfg.name]\\n    # get default root state\\n    root_states = asset.data.default_root_state[env_ids].clone()\\n\\n    # poses\\n    range_list = [pose_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 3), device=asset.device)\\n\\n    positions = root_states[:, 0:3] + env.scene.env_origins[env_ids] + rand_samples\\n    orientations = math_utils.random_orientation(len(env_ids), device=asset.device)\\n\\n    # velocities\\n    range_list = [velocity_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\", \"roll\", \"pitch\", \"yaw\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 6), device=asset.device)\\n\\n    velocities = root_states[:, 7:13] + rand_samples\\n\\n    # set into the physics simulation\\n    asset.write_root_pose_to_sim(torch.cat([positions, orientations], dim=-1), env_ids=env_ids)\\n    asset.write_root_velocity_to_sim(velocities, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def reset_root_state_from_terrain(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    pose_range: dict[str, tuple[float, float]],\\n    velocity_range: dict[str, tuple[float, float]],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Reset the asset root state by sampling a random valid pose from the terrain.\\n\\n    This function samples a random valid pose(based on flat patches) from the terrain and sets the root state\\n    of the asset to this position. The function also samples random velocities from the given ranges and sets them\\n    into the physics simulation.\\n\\n    The function takes a dictionary of position and velocity ranges for each axis and rotation:\\n\\n    * :attr:`pose_range` - a dictionary of pose ranges for each axis. The keys of the dictionary are ``roll``,\\n      ``pitch``, and ``yaw``. The position is sampled from the flat patches of the terrain.\\n    * :attr:`velocity_range` - a dictionary of velocity ranges for each axis and rotation. The keys of the dictionary\\n      are ``x``, ``y``, ``z``, ``roll``, ``pitch``, and ``yaw``.\\n\\n    The values are tuples of the form ``(min, max)``. If the dictionary does not contain a particular key,\\n    the position is set to zero for that axis.\\n\\n    Note:\\n        The function expects the terrain to have valid flat patches under the key \"init_pos\". The flat patches\\n        are used to sample the random pose for the robot.\\n\\n    Raises:\\n        ValueError: If the terrain does not have valid flat patches under the key \"init_pos\".\\n    \"\"\"\\n    # access the used quantities (to enable type-hinting)\\n    asset: RigidObject | Articulation = env.scene[asset_cfg.name]\\n    terrain: TerrainImporter = env.scene.terrain\\n\\n    # obtain all flat patches corresponding to the valid poses\\n    valid_positions: torch.Tensor = terrain.flat_patches.get(\"init_pos\")\\n    if valid_positions is None:\\n        raise ValueError(\\n            \"The event term \\'reset_root_state_from_terrain\\' requires valid flat patches under \\'init_pos\\'.\"\\n            f\" Found: {list(terrain.flat_patches.keys())}\"\\n        )\\n\\n    # sample random valid poses\\n    ids = torch.randint(0, valid_positions.shape[2], size=(len(env_ids),), device=env.device)\\n    positions = valid_positions[terrain.terrain_levels[env_ids], terrain.terrain_types[env_ids], ids]\\n    positions += asset.data.default_root_state[env_ids, :3]\\n\\n    # sample random orientations\\n    range_list = [pose_range.get(key, (0.0, 0.0)) for key in [\"roll\", \"pitch\", \"yaw\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 3), device=asset.device)\\n\\n    # convert to quaternions\\n    orientations = math_utils.quat_from_euler_xyz(rand_samples[:, 0], rand_samples[:, 1], rand_samples[:, 2])\\n\\n    # sample random velocities\\n    range_list = [velocity_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\", \"roll\", \"pitch\", \"yaw\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 6), device=asset.device)\\n\\n    velocities = asset.data.default_root_state[env_ids, 7:13] + rand_samples\\n\\n    # set into the physics simulation\\n    asset.write_root_pose_to_sim(torch.cat([positions, orientations], dim=-1), env_ids=env_ids)\\n    asset.write_root_velocity_to_sim(velocities, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def reset_joints_by_scale(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    position_range: tuple[float, float],\\n    velocity_range: tuple[float, float],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Reset the robot joints by scaling the default position and velocity by the given ranges.\\n\\n    This function samples random values from the given ranges and scales the default joint positions and velocities\\n    by these values. The scaled values are then set into the physics simulation.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # get default joint state\\n    joint_pos = asset.data.default_joint_pos[env_ids].clone()\\n    joint_vel = asset.data.default_joint_vel[env_ids].clone()\\n\\n    # scale these values randomly\\n    joint_pos *= math_utils.sample_uniform(*position_range, joint_pos.shape, joint_pos.device)\\n    joint_vel *= math_utils.sample_uniform(*velocity_range, joint_vel.shape, joint_vel.device)\\n\\n    # clamp joint pos to limits\\n    joint_pos_limits = asset.data.soft_joint_pos_limits[env_ids]\\n    joint_pos = joint_pos.clamp_(joint_pos_limits[..., 0], joint_pos_limits[..., 1])\\n    # clamp joint vel to limits\\n    joint_vel_limits = asset.data.soft_joint_vel_limits[env_ids]\\n    joint_vel = joint_vel.clamp_(-joint_vel_limits, joint_vel_limits)\\n\\n    # set into the physics simulation\\n    asset.write_joint_state_to_sim(joint_pos, joint_vel, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def reset_joints_by_offset(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    position_range: tuple[float, float],\\n    velocity_range: tuple[float, float],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Reset the robot joints with offsets around the default position and velocity by the given ranges.\\n\\n    This function samples random values from the given ranges and biases the default joint positions and velocities\\n    by these values. The biased values are then set into the physics simulation.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n\\n    # get default joint state\\n    joint_pos = asset.data.default_joint_pos[env_ids].clone()\\n    joint_vel = asset.data.default_joint_vel[env_ids].clone()\\n\\n    # bias these values randomly\\n    joint_pos += math_utils.sample_uniform(*position_range, joint_pos.shape, joint_pos.device)\\n    joint_vel += math_utils.sample_uniform(*velocity_range, joint_vel.shape, joint_vel.device)\\n\\n    # clamp joint pos to limits\\n    joint_pos_limits = asset.data.soft_joint_pos_limits[env_ids]\\n    joint_pos = joint_pos.clamp_(joint_pos_limits[..., 0], joint_pos_limits[..., 1])\\n    # clamp joint vel to limits\\n    joint_vel_limits = asset.data.soft_joint_vel_limits[env_ids]\\n    joint_vel = joint_vel.clamp_(-joint_vel_limits, joint_vel_limits)\\n\\n    # set into the physics simulation\\n    asset.write_joint_state_to_sim(joint_pos, joint_vel, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def reset_nodal_state_uniform(\\n    env: ManagerBasedEnv,\\n    env_ids: torch.Tensor,\\n    position_range: dict[str, tuple[float, float]],\\n    velocity_range: dict[str, tuple[float, float]],\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n):\\n    \"\"\"Reset the asset nodal state to a random position and velocity uniformly within the given ranges.\\n\\n    This function randomizes the nodal position and velocity of the asset.\\n\\n    * It samples the root position from the given ranges and adds them to the default nodal position, before setting\\n      them into the physics simulation.\\n    * It samples the root velocity from the given ranges and sets them into the physics simulation.\\n\\n    The function takes a dictionary of position and velocity ranges for each axis. The keys of the\\n    dictionary are ``x``, ``y``, ``z``. The values are tuples of the form ``(min, max)``.\\n    If the dictionary does not contain a key, the position or velocity is set to zero for that axis.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: DeformableObject = env.scene[asset_cfg.name]\\n    # get default root state\\n    nodal_state = asset.data.default_nodal_state_w[env_ids].clone()\\n\\n    # position\\n    range_list = [position_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 1, 3), device=asset.device)\\n\\n    nodal_state[..., :3] += rand_samples\\n\\n    # velocities\\n    range_list = [velocity_range.get(key, (0.0, 0.0)) for key in [\"x\", \"y\", \"z\"]]\\n    ranges = torch.tensor(range_list, device=asset.device)\\n    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 1, 3), device=asset.device)\\n\\n    nodal_state[..., 3:] += rand_samples\\n\\n    # set into the physics simulation\\n    asset.write_nodal_state_to_sim(nodal_state, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='def reset_scene_to_default(env: ManagerBasedEnv, env_ids: torch.Tensor):\\n    \"\"\"Reset the scene to the default state specified in the scene configuration.\"\"\"\\n    # rigid bodies\\n    for rigid_object in env.scene.rigid_objects.values():\\n        # obtain default and deal with the offset for env origins\\n        default_root_state = rigid_object.data.default_root_state[env_ids].clone()\\n        default_root_state[:, 0:3] += env.scene.env_origins[env_ids]\\n        # set into the physics simulation\\n        rigid_object.write_root_pose_to_sim(default_root_state[:, :7], env_ids=env_ids)\\n        rigid_object.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids=env_ids)\\n    # articulations\\n    for articulation_asset in env.scene.articulations.values():\\n        # obtain default and deal with the offset for env origins\\n        default_root_state = articulation_asset.data.default_root_state[env_ids].clone()\\n        default_root_state[:, 0:3] += env.scene.env_origins[env_ids]\\n        # set into the physics simulation\\n        articulation_asset.write_root_pose_to_sim(default_root_state[:, :7], env_ids=env_ids)\\n        articulation_asset.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids=env_ids)\\n        # obtain default joint positions\\n        default_joint_pos = articulation_asset.data.default_joint_pos[env_ids].clone()\\n        default_joint_vel = articulation_asset.data.default_joint_vel[env_ids].clone()\\n        # set into the physics simulation\\n        articulation_asset.write_joint_state_to_sim(default_joint_pos, default_joint_vel, env_ids=env_ids)\\n    # deformable objects\\n    for deformable_object in env.scene.deformable_objects.values():\\n        # obtain default and set into the physics simulation\\n        nodal_state = deformable_object.data.default_nodal_state_w[env_ids].clone()\\n        deformable_object.write_nodal_state_to_sim(nodal_state, env_ids=env_ids)'),\n",
       " Document(metadata={}, page_content='class randomize_visual_texture_material(ManagerTermBase):\\n    \"\"\"Randomize the visual texture of bodies on an asset using Replicator API.\\n\\n    This function randomizes the visual texture of the bodies of the asset using the Replicator API.\\n    The function samples random textures from the given texture paths and applies them to the bodies\\n    of the asset. The textures are projected onto the bodies and rotated by the given angles.\\n\\n    .. note::\\n        The function assumes that the asset follows the prim naming convention as:\\n        \"{asset_prim_path}/{body_name}/visuals\" where the body name is the name of the body to\\n        which the texture is applied. This is the default prim ordering when importing assets\\n        from the asset converters in Isaac Lab.\\n\\n    .. note::\\n        When randomizing the texture of individual assets, please make sure to set\\n        :attr:`isaaclab.scene.InteractiveSceneCfg.replicate_physics` to False. This ensures that physics\\n        parser will parse the individual asset properties separately.\\n    \"\"\"\\n\\n    def __init__(self, cfg: EventTermCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the term.\\n\\n        Args:\\n            cfg: The configuration of the event term.\\n            env: The environment instance.\\n        \"\"\"\\n        super().__init__(cfg, env)\\n\\n        # enable replicator extension if not already enabled\\n        enable_extension(\"omni.replicator.core\")\\n        # we import the module here since we may not always need the replicator\\n        import omni.replicator.core as rep\\n\\n        # read parameters from the configuration\\n        asset_cfg: SceneEntityCfg = cfg.params.get(\"asset_cfg\")\\n        texture_paths = cfg.params.get(\"texture_paths\")\\n        event_name = cfg.params.get(\"event_name\")\\n        texture_rotation = cfg.params.get(\"texture_rotation\", (0.0, 0.0))\\n\\n        # check to make sure replicate_physics is set to False, else raise error\\n        # note: We add an explicit check here since texture randomization can happen outside of \\'prestartup\\' mode\\n        #   and the event manager doesn\\'t check in that case.\\n        if env.cfg.scene.replicate_physics:\\n            raise RuntimeError(\\n                \"Unable to randomize visual texture material with scene replication enabled.\"\\n                \" For stable USD-level randomization, please disable scene replication\"\\n                \" by setting \\'replicate_physics\\' to False in \\'InteractiveSceneCfg\\'.\"\\n            )\\n\\n        # convert from radians to degrees\\n        texture_rotation = tuple(math.degrees(angle) for angle in texture_rotation)\\n\\n        # obtain the asset entity\\n        asset = env.scene[asset_cfg.name]\\n\\n        # join all bodies in the asset\\n        body_names = asset_cfg.body_names\\n        if isinstance(body_names, str):\\n            body_names_regex = body_names\\n        elif isinstance(body_names, list):\\n            body_names_regex = \"|\".join(body_names)\\n        else:\\n            body_names_regex = \".*\"\\n\\n        # create the affected prim path\\n        # TODO: Remove the hard-coded \"/visuals\" part.\\n        prim_path = f\"{asset.cfg.prim_path}/{body_names_regex}/visuals\"\\n\\n        # Create the omni-graph node for the randomization term\\n        def rep_texture_randomization():\\n            prims_group = rep.get.prims(path_pattern=prim_path)\\n\\n            with prims_group:\\n                rep.randomizer.texture(\\n                    textures=texture_paths, project_uvw=True, texture_rotate=rep.distribution.uniform(*texture_rotation)\\n                )\\n\\n            return prims_group.node\\n\\n        # Register the event to the replicator\\n        with rep.trigger.on_custom_event(event_name=event_name):\\n            rep_texture_randomization()\\n\\n    def __call__(\\n        self,\\n        env: ManagerBasedEnv,\\n        env_ids: torch.Tensor,\\n        event_name: str,\\n        asset_cfg: SceneEntityCfg,\\n        texture_paths: list[str],\\n        texture_rotation: tuple[float, float] = (0.0, 0.0),\\n    ):\\n        # import replicator\\n        import omni.replicator.core as rep\\n\\n        # only send the event to the replicator\\n        # note: This triggers the nodes for all the environments.\\n        #   We need to investigate how to make it happen only for a subset based on env_ids.\\n        rep.utils.send_og_event(event_name)'),\n",
       " Document(metadata={}, page_content='class randomize_visual_color(ManagerTermBase):\\n    \"\"\"Randomize the visual color of bodies on an asset using Replicator API.\\n\\n    This function randomizes the visual color of the bodies of the asset using the Replicator API.\\n    The function samples random colors from the given colors and applies them to the bodies\\n    of the asset.\\n\\n    The function assumes that the asset follows the prim naming convention as:\\n    \"{asset_prim_path}/{mesh_name}\" where the mesh name is the name of the mesh to\\n    which the color is applied. For instance, if the asset has a prim path \"/World/asset\"\\n    and a mesh named \"body_0/mesh\", the prim path for the mesh would be\\n    \"/World/asset/body_0/mesh\".\\n\\n    The colors can be specified as a list of tuples of the form ``(r, g, b)`` or as a dictionary\\n    with the keys ``r``, ``g``, ``b`` and values as tuples of the form ``(low, high)``.\\n    If a dictionary is used, the function will sample random colors from the given ranges.\\n\\n    .. note::\\n        When randomizing the color of individual assets, please make sure to set\\n        :attr:`isaaclab.scene.InteractiveSceneCfg.replicate_physics` to False. This ensures that physics\\n        parser will parse the individual asset properties separately.\\n    \"\"\"\\n\\n    def __init__(self, cfg: EventTermCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the randomization term.\"\"\"\\n        super().__init__(cfg, env)\\n\\n        # enable replicator extension if not already enabled\\n        enable_extension(\"omni.replicator.core\")\\n        # we import the module here since we may not always need the replicator\\n        import omni.replicator.core as rep\\n\\n        # read parameters from the configuration\\n        asset_cfg: SceneEntityCfg = cfg.params.get(\"asset_cfg\")\\n        colors = cfg.params.get(\"colors\")\\n        event_name = cfg.params.get(\"event_name\")\\n        mesh_name: str = cfg.params.get(\"mesh_name\", \"\")  # type: ignore\\n\\n        # check to make sure replicate_physics is set to False, else raise error\\n        # note: We add an explicit check here since texture randomization can happen outside of \\'prestartup\\' mode\\n        #   and the event manager doesn\\'t check in that case.\\n        if env.cfg.scene.replicate_physics:\\n            raise RuntimeError(\\n                \"Unable to randomize visual color with scene replication enabled.\"\\n                \" For stable USD-level randomization, please disable scene replication\"\\n                \" by setting \\'replicate_physics\\' to False in \\'InteractiveSceneCfg\\'.\"\\n            )\\n\\n        # obtain the asset entity\\n        asset = env.scene[asset_cfg.name]\\n\\n        # create the affected prim path\\n        if not mesh_name.startswith(\"/\"):\\n            mesh_name = \"/\" + mesh_name\\n        mesh_prim_path = f\"{asset.cfg.prim_path}{mesh_name}\"\\n        # TODO: Need to make it work for multiple meshes.\\n\\n        # parse the colors into replicator format\\n        if isinstance(colors, dict):\\n            # (r, g, b) - low, high --> (low_r, low_g, low_b) and (high_r, high_g, high_b)\\n            color_low = [colors[key][0] for key in [\"r\", \"g\", \"b\"]]\\n            color_high = [colors[key][1] for key in [\"r\", \"g\", \"b\"]]\\n            colors = rep.distribution.uniform(color_low, color_high)\\n        else:\\n            colors = list(colors)\\n\\n        # Create the omni-graph node for the randomization term\\n        def rep_texture_randomization():\\n            prims_group = rep.get.prims(path_pattern=mesh_prim_path)\\n\\n            with prims_group:\\n                rep.randomizer.color(colors=colors)\\n\\n            return prims_group.node\\n\\n        # Register the event to the replicator\\n        with rep.trigger.on_custom_event(event_name=event_name):\\n            rep_texture_randomization()\\n\\n    def __call__(\\n        self,\\n        env: ManagerBasedEnv,\\n        env_ids: torch.Tensor,\\n        event_name: str,\\n        asset_cfg: SceneEntityCfg,\\n        colors: list[tuple[float, float, float]] | dict[str, tuple[float, float]],\\n        mesh_name: str = \"\",\\n    ):\\n        # import replicator\\n        import omni.replicator.core as rep\\n\\n        # only send the event to the replicator\\n        rep.utils.send_og_event(event_name)'),\n",
       " Document(metadata={}, page_content='def _randomize_prop_by_op(\\n    data: torch.Tensor,\\n    distribution_parameters: tuple[float | torch.Tensor, float | torch.Tensor],\\n    dim_0_ids: torch.Tensor | None,\\n    dim_1_ids: torch.Tensor | slice,\\n    operation: Literal[\"add\", \"scale\", \"abs\"],\\n    distribution: Literal[\"uniform\", \"log_uniform\", \"gaussian\"],\\n) -> torch.Tensor:\\n    \"\"\"Perform data randomization based on the given operation and distribution.\\n\\n    Args:\\n        data: The data tensor to be randomized. Shape is (dim_0, dim_1).\\n        distribution_parameters: The parameters for the distribution to sample values from.\\n        dim_0_ids: The indices of the first dimension to randomize.\\n        dim_1_ids: The indices of the second dimension to randomize.\\n        operation: The operation to perform on the data. Options: \\'add\\', \\'scale\\', \\'abs\\'.\\n        distribution: The distribution to sample the random values from. Options: \\'uniform\\', \\'log_uniform\\'.\\n\\n    Returns:\\n        The data tensor after randomization. Shape is (dim_0, dim_1).\\n\\n    Raises:\\n        NotImplementedError: If the operation or distribution is not supported.\\n    \"\"\"\\n    # resolve shape\\n    # -- dim 0\\n    if dim_0_ids is None:\\n        n_dim_0 = data.shape[0]\\n        dim_0_ids = slice(None)\\n    else:\\n        n_dim_0 = len(dim_0_ids)\\n        if not isinstance(dim_1_ids, slice):\\n            dim_0_ids = dim_0_ids[:, None]\\n    # -- dim 1\\n    if isinstance(dim_1_ids, slice):\\n        n_dim_1 = data.shape[1]\\n    else:\\n        n_dim_1 = len(dim_1_ids)\\n\\n    # resolve the distribution\\n    if distribution == \"uniform\":\\n        dist_fn = math_utils.sample_uniform\\n    elif distribution == \"log_uniform\":\\n        dist_fn = math_utils.sample_log_uniform\\n    elif distribution == \"gaussian\":\\n        dist_fn = math_utils.sample_gaussian\\n    else:\\n        raise NotImplementedError(\\n            f\"Unknown distribution: \\'{distribution}\\' for joint properties randomization.\"\\n            \" Please use \\'uniform\\', \\'log_uniform\\', \\'gaussian\\'.\"\\n        )\\n    # perform the operation\\n    if operation == \"add\":\\n        data[dim_0_ids, dim_1_ids] += dist_fn(*distribution_parameters, (n_dim_0, n_dim_1), device=data.device)\\n    elif operation == \"scale\":\\n        data[dim_0_ids, dim_1_ids] *= dist_fn(*distribution_parameters, (n_dim_0, n_dim_1), device=data.device)\\n    elif operation == \"abs\":\\n        data[dim_0_ids, dim_1_ids] = dist_fn(*distribution_parameters, (n_dim_0, n_dim_1), device=data.device)\\n    else:\\n        raise NotImplementedError(\\n            f\"Unknown operation: \\'{operation}\\' for property randomization. Please use \\'add\\', \\'scale\\', or \\'abs\\'.\"\\n        )\\n    return data'),\n",
       " Document(metadata={}, page_content='def base_pos_z(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Root height in the simulation world frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return asset.data.root_pos_w[:, 2].unsqueeze(-1)'),\n",
       " Document(metadata={}, page_content='def base_lin_vel(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Root linear velocity in the asset\\'s root frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.root_lin_vel_b'),\n",
       " Document(metadata={}, page_content='def base_ang_vel(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Root angular velocity in the asset\\'s root frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.root_ang_vel_b'),\n",
       " Document(metadata={}, page_content='def projected_gravity(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Gravity projection on the asset\\'s root frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.projected_gravity_b'),\n",
       " Document(metadata={}, page_content='def root_pos_w(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Asset root position in the environment frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.root_pos_w - env.scene.env_origins'),\n",
       " Document(metadata={}, page_content='def root_quat_w(\\n    env: ManagerBasedEnv, make_quat_unique: bool = False, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Asset root orientation (w, x, y, z) in the environment frame.\\n\\n    If :attr:`make_quat_unique` is True, then returned quaternion is made unique by ensuring\\n    the quaternion has non-negative real component. This is because both ``q`` and ``-q`` represent\\n    the same orientation.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n\\n    quat = asset.data.root_quat_w\\n    # make the quaternion real-part positive if configured\\n    return math_utils.quat_unique(quat) if make_quat_unique else quat'),\n",
       " Document(metadata={}, page_content='def root_lin_vel_w(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Asset root linear velocity in the environment frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.root_lin_vel_w'),\n",
       " Document(metadata={}, page_content='def root_ang_vel_w(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Asset root angular velocity in the environment frame.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.root_ang_vel_w'),\n",
       " Document(metadata={}, page_content='def body_pose_w(\\n    env: ManagerBasedEnv,\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n) -> torch.Tensor:\\n    \"\"\"The flattened body poses of the asset w.r.t the env.scene.origin.\\n\\n    Note: Only the bodies configured in :attr:`asset_cfg.body_ids` will have their poses returned.\\n\\n    Args:\\n        env: The environment.\\n        asset_cfg: The SceneEntity associated with this observation.\\n\\n    Returns:\\n        The poses of bodies in articulation [num_env, 7*num_bodies]. Pose order is [x,y,z,qw,qx,qy,qz]. Output is\\n            stacked horizontally per body.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    pose = asset.data.body_state_w[:, asset_cfg.body_ids, :7]\\n    pose[..., :3] = pose[..., :3] - env.scene.env_origins.unsqueeze(1)\\n    return pose.reshape(env.num_envs, -1)'),\n",
       " Document(metadata={}, page_content='def body_projected_gravity_b(\\n    env: ManagerBasedEnv,\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n) -> torch.Tensor:\\n    \"\"\"The direction of gravity projected on to bodies of an Articulation.\\n\\n    Note: Only the bodies configured in :attr:`asset_cfg.body_ids` will have their poses returned.\\n\\n    Args:\\n        env: The environment.\\n        asset_cfg: The Articulation associated with this observation.\\n\\n    Returns:\\n        The unit vector direction of gravity projected onto body_name\\'s frame. Gravity projection vector order is\\n            [x,y,z]. Output is stacked horizontally per body.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n\\n    body_quat = asset.data.body_quat_w[:, asset_cfg.body_ids]\\n    gravity_dir = asset.data.GRAVITY_VEC_W.unsqueeze(1)\\n    return math_utils.quat_rotate_inverse(body_quat, gravity_dir).view(env.num_envs, -1)'),\n",
       " Document(metadata={}, page_content='def joint_pos(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"The joint positions of the asset.\\n\\n    Note: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their positions returned.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return asset.data.joint_pos[:, asset_cfg.joint_ids]'),\n",
       " Document(metadata={}, page_content='def joint_pos_rel(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"The joint positions of the asset w.r.t. the default joint positions.\\n\\n    Note: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their positions returned.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.default_joint_pos[:, asset_cfg.joint_ids]'),\n",
       " Document(metadata={}, page_content='def joint_pos_limit_normalized(\\n    env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"The joint positions of the asset normalized with the asset\\'s joint limits.\\n\\n    Note: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their normalized positions returned.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return math_utils.scale_transform(\\n        asset.data.joint_pos[:, asset_cfg.joint_ids],\\n        asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 0],\\n        asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 1],\\n    )'),\n",
       " Document(metadata={}, page_content='def joint_vel(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")):\\n    \"\"\"The joint velocities of the asset.\\n\\n    Note: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their velocities returned.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return asset.data.joint_vel[:, asset_cfg.joint_ids]'),\n",
       " Document(metadata={}, page_content='def joint_vel_rel(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")):\\n    \"\"\"The joint velocities of the asset w.r.t. the default joint velocities.\\n\\n    Note: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their velocities returned.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return asset.data.joint_vel[:, asset_cfg.joint_ids] - asset.data.default_joint_vel[:, asset_cfg.joint_ids]'),\n",
       " Document(metadata={}, page_content='def joint_effort(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"The joint applied effort of the robot.\\n\\n    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their effort returned.\\n\\n    Args:\\n        env: The environment.\\n        asset_cfg: The SceneEntity associated with this observation.\\n\\n    Returns:\\n        The joint effort (N or N-m) for joint_names in asset_cfg, shape is [num_env,num_joints].\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return asset.data.applied_torque[:, asset_cfg.joint_ids]'),\n",
       " Document(metadata={}, page_content='def height_scan(env: ManagerBasedEnv, sensor_cfg: SceneEntityCfg, offset: float = 0.5) -> torch.Tensor:\\n    \"\"\"Height scan from the given sensor w.r.t. the sensor\\'s frame.\\n\\n    The provided offset (Defaults to 0.5) is subtracted from the returned values.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]\\n    # height scan: height = sensor_height - hit_point_z - offset\\n    return sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - offset'),\n",
       " Document(metadata={}, page_content='def body_incoming_wrench(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:\\n    \"\"\"Incoming spatial wrench on bodies of an articulation in the simulation world frame.\\n\\n    This is the 6-D wrench (force and torque) applied to the body link by the incoming joint force.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # obtain the link incoming forces in world frame\\n    body_incoming_joint_wrench_b = asset.data.body_incoming_joint_wrench_b[:, asset_cfg.body_ids]\\n    return body_incoming_joint_wrench_b.view(env.num_envs, -1)'),\n",
       " Document(metadata={}, page_content='def imu_orientation(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"imu\")) -> torch.Tensor:\\n    \"\"\"Imu sensor orientation in the simulation world frame.\\n\\n    Args:\\n        env: The environment.\\n        asset_cfg: The SceneEntity associated with an IMU sensor. Defaults to SceneEntityCfg(\"imu\").\\n\\n    Returns:\\n        Orientation in the world frame in (w, x, y, z) quaternion form. Shape is (num_envs, 4).\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Imu = env.scene[asset_cfg.name]\\n    # return the orientation quaternion\\n    return asset.data.quat_w'),\n",
       " Document(metadata={}, page_content='def imu_ang_vel(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"imu\")) -> torch.Tensor:\\n    \"\"\"Imu sensor angular velocity w.r.t. environment origin expressed in the sensor frame.\\n\\n    Args:\\n        env: The environment.\\n        asset_cfg: The SceneEntity associated with an IMU sensor. Defaults to SceneEntityCfg(\"imu\").\\n\\n    Returns:\\n        The angular velocity (rad/s) in the sensor frame. Shape is (num_envs, 3).\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Imu = env.scene[asset_cfg.name]\\n    # return the angular velocity\\n    return asset.data.ang_vel_b'),\n",
       " Document(metadata={}, page_content='def imu_lin_acc(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"imu\")) -> torch.Tensor:\\n    \"\"\"Imu sensor linear acceleration w.r.t. the environment origin expressed in sensor frame.\\n\\n    Args:\\n        env: The environment.\\n        asset_cfg: The SceneEntity associated with an IMU sensor. Defaults to SceneEntityCfg(\"imu\").\\n\\n    Returns:\\n        The linear acceleration (m/s^2) in the sensor frame. Shape is (num_envs, 3).\\n    \"\"\"\\n    asset: Imu = env.scene[asset_cfg.name]\\n    return asset.data.lin_acc_b'),\n",
       " Document(metadata={}, page_content='def image(\\n    env: ManagerBasedEnv,\\n    sensor_cfg: SceneEntityCfg = SceneEntityCfg(\"tiled_camera\"),\\n    data_type: str = \"rgb\",\\n    convert_perspective_to_orthogonal: bool = False,\\n    normalize: bool = True,\\n) -> torch.Tensor:\\n    \"\"\"Images of a specific datatype from the camera sensor.\\n\\n    If the flag :attr:`normalize` is True, post-processing of the images are performed based on their\\n    data-types:\\n\\n    - \"rgb\": Scales the image to (0, 1) and subtracts with the mean of the current image batch.\\n    - \"depth\" or \"distance_to_camera\" or \"distance_to_plane\": Replaces infinity values with zero.\\n\\n    Args:\\n        env: The environment the cameras are placed within.\\n        sensor_cfg: The desired sensor to read from. Defaults to SceneEntityCfg(\"tiled_camera\").\\n        data_type: The data type to pull from the desired camera. Defaults to \"rgb\".\\n        convert_perspective_to_orthogonal: Whether to orthogonalize perspective depth images.\\n            This is used only when the data type is \"distance_to_camera\". Defaults to False.\\n        normalize: Whether to normalize the images. This depends on the selected data type.\\n            Defaults to True.\\n\\n    Returns:\\n        The images produced at the last time-step\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    sensor: TiledCamera | Camera | RayCasterCamera = env.scene.sensors[sensor_cfg.name]\\n\\n    # obtain the input image\\n    images = sensor.data.output[data_type]\\n\\n    # depth image conversion\\n    if (data_type == \"distance_to_camera\") and convert_perspective_to_orthogonal:\\n        images = math_utils.orthogonalize_perspective_depth(images, sensor.data.intrinsic_matrices)\\n\\n    # rgb/depth image normalization\\n    if normalize:\\n        if data_type == \"rgb\":\\n            images = images.float() / 255.0\\n            mean_tensor = torch.mean(images, dim=(1, 2), keepdim=True)\\n            images -= mean_tensor\\n        elif \"distance_to\" in data_type or \"depth\" in data_type:\\n            images[images == float(\"inf\")] = 0\\n\\n    return images.clone()'),\n",
       " Document(metadata={}, page_content='class image_features(ManagerTermBase):\\n    \"\"\"Extracted image features from a pre-trained frozen encoder.\\n\\n    This term uses models from the model zoo in PyTorch and extracts features from the images.\\n\\n    It calls the :func:`image` function to get the images and then processes them using the model zoo.\\n\\n    A user can provide their own model zoo configuration to use different models for feature extraction.\\n    The model zoo configuration should be a dictionary that maps different model names to a dictionary\\n    that defines the model, preprocess and inference functions. The dictionary should have the following\\n    entries:\\n\\n    - \"model\": A callable that returns the model when invoked without arguments.\\n    - \"reset\": A callable that resets the model. This is useful when the model has a state that needs to be reset.\\n    - \"inference\": A callable that, when given the model and the images, returns the extracted features.\\n\\n    If the model zoo configuration is not provided, the default model zoo configurations are used. The default\\n    model zoo configurations include the models from Theia :cite:`shang2024theia` and ResNet :cite:`he2016deep`.\\n    These models are loaded from `Hugging-Face transformers <https://huggingface.co/docs/transformers/index>`_ and\\n    `PyTorch torchvision <https://pytorch.org/vision/stable/models.html>`_ respectively.\\n\\n    Args:\\n        sensor_cfg: The sensor configuration to poll. Defaults to SceneEntityCfg(\"tiled_camera\").\\n        data_type: The sensor data type. Defaults to \"rgb\".\\n        convert_perspective_to_orthogonal: Whether to orthogonalize perspective depth images.\\n            This is used only when the data type is \"distance_to_camera\". Defaults to False.\\n        model_zoo_cfg: A user-defined dictionary that maps different model names to their respective configurations.\\n            Defaults to None. If None, the default model zoo configurations are used.\\n        model_name: The name of the model to use for inference. Defaults to \"resnet18\".\\n        model_device: The device to store and infer the model on. This is useful when offloading the computation\\n            from the environment simulation device. Defaults to the environment device.\\n        inference_kwargs: Additional keyword arguments to pass to the inference function. Defaults to None,\\n            which means no additional arguments are passed.\\n\\n    Returns:\\n        The extracted features tensor. Shape is (num_envs, feature_dim).\\n\\n    Raises:\\n        ValueError: When the model name is not found in the provided model zoo configuration.\\n        ValueError: When the model name is not found in the default model zoo configuration.\\n    \"\"\"\\n\\n    def __init__(self, cfg: ObservationTermCfg, env: ManagerBasedEnv):\\n        # initialize the base class\\n        super().__init__(cfg, env)\\n\\n        # extract parameters from the configuration\\n        self.model_zoo_cfg: dict = cfg.params.get(\"model_zoo_cfg\")  # type: ignore\\n        self.model_name: str = cfg.params.get(\"model_name\", \"resnet18\")  # type: ignore\\n        self.model_device: str = cfg.params.get(\"model_device\", env.device)  # type: ignore\\n\\n        # List of Theia models - These are configured through `_prepare_theia_transformer_model` function\\n        default_theia_models = [\\n            \"theia-tiny-patch16-224-cddsv\",\\n            \"theia-tiny-patch16-224-cdiv\",\\n            \"theia-small-patch16-224-cdiv\",\\n            \"theia-base-patch16-224-cdiv\",\\n            \"theia-small-patch16-224-cddsv\",\\n            \"theia-base-patch16-224-cddsv\",\\n        ]\\n        # List of ResNet models - These are configured through `_prepare_resnet_model` function\\n        default_resnet_models = [\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\"]\\n\\n        # Check if model name is specified in the model zoo configuration\\n        if self.model_zoo_cfg is not None and self.model_name not in self.model_zoo_cfg:\\n            raise ValueError(\\n                f\"Model name \\'{self.model_name}\\' not found in the provided model zoo configuration.\"\\n                \" Please add the model to the model zoo configuration or use a different model name.\"\\n                f\" Available models in the provided list: {list(self.model_zoo_cfg.keys())}.\"\\n                \"\\\\nHint: If you want to use a default model, consider using one of the following models:\"\\n                f\" {default_theia_models + default_resnet_models}. In this case, you can remove the\"\\n                \" \\'model_zoo_cfg\\' parameter from the observation term configuration.\"\\n            )\\n        if self.model_zoo_cfg is None:\\n            if self.model_name in default_theia_models:\\n                model_config = self._prepare_theia_transformer_model(self.model_name, self.model_device)\\n            elif self.model_name in default_resnet_models:\\n                model_config = self._prepare_resnet_model(self.model_name, self.model_device)\\n            else:\\n                raise ValueError(\\n                    f\"Model name \\'{self.model_name}\\' not found in the default model zoo configuration.\"\\n                    f\" Available models: {default_theia_models + default_resnet_models}.\"\\n                )\\n        else:\\n            model_config = self.model_zoo_cfg[self.model_name]\\n\\n        # Retrieve the model, preprocess and inference functions\\n        self._model = model_config[\"model\"]()\\n        self._reset_fn = model_config.get(\"reset\")\\n        self._inference_fn = model_config[\"inference\"]\\n\\n    def reset(self, env_ids: torch.Tensor | None = None):\\n        # reset the model if a reset function is provided\\n        # this might be useful when the model has a state that needs to be reset\\n        # for example: video transformers\\n        if self._reset_fn is not None:\\n            self._reset_fn(self._model, env_ids)\\n\\n    def __call__(\\n        self,\\n        env: ManagerBasedEnv,\\n        sensor_cfg: SceneEntityCfg = SceneEntityCfg(\"tiled_camera\"),\\n        data_type: str = \"rgb\",\\n        convert_perspective_to_orthogonal: bool = False,\\n        model_zoo_cfg: dict | None = None,\\n        model_name: str = \"resnet18\",\\n        model_device: str | None = None,\\n        inference_kwargs: dict | None = None,\\n    ) -> torch.Tensor:\\n        # obtain the images from the sensor\\n        image_data = image(\\n            env=env,\\n            sensor_cfg=sensor_cfg,\\n            data_type=data_type,\\n            convert_perspective_to_orthogonal=convert_perspective_to_orthogonal,\\n            normalize=False,  # we pre-process based on model\\n        )\\n        # store the device of the image\\n        image_device = image_data.device\\n        # forward the images through the model\\n        features = self._inference_fn(self._model, image_data, **(inference_kwargs or {}))\\n\\n        # move the features back to the image device\\n        return features.detach().to(image_device)\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_theia_transformer_model(self, model_name: str, model_device: str) -> dict:\\n        \"\"\"Prepare the Theia transformer model for inference.\\n\\n        Args:\\n            model_name: The name of the Theia transformer model to prepare.\\n            model_device: The device to store and infer the model on.\\n\\n        Returns:\\n            A dictionary containing the model and inference functions.\\n        \"\"\"\\n        from transformers import AutoModel\\n\\n        def _load_model() -> torch.nn.Module:\\n            \"\"\"Load the Theia transformer model.\"\"\"\\n            model = AutoModel.from_pretrained(f\"theaiinstitute/{model_name}\", trust_remote_code=True).eval()\\n            return model.to(model_device)\\n\\n        def _inference(model, images: torch.Tensor) -> torch.Tensor:\\n            \"\"\"Inference the Theia transformer model.\\n\\n            Args:\\n                model: The Theia transformer model.\\n                images: The preprocessed image tensor. Shape is (num_envs, height, width, channel).\\n\\n            Returns:\\n                The extracted features tensor. Shape is (num_envs, feature_dim).\\n            \"\"\"\\n            # Move the image to the model device\\n            image_proc = images.to(model_device)\\n            # permute the image to (num_envs, channel, height, width)\\n            image_proc = image_proc.permute(0, 3, 1, 2).float() / 255.0\\n            # Normalize the image\\n            mean = torch.tensor([0.485, 0.456, 0.406], device=model_device).view(1, 3, 1, 1)\\n            std = torch.tensor([0.229, 0.224, 0.225], device=model_device).view(1, 3, 1, 1)\\n            image_proc = (image_proc - mean) / std\\n\\n            # Taken from Transformers; inference converted to be GPU only\\n            features = model.backbone.model(pixel_values=image_proc, interpolate_pos_encoding=True)\\n            return features.last_hidden_state[:, 1:]\\n\\n        # return the model, preprocess and inference functions\\n        return {\"model\": _load_model, \"inference\": _inference}\\n\\n    def _prepare_resnet_model(self, model_name: str, model_device: str) -> dict:\\n        \"\"\"Prepare the ResNet model for inference.\\n\\n        Args:\\n            model_name: The name of the ResNet model to prepare.\\n            model_device: The device to store and infer the model on.\\n\\n        Returns:\\n            A dictionary containing the model and inference functions.\\n        \"\"\"\\n        from torchvision import models\\n\\n        def _load_model() -> torch.nn.Module:\\n            \"\"\"Load the ResNet model.\"\"\"\\n            # map the model name to the weights\\n            resnet_weights = {\\n                \"resnet18\": \"ResNet18_Weights.IMAGENET1K_V1\",\\n                \"resnet34\": \"ResNet34_Weights.IMAGENET1K_V1\",\\n                \"resnet50\": \"ResNet50_Weights.IMAGENET1K_V1\",\\n                \"resnet101\": \"ResNet101_Weights.IMAGENET1K_V1\",\\n            }\\n\\n            # load the model\\n            model = getattr(models, model_name)(weights=resnet_weights[model_name]).eval()\\n            return model.to(model_device)\\n\\n        def _inference(model, images: torch.Tensor) -> torch.Tensor:\\n            \"\"\"Inference the ResNet model.\\n\\n            Args:\\n                model: The ResNet model.\\n                images: The preprocessed image tensor. Shape is (num_envs, channel, height, width).\\n\\n            Returns:\\n                The extracted features tensor. Shape is (num_envs, feature_dim).\\n            \"\"\"\\n            # move the image to the model device\\n            image_proc = images.to(model_device)\\n            # permute the image to (num_envs, channel, height, width)\\n            image_proc = image_proc.permute(0, 3, 1, 2).float() / 255.0\\n            # normalize the image\\n            mean = torch.tensor([0.485, 0.456, 0.406], device=model_device).view(1, 3, 1, 1)\\n            std = torch.tensor([0.229, 0.224, 0.225], device=model_device).view(1, 3, 1, 1)\\n            image_proc = (image_proc - mean) / std\\n\\n            # forward the image through the model\\n            return model(image_proc)\\n\\n        # return the model, preprocess and inference functions\\n        return {\"model\": _load_model, \"inference\": _inference}'),\n",
       " Document(metadata={}, page_content='def last_action(env: ManagerBasedEnv, action_name: str | None = None) -> torch.Tensor:\\n    \"\"\"The last input action to the environment.\\n\\n    The name of the action term for which the action is required. If None, the\\n    entire action tensor is returned.\\n    \"\"\"\\n    if action_name is None:\\n        return env.action_manager.action\\n    else:\\n        return env.action_manager.get_term(action_name).raw_actions'),\n",
       " Document(metadata={}, page_content='def generated_commands(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:\\n    \"\"\"The generated command from command term in the command manager with the given name.\"\"\"\\n    return env.command_manager.get_command(command_name)'),\n",
       " Document(metadata={}, page_content='def current_time_s(env: ManagerBasedRLEnv) -> torch.Tensor:\\n    \"\"\"The current time in the episode (in seconds).\"\"\"\\n    return env.episode_length_buf.unsqueeze(1) * env.step_dt'),\n",
       " Document(metadata={}, page_content='def remaining_time_s(env: ManagerBasedRLEnv) -> torch.Tensor:\\n    \"\"\"The maximum time remaining in the episode (in seconds).\"\"\"\\n    return env.max_episode_length_s - env.episode_length_buf.unsqueeze(1) * env.step_dt'),\n",
       " Document(metadata={}, page_content='def is_alive(env: ManagerBasedRLEnv) -> torch.Tensor:\\n    \"\"\"Reward for being alive.\"\"\"\\n    return (~env.termination_manager.terminated).float()'),\n",
       " Document(metadata={}, page_content='def is_terminated(env: ManagerBasedRLEnv) -> torch.Tensor:\\n    \"\"\"Penalize terminated episodes that don\\'t correspond to episodic timeouts.\"\"\"\\n    return env.termination_manager.terminated.float()'),\n",
       " Document(metadata={}, page_content='class is_terminated_term(ManagerTermBase):\\n    \"\"\"Penalize termination for specific terms that don\\'t correspond to episodic timeouts.\\n\\n    The parameters are as follows:\\n\\n    * attr:`term_keys`: The termination terms to penalize. This can be a string, a list of strings\\n      or regular expressions. Default is \".*\" which penalizes all terminations.\\n\\n    The reward is computed as the sum of the termination terms that are not episodic timeouts.\\n    This means that the reward is 0 if the episode is terminated due to an episodic timeout. Otherwise,\\n    if two termination terms are active, the reward is 2.\\n    \"\"\"\\n\\n    def __init__(self, cfg: RewardTermCfg, env: ManagerBasedRLEnv):\\n        # initialize the base class\\n        super().__init__(cfg, env)\\n        # find and store the termination terms\\n        term_keys = cfg.params.get(\"term_keys\", \".*\")\\n        self._term_names = env.termination_manager.find_terms(term_keys)\\n\\n    def __call__(self, env: ManagerBasedRLEnv, term_keys: str | list[str] = \".*\") -> torch.Tensor:\\n        # Return the unweighted reward for the termination terms\\n        reset_buf = torch.zeros(env.num_envs, device=env.device)\\n        for term in self._term_names:\\n            # Sums over terminations term values to account for multiple terminations in the same step\\n            reset_buf += env.termination_manager.get_term(term)\\n\\n        return (reset_buf * (~env.termination_manager.time_outs)).float()'),\n",
       " Document(metadata={}, page_content='def lin_vel_z_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize z-axis base linear velocity using L2 squared kernel.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return torch.square(asset.data.root_lin_vel_b[:, 2])'),\n",
       " Document(metadata={}, page_content='def ang_vel_xy_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize xy-axis base angular velocity using L2 squared kernel.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return torch.sum(torch.square(asset.data.root_ang_vel_b[:, :2]), dim=1)'),\n",
       " Document(metadata={}, page_content='def flat_orientation_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize non-flat base orientation using L2 squared kernel.\\n\\n    This is computed by penalizing the xy-components of the projected gravity vector.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)'),\n",
       " Document(metadata={}, page_content='def base_height_l2(\\n    env: ManagerBasedRLEnv,\\n    target_height: float,\\n    asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\"),\\n    sensor_cfg: SceneEntityCfg | None = None,\\n) -> torch.Tensor:\\n    \"\"\"Penalize asset height from its target using L2 squared kernel.\\n\\n    Note:\\n        For flat terrain, target height is in the world frame. For rough terrain,\\n        sensor readings can adjust the target height to account for the terrain.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    if sensor_cfg is not None:\\n        sensor: RayCaster = env.scene[sensor_cfg.name]\\n        # Adjust the target height using the sensor data\\n        adjusted_target_height = target_height + torch.mean(sensor.data.ray_hits_w[..., 2], dim=1)\\n    else:\\n        # Use the provided target height directly for flat terrain\\n        adjusted_target_height = target_height\\n    # Compute the L2 squared penalty\\n    return torch.square(asset.data.root_pos_w[:, 2] - adjusted_target_height)'),\n",
       " Document(metadata={}, page_content='def body_lin_acc_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize the linear acceleration of bodies using L2-kernel.\"\"\"\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return torch.sum(torch.norm(asset.data.body_lin_acc_w[:, asset_cfg.body_ids, :], dim=-1), dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_torques_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize joint torques applied on the articulation using L2 squared kernel.\\n\\n    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint torques contribute to the term.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return torch.sum(torch.square(asset.data.applied_torque[:, asset_cfg.joint_ids]), dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_vel_l1(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:\\n    \"\"\"Penalize joint velocities on the articulation using an L1-kernel.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return torch.sum(torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_vel_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize joint velocities on the articulation using L2 squared kernel.\\n\\n    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint velocities contribute to the term.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return torch.sum(torch.square(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_acc_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize joint accelerations on the articulation using L2 squared kernel.\\n\\n    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint accelerations contribute to the term.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    return torch.sum(torch.square(asset.data.joint_acc[:, asset_cfg.joint_ids]), dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_deviation_l1(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize joint positions that deviate from the default one.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # compute out of limits constraints\\n    angle = asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.default_joint_pos[:, asset_cfg.joint_ids]\\n    return torch.sum(torch.abs(angle), dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_pos_limits(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize joint positions if they cross the soft limits.\\n\\n    This is computed as a sum of the absolute value of the difference between the joint position and the soft limits.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # compute out of limits constraints\\n    out_of_limits = -(\\n        asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 0]\\n    ).clip(max=0.0)\\n    out_of_limits += (\\n        asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 1]\\n    ).clip(min=0.0)\\n    return torch.sum(out_of_limits, dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_vel_limits(\\n    env: ManagerBasedRLEnv, soft_ratio: float, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Penalize joint velocities if they cross the soft limits.\\n\\n    This is computed as a sum of the absolute value of the difference between the joint velocity and the soft limits.\\n\\n    Args:\\n        soft_ratio: The ratio of the soft limits to be used.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # compute out of limits constraints\\n    out_of_limits = (\\n        torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids])\\n        - asset.data.soft_joint_vel_limits[:, asset_cfg.joint_ids] * soft_ratio\\n    )\\n    # clip to max error = 1 rad/s per joint to avoid huge penalties\\n    out_of_limits = out_of_limits.clip_(min=0.0, max=1.0)\\n    return torch.sum(out_of_limits, dim=1)'),\n",
       " Document(metadata={}, page_content='def applied_torque_limits(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Penalize applied torques if they cross the limits.\\n\\n    This is computed as a sum of the absolute value of the difference between the applied torques and the limits.\\n\\n    .. caution::\\n        Currently, this only works for explicit actuators since we manually compute the applied torques.\\n        For implicit actuators, we currently cannot retrieve the applied torques from the physics engine.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # compute out of limits constraints\\n    # TODO: We need to fix this to support implicit joints.\\n    out_of_limits = torch.abs(\\n        asset.data.applied_torque[:, asset_cfg.joint_ids] - asset.data.computed_torque[:, asset_cfg.joint_ids]\\n    )\\n    return torch.sum(out_of_limits, dim=1)'),\n",
       " Document(metadata={}, page_content='def action_rate_l2(env: ManagerBasedRLEnv) -> torch.Tensor:\\n    \"\"\"Penalize the rate of change of the actions using L2 squared kernel.\"\"\"\\n    return torch.sum(torch.square(env.action_manager.action - env.action_manager.prev_action), dim=1)'),\n",
       " Document(metadata={}, page_content='def action_l2(env: ManagerBasedRLEnv) -> torch.Tensor:\\n    \"\"\"Penalize the actions using L2 squared kernel.\"\"\"\\n    return torch.sum(torch.square(env.action_manager.action), dim=1)'),\n",
       " Document(metadata={}, page_content='def undesired_contacts(env: ManagerBasedRLEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:\\n    \"\"\"Penalize undesired contacts as the number of violations that are above a threshold.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]\\n    # check if contact force is above threshold\\n    net_contact_forces = contact_sensor.data.net_forces_w_history\\n    is_contact = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold\\n    # sum over contacts for each environment\\n    return torch.sum(is_contact, dim=1)'),\n",
       " Document(metadata={}, page_content='def contact_forces(env: ManagerBasedRLEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:\\n    \"\"\"Penalize contact forces as the amount of violations of the net contact force.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]\\n    net_contact_forces = contact_sensor.data.net_forces_w_history\\n    # compute the violation\\n    violation = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] - threshold\\n    # compute the penalty\\n    return torch.sum(violation.clip(min=0.0), dim=1)'),\n",
       " Document(metadata={}, page_content='def track_lin_vel_xy_exp(\\n    env: ManagerBasedRLEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Reward tracking of linear velocity commands (xy axes) using exponential kernel.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    # compute the error\\n    lin_vel_error = torch.sum(\\n        torch.square(env.command_manager.get_command(command_name)[:, :2] - asset.data.root_lin_vel_b[:, :2]),\\n        dim=1,\\n    )\\n    return torch.exp(-lin_vel_error / std**2)'),\n",
       " Document(metadata={}, page_content='def track_ang_vel_z_exp(\\n    env: ManagerBasedRLEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Reward tracking of angular velocity commands (yaw) using exponential kernel.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    # compute the error\\n    ang_vel_error = torch.square(env.command_manager.get_command(command_name)[:, 2] - asset.data.root_ang_vel_b[:, 2])\\n    return torch.exp(-ang_vel_error / std**2)'),\n",
       " Document(metadata={}, page_content='def time_out(env: ManagerBasedRLEnv) -> torch.Tensor:\\n    \"\"\"Terminate the episode when the episode length exceeds the maximum episode length.\"\"\"\\n    return env.episode_length_buf >= env.max_episode_length'),\n",
       " Document(metadata={}, page_content='def command_resample(env: ManagerBasedRLEnv, command_name: str, num_resamples: int = 1) -> torch.Tensor:\\n    \"\"\"Terminate the episode based on the total number of times commands have been re-sampled.\\n\\n    This makes the maximum episode length fluid in nature as it depends on how the commands are\\n    sampled. It is useful in situations where delayed rewards are used :cite:`rudin2022advanced`.\\n    \"\"\"\\n    command: CommandTerm = env.command_manager.get_term(command_name)\\n    return torch.logical_and((command.time_left <= env.step_dt), (command.command_counter == num_resamples))'),\n",
       " Document(metadata={}, page_content='def bad_orientation(\\n    env: ManagerBasedRLEnv, limit_angle: float, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Terminate when the asset\\'s orientation is too far from the desired orientation limits.\\n\\n    This is computed by checking the angle between the projected gravity vector and the z-axis.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return torch.acos(-asset.data.projected_gravity_b[:, 2]).abs() > limit_angle'),\n",
       " Document(metadata={}, page_content='def root_height_below_minimum(\\n    env: ManagerBasedRLEnv, minimum_height: float, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Terminate when the asset\\'s root height is below the minimum height.\\n\\n    Note:\\n        This is currently only supported for flat terrains, i.e. the minimum height is in the world frame.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: RigidObject = env.scene[asset_cfg.name]\\n    return asset.data.root_pos_w[:, 2] < minimum_height'),\n",
       " Document(metadata={}, page_content='def joint_pos_out_of_limit(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Terminate when the asset\\'s joint positions are outside of the soft joint limits.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # compute any violations\\n    out_of_upper_limits = torch.any(asset.data.joint_pos > asset.data.soft_joint_pos_limits[..., 1], dim=1)\\n    out_of_lower_limits = torch.any(asset.data.joint_pos < asset.data.soft_joint_pos_limits[..., 0], dim=1)\\n    return torch.logical_or(out_of_upper_limits[:, asset_cfg.joint_ids], out_of_lower_limits[:, asset_cfg.joint_ids])'),\n",
       " Document(metadata={}, page_content='def joint_pos_out_of_manual_limit(\\n    env: ManagerBasedRLEnv, bounds: tuple[float, float], asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Terminate when the asset\\'s joint positions are outside of the configured bounds.\\n\\n    Note:\\n        This function is similar to :func:`joint_pos_out_of_limit` but allows the user to specify the bounds manually.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    if asset_cfg.joint_ids is None:\\n        asset_cfg.joint_ids = slice(None)\\n    # compute any violations\\n    out_of_upper_limits = torch.any(asset.data.joint_pos[:, asset_cfg.joint_ids] > bounds[1], dim=1)\\n    out_of_lower_limits = torch.any(asset.data.joint_pos[:, asset_cfg.joint_ids] < bounds[0], dim=1)\\n    return torch.logical_or(out_of_upper_limits, out_of_lower_limits)'),\n",
       " Document(metadata={}, page_content='def joint_vel_out_of_limit(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")) -> torch.Tensor:\\n    \"\"\"Terminate when the asset\\'s joint velocities are outside of the soft joint limits.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # compute any violations\\n    limits = asset.data.soft_joint_vel_limits\\n    return torch.any(torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids]) > limits[:, asset_cfg.joint_ids], dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_vel_out_of_manual_limit(\\n    env: ManagerBasedRLEnv, max_velocity: float, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Terminate when the asset\\'s joint velocities are outside the provided limits.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # compute any violations\\n    return torch.any(torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids]) > max_velocity, dim=1)'),\n",
       " Document(metadata={}, page_content='def joint_effort_out_of_limit(\\n    env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg(\"robot\")\\n) -> torch.Tensor:\\n    \"\"\"Terminate when effort applied on the asset\\'s joints are outside of the soft joint limits.\\n\\n    In the actuators, the applied torque are the efforts applied on the joints. These are computed by clipping\\n    the computed torques to the joint limits. Hence, we check if the computed torques are equal to the applied\\n    torques.\\n    \"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    asset: Articulation = env.scene[asset_cfg.name]\\n    # check if any joint effort is out of limit\\n    out_of_limits = torch.isclose(\\n        asset.data.computed_torque[:, asset_cfg.joint_ids], asset.data.applied_torque[:, asset_cfg.joint_ids]\\n    )\\n    return torch.any(out_of_limits, dim=1)'),\n",
       " Document(metadata={}, page_content='def illegal_contact(env: ManagerBasedRLEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:\\n    \"\"\"Terminate when the contact force on the sensor exceeds the force threshold.\"\"\"\\n    # extract the used quantities (to enable type-hinting)\\n    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]\\n    net_contact_forces = contact_sensor.data.net_forces_w_history\\n    # check if any contact force exceeds the threshold\\n    return torch.any(\\n        torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold, dim=1\\n    )'),\n",
       " Document(metadata={}, page_content='class JointActionCfg(ActionTermCfg):\\n    \"\"\"Configuration for the base joint action term.\\n\\n    See :class:`JointAction` for more details.\\n    \"\"\"\\n\\n    joint_names: list[str] = MISSING\\n    \"\"\"List of joint names or regex expressions that the action will be mapped to.\"\"\"\\n    scale: float | dict[str, float] = 1.0\\n    \"\"\"Scale factor for the action (float or dict of regex expressions). Defaults to 1.0.\"\"\"\\n    offset: float | dict[str, float] = 0.0\\n    \"\"\"Offset factor for the action (float or dict of regex expressions). Defaults to 0.0.\"\"\"\\n    preserve_order: bool = False\\n    \"\"\"Whether to preserve the order of the joint names in the action output. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='class JointPositionActionCfg(JointActionCfg):\\n    \"\"\"Configuration for the joint position action term.\\n\\n    See :class:`JointPositionAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = joint_actions.JointPositionAction\\n\\n    use_default_offset: bool = True\\n    \"\"\"Whether to use default joint positions configured in the articulation asset as offset.\\n    Defaults to True.\\n\\n    If True, this flag results in overwriting the values of :attr:`offset` to the default joint positions\\n    from the articulation asset.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RelativeJointPositionActionCfg(JointActionCfg):\\n    \"\"\"Configuration for the relative joint position action term.\\n\\n    See :class:`RelativeJointPositionAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = joint_actions.RelativeJointPositionAction\\n\\n    use_zero_offset: bool = True\\n    \"\"\"Whether to ignore the offset defined in articulation asset. Defaults to True.\\n\\n    If True, this flag results in overwriting the values of :attr:`offset` to zero.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class JointVelocityActionCfg(JointActionCfg):\\n    \"\"\"Configuration for the joint velocity action term.\\n\\n    See :class:`JointVelocityAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = joint_actions.JointVelocityAction\\n\\n    use_default_offset: bool = True\\n    \"\"\"Whether to use default joint velocities configured in the articulation asset as offset.\\n    Defaults to True.\\n\\n    This overrides the settings from :attr:`offset` if set to True.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class JointEffortActionCfg(JointActionCfg):\\n    \"\"\"Configuration for the joint effort action term.\\n\\n    See :class:`JointEffortAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = joint_actions.JointEffortAction'),\n",
       " Document(metadata={}, page_content='class JointPositionToLimitsActionCfg(ActionTermCfg):\\n    \"\"\"Configuration for the bounded joint position action term.\\n\\n    See :class:`JointPositionToLimitsAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = joint_actions_to_limits.JointPositionToLimitsAction\\n\\n    joint_names: list[str] = MISSING\\n    \"\"\"List of joint names or regex expressions that the action will be mapped to.\"\"\"\\n\\n    scale: float | dict[str, float] = 1.0\\n    \"\"\"Scale factor for the action (float or dict of regex expressions). Defaults to 1.0.\"\"\"\\n\\n    rescale_to_limits: bool = True\\n    \"\"\"Whether to rescale the action to the joint limits. Defaults to True.\\n\\n    If True, the input actions are rescaled to the joint limits, i.e., the action value in\\n    the range [-1, 1] corresponds to the joint lower and upper limits respectively.\\n\\n    Note:\\n        This operation is performed after applying the scale factor.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class EMAJointPositionToLimitsActionCfg(JointPositionToLimitsActionCfg):\\n    \"\"\"Configuration for the exponential moving average (EMA) joint position action term.\\n\\n    See :class:`EMAJointPositionToLimitsAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = joint_actions_to_limits.EMAJointPositionToLimitsAction\\n\\n    alpha: float | dict[str, float] = 1.0\\n    \"\"\"The weight for the moving average (float or dict of regex expressions). Defaults to 1.0.\\n\\n    If set to 1.0, the processed action is applied directly without any moving average window.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class BinaryJointActionCfg(ActionTermCfg):\\n    \"\"\"Configuration for the base binary joint action term.\\n\\n    See :class:`BinaryJointAction` for more details.\\n    \"\"\"\\n\\n    joint_names: list[str] = MISSING\\n    \"\"\"List of joint names or regex expressions that the action will be mapped to.\"\"\"\\n    open_command_expr: dict[str, float] = MISSING\\n    \"\"\"The joint command to move to *open* configuration.\"\"\"\\n    close_command_expr: dict[str, float] = MISSING\\n    \"\"\"The joint command to move to *close* configuration.\"\"\"'),\n",
       " Document(metadata={}, page_content='class BinaryJointPositionActionCfg(BinaryJointActionCfg):\\n    \"\"\"Configuration for the binary joint position action term.\\n\\n    See :class:`BinaryJointPositionAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = binary_joint_actions.BinaryJointPositionAction'),\n",
       " Document(metadata={}, page_content='class BinaryJointVelocityActionCfg(BinaryJointActionCfg):\\n    \"\"\"Configuration for the binary joint velocity action term.\\n\\n    See :class:`BinaryJointVelocityAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = binary_joint_actions.BinaryJointVelocityAction'),\n",
       " Document(metadata={}, page_content='class NonHolonomicActionCfg(ActionTermCfg):\\n    \"\"\"Configuration for the non-holonomic action term with dummy joints at the base.\\n\\n    See :class:`NonHolonomicAction` for more details.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = non_holonomic_actions.NonHolonomicAction\\n\\n    body_name: str = MISSING\\n    \"\"\"Name of the body which has the dummy mechanism connected to.\"\"\"\\n    x_joint_name: str = MISSING\\n    \"\"\"The dummy joint name in the x direction.\"\"\"\\n    y_joint_name: str = MISSING\\n    \"\"\"The dummy joint name in the y direction.\"\"\"\\n    yaw_joint_name: str = MISSING\\n    \"\"\"The dummy joint name in the yaw direction.\"\"\"\\n    scale: tuple[float, float] = (1.0, 1.0)\\n    \"\"\"Scale factor for the action. Defaults to (1.0, 1.0).\"\"\"\\n    offset: tuple[float, float] = (0.0, 0.0)\\n    \"\"\"Offset factor for the action. Defaults to (0.0, 0.0).\"\"\"'),\n",
       " Document(metadata={}, page_content='class DifferentialInverseKinematicsActionCfg(ActionTermCfg):\\n    \"\"\"Configuration for inverse differential kinematics action term.\\n\\n    See :class:`DifferentialInverseKinematicsAction` for more details.\\n    \"\"\"\\n\\n    @configclass\\n    class OffsetCfg:\\n        \"\"\"The offset pose from parent frame to child frame.\\n\\n        On many robots, end-effector frames are fictitious frames that do not have a corresponding\\n        rigid body. In such cases, it is easier to define this transform w.r.t. their parent rigid body.\\n        For instance, for the Franka Emika arm, the end-effector is defined at an offset to the the\\n        \"panda_hand\" frame.\\n        \"\"\"\\n\\n        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Translation w.r.t. the parent frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n        \"\"\"Quaternion rotation ``(w, x, y, z)`` w.r.t. the parent frame. Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"\\n\\n    class_type: type[ActionTerm] = task_space_actions.DifferentialInverseKinematicsAction\\n\\n    joint_names: list[str] = MISSING\\n    \"\"\"List of joint names or regex expressions that the action will be mapped to.\"\"\"\\n    body_name: str = MISSING\\n    \"\"\"Name of the body or frame for which IK is performed.\"\"\"\\n    body_offset: OffsetCfg | None = None\\n    \"\"\"Offset of target frame w.r.t. to the body frame. Defaults to None, in which case no offset is applied.\"\"\"\\n    scale: float | tuple[float, ...] = 1.0\\n    \"\"\"Scale factor for the action. Defaults to 1.0.\"\"\"\\n    controller: DifferentialIKControllerCfg = MISSING\\n    \"\"\"The configuration for the differential IK controller.\"\"\"'),\n",
       " Document(metadata={}, page_content='class OperationalSpaceControllerActionCfg(ActionTermCfg):\\n    \"\"\"Configuration for operational space controller action term.\\n\\n    See :class:`OperationalSpaceControllerAction` for more details.\\n    \"\"\"\\n\\n    @configclass\\n    class OffsetCfg:\\n        \"\"\"The offset pose from parent frame to child frame.\\n\\n        On many robots, end-effector frames are fictitious frames that do not have a corresponding\\n        rigid body. In such cases, it is easier to define this transform w.r.t. their parent rigid body.\\n        For instance, for the Franka Emika arm, the end-effector is defined at an offset to the the\\n        \"panda_hand\" frame.\\n        \"\"\"\\n\\n        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Translation w.r.t. the parent frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n        \"\"\"Quaternion rotation ``(w, x, y, z)`` w.r.t. the parent frame. Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"\\n\\n    class_type: type[ActionTerm] = task_space_actions.OperationalSpaceControllerAction\\n\\n    joint_names: list[str] = MISSING\\n    \"\"\"List of joint names or regex expressions that the action will be mapped to.\"\"\"\\n\\n    body_name: str = MISSING\\n    \"\"\"Name of the body or frame for which motion/force control is performed.\"\"\"\\n\\n    body_offset: OffsetCfg | None = None\\n    \"\"\"Offset of target frame w.r.t. to the body frame. Defaults to None, in which case no offset is applied.\"\"\"\\n\\n    task_frame_rel_path: str = None\\n    \"\"\"The path of a ``RigidObject``, relative to the sub-environment, representing task frame. Defaults to None.\"\"\"\\n\\n    controller_cfg: OperationalSpaceControllerCfg = MISSING\\n    \"\"\"The configuration for the operational space controller.\"\"\"\\n\\n    position_scale: float = 1.0\\n    \"\"\"Scale factor for the position targets. Defaults to 1.0.\"\"\"\\n\\n    orientation_scale: float = 1.0\\n    \"\"\"Scale factor for the orientation (quad for ``pose_abs`` or axis-angle for ``pose_rel``). Defaults to 1.0.\"\"\"\\n\\n    wrench_scale: float = 1.0\\n    \"\"\"Scale factor for the wrench targets. Defaults to 1.0.\"\"\"\\n\\n    stiffness_scale: float = 1.0\\n    \"\"\"Scale factor for the stiffness commands. Defaults to 1.0.\"\"\"\\n\\n    damping_ratio_scale: float = 1.0\\n    \"\"\"Scale factor for the damping ratio commands. Defaults to 1.0.\"\"\"\\n\\n    nullspace_joint_pos_target: str = \"none\"\\n    \"\"\"The joint targets for the null-space control: ``\"none\"``, ``\"zero\"``, ``\"default\"``, ``\"center\"``.\\n\\n    Note: Functional only when ``nullspace_control`` is set to ``\"position\"`` within the\\n        ``OperationalSpaceControllerCfg``.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class BinaryJointAction(ActionTerm):\\n    \"\"\"Base class for binary joint actions.\\n\\n    This action term maps a binary action to the *open* or *close* joint configurations. These configurations are\\n    specified through the :class:`BinaryJointActionCfg` object. If the input action is a float vector, the action\\n    is considered binary based on the sign of the action values.\\n\\n    Based on above, we follow the following convention for the binary action:\\n\\n    1. Open action: 1 (bool) or positive values (float).\\n    2. Close action: 0 (bool) or negative values (float).\\n\\n    The action term can mostly be used for gripper actions, where the gripper is either open or closed. This\\n    helps in devising a mimicking mechanism for the gripper, since in simulation it is often not possible to\\n    add such constraints to the gripper.\\n    \"\"\"\\n\\n    cfg: actions_cfg.BinaryJointActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n    _asset: Articulation\\n    \"\"\"The articulation asset on which the action term is applied.\"\"\"\\n    _clip: torch.Tensor\\n    \"\"\"The clip applied to the input action.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.BinaryJointActionCfg, env: ManagerBasedEnv) -> None:\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n\\n        # resolve the joints over which the action term is applied\\n        self._joint_ids, self._joint_names = self._asset.find_joints(self.cfg.joint_names)\\n        self._num_joints = len(self._joint_ids)\\n        # log the resolved joint names for debugging\\n        omni.log.info(\\n            f\"Resolved joint names for the action term {self.__class__.__name__}:\"\\n            f\" {self._joint_names} [{self._joint_ids}]\"\\n        )\\n\\n        # create tensors for raw and processed actions\\n        self._raw_actions = torch.zeros(self.num_envs, 1, device=self.device)\\n        self._processed_actions = torch.zeros(self.num_envs, self._num_joints, device=self.device)\\n\\n        # parse open command\\n        self._open_command = torch.zeros(self._num_joints, device=self.device)\\n        index_list, name_list, value_list = string_utils.resolve_matching_names_values(\\n            self.cfg.open_command_expr, self._joint_names\\n        )\\n        if len(index_list) != self._num_joints:\\n            raise ValueError(\\n                f\"Could not resolve all joints for the action term. Missing: {set(self._joint_names) - set(name_list)}\"\\n            )\\n        self._open_command[index_list] = torch.tensor(value_list, device=self.device)\\n\\n        # parse close command\\n        self._close_command = torch.zeros_like(self._open_command)\\n        index_list, name_list, value_list = string_utils.resolve_matching_names_values(\\n            self.cfg.close_command_expr, self._joint_names\\n        )\\n        if len(index_list) != self._num_joints:\\n            raise ValueError(\\n                f\"Could not resolve all joints for the action term. Missing: {set(self._joint_names) - set(name_list)}\"\\n            )\\n        self._close_command[index_list] = torch.tensor(value_list, device=self.device)\\n\\n        # parse clip\\n        if self.cfg.clip is not None:\\n            if isinstance(cfg.clip, dict):\\n                self._clip = torch.tensor([[-float(\"inf\"), float(\"inf\")]], device=self.device).repeat(\\n                    self.num_envs, self.action_dim, 1\\n                )\\n                index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.clip, self._joint_names)\\n                self._clip[:, index_list] = torch.tensor(value_list, device=self.device)\\n            else:\\n                raise ValueError(f\"Unsupported clip type: {type(cfg.clip)}. Supported types are dict.\")\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        return 1\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        return self._processed_actions\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        # store the raw actions\\n        self._raw_actions[:] = actions\\n        # compute the binary mask\\n        if actions.dtype == torch.bool:\\n            # true: close, false: open\\n            binary_mask = actions == 0\\n        else:\\n            # true: close, false: open\\n            binary_mask = actions < 0\\n        # compute the command\\n        self._processed_actions = torch.where(binary_mask, self._close_command, self._open_command)\\n        if self.cfg.clip is not None:\\n            self._processed_actions = torch.clamp(\\n                self._processed_actions, min=self._clip[:, :, 0], max=self._clip[:, :, 1]\\n            )\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        self._raw_actions[env_ids] = 0.0'),\n",
       " Document(metadata={}, page_content='class BinaryJointPositionAction(BinaryJointAction):\\n    \"\"\"Binary joint action that sets the binary action into joint position targets.\"\"\"\\n\\n    cfg: actions_cfg.BinaryJointPositionActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n\\n    def apply_actions(self):\\n        self._asset.set_joint_position_target(self._processed_actions, joint_ids=self._joint_ids)'),\n",
       " Document(metadata={}, page_content='class BinaryJointVelocityAction(BinaryJointAction):\\n    \"\"\"Binary joint action that sets the binary action into joint velocity targets.\"\"\"\\n\\n    cfg: actions_cfg.BinaryJointVelocityActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n\\n    def apply_actions(self):\\n        self._asset.set_joint_velocity_target(self._processed_actions, joint_ids=self._joint_ids)'),\n",
       " Document(metadata={}, page_content='class JointAction(ActionTerm):\\n    r\"\"\"Base class for joint actions.\\n\\n    This action term performs pre-processing of the raw actions using affine transformations (scale and offset).\\n    These transformations can be configured to be applied to a subset of the articulation\\'s joints.\\n\\n    Mathematically, the action term is defined as:\\n\\n    .. math::\\n\\n       \\\\text{action} = \\\\text{offset} + \\\\text{scaling} \\\\times \\\\text{input action}\\n\\n    where :math:`\\\\text{action}` is the action that is sent to the articulation\\'s actuated joints, :math:`\\\\text{offset}`\\n    is the offset applied to the input action, :math:`\\\\text{scaling}` is the scaling applied to the input\\n    action, and :math:`\\\\text{input action}` is the input action from the user.\\n\\n    Based on above, this kind of action transformation ensures that the input and output actions are in the same\\n    units and dimensions. The child classes of this action term can then map the output action to a specific\\n    desired command of the articulation\\'s joints (e.g. position, velocity, etc.).\\n    \"\"\"\\n\\n    cfg: actions_cfg.JointActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n    _asset: Articulation\\n    \"\"\"The articulation asset on which the action term is applied.\"\"\"\\n    _scale: torch.Tensor | float\\n    \"\"\"The scaling factor applied to the input action.\"\"\"\\n    _offset: torch.Tensor | float\\n    \"\"\"The offset applied to the input action.\"\"\"\\n    _clip: torch.Tensor\\n    \"\"\"The clip applied to the input action.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.JointActionCfg, env: ManagerBasedEnv) -> None:\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n\\n        # resolve the joints over which the action term is applied\\n        self._joint_ids, self._joint_names = self._asset.find_joints(\\n            self.cfg.joint_names, preserve_order=self.cfg.preserve_order\\n        )\\n        self._num_joints = len(self._joint_ids)\\n        # log the resolved joint names for debugging\\n        omni.log.info(\\n            f\"Resolved joint names for the action term {self.__class__.__name__}:\"\\n            f\" {self._joint_names} [{self._joint_ids}]\"\\n        )\\n\\n        # Avoid indexing across all joints for efficiency\\n        if self._num_joints == self._asset.num_joints and not self.cfg.preserve_order:\\n            self._joint_ids = slice(None)\\n\\n        # create tensors for raw and processed actions\\n        self._raw_actions = torch.zeros(self.num_envs, self.action_dim, device=self.device)\\n        self._processed_actions = torch.zeros_like(self.raw_actions)\\n\\n        # parse scale\\n        if isinstance(cfg.scale, (float, int)):\\n            self._scale = float(cfg.scale)\\n        elif isinstance(cfg.scale, dict):\\n            self._scale = torch.ones(self.num_envs, self.action_dim, device=self.device)\\n            # resolve the dictionary config\\n            index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.scale, self._joint_names)\\n            self._scale[:, index_list] = torch.tensor(value_list, device=self.device)\\n        else:\\n            raise ValueError(f\"Unsupported scale type: {type(cfg.scale)}. Supported types are float and dict.\")\\n        # parse offset\\n        if isinstance(cfg.offset, (float, int)):\\n            self._offset = float(cfg.offset)\\n        elif isinstance(cfg.offset, dict):\\n            self._offset = torch.zeros_like(self._raw_actions)\\n            # resolve the dictionary config\\n            index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.offset, self._joint_names)\\n            self._offset[:, index_list] = torch.tensor(value_list, device=self.device)\\n        else:\\n            raise ValueError(f\"Unsupported offset type: {type(cfg.offset)}. Supported types are float and dict.\")\\n        # parse clip\\n        if self.cfg.clip is not None:\\n            if isinstance(cfg.clip, dict):\\n                self._clip = torch.tensor([[-float(\"inf\"), float(\"inf\")]], device=self.device).repeat(\\n                    self.num_envs, self.action_dim, 1\\n                )\\n                index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.clip, self._joint_names)\\n                self._clip[:, index_list] = torch.tensor(value_list, device=self.device)\\n            else:\\n                raise ValueError(f\"Unsupported clip type: {type(cfg.clip)}. Supported types are dict.\")\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        return self._num_joints\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        return self._processed_actions\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        # store the raw actions\\n        self._raw_actions[:] = actions\\n        # apply the affine transformations\\n        self._processed_actions = self._raw_actions * self._scale + self._offset\\n        # clip actions\\n        if self.cfg.clip is not None:\\n            self._processed_actions = torch.clamp(\\n                self._processed_actions, min=self._clip[:, :, 0], max=self._clip[:, :, 1]\\n            )\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        self._raw_actions[env_ids] = 0.0'),\n",
       " Document(metadata={}, page_content='class JointPositionAction(JointAction):\\n    \"\"\"Joint action term that applies the processed actions to the articulation\\'s joints as position commands.\"\"\"\\n\\n    cfg: actions_cfg.JointPositionActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.JointPositionActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n        # use default joint positions as offset\\n        if cfg.use_default_offset:\\n            self._offset = self._asset.data.default_joint_pos[:, self._joint_ids].clone()\\n\\n    def apply_actions(self):\\n        # set position targets\\n        self._asset.set_joint_position_target(self.processed_actions, joint_ids=self._joint_ids)'),\n",
       " Document(metadata={}, page_content='class RelativeJointPositionAction(JointAction):\\n    r\"\"\"Joint action term that applies the processed actions to the articulation\\'s joints as relative position commands.\\n\\n    Unlike :class:`JointPositionAction`, this action term applies the processed actions as relative position commands.\\n    This means that the processed actions are added to the current joint positions of the articulation\\'s joints\\n    before being sent as position commands.\\n\\n    This means that the action applied at every step is:\\n\\n    .. math::\\n\\n         \\\\text{applied action} = \\\\text{current joint positions} + \\\\text{processed actions}\\n\\n    where :math:`\\\\text{current joint positions}` are the current joint positions of the articulation\\'s joints.\\n    \"\"\"\\n\\n    cfg: actions_cfg.RelativeJointPositionActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.RelativeJointPositionActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n        # use zero offset for relative position\\n        if cfg.use_zero_offset:\\n            self._offset = 0.0\\n\\n    def apply_actions(self):\\n        # add current joint positions to the processed actions\\n        current_actions = self.processed_actions + self._asset.data.joint_pos[:, self._joint_ids]\\n        # set position targets\\n        self._asset.set_joint_position_target(current_actions, joint_ids=self._joint_ids)'),\n",
       " Document(metadata={}, page_content='class JointVelocityAction(JointAction):\\n    \"\"\"Joint action term that applies the processed actions to the articulation\\'s joints as velocity commands.\"\"\"\\n\\n    cfg: actions_cfg.JointVelocityActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.JointVelocityActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n        # use default joint velocity as offset\\n        if cfg.use_default_offset:\\n            self._offset = self._asset.data.default_joint_vel[:, self._joint_ids].clone()\\n\\n    def apply_actions(self):\\n        # set joint velocity targets\\n        self._asset.set_joint_velocity_target(self.processed_actions, joint_ids=self._joint_ids)'),\n",
       " Document(metadata={}, page_content='class JointEffortAction(JointAction):\\n    \"\"\"Joint action term that applies the processed actions to the articulation\\'s joints as effort commands.\"\"\"\\n\\n    cfg: actions_cfg.JointEffortActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.JointEffortActionCfg, env: ManagerBasedEnv):\\n        super().__init__(cfg, env)\\n\\n    def apply_actions(self):\\n        # set joint effort targets\\n        self._asset.set_joint_effort_target(self.processed_actions, joint_ids=self._joint_ids)'),\n",
       " Document(metadata={}, page_content='class JointPositionToLimitsAction(ActionTerm):\\n    \"\"\"Joint position action term that scales the input actions to the joint limits and applies them to the\\n    articulation\\'s joints.\\n\\n    This class is similar to the :class:`JointPositionAction` class. However, it performs additional\\n    re-scaling of input actions to the actuator joint position limits.\\n\\n    While processing the actions, it performs the following operations:\\n\\n    1. Apply scaling to the raw actions based on :attr:`actions_cfg.JointPositionToLimitsActionCfg.scale`.\\n    2. Clip the scaled actions to the range [-1, 1] and re-scale them to the joint limits if\\n       :attr:`actions_cfg.JointPositionToLimitsActionCfg.rescale_to_limits` is set to True.\\n\\n    The processed actions are then sent as position commands to the articulation\\'s joints.\\n    \"\"\"\\n\\n    cfg: actions_cfg.JointPositionToLimitsActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n    _asset: Articulation\\n    \"\"\"The articulation asset on which the action term is applied.\"\"\"\\n    _scale: torch.Tensor | float\\n    \"\"\"The scaling factor applied to the input action.\"\"\"\\n    _clip: torch.Tensor\\n    \"\"\"The clip applied to the input action.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.JointPositionToLimitsActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n\\n        # resolve the joints over which the action term is applied\\n        self._joint_ids, self._joint_names = self._asset.find_joints(self.cfg.joint_names)\\n        self._num_joints = len(self._joint_ids)\\n        # log the resolved joint names for debugging\\n        omni.log.info(\\n            f\"Resolved joint names for the action term {self.__class__.__name__}:\"\\n            f\" {self._joint_names} [{self._joint_ids}]\"\\n        )\\n\\n        # Avoid indexing across all joints for efficiency\\n        if self._num_joints == self._asset.num_joints:\\n            self._joint_ids = slice(None)\\n\\n        # create tensors for raw and processed actions\\n        self._raw_actions = torch.zeros(self.num_envs, self.action_dim, device=self.device)\\n        self._processed_actions = torch.zeros_like(self.raw_actions)\\n\\n        # parse scale\\n        if isinstance(cfg.scale, (float, int)):\\n            self._scale = float(cfg.scale)\\n        elif isinstance(cfg.scale, dict):\\n            self._scale = torch.ones(self.num_envs, self.action_dim, device=self.device)\\n            # resolve the dictionary config\\n            index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.scale, self._joint_names)\\n            self._scale[:, index_list] = torch.tensor(value_list, device=self.device)\\n        else:\\n            raise ValueError(f\"Unsupported scale type: {type(cfg.scale)}. Supported types are float and dict.\")\\n        # parse clip\\n        if self.cfg.clip is not None:\\n            if isinstance(cfg.clip, dict):\\n                self._clip = torch.tensor([[-float(\"inf\"), float(\"inf\")]], device=self.device).repeat(\\n                    self.num_envs, self.action_dim, 1\\n                )\\n                index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.clip, self._joint_names)\\n                self._clip[:, index_list] = torch.tensor(value_list, device=self.device)\\n            else:\\n                raise ValueError(f\"Unsupported clip type: {type(cfg.clip)}. Supported types are dict.\")\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        return self._num_joints\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        return self._processed_actions\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        # store the raw actions\\n        self._raw_actions[:] = actions\\n        # apply affine transformations\\n        self._processed_actions = self._raw_actions * self._scale\\n        if self.cfg.clip is not None:\\n            self._processed_actions = torch.clamp(\\n                self._processed_actions, min=self._clip[:, :, 0], max=self._clip[:, :, 1]\\n            )\\n        # rescale the position targets if configured\\n        # this is useful when the input actions are in the range [-1, 1]\\n        if self.cfg.rescale_to_limits:\\n            # clip to [-1, 1]\\n            actions = self._processed_actions.clamp(-1.0, 1.0)\\n            # rescale within the joint limits\\n            actions = math_utils.unscale_transform(\\n                actions,\\n                self._asset.data.soft_joint_pos_limits[:, self._joint_ids, 0],\\n                self._asset.data.soft_joint_pos_limits[:, self._joint_ids, 1],\\n            )\\n            self._processed_actions[:] = actions[:]\\n\\n    def apply_actions(self):\\n        # set position targets\\n        self._asset.set_joint_position_target(self.processed_actions, joint_ids=self._joint_ids)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        self._raw_actions[env_ids] = 0.0'),\n",
       " Document(metadata={}, page_content='class EMAJointPositionToLimitsAction(JointPositionToLimitsAction):\\n    r\"\"\"Joint action term that applies exponential moving average (EMA) over the processed actions as the\\n    articulation\\'s joints position commands.\\n\\n    Exponential moving average (EMA) is a type of moving average that gives more weight to the most recent data points.\\n    This action term applies the processed actions as moving average position action commands.\\n    The moving average is computed as:\\n\\n    .. math::\\n\\n        \\\\text{applied action} = \\\\alpha \\\\times \\\\text{processed actions} + (1 - \\\\alpha) \\\\times \\\\text{previous applied action}\\n\\n    where :math:`\\\\alpha` is the weight for the moving average, :math:`\\\\text{processed actions}` are the\\n    processed actions, and :math:`\\\\text{previous action}` is the previous action that was applied to the articulation\\'s\\n    joints.\\n\\n    In the trivial case where the weight is 1.0, the action term behaves exactly like\\n    the :class:`JointPositionToLimitsAction` class.\\n\\n    On reset, the previous action is initialized to the current joint positions of the articulation\\'s joints.\\n    \"\"\"\\n\\n    cfg: actions_cfg.EMAJointPositionToLimitsActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.EMAJointPositionToLimitsActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n\\n        # parse and save the moving average weight\\n        if isinstance(cfg.alpha, float):\\n            # check that the weight is in the valid range\\n            if not 0.0 <= cfg.alpha <= 1.0:\\n                raise ValueError(f\"Moving average weight must be in the range [0, 1]. Got {cfg.alpha}.\")\\n            self._alpha = cfg.alpha\\n        elif isinstance(cfg.alpha, dict):\\n            self._alpha = torch.ones((env.num_envs, self.action_dim), device=self.device)\\n            # resolve the dictionary config\\n            index_list, names_list, value_list = string_utils.resolve_matching_names_values(\\n                cfg.alpha, self._joint_names\\n            )\\n            # check that the weights are in the valid range\\n            for name, value in zip(names_list, value_list):\\n                if not 0.0 <= value <= 1.0:\\n                    raise ValueError(\\n                        f\"Moving average weight must be in the range [0, 1]. Got {value} for joint {name}.\"\\n                    )\\n            self._alpha[:, index_list] = torch.tensor(value_list, device=self.device)\\n        else:\\n            raise ValueError(\\n                f\"Unsupported moving average weight type: {type(cfg.alpha)}. Supported types are float and dict.\"\\n            )\\n\\n        # initialize the previous targets\\n        self._prev_applied_actions = torch.zeros_like(self.processed_actions)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        # check if specific environment ids are provided\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        else:\\n            env_ids = env_ids[:, None]\\n        super().reset(env_ids)\\n        # reset history to current joint positions\\n        self._prev_applied_actions[env_ids, :] = self._asset.data.joint_pos[env_ids, self._joint_ids]\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        # apply affine transformations\\n        super().process_actions(actions)\\n        # set position targets as moving average\\n        ema_actions = self._alpha * self._processed_actions\\n        ema_actions += (1.0 - self._alpha) * self._prev_applied_actions\\n        # clamp the targets\\n        self._processed_actions[:] = torch.clamp(\\n            ema_actions,\\n            self._asset.data.soft_joint_pos_limits[:, self._joint_ids, 0],\\n            self._asset.data.soft_joint_pos_limits[:, self._joint_ids, 1],\\n        )\\n        # update previous targets\\n        self._prev_applied_actions[:] = self._processed_actions[:]'),\n",
       " Document(metadata={}, page_content='class NonHolonomicAction(ActionTerm):\\n    r\"\"\"Non-holonomic action that maps a two dimensional action to the velocity of the robot in\\n    the x, y and yaw directions.\\n\\n    This action term helps model a skid-steer robot base. The action is a 2D vector which comprises of the\\n    forward velocity :math:`v_{B,x}` and the turning rate :\\\\omega_{B,z}: in the base frame. Using the current\\n    base orientation, the commands are transformed into dummy joint velocity targets as:\\n\\n    .. math::\\n\\n        \\\\dot{q}_{0, des} &= v_{B,x} \\\\cos(\\\\theta) \\\\\\\\\\n        \\\\dot{q}_{1, des} &= v_{B,x} \\\\sin(\\\\theta) \\\\\\\\\\n        \\\\dot{q}_{2, des} &= \\\\omega_{B,z}\\n\\n    where :math:`\\\\theta` is the yaw of the 2-D base. Since the base is simulated as a dummy joint, the yaw is directly\\n    the value of the revolute joint along z, i.e., :math:`q_2 = \\\\theta`.\\n\\n    .. note::\\n        The current implementation assumes that the base is simulated with three dummy joints (prismatic joints along x\\n        and y, and revolute joint along z). This is because it is easier to consider the mobile base as a floating link\\n        controlled by three dummy joints, in comparison to simulating wheels which is at times is tricky because of\\n        friction settings.\\n\\n        However, the action term can be extended to support other base configurations as well.\\n\\n    .. tip::\\n        For velocity control of the base with dummy mechanism, we recommend setting high damping gains to the joints.\\n        This ensures that the base remains unperturbed from external disturbances, such as an arm mounted on the base.\\n    \"\"\"\\n\\n    cfg: actions_cfg.NonHolonomicActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n    _asset: Articulation\\n    \"\"\"The articulation asset on which the action term is applied.\"\"\"\\n    _scale: torch.Tensor\\n    \"\"\"The scaling factor applied to the input action. Shape is (1, 2).\"\"\"\\n    _offset: torch.Tensor\\n    \"\"\"The offset applied to the input action. Shape is (1, 2).\"\"\"\\n    _clip: torch.Tensor\\n    \"\"\"The clip applied to the input action.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.NonHolonomicActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n\\n        # parse the joint information\\n        # -- x joint\\n        x_joint_id, x_joint_name = self._asset.find_joints(self.cfg.x_joint_name)\\n        if len(x_joint_id) != 1:\\n            raise ValueError(\\n                f\"Expected a single joint match for the x joint name: {self.cfg.x_joint_name}, got {len(x_joint_id)}\"\\n            )\\n        # -- y joint\\n        y_joint_id, y_joint_name = self._asset.find_joints(self.cfg.y_joint_name)\\n        if len(y_joint_id) != 1:\\n            raise ValueError(f\"Found more than one joint match for the y joint name: {self.cfg.y_joint_name}\")\\n        # -- yaw joint\\n        yaw_joint_id, yaw_joint_name = self._asset.find_joints(self.cfg.yaw_joint_name)\\n        if len(yaw_joint_id) != 1:\\n            raise ValueError(f\"Found more than one joint match for the yaw joint name: {self.cfg.yaw_joint_name}\")\\n        # parse the body index\\n        self._body_idx, self._body_name = self._asset.find_bodies(self.cfg.body_name)\\n        if len(self._body_idx) != 1:\\n            raise ValueError(f\"Found more than one body match for the body name: {self.cfg.body_name}\")\\n\\n        # process into a list of joint ids\\n        self._joint_ids = [x_joint_id[0], y_joint_id[0], yaw_joint_id[0]]\\n        self._joint_names = [x_joint_name[0], y_joint_name[0], yaw_joint_name[0]]\\n        # log info for debugging\\n        omni.log.info(\\n            f\"Resolved joint names for the action term {self.__class__.__name__}:\"\\n            f\" {self._joint_names} [{self._joint_ids}]\"\\n        )\\n        omni.log.info(\\n            f\"Resolved body name for the action term {self.__class__.__name__}: {self._body_name} [{self._body_idx}]\"\\n        )\\n\\n        # create tensors for raw and processed actions\\n        self._raw_actions = torch.zeros(self.num_envs, self.action_dim, device=self.device)\\n        self._processed_actions = torch.zeros_like(self.raw_actions)\\n        self._joint_vel_command = torch.zeros(self.num_envs, 3, device=self.device)\\n\\n        # save the scale and offset as tensors\\n        self._scale = torch.tensor(self.cfg.scale, device=self.device).unsqueeze(0)\\n        self._offset = torch.tensor(self.cfg.offset, device=self.device).unsqueeze(0)\\n        # parse clip\\n        if self.cfg.clip is not None:\\n            if isinstance(cfg.clip, dict):\\n                self._clip = torch.tensor([[-float(\"inf\"), float(\"inf\")]], device=self.device).repeat(\\n                    self.num_envs, self.action_dim, 1\\n                )\\n                index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.clip, self._joint_names)\\n                self._clip[:, index_list] = torch.tensor(value_list, device=self.device)\\n            else:\\n                raise ValueError(f\"Unsupported clip type: {type(cfg.clip)}. Supported types are dict.\")\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        return 2\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        return self._processed_actions\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def process_actions(self, actions):\\n        # store the raw actions\\n        self._raw_actions[:] = actions\\n        self._processed_actions = self.raw_actions * self._scale + self._offset\\n        # clip actions\\n        if self.cfg.clip is not None:\\n            self._processed_actions = torch.clamp(\\n                self._processed_actions, min=self._clip[:, :, 0], max=self._clip[:, :, 1]\\n            )\\n\\n    def apply_actions(self):\\n        # obtain current heading\\n        quat_w = self._asset.data.body_quat_w[:, self._body_idx].view(self.num_envs, 4)\\n        yaw_w = euler_xyz_from_quat(quat_w)[2]\\n        # compute joint velocities targets\\n        self._joint_vel_command[:, 0] = torch.cos(yaw_w) * self.processed_actions[:, 0]  # x\\n        self._joint_vel_command[:, 1] = torch.sin(yaw_w) * self.processed_actions[:, 0]  # y\\n        self._joint_vel_command[:, 2] = self.processed_actions[:, 1]  # yaw\\n        # set the joint velocity targets\\n        self._asset.set_joint_velocity_target(self._joint_vel_command, joint_ids=self._joint_ids)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        self._raw_actions[env_ids] = 0.0'),\n",
       " Document(metadata={}, page_content='class PinkInverseKinematicsActionCfg(ActionTermCfg):\\n    \"\"\"Configuration for Pink inverse kinematics action term.\\n\\n    This configuration is used to define settings for the Pink inverse kinematics action term,\\n    which is a inverse kinematics framework.\\n    \"\"\"\\n\\n    class_type: type[ActionTerm] = pink_task_space_actions.PinkInverseKinematicsAction\\n    \"\"\"Specifies the action term class type for Pink inverse kinematics action.\"\"\"\\n\\n    pink_controlled_joint_names: list[str] = MISSING\\n    \"\"\"List of joint names or regular expression patterns that specify the joints controlled by pink IK.\"\"\"\\n\\n    ik_urdf_fixed_joint_names: list[str] = MISSING\\n    \"\"\"List of joint names that specify the joints to be locked in URDF.\"\"\"\\n\\n    hand_joint_names: list[str] = MISSING\\n    \"\"\"List of joint names or regular expression patterns that specify the joints controlled by hand retargeting.\"\"\"\\n\\n    controller: PinkIKControllerCfg = MISSING\\n    \"\"\"Configuration for the Pink IK controller that will be used to solve the inverse kinematics.\"\"\"'),\n",
       " Document(metadata={}, page_content='class PinkInverseKinematicsAction(ActionTerm):\\n    r\"\"\"Pink Inverse Kinematics action term.\\n\\n    This action term processes the action tensor and sets these setpoints in the pink IK framework\\n    The action tensor is ordered in the order of the tasks defined in PinkIKControllerCfg\\n    \"\"\"\\n\\n    cfg: pink_actions_cfg.PinkInverseKinematicsActionCfg\\n    \"\"\"Configuration for the Pink Inverse Kinematics action term.\"\"\"\\n\\n    _asset: Articulation\\n    \"\"\"The articulation asset to which the action term is applied.\"\"\"\\n\\n    def __init__(self, cfg: pink_actions_cfg.PinkInverseKinematicsActionCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the Pink Inverse Kinematics action term.\\n\\n        Args:\\n            cfg: The configuration for this action term.\\n            env: The environment in which the action term will be applied.\\n        \"\"\"\\n        super().__init__(cfg, env)\\n\\n        # Resolve joint IDs and names based on the configuration\\n        self._pink_controlled_joint_ids, self._pink_controlled_joint_names = self._asset.find_joints(\\n            self.cfg.pink_controlled_joint_names\\n        )\\n        self.cfg.controller.joint_names = self._pink_controlled_joint_names\\n        self._hand_joint_ids, self._hand_joint_names = self._asset.find_joints(self.cfg.hand_joint_names)\\n        self._joint_ids = self._pink_controlled_joint_ids + self._hand_joint_ids\\n        self._joint_names = self._pink_controlled_joint_names + self._hand_joint_names\\n\\n        # Initialize the Pink IK controller\\n        assert env.num_envs > 0, \"Number of environments specified are less than 1.\"\\n        self._ik_controllers = []\\n        for _ in range(env.num_envs):\\n            self._ik_controllers.append(PinkIKController(cfg=copy.deepcopy(self.cfg.controller), device=self.device))\\n\\n        # Create tensors to store raw and processed actions\\n        self._raw_actions = torch.zeros(self.num_envs, self.action_dim, device=self.device)\\n        self._processed_actions = torch.zeros_like(self.raw_actions)\\n\\n        # Get the simulation time step\\n        self._sim_dt = env.sim.get_physics_dt()\\n\\n        self.total_time = 0  # Variable to accumulate the total time\\n        self.num_runs = 0  # Counter for the number of runs\\n\\n        # Save the base_link_frame pose in the world frame as a transformation matrix in\\n        # order to transform the desired pose of the controlled_frame to be with respect to the base_link_frame\\n        # Shape of env.scene[self.cfg.articulation_name].data.body_link_state_w is (num_instances, num_bodies, 13)\\n        base_link_frame_in_world_origin = env.scene[self.cfg.controller.articulation_name].data.body_link_state_w[\\n            :,\\n            env.scene[self.cfg.controller.articulation_name].data.body_names.index(self.cfg.controller.base_link_name),\\n            :7,\\n        ]\\n\\n        # Get robot base link frame in env origin frame\\n        base_link_frame_in_env_origin = copy.deepcopy(base_link_frame_in_world_origin)\\n        base_link_frame_in_env_origin[:, :3] -= self._env.scene.env_origins\\n\\n        self.base_link_frame_in_env_origin = math_utils.make_pose(\\n            base_link_frame_in_env_origin[:, :3], math_utils.matrix_from_quat(base_link_frame_in_env_origin[:, 3:7])\\n        )\\n\\n    # \"\"\"\\n    # Properties.\\n    # \"\"\"\\n\\n    @property\\n    def hand_joint_dim(self) -> int:\\n        \"\"\"Dimension for hand joint positions.\"\"\"\\n        return self.cfg.controller.num_hand_joints\\n\\n    @property\\n    def position_dim(self) -> int:\\n        \"\"\"Dimension for position (x, y, z).\"\"\"\\n        return 3\\n\\n    @property\\n    def orientation_dim(self) -> int:\\n        \"\"\"Dimension for orientation (w, x, y, z).\"\"\"\\n        return 4\\n\\n    @property\\n    def pose_dim(self) -> int:\\n        \"\"\"Total pose dimension (position + orientation).\"\"\"\\n        return self.position_dim + self.orientation_dim\\n\\n    @property\\n    def action_dim(self) -> int:\\n        \"\"\"Dimension of the action space (based on number of tasks and pose dimension).\"\"\"\\n        # The tasks for all the controllers are the same, hence just using the first one to calculate the action_dim\\n        return len(self._ik_controllers[0].cfg.variable_input_tasks) * self.pose_dim + self.hand_joint_dim\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        \"\"\"Get the raw actions tensor.\"\"\"\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        \"\"\"Get the processed actions tensor.\"\"\"\\n        return self._processed_actions\\n\\n    # \"\"\"\\n    # Operations.\\n    # \"\"\"\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        \"\"\"Process the input actions and set targets for each task.\\n\\n        Args:\\n            actions: The input actions tensor.\\n        \"\"\"\\n        # Store the raw actions\\n        self._raw_actions[:] = actions\\n        self._processed_actions[:] = self.raw_actions\\n\\n        # Make a copy of actions before modifying so that raw actions are not modified\\n        actions_clone = actions.clone()\\n\\n        # Extract hand joint positions (last 22 values)\\n        self._target_hand_joint_positions = actions_clone[:, -self.hand_joint_dim :]\\n\\n        # The action tensor provides the desired pose of the controlled_frame with respect to the env origin frame\\n        # But the pink IK controller expects the desired pose of the controlled_frame with respect to the base_link_frame\\n        # So we need to transform the desired pose of the controlled_frame to be with respect to the base_link_frame\\n\\n        # Get the controlled_frame pose wrt to the env origin frame\\n        all_controlled_frames_in_env_origin = []\\n        # The contrllers for all envs are the same, hence just using the first one to get the number of variable_input_tasks\\n        for task_index in range(len(self._ik_controllers[0].cfg.variable_input_tasks)):\\n            controlled_frame_in_env_origin_pos = actions_clone[\\n                :, task_index * self.pose_dim : task_index * self.pose_dim + self.position_dim\\n            ]\\n            controlled_frame_in_env_origin_quat = actions_clone[\\n                :, task_index * self.pose_dim + self.position_dim : (task_index + 1) * self.pose_dim\\n            ]\\n            controlled_frame_in_env_origin = math_utils.make_pose(\\n                controlled_frame_in_env_origin_pos, math_utils.matrix_from_quat(controlled_frame_in_env_origin_quat)\\n            )\\n            all_controlled_frames_in_env_origin.append(controlled_frame_in_env_origin)\\n        # Stack all the controlled_frame poses in the env origin frame. Shape is (num_tasks, num_envs , 4, 4)\\n        all_controlled_frames_in_env_origin = torch.stack(all_controlled_frames_in_env_origin)\\n\\n        # Transform the controlled_frame to be with respect to the base_link_frame using batched matrix multiplication\\n        controlled_frame_in_base_link_frame = math_utils.pose_in_A_to_pose_in_B(\\n            all_controlled_frames_in_env_origin, math_utils.pose_inv(self.base_link_frame_in_env_origin)\\n        )\\n\\n        controlled_frame_in_base_link_frame_pos, controlled_frame_in_base_link_frame_mat = math_utils.unmake_pose(\\n            controlled_frame_in_base_link_frame\\n        )\\n\\n        # Loop through each task and set the target\\n        for env_index, ik_controller in enumerate(self._ik_controllers):\\n            for task_index, task in enumerate(ik_controller.cfg.variable_input_tasks):\\n                target = task.transform_target_to_world\\n                target.translation = controlled_frame_in_base_link_frame_pos[task_index, env_index, :].cpu().numpy()\\n                target.rotation = controlled_frame_in_base_link_frame_mat[task_index, env_index, :].cpu().numpy()\\n                task.set_target(target)\\n\\n    def apply_actions(self):\\n        # start_time = time.time()  # Capture the time before the step\\n        \"\"\"Apply the computed joint positions based on the inverse kinematics solution.\"\"\"\\n        all_envs_joint_pos_des = []\\n        for env_index, ik_controller in enumerate(self._ik_controllers):\\n            curr_joint_pos = self._asset.data.joint_pos[:, self._pink_controlled_joint_ids].cpu().numpy()[env_index]\\n            joint_pos_des = ik_controller.compute(curr_joint_pos, self._sim_dt)\\n            all_envs_joint_pos_des.append(joint_pos_des)\\n        all_envs_joint_pos_des = torch.stack(all_envs_joint_pos_des)\\n        # Combine IK joint positions with hand joint positions\\n        all_envs_joint_pos_des = torch.cat((all_envs_joint_pos_des, self._target_hand_joint_positions), dim=1)\\n\\n        self._asset.set_joint_position_target(all_envs_joint_pos_des, self._joint_ids)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        \"\"\"Reset the action term for specified environments.\\n\\n        Args:\\n            env_ids: A list of environment IDs to reset. If None, all environments are reset.\\n        \"\"\"\\n        self._raw_actions[env_ids] = torch.zeros(self.action_dim, device=self.device)'),\n",
       " Document(metadata={}, page_content='class DifferentialInverseKinematicsAction(ActionTerm):\\n    r\"\"\"Inverse Kinematics action term.\\n\\n    This action term performs pre-processing of the raw actions using scaling transformation.\\n\\n    .. math::\\n        \\\\text{action} = \\\\text{scaling} \\\\times \\\\text{input action}\\n        \\\\text{joint position} = J^{-} \\\\times \\\\text{action}\\n\\n    where :math:`\\\\text{scaling}` is the scaling applied to the input action, and :math:`\\\\text{input action}`\\n    is the input action from the user, :math:`J` is the Jacobian over the articulation\\'s actuated joints,\\n    and \\\\text{joint position} is the desired joint position command for the articulation\\'s joints.\\n    \"\"\"\\n\\n    cfg: actions_cfg.DifferentialInverseKinematicsActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n    _asset: Articulation\\n    \"\"\"The articulation asset on which the action term is applied.\"\"\"\\n    _scale: torch.Tensor\\n    \"\"\"The scaling factor applied to the input action. Shape is (1, action_dim).\"\"\"\\n    _clip: torch.Tensor\\n    \"\"\"The clip applied to the input action.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.DifferentialInverseKinematicsActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n\\n        # resolve the joints over which the action term is applied\\n        self._joint_ids, self._joint_names = self._asset.find_joints(self.cfg.joint_names)\\n        self._num_joints = len(self._joint_ids)\\n        # parse the body index\\n        body_ids, body_names = self._asset.find_bodies(self.cfg.body_name)\\n        if len(body_ids) != 1:\\n            raise ValueError(\\n                f\"Expected one match for the body name: {self.cfg.body_name}. Found {len(body_ids)}: {body_names}.\"\\n            )\\n        # save only the first body index\\n        self._body_idx = body_ids[0]\\n        self._body_name = body_names[0]\\n        # check if articulation is fixed-base\\n        # if fixed-base then the jacobian for the base is not computed\\n        # this means that number of bodies is one less than the articulation\\'s number of bodies\\n        if self._asset.is_fixed_base:\\n            self._jacobi_body_idx = self._body_idx - 1\\n            self._jacobi_joint_ids = self._joint_ids\\n        else:\\n            self._jacobi_body_idx = self._body_idx\\n            self._jacobi_joint_ids = [i + 6 for i in self._joint_ids]\\n\\n        # log info for debugging\\n        omni.log.info(\\n            f\"Resolved joint names for the action term {self.__class__.__name__}:\"\\n            f\" {self._joint_names} [{self._joint_ids}]\"\\n        )\\n        omni.log.info(\\n            f\"Resolved body name for the action term {self.__class__.__name__}: {self._body_name} [{self._body_idx}]\"\\n        )\\n        # Avoid indexing across all joints for efficiency\\n        if self._num_joints == self._asset.num_joints:\\n            self._joint_ids = slice(None)\\n\\n        # create the differential IK controller\\n        self._ik_controller = DifferentialIKController(\\n            cfg=self.cfg.controller, num_envs=self.num_envs, device=self.device\\n        )\\n\\n        # create tensors for raw and processed actions\\n        self._raw_actions = torch.zeros(self.num_envs, self.action_dim, device=self.device)\\n        self._processed_actions = torch.zeros_like(self.raw_actions)\\n\\n        # save the scale as tensors\\n        self._scale = torch.zeros((self.num_envs, self.action_dim), device=self.device)\\n        self._scale[:] = torch.tensor(self.cfg.scale, device=self.device)\\n\\n        # convert the fixed offsets to torch tensors of batched shape\\n        if self.cfg.body_offset is not None:\\n            self._offset_pos = torch.tensor(self.cfg.body_offset.pos, device=self.device).repeat(self.num_envs, 1)\\n            self._offset_rot = torch.tensor(self.cfg.body_offset.rot, device=self.device).repeat(self.num_envs, 1)\\n        else:\\n            self._offset_pos, self._offset_rot = None, None\\n\\n        # parse clip\\n        if self.cfg.clip is not None:\\n            if isinstance(cfg.clip, dict):\\n                self._clip = torch.tensor([[-float(\"inf\"), float(\"inf\")]], device=self.device).repeat(\\n                    self.num_envs, self.action_dim, 1\\n                )\\n                index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.clip, self._joint_names)\\n                self._clip[:, index_list] = torch.tensor(value_list, device=self.device)\\n            else:\\n                raise ValueError(f\"Unsupported clip type: {type(cfg.clip)}. Supported types are dict.\")\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        return self._ik_controller.action_dim\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        return self._processed_actions\\n\\n    @property\\n    def jacobian_w(self) -> torch.Tensor:\\n        return self._asset.root_physx_view.get_jacobians()[:, self._jacobi_body_idx, :, self._jacobi_joint_ids]\\n\\n    @property\\n    def jacobian_b(self) -> torch.Tensor:\\n        jacobian = self.jacobian_w\\n        base_rot = self._asset.data.root_quat_w\\n        base_rot_matrix = math_utils.matrix_from_quat(math_utils.quat_inv(base_rot))\\n        jacobian[:, :3, :] = torch.bmm(base_rot_matrix, jacobian[:, :3, :])\\n        jacobian[:, 3:, :] = torch.bmm(base_rot_matrix, jacobian[:, 3:, :])\\n        return jacobian\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        # store the raw actions\\n        self._raw_actions[:] = actions\\n        self._processed_actions[:] = self.raw_actions * self._scale\\n        if self.cfg.clip is not None:\\n            self._processed_actions = torch.clamp(\\n                self._processed_actions, min=self._clip[:, :, 0], max=self._clip[:, :, 1]\\n            )\\n        # obtain quantities from simulation\\n        ee_pos_curr, ee_quat_curr = self._compute_frame_pose()\\n        # set command into controller\\n        self._ik_controller.set_command(self._processed_actions, ee_pos_curr, ee_quat_curr)\\n\\n    def apply_actions(self):\\n        # obtain quantities from simulation\\n        ee_pos_curr, ee_quat_curr = self._compute_frame_pose()\\n        joint_pos = self._asset.data.joint_pos[:, self._joint_ids]\\n        # compute the delta in joint-space\\n        if ee_quat_curr.norm() != 0:\\n            jacobian = self._compute_frame_jacobian()\\n            joint_pos_des = self._ik_controller.compute(ee_pos_curr, ee_quat_curr, jacobian, joint_pos)\\n        else:\\n            joint_pos_des = joint_pos.clone()\\n        # set the joint position command\\n        self._asset.set_joint_position_target(joint_pos_des, self._joint_ids)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        self._raw_actions[env_ids] = 0.0\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _compute_frame_pose(self) -> tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"Computes the pose of the target frame in the root frame.\\n\\n        Returns:\\n            A tuple of the body\\'s position and orientation in the root frame.\\n        \"\"\"\\n        # obtain quantities from simulation\\n        ee_pos_w = self._asset.data.body_pos_w[:, self._body_idx]\\n        ee_quat_w = self._asset.data.body_quat_w[:, self._body_idx]\\n        root_pos_w = self._asset.data.root_pos_w\\n        root_quat_w = self._asset.data.root_quat_w\\n        # compute the pose of the body in the root frame\\n        ee_pose_b, ee_quat_b = math_utils.subtract_frame_transforms(root_pos_w, root_quat_w, ee_pos_w, ee_quat_w)\\n        # account for the offset\\n        if self.cfg.body_offset is not None:\\n            ee_pose_b, ee_quat_b = math_utils.combine_frame_transforms(\\n                ee_pose_b, ee_quat_b, self._offset_pos, self._offset_rot\\n            )\\n\\n        return ee_pose_b, ee_quat_b\\n\\n    def _compute_frame_jacobian(self):\\n        \"\"\"Computes the geometric Jacobian of the target frame in the root frame.\\n\\n        This function accounts for the target frame offset and applies the necessary transformations to obtain\\n        the right Jacobian from the parent body Jacobian.\\n        \"\"\"\\n        # read the parent jacobian\\n        jacobian = self.jacobian_b\\n        # account for the offset\\n        if self.cfg.body_offset is not None:\\n            # Modify the jacobian to account for the offset\\n            # -- translational part\\n            # v_link = v_ee + w_ee x r_link_ee = v_J_ee * q + w_J_ee * q x r_link_ee\\n            #        = (v_J_ee + w_J_ee x r_link_ee ) * q\\n            #        = (v_J_ee - r_link_ee_[x] @ w_J_ee) * q\\n            jacobian[:, 0:3, :] += torch.bmm(-math_utils.skew_symmetric_matrix(self._offset_pos), jacobian[:, 3:, :])\\n            # -- rotational part\\n            # w_link = R_link_ee @ w_ee\\n            jacobian[:, 3:, :] = torch.bmm(math_utils.matrix_from_quat(self._offset_rot), jacobian[:, 3:, :])\\n\\n        return jacobian'),\n",
       " Document(metadata={}, page_content='class OperationalSpaceControllerAction(ActionTerm):\\n    r\"\"\"Operational space controller action term.\\n\\n    This action term performs pre-processing of the raw actions for operational space control.\\n\\n    \"\"\"\\n\\n    cfg: actions_cfg.OperationalSpaceControllerActionCfg\\n    \"\"\"The configuration of the action term.\"\"\"\\n    _asset: Articulation\\n    \"\"\"The articulation asset on which the action term is applied.\"\"\"\\n    _contact_sensor: ContactSensor = None\\n    \"\"\"The contact sensor for the end-effector body.\"\"\"\\n    _task_frame_transformer: FrameTransformer = None\\n    \"\"\"The frame transformer for the task frame.\"\"\"\\n\\n    def __init__(self, cfg: actions_cfg.OperationalSpaceControllerActionCfg, env: ManagerBasedEnv):\\n        # initialize the action term\\n        super().__init__(cfg, env)\\n\\n        self._sim_dt = env.sim.get_physics_dt()\\n\\n        # resolve the joints over which the action term is applied\\n        self._joint_ids, self._joint_names = self._asset.find_joints(self.cfg.joint_names)\\n        self._num_DoF = len(self._joint_ids)\\n        # parse the ee body index\\n        body_ids, body_names = self._asset.find_bodies(self.cfg.body_name)\\n        if len(body_ids) != 1:\\n            raise ValueError(\\n                f\"Expected one match for the ee body name: {self.cfg.body_name}. Found {len(body_ids)}: {body_names}.\"\\n            )\\n        # save only the first ee body index\\n        self._ee_body_idx = body_ids[0]\\n        self._ee_body_name = body_names[0]\\n        # check if articulation is fixed-base\\n        # if fixed-base then the jacobian for the base is not computed\\n        # this means that number of bodies is one less than the articulation\\'s number of bodies\\n        if self._asset.is_fixed_base:\\n            self._jacobi_ee_body_idx = self._ee_body_idx - 1\\n            self._jacobi_joint_idx = self._joint_ids\\n        else:\\n            self._jacobi_ee_body_idx = self._ee_body_idx\\n            self._jacobi_joint_idx = [i + 6 for i in self._joint_ids]\\n\\n        # log info for debugging\\n        omni.log.info(\\n            f\"Resolved joint names for the action term {self.__class__.__name__}:\"\\n            f\" {self._joint_names} [{self._joint_ids}]\"\\n        )\\n        omni.log.info(\\n            f\"Resolved ee body name for the action term {self.__class__.__name__}:\"\\n            f\" {self._ee_body_name} [{self._ee_body_idx}]\"\\n        )\\n        # Avoid indexing across all joints for efficiency\\n        if self._num_DoF == self._asset.num_joints:\\n            self._joint_ids = slice(None)\\n\\n        # convert the fixed offsets to torch tensors of batched shape\\n        if self.cfg.body_offset is not None:\\n            self._offset_pos = torch.tensor(self.cfg.body_offset.pos, device=self.device).repeat(self.num_envs, 1)\\n            self._offset_rot = torch.tensor(self.cfg.body_offset.rot, device=self.device).repeat(self.num_envs, 1)\\n        else:\\n            self._offset_pos, self._offset_rot = None, None\\n\\n        # create contact sensor if any of the command is wrench_abs, and if stiffness is provided\\n        if (\\n            \"wrench_abs\" in self.cfg.controller_cfg.target_types\\n            and self.cfg.controller_cfg.contact_wrench_stiffness_task is not None\\n        ):\\n            self._contact_sensor_cfg = ContactSensorCfg(prim_path=self._asset.cfg.prim_path + \"/\" + self._ee_body_name)\\n            self._contact_sensor = ContactSensor(self._contact_sensor_cfg)\\n            if not self._contact_sensor.is_initialized:\\n                self._contact_sensor._initialize_impl()\\n                self._contact_sensor._is_initialized = True\\n\\n        # Initialize the task frame transformer if a relative path for the RigidObject, representing the task frame,\\n        # is provided.\\n        if self.cfg.task_frame_rel_path is not None:\\n            # The source RigidObject can be any child of the articulation asset (we will not use it),\\n            # hence, we will use the first RigidObject child.\\n            root_rigidbody_path = self._first_RigidObject_child_path()\\n            task_frame_transformer_path = \"/World/envs/env_.*/\" + self.cfg.task_frame_rel_path\\n            task_frame_transformer_cfg = FrameTransformerCfg(\\n                prim_path=root_rigidbody_path,\\n                target_frames=[\\n                    FrameTransformerCfg.FrameCfg(\\n                        name=\"task_frame\",\\n                        prim_path=task_frame_transformer_path,\\n                    ),\\n                ],\\n            )\\n            self._task_frame_transformer = FrameTransformer(task_frame_transformer_cfg)\\n            if not self._task_frame_transformer.is_initialized:\\n                self._task_frame_transformer._initialize_impl()\\n                self._task_frame_transformer._is_initialized = True\\n            # create tensor for task frame pose in the root frame\\n            self._task_frame_pose_b = torch.zeros(self.num_envs, 7, device=self.device)\\n        else:\\n            # create an empty reference for task frame pose\\n            self._task_frame_pose_b = None\\n\\n        # create the operational space controller\\n        self._osc = OperationalSpaceController(cfg=self.cfg.controller_cfg, num_envs=self.num_envs, device=self.device)\\n\\n        # create tensors for raw and processed actions\\n        self._raw_actions = torch.zeros(self.num_envs, self.action_dim, device=self.device)\\n        self._processed_actions = torch.zeros_like(self.raw_actions)\\n\\n        # create tensors for the dynamic-related quantities\\n        self._jacobian_b = torch.zeros(self.num_envs, 6, self._num_DoF, device=self.device)\\n        self._mass_matrix = torch.zeros(self.num_envs, self._num_DoF, self._num_DoF, device=self.device)\\n        self._gravity = torch.zeros(self.num_envs, self._num_DoF, device=self.device)\\n\\n        # create tensors for the ee states\\n        self._ee_pose_w = torch.zeros(self.num_envs, 7, device=self.device)\\n        self._ee_pose_b = torch.zeros(self.num_envs, 7, device=self.device)\\n        self._ee_pose_b_no_offset = torch.zeros(self.num_envs, 7, device=self.device)  # The original ee without offset\\n        self._ee_vel_w = torch.zeros(self.num_envs, 6, device=self.device)\\n        self._ee_vel_b = torch.zeros(self.num_envs, 6, device=self.device)\\n        self._ee_force_w = torch.zeros(self.num_envs, 3, device=self.device)  # Only the forces are used for now\\n        self._ee_force_b = torch.zeros(self.num_envs, 3, device=self.device)  # Only the forces are used for now\\n\\n        # create tensors for the joint states\\n        self._joint_pos = torch.zeros(self.num_envs, self._num_DoF, device=self.device)\\n        self._joint_vel = torch.zeros(self.num_envs, self._num_DoF, device=self.device)\\n\\n        # create the joint effort tensor\\n        self._joint_efforts = torch.zeros(self.num_envs, self._num_DoF, device=self.device)\\n\\n        # save the scale as tensors\\n        self._position_scale = torch.tensor(self.cfg.position_scale, device=self.device)\\n        self._orientation_scale = torch.tensor(self.cfg.orientation_scale, device=self.device)\\n        self._wrench_scale = torch.tensor(self.cfg.wrench_scale, device=self.device)\\n        self._stiffness_scale = torch.tensor(self.cfg.stiffness_scale, device=self.device)\\n        self._damping_ratio_scale = torch.tensor(self.cfg.damping_ratio_scale, device=self.device)\\n\\n        # indexes for the various command elements (e.g., pose_rel, stifness, etc.) within the command tensor\\n        self._pose_abs_idx = None\\n        self._pose_rel_idx = None\\n        self._wrench_abs_idx = None\\n        self._stiffness_idx = None\\n        self._damping_ratio_idx = None\\n        self._resolve_command_indexes()\\n\\n        # Nullspace position control joint targets\\n        self._nullspace_joint_pos_target = None\\n        self._resolve_nullspace_joint_pos_targets()\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def action_dim(self) -> int:\\n        \"\"\"Dimension of the action space of operational space control.\"\"\"\\n        return self._osc.action_dim\\n\\n    @property\\n    def raw_actions(self) -> torch.Tensor:\\n        \"\"\"Raw actions for operational space control.\"\"\"\\n        return self._raw_actions\\n\\n    @property\\n    def processed_actions(self) -> torch.Tensor:\\n        \"\"\"Processed actions for operational space control.\"\"\"\\n        return self._processed_actions\\n\\n    @property\\n    def jacobian_w(self) -> torch.Tensor:\\n        return self._asset.root_physx_view.get_jacobians()[:, self._jacobi_ee_body_idx, :, self._jacobi_joint_idx]\\n\\n    @property\\n    def jacobian_b(self) -> torch.Tensor:\\n        jacobian = self.jacobian_w\\n        base_rot = self._asset.data.root_quat_w\\n        base_rot_matrix = math_utils.matrix_from_quat(math_utils.quat_inv(base_rot))\\n        jacobian[:, :3, :] = torch.bmm(base_rot_matrix, jacobian[:, :3, :])\\n        jacobian[:, 3:, :] = torch.bmm(base_rot_matrix, jacobian[:, 3:, :])\\n        return jacobian\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def process_actions(self, actions: torch.Tensor):\\n        \"\"\"Pre-processes the raw actions and sets them as commands for for operational space control.\\n\\n        Args:\\n            actions (torch.Tensor): The raw actions for operational space control. It is a tensor of\\n                shape (``num_envs``, ``action_dim``).\\n        \"\"\"\\n\\n        # Update ee pose, which would be used by relative targets (i.e., pose_rel)\\n        self._compute_ee_pose()\\n\\n        # Update task frame pose w.r.t. the root frame.\\n        self._compute_task_frame_pose()\\n\\n        # Pre-process the raw actions for operational space control.\\n        self._preprocess_actions(actions)\\n\\n        # set command into controller\\n        self._osc.set_command(\\n            command=self._processed_actions,\\n            current_ee_pose_b=self._ee_pose_b,\\n            current_task_frame_pose_b=self._task_frame_pose_b,\\n        )\\n\\n    def apply_actions(self):\\n        \"\"\"Computes the joint efforts for operational space control and applies them to the articulation.\"\"\"\\n\\n        # Update the relevant states and dynamical quantities\\n        self._compute_dynamic_quantities()\\n        self._compute_ee_jacobian()\\n        self._compute_ee_pose()\\n        self._compute_ee_velocity()\\n        self._compute_ee_force()\\n        self._compute_joint_states()\\n        # Calculate the joint efforts\\n        self._joint_efforts[:] = self._osc.compute(\\n            jacobian_b=self._jacobian_b,\\n            current_ee_pose_b=self._ee_pose_b,\\n            current_ee_vel_b=self._ee_vel_b,\\n            current_ee_force_b=self._ee_force_b,\\n            mass_matrix=self._mass_matrix,\\n            gravity=self._gravity,\\n            current_joint_pos=self._joint_pos,\\n            current_joint_vel=self._joint_vel,\\n            nullspace_joint_pos_target=self._nullspace_joint_pos_target,\\n        )\\n        self._asset.set_joint_effort_target(self._joint_efforts, joint_ids=self._joint_ids)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        \"\"\"Resets the raw actions and the sensors if available.\\n\\n        Args:\\n            env_ids (Sequence[int] | None): The environment indices to reset. If ``None``, all environments are reset.\\n        \"\"\"\\n        self._raw_actions[env_ids] = 0.0\\n        if self._contact_sensor is not None:\\n            self._contact_sensor.reset(env_ids)\\n        if self._task_frame_transformer is not None:\\n            self._task_frame_transformer.reset(env_ids)\\n\\n    \"\"\"\\n    Helper functions.\\n\\n    \"\"\"\\n\\n    def _first_RigidObject_child_path(self):\\n        \"\"\"Finds the first ``RigidObject`` child under the articulation asset.\\n\\n        Raises:\\n            ValueError: If no child ``RigidObject`` is found under the articulation asset.\\n\\n        Returns:\\n            str: The path to the first ``RigidObject`` child under the articulation asset.\\n        \"\"\"\\n        child_prims = find_matching_prims(self._asset.cfg.prim_path + \"/.*\")\\n        rigid_child_prim = None\\n        # Loop through the list and stop at the first RigidObject found\\n        for prim in child_prims:\\n            if prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n                rigid_child_prim = prim\\n                break\\n        if rigid_child_prim is None:\\n            raise ValueError(\"No child rigid body found under the expression: \\'{self._asset.cfg.prim_path}\\'/.\")\\n        rigid_child_prim_path = rigid_child_prim.GetPath().pathString\\n        # Remove the specific env index from the path string\\n        rigid_child_prim_path = self._asset.cfg.prim_path + \"/\" + rigid_child_prim_path.split(\"/\")[-1]\\n        return rigid_child_prim_path\\n\\n    def _resolve_command_indexes(self):\\n        \"\"\"Resolves the indexes for the various command elements within the command tensor.\\n\\n        Raises:\\n            ValueError: If any command index is left unresolved.\\n        \"\"\"\\n        # First iterate over the target types to find the indexes of the different command elements\\n        cmd_idx = 0\\n        for target_type in self.cfg.controller_cfg.target_types:\\n            if target_type == \"pose_abs\":\\n                self._pose_abs_idx = cmd_idx\\n                cmd_idx += 7\\n            elif target_type == \"pose_rel\":\\n                self._pose_rel_idx = cmd_idx\\n                cmd_idx += 6\\n            elif target_type == \"wrench_abs\":\\n                self._wrench_abs_idx = cmd_idx\\n                cmd_idx += 6\\n            else:\\n                raise ValueError(\"Undefined target_type for OSC within OperationalSpaceControllerAction.\")\\n        # Then iterate over the impedance parameters depending on the impedance mode\\n        if (\\n            self.cfg.controller_cfg.impedance_mode == \"variable_kp\"\\n            or self.cfg.controller_cfg.impedance_mode == \"variable\"\\n        ):\\n            self._stiffness_idx = cmd_idx\\n            cmd_idx += 6\\n            if self.cfg.controller_cfg.impedance_mode == \"variable\":\\n                self._damping_ratio_idx = cmd_idx\\n                cmd_idx += 6\\n\\n        # Check if any command is left unresolved\\n        if self.action_dim != cmd_idx:\\n            raise ValueError(\"Not all command indexes have been resolved.\")\\n\\n    def _resolve_nullspace_joint_pos_targets(self):\\n        \"\"\"Resolves the nullspace joint pos targets for the operational space controller.\\n\\n        Raises:\\n            ValueError: If the nullspace joint pos targets are set when null space control is not set to \\'position\\'.\\n            ValueError: If the nullspace joint pos targets are not set when null space control is set to \\'position\\'.\\n            ValueError: If an invalid value is set for nullspace joint pos targets.\\n        \"\"\"\\n\\n        if self.cfg.nullspace_joint_pos_target != \"none\" and self.cfg.controller_cfg.nullspace_control != \"position\":\\n            raise ValueError(\"Nullspace joint targets can only be set when null space control is set to \\'position\\'.\")\\n\\n        if self.cfg.nullspace_joint_pos_target == \"none\" and self.cfg.controller_cfg.nullspace_control == \"position\":\\n            raise ValueError(\"Nullspace joint targets must be set when null space control is set to \\'position\\'.\")\\n\\n        if self.cfg.nullspace_joint_pos_target == \"zero\" or self.cfg.nullspace_joint_pos_target == \"none\":\\n            # Keep the nullspace joint targets as None as this is later processed as zero in the controller\\n            self._nullspace_joint_pos_target = None\\n        elif self.cfg.nullspace_joint_pos_target == \"center\":\\n            # Get the center of the robot soft joint limits\\n            self._nullspace_joint_pos_target = torch.mean(\\n                self._asset.data.soft_joint_pos_limits[:, self._joint_ids, :], dim=-1\\n            )\\n        elif self.cfg.nullspace_joint_pos_target == \"default\":\\n            # Get the default joint positions\\n            self._nullspace_joint_pos_target = self._asset.data.default_joint_pos[:, self._joint_ids]\\n        else:\\n            raise ValueError(\"Invalid value for nullspace joint pos targets.\")\\n\\n    def _compute_dynamic_quantities(self):\\n        \"\"\"Computes the dynamic quantities for operational space control.\"\"\"\\n\\n        self._mass_matrix[:] = self._asset.root_physx_view.get_generalized_mass_matrices()[:, self._joint_ids, :][\\n            :, :, self._joint_ids\\n        ]\\n        self._gravity[:] = self._asset.root_physx_view.get_gravity_compensation_forces()[:, self._joint_ids]\\n\\n    def _compute_ee_jacobian(self):\\n        \"\"\"Computes the geometric Jacobian of the ee body frame in root frame.\\n\\n        This function accounts for the target frame offset and applies the necessary transformations to obtain\\n        the right Jacobian from the parent body Jacobian.\\n        \"\"\"\\n        # Get the Jacobian in root frame\\n        self._jacobian_b[:] = self.jacobian_b\\n\\n        # account for the offset\\n        if self.cfg.body_offset is not None:\\n            # Modify the jacobian to account for the offset\\n            # -- translational part\\n            # v_link = v_ee + w_ee x r_link_ee = v_J_ee * q + w_J_ee * q x r_link_ee\\n            #        = (v_J_ee + w_J_ee x r_link_ee ) * q\\n            #        = (v_J_ee - r_link_ee_[x] @ w_J_ee) * q\\n            self._jacobian_b[:, 0:3, :] += torch.bmm(-math_utils.skew_symmetric_matrix(self._offset_pos), self._jacobian_b[:, 3:, :])  # type: ignore\\n            # -- rotational part\\n            # w_link = R_link_ee @ w_ee\\n            self._jacobian_b[:, 3:, :] = torch.bmm(math_utils.matrix_from_quat(self._offset_rot), self._jacobian_b[:, 3:, :])  # type: ignore\\n\\n    def _compute_ee_pose(self):\\n        \"\"\"Computes the pose of the ee frame in root frame.\"\"\"\\n        # Obtain quantities from simulation\\n        self._ee_pose_w[:, 0:3] = self._asset.data.body_pos_w[:, self._ee_body_idx]\\n        self._ee_pose_w[:, 3:7] = self._asset.data.body_quat_w[:, self._ee_body_idx]\\n        # Compute the pose of the ee body in the root frame\\n        self._ee_pose_b_no_offset[:, 0:3], self._ee_pose_b_no_offset[:, 3:7] = math_utils.subtract_frame_transforms(\\n            self._asset.data.root_pos_w,\\n            self._asset.data.root_quat_w,\\n            self._ee_pose_w[:, 0:3],\\n            self._ee_pose_w[:, 3:7],\\n        )\\n        # Account for the offset\\n        if self.cfg.body_offset is not None:\\n            self._ee_pose_b[:, 0:3], self._ee_pose_b[:, 3:7] = math_utils.combine_frame_transforms(\\n                self._ee_pose_b_no_offset[:, 0:3], self._ee_pose_b_no_offset[:, 3:7], self._offset_pos, self._offset_rot\\n            )\\n        else:\\n            self._ee_pose_b[:] = self._ee_pose_b_no_offset\\n\\n    def _compute_ee_velocity(self):\\n        \"\"\"Computes the velocity of the ee frame in root frame.\"\"\"\\n        # Extract end-effector velocity in the world frame\\n        self._ee_vel_w[:] = self._asset.data.body_vel_w[:, self._ee_body_idx, :]\\n        # Compute the relative velocity in the world frame\\n        relative_vel_w = self._ee_vel_w - self._asset.data.root_vel_w\\n\\n        # Convert ee velocities from world to root frame\\n        self._ee_vel_b[:, 0:3] = math_utils.quat_apply_inverse(self._asset.data.root_quat_w, relative_vel_w[:, 0:3])\\n        self._ee_vel_b[:, 3:6] = math_utils.quat_apply_inverse(self._asset.data.root_quat_w, relative_vel_w[:, 3:6])\\n\\n        # Account for the offset\\n        if self.cfg.body_offset is not None:\\n            # Compute offset vector in root frame\\n            r_offset_b = math_utils.quat_apply(self._ee_pose_b_no_offset[:, 3:7], self._offset_pos)\\n            # Adjust the linear velocity to account for the offset\\n            self._ee_vel_b[:, :3] += torch.cross(self._ee_vel_b[:, 3:], r_offset_b, dim=-1)\\n            # Angular velocity is not affected by the offset\\n\\n    def _compute_ee_force(self):\\n        \"\"\"Computes the contact forces on the ee frame in root frame.\"\"\"\\n        # Obtain contact forces only if the contact sensor is available\\n        if self._contact_sensor is not None:\\n            self._contact_sensor.update(self._sim_dt)\\n            self._ee_force_w[:] = self._contact_sensor.data.net_forces_w[:, 0, :]  # type: ignore\\n            # Rotate forces and torques into root frame\\n            self._ee_force_b[:] = math_utils.quat_apply_inverse(self._asset.data.root_quat_w, self._ee_force_w)\\n\\n    def _compute_joint_states(self):\\n        \"\"\"Computes the joint states for operational space control.\"\"\"\\n        # Extract joint positions and velocities\\n        self._joint_pos[:] = self._asset.data.joint_pos[:, self._joint_ids]\\n        self._joint_vel[:] = self._asset.data.joint_vel[:, self._joint_ids]\\n\\n    def _compute_task_frame_pose(self):\\n        \"\"\"Computes the pose of the task frame in root frame.\"\"\"\\n        # Update task frame pose if task frame rigidbody is provided\\n        if self._task_frame_transformer is not None and self._task_frame_pose_b is not None:\\n            self._task_frame_transformer.update(self._sim_dt)\\n            # Calculate the pose of the task frame in the root frame\\n            self._task_frame_pose_b[:, :3], self._task_frame_pose_b[:, 3:] = math_utils.subtract_frame_transforms(\\n                self._asset.data.root_pos_w,\\n                self._asset.data.root_quat_w,\\n                self._task_frame_transformer.data.target_pos_w[:, 0, :],\\n                self._task_frame_transformer.data.target_quat_w[:, 0, :],\\n            )\\n\\n    def _preprocess_actions(self, actions: torch.Tensor):\\n        \"\"\"Pre-processes the raw actions for operational space control.\\n\\n        Args:\\n            actions (torch.Tensor): The raw actions for operational space control. It is a tensor of\\n                shape (``num_envs``, ``action_dim``).\\n        \"\"\"\\n        # Store the raw actions. Please note that the actions contain task space targets\\n        # (in the order of the target_types), and possibly the impedance parameters depending on impedance_mode.\\n        self._raw_actions[:] = actions\\n        # Initialize the processed actions with raw actions.\\n        self._processed_actions[:] = self._raw_actions\\n        # Go through the command types one by one, and apply the pre-processing if needed.\\n        if self._pose_abs_idx is not None:\\n            self._processed_actions[:, self._pose_abs_idx : self._pose_abs_idx + 3] *= self._position_scale\\n            self._processed_actions[:, self._pose_abs_idx + 3 : self._pose_abs_idx + 7] *= self._orientation_scale\\n        if self._pose_rel_idx is not None:\\n            self._processed_actions[:, self._pose_rel_idx : self._pose_rel_idx + 3] *= self._position_scale\\n            self._processed_actions[:, self._pose_rel_idx + 3 : self._pose_rel_idx + 6] *= self._orientation_scale\\n        if self._wrench_abs_idx is not None:\\n            self._processed_actions[:, self._wrench_abs_idx : self._wrench_abs_idx + 6] *= self._wrench_scale\\n        if self._stiffness_idx is not None:\\n            self._processed_actions[:, self._stiffness_idx : self._stiffness_idx + 6] *= self._stiffness_scale\\n            self._processed_actions[:, self._stiffness_idx : self._stiffness_idx + 6] = torch.clamp(\\n                self._processed_actions[:, self._stiffness_idx : self._stiffness_idx + 6],\\n                min=self.cfg.controller_cfg.motion_stiffness_limits_task[0],\\n                max=self.cfg.controller_cfg.motion_stiffness_limits_task[1],\\n            )\\n        if self._damping_ratio_idx is not None:\\n            self._processed_actions[\\n                :, self._damping_ratio_idx : self._damping_ratio_idx + 6\\n            ] *= self._damping_ratio_scale\\n            self._processed_actions[:, self._damping_ratio_idx : self._damping_ratio_idx + 6] = torch.clamp(\\n                self._processed_actions[:, self._damping_ratio_idx : self._damping_ratio_idx + 6],\\n                min=self.cfg.controller_cfg.motion_damping_ratio_limits_task[0],\\n                max=self.cfg.controller_cfg.motion_damping_ratio_limits_task[1],\\n            )'),\n",
       " Document(metadata={}, page_content='class NullCommandCfg(CommandTermCfg):\\n    \"\"\"Configuration for the null command generator.\"\"\"\\n\\n    class_type: type = NullCommand\\n\\n    def __post_init__(self):\\n        \"\"\"Post initialization.\"\"\"\\n        # set the resampling time range to infinity to avoid resampling\\n        self.resampling_time_range = (math.inf, math.inf)'),\n",
       " Document(metadata={}, page_content='class UniformVelocityCommandCfg(CommandTermCfg):\\n    \"\"\"Configuration for the uniform velocity command generator.\"\"\"\\n\\n    class_type: type = UniformVelocityCommand\\n\\n    asset_name: str = MISSING\\n    \"\"\"Name of the asset in the environment for which the commands are generated.\"\"\"\\n\\n    heading_command: bool = False\\n    \"\"\"Whether to use heading command or angular velocity command. Defaults to False.\\n\\n    If True, the angular velocity command is computed from the heading error, where the\\n    target heading is sampled uniformly from provided range. Otherwise, the angular velocity\\n    command is sampled uniformly from provided range.\\n    \"\"\"\\n\\n    heading_control_stiffness: float = 1.0\\n    \"\"\"Scale factor to convert the heading error to angular velocity command. Defaults to 1.0.\"\"\"\\n\\n    rel_standing_envs: float = 0.0\\n    \"\"\"The sampled probability of environments that should be standing still. Defaults to 0.0.\"\"\"\\n\\n    rel_heading_envs: float = 1.0\\n    \"\"\"The sampled probability of environments where the robots follow the heading-based angular velocity command\\n    (the others follow the sampled angular velocity command). Defaults to 1.0.\\n\\n    This parameter is only used if :attr:`heading_command` is True.\\n    \"\"\"\\n\\n    @configclass\\n    class Ranges:\\n        \"\"\"Uniform distribution ranges for the velocity commands.\"\"\"\\n\\n        lin_vel_x: tuple[float, float] = MISSING\\n        \"\"\"Range for the linear-x velocity command (in m/s).\"\"\"\\n\\n        lin_vel_y: tuple[float, float] = MISSING\\n        \"\"\"Range for the linear-y velocity command (in m/s).\"\"\"\\n\\n        ang_vel_z: tuple[float, float] = MISSING\\n        \"\"\"Range for the angular-z velocity command (in rad/s).\"\"\"\\n\\n        heading: tuple[float, float] | None = None\\n        \"\"\"Range for the heading command (in rad). Defaults to None.\\n\\n        This parameter is only used if :attr:`~UniformVelocityCommandCfg.heading_command` is True.\\n        \"\"\"\\n\\n    ranges: Ranges = MISSING\\n    \"\"\"Distribution ranges for the velocity commands.\"\"\"\\n\\n    goal_vel_visualizer_cfg: VisualizationMarkersCfg = GREEN_ARROW_X_MARKER_CFG.replace(\\n        prim_path=\"/Visuals/Command/velocity_goal\"\\n    )\\n    \"\"\"The configuration for the goal velocity visualization marker. Defaults to GREEN_ARROW_X_MARKER_CFG.\"\"\"\\n\\n    current_vel_visualizer_cfg: VisualizationMarkersCfg = BLUE_ARROW_X_MARKER_CFG.replace(\\n        prim_path=\"/Visuals/Command/velocity_current\"\\n    )\\n    \"\"\"The configuration for the current velocity visualization marker. Defaults to BLUE_ARROW_X_MARKER_CFG.\"\"\"\\n\\n    # Set the scale of the visualization markers to (0.5, 0.5, 0.5)\\n    goal_vel_visualizer_cfg.markers[\"arrow\"].scale = (0.5, 0.5, 0.5)\\n    current_vel_visualizer_cfg.markers[\"arrow\"].scale = (0.5, 0.5, 0.5)'),\n",
       " Document(metadata={}, page_content='class NormalVelocityCommandCfg(UniformVelocityCommandCfg):\\n    \"\"\"Configuration for the normal velocity command generator.\"\"\"\\n\\n    class_type: type = NormalVelocityCommand\\n    heading_command: bool = False  # --> we don\\'t use heading command for normal velocity command.\\n\\n    @configclass\\n    class Ranges:\\n        \"\"\"Normal distribution ranges for the velocity commands.\"\"\"\\n\\n        mean_vel: tuple[float, float, float] = MISSING\\n        \"\"\"Mean velocity for the normal distribution (in m/s).\\n\\n        The tuple contains the mean linear-x, linear-y, and angular-z velocity.\\n        \"\"\"\\n\\n        std_vel: tuple[float, float, float] = MISSING\\n        \"\"\"Standard deviation for the normal distribution (in m/s).\\n\\n        The tuple contains the standard deviation linear-x, linear-y, and angular-z velocity.\\n        \"\"\"\\n\\n        zero_prob: tuple[float, float, float] = MISSING\\n        \"\"\"Probability of zero velocity for the normal distribution.\\n\\n        The tuple contains the probability of zero linear-x, linear-y, and angular-z velocity.\\n        \"\"\"\\n\\n    ranges: Ranges = MISSING\\n    \"\"\"Distribution ranges for the velocity commands.\"\"\"'),\n",
       " Document(metadata={}, page_content='class UniformPoseCommandCfg(CommandTermCfg):\\n    \"\"\"Configuration for uniform pose command generator.\"\"\"\\n\\n    class_type: type = UniformPoseCommand\\n\\n    asset_name: str = MISSING\\n    \"\"\"Name of the asset in the environment for which the commands are generated.\"\"\"\\n\\n    body_name: str = MISSING\\n    \"\"\"Name of the body in the asset for which the commands are generated.\"\"\"\\n\\n    make_quat_unique: bool = False\\n    \"\"\"Whether to make the quaternion unique or not. Defaults to False.\\n\\n    If True, the quaternion is made unique by ensuring the real part is positive.\\n    \"\"\"\\n\\n    @configclass\\n    class Ranges:\\n        \"\"\"Uniform distribution ranges for the pose commands.\"\"\"\\n\\n        pos_x: tuple[float, float] = MISSING\\n        \"\"\"Range for the x position (in m).\"\"\"\\n\\n        pos_y: tuple[float, float] = MISSING\\n        \"\"\"Range for the y position (in m).\"\"\"\\n\\n        pos_z: tuple[float, float] = MISSING\\n        \"\"\"Range for the z position (in m).\"\"\"\\n\\n        roll: tuple[float, float] = MISSING\\n        \"\"\"Range for the roll angle (in rad).\"\"\"\\n\\n        pitch: tuple[float, float] = MISSING\\n        \"\"\"Range for the pitch angle (in rad).\"\"\"\\n\\n        yaw: tuple[float, float] = MISSING\\n        \"\"\"Range for the yaw angle (in rad).\"\"\"\\n\\n    ranges: Ranges = MISSING\\n    \"\"\"Ranges for the commands.\"\"\"\\n\\n    goal_pose_visualizer_cfg: VisualizationMarkersCfg = FRAME_MARKER_CFG.replace(prim_path=\"/Visuals/Command/goal_pose\")\\n    \"\"\"The configuration for the goal pose visualization marker. Defaults to FRAME_MARKER_CFG.\"\"\"\\n\\n    current_pose_visualizer_cfg: VisualizationMarkersCfg = FRAME_MARKER_CFG.replace(\\n        prim_path=\"/Visuals/Command/body_pose\"\\n    )\\n    \"\"\"The configuration for the current pose visualization marker. Defaults to FRAME_MARKER_CFG.\"\"\"\\n\\n    # Set the scale of the visualization markers to (0.1, 0.1, 0.1)\\n    goal_pose_visualizer_cfg.markers[\"frame\"].scale = (0.1, 0.1, 0.1)\\n    current_pose_visualizer_cfg.markers[\"frame\"].scale = (0.1, 0.1, 0.1)'),\n",
       " Document(metadata={}, page_content='class UniformPose2dCommandCfg(CommandTermCfg):\\n    \"\"\"Configuration for the uniform 2D-pose command generator.\"\"\"\\n\\n    class_type: type = UniformPose2dCommand\\n\\n    asset_name: str = MISSING\\n    \"\"\"Name of the asset in the environment for which the commands are generated.\"\"\"\\n\\n    simple_heading: bool = MISSING\\n    \"\"\"Whether to use simple heading or not.\\n\\n    If True, the heading is in the direction of the target position.\\n    \"\"\"\\n\\n    @configclass\\n    class Ranges:\\n        \"\"\"Uniform distribution ranges for the position commands.\"\"\"\\n\\n        pos_x: tuple[float, float] = MISSING\\n        \"\"\"Range for the x position (in m).\"\"\"\\n\\n        pos_y: tuple[float, float] = MISSING\\n        \"\"\"Range for the y position (in m).\"\"\"\\n\\n        heading: tuple[float, float] = MISSING\\n        \"\"\"Heading range for the position commands (in rad).\\n\\n        Used only if :attr:`simple_heading` is False.\\n        \"\"\"\\n\\n    ranges: Ranges = MISSING\\n    \"\"\"Distribution ranges for the position commands.\"\"\"\\n\\n    goal_pose_visualizer_cfg: VisualizationMarkersCfg = GREEN_ARROW_X_MARKER_CFG.replace(\\n        prim_path=\"/Visuals/Command/pose_goal\"\\n    )\\n    \"\"\"The configuration for the goal pose visualization marker. Defaults to GREEN_ARROW_X_MARKER_CFG.\"\"\"\\n\\n    # Set the scale of the visualization markers to (0.2, 0.2, 0.8)\\n    goal_pose_visualizer_cfg.markers[\"arrow\"].scale = (0.2, 0.2, 0.8)'),\n",
       " Document(metadata={}, page_content='class TerrainBasedPose2dCommandCfg(UniformPose2dCommandCfg):\\n    \"\"\"Configuration for the terrain-based position command generator.\"\"\"\\n\\n    class_type = TerrainBasedPose2dCommand\\n\\n    @configclass\\n    class Ranges:\\n        \"\"\"Uniform distribution ranges for the position commands.\"\"\"\\n\\n        heading: tuple[float, float] = MISSING\\n        \"\"\"Heading range for the position commands (in rad).\\n\\n        Used only if :attr:`simple_heading` is False.\\n        \"\"\"\\n\\n    ranges: Ranges = MISSING\\n    \"\"\"Distribution ranges for the sampled commands.\"\"\"'),\n",
       " Document(metadata={}, page_content='class NullCommand(CommandTerm):\\n    \"\"\"Command generator that does nothing.\\n\\n    This command generator does not generate any commands. It is used for environments that do not\\n    require any commands.\\n    \"\"\"\\n\\n    cfg: NullCommandCfg\\n    \"\"\"Configuration for the command generator.\"\"\"\\n\\n    def __str__(self) -> str:\\n        msg = \"NullCommand:\\\\n\"\\n        msg += \"\\\\tCommand dimension: N/A\\\\n\"\\n        msg += f\"\\\\tResampling time range: {self.cfg.resampling_time_range}\"\\n        return msg\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def command(self):\\n        \"\"\"Null command.\\n\\n        Raises:\\n            RuntimeError: No command is generated. Always raises this error.\\n        \"\"\"\\n        raise RuntimeError(\"NullCommandTerm does not generate any commands.\")\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:\\n        return {}\\n\\n    def compute(self, dt: float):\\n        pass\\n\\n    \"\"\"\\n    Implementation specific functions.\\n    \"\"\"\\n\\n    def _update_metrics(self):\\n        pass\\n\\n    def _resample_command(self, env_ids: Sequence[int]):\\n        pass\\n\\n    def _update_command(self):\\n        pass'),\n",
       " Document(metadata={}, page_content='class UniformPose2dCommand(CommandTerm):\\n    \"\"\"Command generator that generates pose commands containing a 3-D position and heading.\\n\\n    The command generator samples uniform 2D positions around the environment origin. It sets\\n    the height of the position command to the default root height of the robot. The heading\\n    command is either set to point towards the target or is sampled uniformly.\\n    This can be configured through the :attr:`Pose2dCommandCfg.simple_heading` parameter in\\n    the configuration.\\n    \"\"\"\\n\\n    cfg: UniformPose2dCommandCfg\\n    \"\"\"Configuration for the command generator.\"\"\"\\n\\n    def __init__(self, cfg: UniformPose2dCommandCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the command generator class.\\n\\n        Args:\\n            cfg: The configuration parameters for the command generator.\\n            env: The environment object.\\n        \"\"\"\\n        # initialize the base class\\n        super().__init__(cfg, env)\\n\\n        # obtain the robot and terrain assets\\n        # -- robot\\n        self.robot: Articulation = env.scene[cfg.asset_name]\\n\\n        # crete buffers to store the command\\n        # -- commands: (x, y, z, heading)\\n        self.pos_command_w = torch.zeros(self.num_envs, 3, device=self.device)\\n        self.heading_command_w = torch.zeros(self.num_envs, device=self.device)\\n        self.pos_command_b = torch.zeros_like(self.pos_command_w)\\n        self.heading_command_b = torch.zeros_like(self.heading_command_w)\\n        # -- metrics\\n        self.metrics[\"error_pos\"] = torch.zeros(self.num_envs, device=self.device)\\n        self.metrics[\"error_heading\"] = torch.zeros(self.num_envs, device=self.device)\\n\\n    def __str__(self) -> str:\\n        msg = \"PositionCommand:\\\\n\"\\n        msg += f\"\\\\tCommand dimension: {tuple(self.command.shape[1:])}\\\\n\"\\n        msg += f\"\\\\tResampling time range: {self.cfg.resampling_time_range}\"\\n        return msg\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def command(self) -> torch.Tensor:\\n        \"\"\"The desired 2D-pose in base frame. Shape is (num_envs, 4).\"\"\"\\n        return torch.cat([self.pos_command_b, self.heading_command_b.unsqueeze(1)], dim=1)\\n\\n    \"\"\"\\n    Implementation specific functions.\\n    \"\"\"\\n\\n    def _update_metrics(self):\\n        # logs data\\n        self.metrics[\"error_pos_2d\"] = torch.norm(self.pos_command_w[:, :2] - self.robot.data.root_pos_w[:, :2], dim=1)\\n        self.metrics[\"error_heading\"] = torch.abs(wrap_to_pi(self.heading_command_w - self.robot.data.heading_w))\\n\\n    def _resample_command(self, env_ids: Sequence[int]):\\n        # obtain env origins for the environments\\n        self.pos_command_w[env_ids] = self._env.scene.env_origins[env_ids]\\n        # offset the position command by the current root position\\n        r = torch.empty(len(env_ids), device=self.device)\\n        self.pos_command_w[env_ids, 0] += r.uniform_(*self.cfg.ranges.pos_x)\\n        self.pos_command_w[env_ids, 1] += r.uniform_(*self.cfg.ranges.pos_y)\\n        self.pos_command_w[env_ids, 2] += self.robot.data.default_root_state[env_ids, 2]\\n\\n        if self.cfg.simple_heading:\\n            # set heading command to point towards target\\n            target_vec = self.pos_command_w[env_ids] - self.robot.data.root_pos_w[env_ids]\\n            target_direction = torch.atan2(target_vec[:, 1], target_vec[:, 0])\\n            flipped_target_direction = wrap_to_pi(target_direction + torch.pi)\\n\\n            # compute errors to find the closest direction to the current heading\\n            # this is done to avoid the discontinuity at the -pi/pi boundary\\n            curr_to_target = wrap_to_pi(target_direction - self.robot.data.heading_w[env_ids]).abs()\\n            curr_to_flipped_target = wrap_to_pi(flipped_target_direction - self.robot.data.heading_w[env_ids]).abs()\\n\\n            # set the heading command to the closest direction\\n            self.heading_command_w[env_ids] = torch.where(\\n                curr_to_target < curr_to_flipped_target,\\n                target_direction,\\n                flipped_target_direction,\\n            )\\n        else:\\n            # random heading command\\n            self.heading_command_w[env_ids] = r.uniform_(*self.cfg.ranges.heading)\\n\\n    def _update_command(self):\\n        \"\"\"Re-target the position command to the current root state.\"\"\"\\n        target_vec = self.pos_command_w - self.robot.data.root_pos_w[:, :3]\\n        self.pos_command_b[:] = quat_apply_inverse(yaw_quat(self.robot.data.root_quat_w), target_vec)\\n        self.heading_command_b[:] = wrap_to_pi(self.heading_command_w - self.robot.data.heading_w)\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # create markers if necessary for the first tome\\n        if debug_vis:\\n            if not hasattr(self, \"goal_pose_visualizer\"):\\n                self.goal_pose_visualizer = VisualizationMarkers(self.cfg.goal_pose_visualizer_cfg)\\n            # set their visibility to true\\n            self.goal_pose_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"goal_pose_visualizer\"):\\n                self.goal_pose_visualizer.set_visibility(False)\\n\\n    def _debug_vis_callback(self, event):\\n        # update the box marker\\n        self.goal_pose_visualizer.visualize(\\n            translations=self.pos_command_w,\\n            orientations=quat_from_euler_xyz(\\n                torch.zeros_like(self.heading_command_w),\\n                torch.zeros_like(self.heading_command_w),\\n                self.heading_command_w,\\n            ),\\n        )'),\n",
       " Document(metadata={}, page_content='class TerrainBasedPose2dCommand(UniformPose2dCommand):\\n    \"\"\"Command generator that generates pose commands based on the terrain.\\n\\n    This command generator samples the position commands from the valid patches of the terrain.\\n    The heading commands are either set to point towards the target or are sampled uniformly.\\n\\n    It expects the terrain to have a valid flat patches under the key \\'target\\'.\\n    \"\"\"\\n\\n    cfg: TerrainBasedPose2dCommandCfg\\n    \"\"\"Configuration for the command generator.\"\"\"\\n\\n    def __init__(self, cfg: TerrainBasedPose2dCommandCfg, env: ManagerBasedEnv):\\n        # initialize the base class\\n        super().__init__(cfg, env)\\n\\n        # obtain the terrain asset\\n        self.terrain: TerrainImporter = env.scene[\"terrain\"]\\n\\n        # obtain the valid targets from the terrain\\n        if \"target\" not in self.terrain.flat_patches:\\n            raise RuntimeError(\\n                \"The terrain-based command generator requires a valid flat patch under \\'target\\' in the terrain.\"\\n                f\" Found: {list(self.terrain.flat_patches.keys())}\"\\n            )\\n        # valid targets: (terrain_level, terrain_type, num_patches, 3)\\n        self.valid_targets: torch.Tensor = self.terrain.flat_patches[\"target\"]\\n\\n    def _resample_command(self, env_ids: Sequence[int]):\\n        # sample new position targets from the terrain\\n        ids = torch.randint(0, self.valid_targets.shape[2], size=(len(env_ids),), device=self.device)\\n        self.pos_command_w[env_ids] = self.valid_targets[\\n            self.terrain.terrain_levels[env_ids], self.terrain.terrain_types[env_ids], ids\\n        ]\\n        # offset the position command by the current root height\\n        self.pos_command_w[env_ids, 2] += self.robot.data.default_root_state[env_ids, 2]\\n\\n        if self.cfg.simple_heading:\\n            # set heading command to point towards target\\n            target_vec = self.pos_command_w[env_ids] - self.robot.data.root_pos_w[env_ids]\\n            target_direction = torch.atan2(target_vec[:, 1], target_vec[:, 0])\\n            flipped_target_direction = wrap_to_pi(target_direction + torch.pi)\\n\\n            # compute errors to find the closest direction to the current heading\\n            # this is done to avoid the discontinuity at the -pi/pi boundary\\n            curr_to_target = wrap_to_pi(target_direction - self.robot.data.heading_w[env_ids]).abs()\\n            curr_to_flipped_target = wrap_to_pi(flipped_target_direction - self.robot.data.heading_w[env_ids]).abs()\\n\\n            # set the heading command to the closest direction\\n            self.heading_command_w[env_ids] = torch.where(\\n                curr_to_target < curr_to_flipped_target,\\n                target_direction,\\n                flipped_target_direction,\\n            )\\n        else:\\n            # random heading command\\n            r = torch.empty(len(env_ids), device=self.device)\\n            self.heading_command_w[env_ids] = r.uniform_(*self.cfg.ranges.heading)'),\n",
       " Document(metadata={}, page_content='class UniformPoseCommand(CommandTerm):\\n    \"\"\"Command generator for generating pose commands uniformly.\\n\\n    The command generator generates poses by sampling positions uniformly within specified\\n    regions in cartesian space. For orientation, it samples uniformly the euler angles\\n    (roll-pitch-yaw) and converts them into quaternion representation (w, x, y, z).\\n\\n    The position and orientation commands are generated in the base frame of the robot, and not the\\n    simulation world frame. This means that users need to handle the transformation from the\\n    base frame to the simulation world frame themselves.\\n\\n    .. caution::\\n\\n        Sampling orientations uniformly is not strictly the same as sampling euler angles uniformly.\\n        This is because rotations are defined by 3D non-Euclidean space, and the mapping\\n        from euler angles to rotations is not one-to-one.\\n\\n    \"\"\"\\n\\n    cfg: UniformPoseCommandCfg\\n    \"\"\"Configuration for the command generator.\"\"\"\\n\\n    def __init__(self, cfg: UniformPoseCommandCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the command generator class.\\n\\n        Args:\\n            cfg: The configuration parameters for the command generator.\\n            env: The environment object.\\n        \"\"\"\\n        # initialize the base class\\n        super().__init__(cfg, env)\\n\\n        # extract the robot and body index for which the command is generated\\n        self.robot: Articulation = env.scene[cfg.asset_name]\\n        self.body_idx = self.robot.find_bodies(cfg.body_name)[0][0]\\n\\n        # create buffers\\n        # -- commands: (x, y, z, qw, qx, qy, qz) in root frame\\n        self.pose_command_b = torch.zeros(self.num_envs, 7, device=self.device)\\n        self.pose_command_b[:, 3] = 1.0\\n        self.pose_command_w = torch.zeros_like(self.pose_command_b)\\n        # -- metrics\\n        self.metrics[\"position_error\"] = torch.zeros(self.num_envs, device=self.device)\\n        self.metrics[\"orientation_error\"] = torch.zeros(self.num_envs, device=self.device)\\n\\n    def __str__(self) -> str:\\n        msg = \"UniformPoseCommand:\\\\n\"\\n        msg += f\"\\\\tCommand dimension: {tuple(self.command.shape[1:])}\\\\n\"\\n        msg += f\"\\\\tResampling time range: {self.cfg.resampling_time_range}\\\\n\"\\n        return msg\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def command(self) -> torch.Tensor:\\n        \"\"\"The desired pose command. Shape is (num_envs, 7).\\n\\n        The first three elements correspond to the position, followed by the quaternion orientation in (w, x, y, z).\\n        \"\"\"\\n        return self.pose_command_b\\n\\n    \"\"\"\\n    Implementation specific functions.\\n    \"\"\"\\n\\n    def _update_metrics(self):\\n        # transform command from base frame to simulation world frame\\n        self.pose_command_w[:, :3], self.pose_command_w[:, 3:] = combine_frame_transforms(\\n            self.robot.data.root_pos_w,\\n            self.robot.data.root_quat_w,\\n            self.pose_command_b[:, :3],\\n            self.pose_command_b[:, 3:],\\n        )\\n        # compute the error\\n        pos_error, rot_error = compute_pose_error(\\n            self.pose_command_w[:, :3],\\n            self.pose_command_w[:, 3:],\\n            self.robot.data.body_state_w[:, self.body_idx, :3],\\n            self.robot.data.body_state_w[:, self.body_idx, 3:7],\\n        )\\n        self.metrics[\"position_error\"] = torch.norm(pos_error, dim=-1)\\n        self.metrics[\"orientation_error\"] = torch.norm(rot_error, dim=-1)\\n\\n    def _resample_command(self, env_ids: Sequence[int]):\\n        # sample new pose targets\\n        # -- position\\n        r = torch.empty(len(env_ids), device=self.device)\\n        self.pose_command_b[env_ids, 0] = r.uniform_(*self.cfg.ranges.pos_x)\\n        self.pose_command_b[env_ids, 1] = r.uniform_(*self.cfg.ranges.pos_y)\\n        self.pose_command_b[env_ids, 2] = r.uniform_(*self.cfg.ranges.pos_z)\\n        # -- orientation\\n        euler_angles = torch.zeros_like(self.pose_command_b[env_ids, :3])\\n        euler_angles[:, 0].uniform_(*self.cfg.ranges.roll)\\n        euler_angles[:, 1].uniform_(*self.cfg.ranges.pitch)\\n        euler_angles[:, 2].uniform_(*self.cfg.ranges.yaw)\\n        quat = quat_from_euler_xyz(euler_angles[:, 0], euler_angles[:, 1], euler_angles[:, 2])\\n        # make sure the quaternion has real part as positive\\n        self.pose_command_b[env_ids, 3:] = quat_unique(quat) if self.cfg.make_quat_unique else quat\\n\\n    def _update_command(self):\\n        pass\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # create markers if necessary for the first tome\\n        if debug_vis:\\n            if not hasattr(self, \"goal_pose_visualizer\"):\\n                # -- goal pose\\n                self.goal_pose_visualizer = VisualizationMarkers(self.cfg.goal_pose_visualizer_cfg)\\n                # -- current body pose\\n                self.current_pose_visualizer = VisualizationMarkers(self.cfg.current_pose_visualizer_cfg)\\n            # set their visibility to true\\n            self.goal_pose_visualizer.set_visibility(True)\\n            self.current_pose_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"goal_pose_visualizer\"):\\n                self.goal_pose_visualizer.set_visibility(False)\\n                self.current_pose_visualizer.set_visibility(False)\\n\\n    def _debug_vis_callback(self, event):\\n        # check if robot is initialized\\n        # note: this is needed in-case the robot is de-initialized. we can\\'t access the data\\n        if not self.robot.is_initialized:\\n            return\\n        # update the markers\\n        # -- goal pose\\n        self.goal_pose_visualizer.visualize(self.pose_command_w[:, :3], self.pose_command_w[:, 3:])\\n        # -- current body pose\\n        body_link_state_w = self.robot.data.body_state_w[:, self.body_idx]\\n        self.current_pose_visualizer.visualize(body_link_state_w[:, :3], body_link_state_w[:, 3:7])'),\n",
       " Document(metadata={}, page_content='class UniformVelocityCommand(CommandTerm):\\n    r\"\"\"Command generator that generates a velocity command in SE(2) from uniform distribution.\\n\\n    The command comprises of a linear velocity in x and y direction and an angular velocity around\\n    the z-axis. It is given in the robot\\'s base frame.\\n\\n    If the :attr:`cfg.heading_command` flag is set to True, the angular velocity is computed from the heading\\n    error similar to doing a proportional control on the heading error. The target heading is sampled uniformly\\n    from the provided range. Otherwise, the angular velocity is sampled uniformly from the provided range.\\n\\n    Mathematically, the angular velocity is computed as follows from the heading command:\\n\\n    .. math::\\n\\n        \\\\omega_z = \\\\frac{1}{2} \\\\text{wrap_to_pi}(\\\\theta_{\\\\text{target}} - \\\\theta_{\\\\text{current}})\\n\\n    \"\"\"\\n\\n    cfg: UniformVelocityCommandCfg\\n    \"\"\"The configuration of the command generator.\"\"\"\\n\\n    def __init__(self, cfg: UniformVelocityCommandCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the command generator.\\n\\n        Args:\\n            cfg: The configuration of the command generator.\\n            env: The environment.\\n\\n        Raises:\\n            ValueError: If the heading command is active but the heading range is not provided.\\n        \"\"\"\\n        # initialize the base class\\n        super().__init__(cfg, env)\\n\\n        # check configuration\\n        if self.cfg.heading_command and self.cfg.ranges.heading is None:\\n            raise ValueError(\\n                \"The velocity command has heading commands active (heading_command=True) but the `ranges.heading`\"\\n                \" parameter is set to None.\"\\n            )\\n        if self.cfg.ranges.heading and not self.cfg.heading_command:\\n            omni.log.warn(\\n                f\"The velocity command has the \\'ranges.heading\\' attribute set to \\'{self.cfg.ranges.heading}\\'\"\\n                \" but the heading command is not active. Consider setting the flag for the heading command to True.\"\\n            )\\n\\n        # obtain the robot asset\\n        # -- robot\\n        self.robot: Articulation = env.scene[cfg.asset_name]\\n\\n        # crete buffers to store the command\\n        # -- command: x vel, y vel, yaw vel, heading\\n        self.vel_command_b = torch.zeros(self.num_envs, 3, device=self.device)\\n        self.heading_target = torch.zeros(self.num_envs, device=self.device)\\n        self.is_heading_env = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)\\n        self.is_standing_env = torch.zeros_like(self.is_heading_env)\\n        # -- metrics\\n        self.metrics[\"error_vel_xy\"] = torch.zeros(self.num_envs, device=self.device)\\n        self.metrics[\"error_vel_yaw\"] = torch.zeros(self.num_envs, device=self.device)\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the command generator.\"\"\"\\n        msg = \"UniformVelocityCommand:\\\\n\"\\n        msg += f\"\\\\tCommand dimension: {tuple(self.command.shape[1:])}\\\\n\"\\n        msg += f\"\\\\tResampling time range: {self.cfg.resampling_time_range}\\\\n\"\\n        msg += f\"\\\\tHeading command: {self.cfg.heading_command}\\\\n\"\\n        if self.cfg.heading_command:\\n            msg += f\"\\\\tHeading probability: {self.cfg.rel_heading_envs}\\\\n\"\\n        msg += f\"\\\\tStanding probability: {self.cfg.rel_standing_envs}\"\\n        return msg\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def command(self) -> torch.Tensor:\\n        \"\"\"The desired base velocity command in the base frame. Shape is (num_envs, 3).\"\"\"\\n        return self.vel_command_b\\n\\n    \"\"\"\\n    Implementation specific functions.\\n    \"\"\"\\n\\n    def _update_metrics(self):\\n        # time for which the command was executed\\n        max_command_time = self.cfg.resampling_time_range[1]\\n        max_command_step = max_command_time / self._env.step_dt\\n        # logs data\\n        self.metrics[\"error_vel_xy\"] += (\\n            torch.norm(self.vel_command_b[:, :2] - self.robot.data.root_lin_vel_b[:, :2], dim=-1) / max_command_step\\n        )\\n        self.metrics[\"error_vel_yaw\"] += (\\n            torch.abs(self.vel_command_b[:, 2] - self.robot.data.root_ang_vel_b[:, 2]) / max_command_step\\n        )\\n\\n    def _resample_command(self, env_ids: Sequence[int]):\\n        # sample velocity commands\\n        r = torch.empty(len(env_ids), device=self.device)\\n        # -- linear velocity - x direction\\n        self.vel_command_b[env_ids, 0] = r.uniform_(*self.cfg.ranges.lin_vel_x)\\n        # -- linear velocity - y direction\\n        self.vel_command_b[env_ids, 1] = r.uniform_(*self.cfg.ranges.lin_vel_y)\\n        # -- ang vel yaw - rotation around z\\n        self.vel_command_b[env_ids, 2] = r.uniform_(*self.cfg.ranges.ang_vel_z)\\n        # heading target\\n        if self.cfg.heading_command:\\n            self.heading_target[env_ids] = r.uniform_(*self.cfg.ranges.heading)\\n            # update heading envs\\n            self.is_heading_env[env_ids] = r.uniform_(0.0, 1.0) <= self.cfg.rel_heading_envs\\n        # update standing envs\\n        self.is_standing_env[env_ids] = r.uniform_(0.0, 1.0) <= self.cfg.rel_standing_envs\\n\\n    def _update_command(self):\\n        \"\"\"Post-processes the velocity command.\\n\\n        This function sets velocity command to zero for standing environments and computes angular\\n        velocity from heading direction if the heading_command flag is set.\\n        \"\"\"\\n        # Compute angular velocity from heading direction\\n        if self.cfg.heading_command:\\n            # resolve indices of heading envs\\n            env_ids = self.is_heading_env.nonzero(as_tuple=False).flatten()\\n            # compute angular velocity\\n            heading_error = math_utils.wrap_to_pi(self.heading_target[env_ids] - self.robot.data.heading_w[env_ids])\\n            self.vel_command_b[env_ids, 2] = torch.clip(\\n                self.cfg.heading_control_stiffness * heading_error,\\n                min=self.cfg.ranges.ang_vel_z[0],\\n                max=self.cfg.ranges.ang_vel_z[1],\\n            )\\n        # Enforce standing (i.e., zero velocity command) for standing envs\\n        # TODO: check if conversion is needed\\n        standing_env_ids = self.is_standing_env.nonzero(as_tuple=False).flatten()\\n        self.vel_command_b[standing_env_ids, :] = 0.0\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # set visibility of markers\\n        # note: parent only deals with callbacks. not their visibility\\n        if debug_vis:\\n            # create markers if necessary for the first tome\\n            if not hasattr(self, \"goal_vel_visualizer\"):\\n                # -- goal\\n                self.goal_vel_visualizer = VisualizationMarkers(self.cfg.goal_vel_visualizer_cfg)\\n                # -- current\\n                self.current_vel_visualizer = VisualizationMarkers(self.cfg.current_vel_visualizer_cfg)\\n            # set their visibility to true\\n            self.goal_vel_visualizer.set_visibility(True)\\n            self.current_vel_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"goal_vel_visualizer\"):\\n                self.goal_vel_visualizer.set_visibility(False)\\n                self.current_vel_visualizer.set_visibility(False)\\n\\n    def _debug_vis_callback(self, event):\\n        # check if robot is initialized\\n        # note: this is needed in-case the robot is de-initialized. we can\\'t access the data\\n        if not self.robot.is_initialized:\\n            return\\n        # get marker location\\n        # -- base state\\n        base_pos_w = self.robot.data.root_pos_w.clone()\\n        base_pos_w[:, 2] += 0.5\\n        # -- resolve the scales and quaternions\\n        vel_des_arrow_scale, vel_des_arrow_quat = self._resolve_xy_velocity_to_arrow(self.command[:, :2])\\n        vel_arrow_scale, vel_arrow_quat = self._resolve_xy_velocity_to_arrow(self.robot.data.root_lin_vel_b[:, :2])\\n        # display markers\\n        self.goal_vel_visualizer.visualize(base_pos_w, vel_des_arrow_quat, vel_des_arrow_scale)\\n        self.current_vel_visualizer.visualize(base_pos_w, vel_arrow_quat, vel_arrow_scale)\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _resolve_xy_velocity_to_arrow(self, xy_velocity: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"Converts the XY base velocity command to arrow direction rotation.\"\"\"\\n        # obtain default scale of the marker\\n        default_scale = self.goal_vel_visualizer.cfg.markers[\"arrow\"].scale\\n        # arrow-scale\\n        arrow_scale = torch.tensor(default_scale, device=self.device).repeat(xy_velocity.shape[0], 1)\\n        arrow_scale[:, 0] *= torch.linalg.norm(xy_velocity, dim=1) * 3.0\\n        # arrow-direction\\n        heading_angle = torch.atan2(xy_velocity[:, 1], xy_velocity[:, 0])\\n        zeros = torch.zeros_like(heading_angle)\\n        arrow_quat = math_utils.quat_from_euler_xyz(zeros, zeros, heading_angle)\\n        # convert everything back from base to world frame\\n        base_quat_w = self.robot.data.root_quat_w\\n        arrow_quat = math_utils.quat_mul(base_quat_w, arrow_quat)\\n\\n        return arrow_scale, arrow_quat'),\n",
       " Document(metadata={}, page_content='class NormalVelocityCommand(UniformVelocityCommand):\\n    \"\"\"Command generator that generates a velocity command in SE(2) from a normal distribution.\\n\\n    The command comprises of a linear velocity in x and y direction and an angular velocity around\\n    the z-axis. It is given in the robot\\'s base frame.\\n\\n    The command is sampled from a normal distribution with mean and standard deviation specified in\\n    the configuration. With equal probability, the sign of the individual components is flipped.\\n    \"\"\"\\n\\n    cfg: NormalVelocityCommandCfg\\n    \"\"\"The command generator configuration.\"\"\"\\n\\n    def __init__(self, cfg: NormalVelocityCommandCfg, env: ManagerBasedEnv):\\n        \"\"\"Initializes the command generator.\\n\\n        Args:\\n            cfg: The command generator configuration.\\n            env: The environment.\\n        \"\"\"\\n        super().__init__(cfg, env)\\n        # create buffers for zero commands envs\\n        self.is_zero_vel_x_env = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)\\n        self.is_zero_vel_y_env = torch.zeros_like(self.is_zero_vel_x_env)\\n        self.is_zero_vel_yaw_env = torch.zeros_like(self.is_zero_vel_x_env)\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the command generator.\"\"\"\\n        msg = \"NormalVelocityCommand:\\\\n\"\\n        msg += f\"\\\\tCommand dimension: {tuple(self.command.shape[1:])}\\\\n\"\\n        msg += f\"\\\\tResampling time range: {self.cfg.resampling_time_range}\\\\n\"\\n        msg += f\"\\\\tStanding probability: {self.cfg.rel_standing_envs}\"\\n        return msg\\n\\n    def _resample_command(self, env_ids):\\n        # sample velocity commands\\n        r = torch.empty(len(env_ids), device=self.device)\\n        # -- linear velocity - x direction\\n        self.vel_command_b[env_ids, 0] = r.normal_(mean=self.cfg.ranges.mean_vel[0], std=self.cfg.ranges.std_vel[0])\\n        self.vel_command_b[env_ids, 0] *= torch.where(r.uniform_(0.0, 1.0) <= 0.5, 1.0, -1.0)\\n        # -- linear velocity - y direction\\n        self.vel_command_b[env_ids, 1] = r.normal_(mean=self.cfg.ranges.mean_vel[1], std=self.cfg.ranges.std_vel[1])\\n        self.vel_command_b[env_ids, 1] *= torch.where(r.uniform_(0.0, 1.0) <= 0.5, 1.0, -1.0)\\n        # -- angular velocity - yaw direction\\n        self.vel_command_b[env_ids, 2] = r.normal_(mean=self.cfg.ranges.mean_vel[2], std=self.cfg.ranges.std_vel[2])\\n        self.vel_command_b[env_ids, 2] *= torch.where(r.uniform_(0.0, 1.0) <= 0.5, 1.0, -1.0)\\n\\n        # update element wise zero velocity command\\n        # TODO what is zero prob ?\\n        self.is_zero_vel_x_env[env_ids] = r.uniform_(0.0, 1.0) <= self.cfg.ranges.zero_prob[0]\\n        self.is_zero_vel_y_env[env_ids] = r.uniform_(0.0, 1.0) <= self.cfg.ranges.zero_prob[1]\\n        self.is_zero_vel_yaw_env[env_ids] = r.uniform_(0.0, 1.0) <= self.cfg.ranges.zero_prob[2]\\n\\n        # update standing envs\\n        self.is_standing_env[env_ids] = r.uniform_(0.0, 1.0) <= self.cfg.rel_standing_envs\\n\\n    def _update_command(self):\\n        \"\"\"Sets velocity command to zero for standing envs.\"\"\"\\n        # Enforce standing (i.e., zero velocity command) for standing envs\\n        standing_env_ids = self.is_standing_env.nonzero(as_tuple=False).flatten()  # TODO check if conversion is needed\\n        self.vel_command_b[standing_env_ids, :] = 0.0\\n\\n        # Enforce zero velocity for individual elements\\n        # TODO: check if conversion is needed\\n        zero_vel_x_env_ids = self.is_zero_vel_x_env.nonzero(as_tuple=False).flatten()\\n        zero_vel_y_env_ids = self.is_zero_vel_y_env.nonzero(as_tuple=False).flatten()\\n        zero_vel_yaw_env_ids = self.is_zero_vel_yaw_env.nonzero(as_tuple=False).flatten()\\n        self.vel_command_b[zero_vel_x_env_ids, 0] = 0.0\\n        self.vel_command_b[zero_vel_y_env_ids, 1] = 0.0\\n        self.vel_command_b[zero_vel_yaw_env_ids, 2] = 0.0'),\n",
       " Document(metadata={}, page_content='class InitialStateRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the initial state of the environment after reset.\"\"\"\\n\\n    def record_post_reset(self, env_ids: Sequence[int] | None):\\n        def extract_env_ids_values(value):\\n            nonlocal env_ids\\n            if isinstance(value, dict):\\n                return {k: extract_env_ids_values(v) for k, v in value.items()}\\n            return value[env_ids]\\n\\n        return \"initial_state\", extract_env_ids_values(self._env.scene.get_state(is_relative=True))'),\n",
       " Document(metadata={}, page_content='class PostStepStatesRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the state of the environment at the end of each step.\"\"\"\\n\\n    def record_post_step(self):\\n        return \"states\", self._env.scene.get_state(is_relative=True)'),\n",
       " Document(metadata={}, page_content='class PreStepActionsRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the actions in the beginning of each step.\"\"\"\\n\\n    def record_pre_step(self):\\n        return \"actions\", self._env.action_manager.action'),\n",
       " Document(metadata={}, page_content='class PreStepFlatPolicyObservationsRecorder(RecorderTerm):\\n    \"\"\"Recorder term that records the policy group observations in each step.\"\"\"\\n\\n    def record_pre_step(self):\\n        return \"obs\", self._env.obs_buf[\"policy\"]'),\n",
       " Document(metadata={}, page_content='class InitialStateRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the initial state recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = recorders.InitialStateRecorder'),\n",
       " Document(metadata={}, page_content='class PostStepStatesRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the step state recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = recorders.PostStepStatesRecorder'),\n",
       " Document(metadata={}, page_content='class PreStepActionsRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the step action recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = recorders.PreStepActionsRecorder'),\n",
       " Document(metadata={}, page_content='class PreStepFlatPolicyObservationsRecorderCfg(RecorderTermCfg):\\n    \"\"\"Configuration for the step policy observation recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = recorders.PreStepFlatPolicyObservationsRecorder'),\n",
       " Document(metadata={}, page_content='class ActionStateRecorderManagerCfg(RecorderManagerBaseCfg):\\n    \"\"\"Recorder configurations for recording actions and states.\"\"\"\\n\\n    record_initial_state = InitialStateRecorderCfg()\\n    record_post_step_states = PostStepStatesRecorderCfg()\\n    record_pre_step_actions = PreStepActionsRecorderCfg()\\n    record_pre_step_flat_policy_observations = PreStepFlatPolicyObservationsRecorderCfg()'),\n",
       " Document(metadata={}, page_content='class BaseEnvWindow:\\n    \"\"\"Window manager for the basic environment.\\n\\n    This class creates a window that is used to control the environment. The window\\n    contains controls for rendering, debug visualization, and other environment-specific\\n    UI elements.\\n\\n    Users can add their own UI elements to the window by using the `with` context manager.\\n    This can be done either be inheriting the class or by using the `env.window` object\\n    directly from the standalone execution script.\\n\\n    Example for adding a UI element from the standalone execution script:\\n        >>> with env.window.ui_window_elements[\"main_vstack\"]:\\n        >>>     ui.Label(\"My UI element\")\\n\\n    \"\"\"\\n\\n    def __init__(self, env: ManagerBasedEnv, window_name: str = \"IsaacLab\"):\\n        \"\"\"Initialize the window.\\n\\n        Args:\\n            env: The environment object.\\n            window_name: The name of the window. Defaults to \"IsaacLab\".\\n        \"\"\"\\n        # store inputs\\n        self.env = env\\n        # prepare the list of assets that can be followed by the viewport camera\\n        # note that the first two options are \"World\" and \"Env\" which are special cases\\n        self._viewer_assets_options = [\\n            \"World\",\\n            \"Env\",\\n            *self.env.scene.rigid_objects.keys(),\\n            *self.env.scene.articulations.keys(),\\n        ]\\n\\n        # Listeners for environment selection changes\\n        self._ui_listeners: list[ManagerLiveVisualizer] = []\\n\\n        print(\"Creating window for environment.\")\\n        # create window for UI\\n        self.ui_window = omni.ui.Window(\\n            window_name, width=400, height=500, visible=True, dock_preference=omni.ui.DockPreference.RIGHT_TOP\\n        )\\n        # dock next to properties window\\n        asyncio.ensure_future(self._dock_window(window_title=self.ui_window.title))\\n\\n        # keep a dictionary of stacks so that child environments can add their own UI elements\\n        # this can be done by using the `with` context manager\\n        self.ui_window_elements = dict()\\n        # create main frame\\n        self.ui_window_elements[\"main_frame\"] = self.ui_window.frame\\n        with self.ui_window_elements[\"main_frame\"]:\\n            # create main stack\\n            self.ui_window_elements[\"main_vstack\"] = omni.ui.VStack(spacing=5, height=0)\\n            with self.ui_window_elements[\"main_vstack\"]:\\n                # create collapsable frame for simulation\\n                self._build_sim_frame()\\n                # create collapsable frame for viewer\\n                self._build_viewer_frame()\\n                # create collapsable frame for debug visualization\\n                self._build_debug_vis_frame()\\n                with self.ui_window_elements[\"debug_frame\"]:\\n                    with self.ui_window_elements[\"debug_vstack\"]:\\n                        self._visualize_manager(title=\"Actions\", class_name=\"action_manager\")\\n                        self._visualize_manager(title=\"Observations\", class_name=\"observation_manager\")\\n\\n    def __del__(self):\\n        \"\"\"Destructor for the window.\"\"\"\\n        # destroy the window\\n        if self.ui_window is not None:\\n            self.ui_window.visible = False\\n            self.ui_window.destroy()\\n            self.ui_window = None\\n\\n    \"\"\"\\n    Build sub-sections of the UI.\\n    \"\"\"\\n\\n    def _build_sim_frame(self):\\n        \"\"\"Builds the sim-related controls frame for the UI.\"\"\"\\n        # create collapsable frame for controls\\n        self.ui_window_elements[\"sim_frame\"] = omni.ui.CollapsableFrame(\\n            title=\"Simulation Settings\",\\n            width=omni.ui.Fraction(1),\\n            height=0,\\n            collapsed=False,\\n            style=isaacsim.gui.components.ui_utils.get_style(),\\n            horizontal_scrollbar_policy=omni.ui.ScrollBarPolicy.SCROLLBAR_AS_NEEDED,\\n            vertical_scrollbar_policy=omni.ui.ScrollBarPolicy.SCROLLBAR_ALWAYS_ON,\\n        )\\n        with self.ui_window_elements[\"sim_frame\"]:\\n            # create stack for controls\\n            self.ui_window_elements[\"sim_vstack\"] = omni.ui.VStack(spacing=5, height=0)\\n            with self.ui_window_elements[\"sim_vstack\"]:\\n                # create rendering mode dropdown\\n                render_mode_cfg = {\\n                    \"label\": \"Rendering Mode\",\\n                    \"type\": \"dropdown\",\\n                    \"default_val\": self.env.sim.render_mode.value,\\n                    \"items\": [member.name for member in self.env.sim.RenderMode if member.value >= 0],\\n                    \"tooltip\": \"Select a rendering mode\\\\n\" + self.env.sim.RenderMode.__doc__,\\n                    \"on_clicked_fn\": lambda value: self.env.sim.set_render_mode(self.env.sim.RenderMode[value]),\\n                }\\n                self.ui_window_elements[\"render_dropdown\"] = isaacsim.gui.components.ui_utils.dropdown_builder(\\n                    **render_mode_cfg\\n                )\\n\\n                # create animation recording box\\n                record_animate_cfg = {\\n                    \"label\": \"Record Animation\",\\n                    \"type\": \"state_button\",\\n                    \"a_text\": \"START\",\\n                    \"b_text\": \"STOP\",\\n                    \"tooltip\": \"Record the animation of the scene. Only effective if fabric is disabled.\",\\n                    \"on_clicked_fn\": lambda value: self._toggle_recording_animation_fn(value),\\n                }\\n                self.ui_window_elements[\"record_animation\"] = isaacsim.gui.components.ui_utils.state_btn_builder(\\n                    **record_animate_cfg\\n                )\\n                # disable the button if fabric is not enabled\\n                self.ui_window_elements[\"record_animation\"].enabled = not self.env.sim.is_fabric_enabled()\\n\\n    def _build_viewer_frame(self):\\n        \"\"\"Build the viewer-related control frame for the UI.\"\"\"\\n        # create collapsable frame for viewer\\n        self.ui_window_elements[\"viewer_frame\"] = omni.ui.CollapsableFrame(\\n            title=\"Viewer Settings\",\\n            width=omni.ui.Fraction(1),\\n            height=0,\\n            collapsed=False,\\n            style=isaacsim.gui.components.ui_utils.get_style(),\\n            horizontal_scrollbar_policy=omni.ui.ScrollBarPolicy.SCROLLBAR_AS_NEEDED,\\n            vertical_scrollbar_policy=omni.ui.ScrollBarPolicy.SCROLLBAR_ALWAYS_ON,\\n        )\\n        with self.ui_window_elements[\"viewer_frame\"]:\\n            # create stack for controls\\n            self.ui_window_elements[\"viewer_vstack\"] = omni.ui.VStack(spacing=5, height=0)\\n            with self.ui_window_elements[\"viewer_vstack\"]:\\n                # create a number slider to move to environment origin\\n                # NOTE: slider is 1-indexed, whereas the env index is 0-indexed\\n                viewport_origin_cfg = {\\n                    \"label\": \"Environment Index\",\\n                    \"type\": \"button\",\\n                    \"default_val\": self.env.cfg.viewer.env_index + 1,\\n                    \"min\": 1,\\n                    \"max\": self.env.num_envs,\\n                    \"tooltip\": \"The environment index to follow. Only effective if follow mode is not \\'World\\'.\",\\n                }\\n                self.ui_window_elements[\"viewer_env_index\"] = isaacsim.gui.components.ui_utils.int_builder(\\n                    **viewport_origin_cfg\\n                )\\n                # create a number slider to move to environment origin\\n                self.ui_window_elements[\"viewer_env_index\"].add_value_changed_fn(self._set_viewer_env_index_fn)\\n\\n                # create a tracker for the camera location\\n                viewer_follow_cfg = {\\n                    \"label\": \"Follow Mode\",\\n                    \"type\": \"dropdown\",\\n                    \"default_val\": 0,\\n                    \"items\": [name.replace(\"_\", \" \").title() for name in self._viewer_assets_options],\\n                    \"tooltip\": \"Select the viewport camera following mode.\",\\n                    \"on_clicked_fn\": self._set_viewer_origin_type_fn,\\n                }\\n                self.ui_window_elements[\"viewer_follow\"] = isaacsim.gui.components.ui_utils.dropdown_builder(\\n                    **viewer_follow_cfg\\n                )\\n\\n                # add viewer default eye and lookat locations\\n                self.ui_window_elements[\"viewer_eye\"] = isaacsim.gui.components.ui_utils.xyz_builder(\\n                    label=\"Camera Eye\",\\n                    tooltip=\"Modify the XYZ location of the viewer eye.\",\\n                    default_val=self.env.cfg.viewer.eye,\\n                    step=0.1,\\n                    on_value_changed_fn=[self._set_viewer_location_fn] * 3,\\n                )\\n                self.ui_window_elements[\"viewer_lookat\"] = isaacsim.gui.components.ui_utils.xyz_builder(\\n                    label=\"Camera Target\",\\n                    tooltip=\"Modify the XYZ location of the viewer target.\",\\n                    default_val=self.env.cfg.viewer.lookat,\\n                    step=0.1,\\n                    on_value_changed_fn=[self._set_viewer_location_fn] * 3,\\n                )\\n\\n    def _build_debug_vis_frame(self):\\n        \"\"\"Builds the debug visualization frame for various scene elements.\\n\\n        This function inquires the scene for all elements that have a debug visualization\\n        implemented and creates a checkbox to toggle the debug visualization for each element\\n        that has it implemented. If the element does not have a debug visualization implemented,\\n        a label is created instead.\\n        \"\"\"\\n        # create collapsable frame for debug visualization\\n        self.ui_window_elements[\"debug_frame\"] = omni.ui.CollapsableFrame(\\n            title=\"Scene Debug Visualization\",\\n            width=omni.ui.Fraction(1),\\n            height=0,\\n            collapsed=False,\\n            style=isaacsim.gui.components.ui_utils.get_style(),\\n            horizontal_scrollbar_policy=omni.ui.ScrollBarPolicy.SCROLLBAR_AS_NEEDED,\\n            vertical_scrollbar_policy=omni.ui.ScrollBarPolicy.SCROLLBAR_ALWAYS_ON,\\n        )\\n        with self.ui_window_elements[\"debug_frame\"]:\\n            # create stack for debug visualization\\n            self.ui_window_elements[\"debug_vstack\"] = omni.ui.VStack(spacing=5, height=0)\\n            with self.ui_window_elements[\"debug_vstack\"]:\\n                elements = [\\n                    self.env.scene.terrain,\\n                    *self.env.scene.rigid_objects.values(),\\n                    *self.env.scene.articulations.values(),\\n                    *self.env.scene.sensors.values(),\\n                ]\\n                names = [\\n                    \"terrain\",\\n                    *self.env.scene.rigid_objects.keys(),\\n                    *self.env.scene.articulations.keys(),\\n                    *self.env.scene.sensors.keys(),\\n                ]\\n                # create one for the terrain\\n                for elem, name in zip(elements, names):\\n                    if elem is not None:\\n                        self._create_debug_vis_ui_element(name, elem)\\n\\n    def _visualize_manager(self, title: str, class_name: str) -> None:\\n        \"\"\"Checks if the attribute with the name \\'class_name\\' can be visualized. If yes, create vis interface.\\n\\n        Args:\\n            title: The title of the manager visualization frame.\\n            class_name: The name of the manager to visualize.\\n        \"\"\"\\n\\n        if hasattr(self.env, class_name) and class_name in self.env.manager_visualizers:\\n            manager = self.env.manager_visualizers[class_name]\\n            if hasattr(manager, \"has_debug_vis_implementation\"):\\n                self._create_debug_vis_ui_element(title, manager)\\n            else:\\n                print(\\n                    f\"ManagerLiveVisualizer cannot be created for manager: {class_name}, has_debug_vis_implementation\"\\n                    \" does not exist\"\\n                )\\n        else:\\n            print(f\"ManagerLiveVisualizer cannot be created for manager: {class_name}, Manager does not exist\")\\n\\n    \"\"\"\\n    Custom callbacks for UI elements.\\n    \"\"\"\\n\\n    def _toggle_recording_animation_fn(self, value: bool):\\n        \"\"\"Toggles the animation recording.\"\"\"\\n        if value:\\n            # log directory to save the recording\\n            if not hasattr(self, \"animation_log_dir\"):\\n                # create a new log directory\\n                log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\n                self.animation_log_dir = os.path.join(os.getcwd(), \"recordings\", log_dir)\\n            # start the recording\\n            _ = omni.kit.commands.execute(\\n                \"StartRecording\",\\n                target_paths=[(\"/World\", True)],\\n                live_mode=True,\\n                use_frame_range=False,\\n                start_frame=0,\\n                end_frame=0,\\n                use_preroll=False,\\n                preroll_frame=0,\\n                record_to=\"FILE\",\\n                fps=0,\\n                apply_root_anim=False,\\n                increment_name=True,\\n                record_folder=self.animation_log_dir,\\n                take_name=\"TimeSample\",\\n            )\\n        else:\\n            # stop the recording\\n            _ = omni.kit.commands.execute(\"StopRecording\")\\n            # save the current stage\\n            stage = omni.usd.get_context().get_stage()\\n            source_layer = stage.GetRootLayer()\\n            # output the stage to a file\\n            stage_usd_path = os.path.join(self.animation_log_dir, \"Stage.usd\")\\n            source_prim_path = \"/\"\\n            # creates empty anon layer\\n            temp_layer = Sdf.Find(stage_usd_path)\\n            if temp_layer is None:\\n                temp_layer = Sdf.Layer.CreateNew(stage_usd_path)\\n            temp_stage = Usd.Stage.Open(temp_layer)\\n            # update stage data\\n            UsdGeom.SetStageUpAxis(temp_stage, UsdGeom.GetStageUpAxis(stage))\\n            UsdGeom.SetStageMetersPerUnit(temp_stage, UsdGeom.GetStageMetersPerUnit(stage))\\n            # copy the prim\\n            Sdf.CreatePrimInLayer(temp_layer, source_prim_path)\\n            Sdf.CopySpec(source_layer, source_prim_path, temp_layer, source_prim_path)\\n            # set the default prim\\n            temp_layer.defaultPrim = Sdf.Path(source_prim_path).name\\n            # remove all physics from the stage\\n            for prim in temp_stage.TraverseAll():\\n                # skip if the prim is an instance\\n                if prim.IsInstanceable():\\n                    continue\\n                # if prim has articulation then disable it\\n                if prim.HasAPI(UsdPhysics.ArticulationRootAPI):\\n                    prim.RemoveAPI(UsdPhysics.ArticulationRootAPI)\\n                    prim.RemoveAPI(PhysxSchema.PhysxArticulationAPI)\\n                # if prim has rigid body then disable it\\n                if prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n                    prim.RemoveAPI(UsdPhysics.RigidBodyAPI)\\n                    prim.RemoveAPI(PhysxSchema.PhysxRigidBodyAPI)\\n                # if prim is a joint type then disable it\\n                if prim.IsA(UsdPhysics.Joint):\\n                    prim.GetAttribute(\"physics:jointEnabled\").Set(False)\\n            # resolve all paths relative to layer path\\n            omni.usd.resolve_paths(source_layer.identifier, temp_layer.identifier)\\n            # save the stage\\n            temp_layer.Save()\\n            # print the path to the saved stage\\n            print(\"Recording completed.\")\\n            print(f\"\\\\tSaved recorded stage to    : {stage_usd_path}\")\\n            print(f\"\\\\tSaved recorded animation to: {os.path.join(self.animation_log_dir, \\'TimeSample_tk001.usd\\')}\")\\n            print(\"\\\\nTo play the animation, check the instructions in the following link:\")\\n            print(\\n                \"\\\\thttps://docs.omniverse.nvidia.com/extensions/latest/ext_animation_stage-recorder.html#using-the-captured-timesamples\"\\n            )\\n            print(\"\\\\n\")\\n            # reset the log directory\\n            self.animation_log_dir = None\\n\\n    def _set_viewer_origin_type_fn(self, value: str):\\n        \"\"\"Sets the origin of the viewport\\'s camera. This is based on the drop-down menu in the UI.\"\"\"\\n        # Extract the viewport camera controller from environment\\n        vcc = self.env.viewport_camera_controller\\n        if vcc is None:\\n            raise ValueError(\"Viewport camera controller is not initialized! Please check the rendering mode.\")\\n\\n        # Based on origin type, update the camera view\\n        if value == \"World\":\\n            vcc.update_view_to_world()\\n        elif value == \"Env\":\\n            vcc.update_view_to_env()\\n        else:\\n            # find which index the asset is\\n            fancy_names = [name.replace(\"_\", \" \").title() for name in self._viewer_assets_options]\\n            # store the desired env index\\n            viewer_asset_name = self._viewer_assets_options[fancy_names.index(value)]\\n            # update the camera view\\n            vcc.update_view_to_asset_root(viewer_asset_name)\\n\\n    def _set_viewer_location_fn(self, model: omni.ui.SimpleFloatModel):\\n        \"\"\"Sets the viewport camera location based on the UI.\"\"\"\\n        # access the viewport camera controller (for brevity)\\n        vcc = self.env.viewport_camera_controller\\n        if vcc is None:\\n            raise ValueError(\"Viewport camera controller is not initialized! Please check the rendering mode.\")\\n        # obtain the camera locations and set them in the viewpoint camera controller\\n        eye = [self.ui_window_elements[\"viewer_eye\"][i].get_value_as_float() for i in range(3)]\\n        lookat = [self.ui_window_elements[\"viewer_lookat\"][i].get_value_as_float() for i in range(3)]\\n        # update the camera view\\n        vcc.update_view_location(eye, lookat)\\n\\n    def _set_viewer_env_index_fn(self, model: omni.ui.SimpleIntModel):\\n        \"\"\"Sets the environment index and updates the camera if in \\'env\\' origin mode.\"\"\"\\n        # access the viewport camera controller (for brevity)\\n        vcc = self.env.viewport_camera_controller\\n        if vcc is None:\\n            raise ValueError(\"Viewport camera controller is not initialized! Please check the rendering mode.\")\\n        # store the desired env index, UI is 1-indexed\\n        vcc.set_view_env_index(model.as_int - 1)\\n        # notify additional listeners\\n        for listener in self._ui_listeners:\\n            listener.set_env_selection(model.as_int - 1)\\n\\n    \"\"\"\\n    Helper functions - UI building.\\n    \"\"\"\\n\\n    def _create_debug_vis_ui_element(self, name: str, elem: object):\\n        \"\"\"Create a checkbox for toggling debug visualization for the given element.\"\"\"\\n        from omni.kit.window.extensions import SimpleCheckBox\\n\\n        with omni.ui.HStack():\\n            # create the UI element\\n            text = (\\n                \"Toggle debug visualization.\"\\n                if elem.has_debug_vis_implementation\\n                else \"Debug visualization not implemented.\"\\n            )\\n            omni.ui.Label(\\n                name.replace(\"_\", \" \").title(),\\n                width=isaacsim.gui.components.ui_utils.LABEL_WIDTH - 12,\\n                alignment=omni.ui.Alignment.LEFT_CENTER,\\n                tooltip=text,\\n            )\\n            has_cfg = hasattr(elem, \"cfg\") and elem.cfg is not None\\n            is_checked = False\\n            if has_cfg:\\n                is_checked = (hasattr(elem.cfg, \"debug_vis\") and elem.cfg.debug_vis) or (\\n                    hasattr(elem, \"debug_vis\") and elem.debug_vis\\n                )\\n            self.ui_window_elements[f\"{name}_cb\"] = SimpleCheckBox(\\n                model=omni.ui.SimpleBoolModel(),\\n                enabled=elem.has_debug_vis_implementation,\\n                checked=is_checked,\\n                on_checked_fn=lambda value, e=weakref.proxy(elem): e.set_debug_vis(value),\\n            )\\n            isaacsim.gui.components.ui_utils.add_line_rect_flourish()\\n\\n        # Create a panel for the debug visualization\\n        if isinstance(elem, ManagerLiveVisualizer):\\n            self.ui_window_elements[f\"{name}_panel\"] = omni.ui.Frame(width=omni.ui.Fraction(1))\\n            if not elem.set_vis_frame(self.ui_window_elements[f\"{name}_panel\"]):\\n                print(f\"Frame failed to set for ManagerLiveVisualizer: {name}\")\\n\\n        # Add listener for environment selection changes\\n        if isinstance(elem, ManagerLiveVisualizer):\\n            self._ui_listeners.append(elem)\\n\\n    async def _dock_window(self, window_title: str):\\n        \"\"\"Docks the custom UI window to the property window.\"\"\"\\n        # wait for the window to be created\\n        for _ in range(5):\\n            if omni.ui.Workspace.get_window(window_title):\\n                break\\n            await self.env.sim.app.next_update_async()\\n\\n        # dock next to properties window\\n        custom_window = omni.ui.Workspace.get_window(window_title)\\n        property_window = omni.ui.Workspace.get_window(\"Property\")\\n        if custom_window and property_window:\\n            custom_window.dock_in(property_window, omni.ui.DockPosition.SAME, 1.0)\\n            custom_window.focus()'),\n",
       " Document(metadata={}, page_content='class EmptyWindow:\\n    \"\"\"\\n    Creates an empty UI window that can be docked in the Omniverse Kit environment.\\n\\n    The class initializes a dockable UI window and provides a main frame with a vertical stack.\\n    You can add custom UI elements to this vertical stack.\\n\\n    Example for adding a UI element from the standalone execution script:\\n        >>> with env.window.ui_window_elements[\"main_vstack\"]:\\n        >>>     ui.Label(\"My UI element\")\\n\\n    \"\"\"\\n\\n    def __init__(self, env: ManagerBasedEnv, window_name: str):\\n        \"\"\"Initialize the window.\\n\\n        Args:\\n            env: The environment object.\\n            window_name: The name of the window.\\n        \"\"\"\\n        # store environment\\n        self.env = env\\n\\n        # create window for UI\\n        self.ui_window = omni.ui.Window(\\n            window_name, width=400, height=500, visible=True, dock_preference=omni.ui.DockPreference.RIGHT_TOP\\n        )\\n        # dock next to properties window\\n        asyncio.ensure_future(self._dock_window(window_title=self.ui_window.title))\\n\\n        # keep a dictionary of stacks so that child environments can add their own UI elements\\n        # this can be done by using the `with` context manager\\n        self.ui_window_elements = dict()\\n        # create main frame\\n        self.ui_window_elements[\"main_frame\"] = self.ui_window.frame\\n        with self.ui_window_elements[\"main_frame\"]:\\n            # create main vstack\\n            self.ui_window_elements[\"main_vstack\"] = omni.ui.VStack(spacing=5, height=0)\\n\\n    def __del__(self):\\n        \"\"\"Destructor for the window.\"\"\"\\n        # destroy the window\\n        if self.ui_window is not None:\\n            self.ui_window.visible = False\\n            self.ui_window.destroy()\\n            self.ui_window = None\\n\\n    async def _dock_window(self, window_title: str):\\n        \"\"\"Docks the custom UI window to the property window.\"\"\"\\n        # wait for the window to be created\\n        for _ in range(5):\\n            if omni.ui.Workspace.get_window(window_title):\\n                break\\n            await self.env.sim.app.next_update_async()\\n\\n        # dock next to properties window\\n        custom_window = omni.ui.Workspace.get_window(window_title)\\n        property_window = omni.ui.Workspace.get_window(\"Property\")\\n        if custom_window and property_window:\\n            custom_window.dock_in(property_window, omni.ui.DockPosition.SAME, 1.0)\\n            custom_window.focus()'),\n",
       " Document(metadata={}, page_content='class ManagerBasedRLEnvWindow(BaseEnvWindow):\\n    \"\"\"Window manager for the RL environment.\\n\\n    On top of the basic environment window, this class adds controls for the RL environment.\\n    This includes visualization of the command manager.\\n    \"\"\"\\n\\n    def __init__(self, env: ManagerBasedRLEnv, window_name: str = \"IsaacLab\"):\\n        \"\"\"Initialize the window.\\n\\n        Args:\\n            env: The environment object.\\n            window_name: The name of the window. Defaults to \"IsaacLab\".\\n        \"\"\"\\n        # initialize base window\\n        super().__init__(env, window_name)\\n\\n        # add custom UI elements\\n        with self.ui_window_elements[\"main_vstack\"]:\\n            with self.ui_window_elements[\"debug_frame\"]:\\n                with self.ui_window_elements[\"debug_vstack\"]:\\n                    self._visualize_manager(title=\"Commands\", class_name=\"command_manager\")\\n                    self._visualize_manager(title=\"Rewards\", class_name=\"reward_manager\")\\n                    self._visualize_manager(title=\"Curriculum\", class_name=\"curriculum_manager\")\\n                    self._visualize_manager(title=\"Termination\", class_name=\"termination_manager\")'),\n",
       " Document(metadata={}, page_content='class ViewportCameraController:\\n    \"\"\"This class handles controlling the camera associated with a viewport in the simulator.\\n\\n    It can be used to set the viewpoint camera to track different origin types:\\n\\n    - **world**: the center of the world (static)\\n    - **env**: the center of an environment (static)\\n    - **asset_root**: the root of an asset in the scene (e.g. tracking a robot moving in the scene)\\n\\n    On creation, the camera is set to track the origin type specified in the configuration.\\n\\n    For the :attr:`asset_root` origin type, the camera is updated at each rendering step to track the asset\\'s\\n    root position. For this, it registers a callback to the post update event stream from the simulation app.\\n    \"\"\"\\n\\n    def __init__(self, env: ManagerBasedEnv | DirectRLEnv, cfg: ViewerCfg):\\n        \"\"\"Initialize the ViewportCameraController.\\n\\n        Args:\\n            env: The environment.\\n            cfg: The configuration for the viewport camera controller.\\n\\n        Raises:\\n            ValueError: If origin type is configured to be \"env\" but :attr:`cfg.env_index` is out of bounds.\\n            ValueError: If origin type is configured to be \"asset_root\" but :attr:`cfg.asset_name` is unset.\\n\\n        \"\"\"\\n        # store inputs\\n        self._env = env\\n        self._cfg = copy.deepcopy(cfg)\\n        # cast viewer eye and look-at to numpy arrays\\n        self.default_cam_eye = np.array(self._cfg.eye)\\n        self.default_cam_lookat = np.array(self._cfg.lookat)\\n\\n        # set the camera origins\\n        if self.cfg.origin_type == \"env\":\\n            # check that the env_index is within bounds\\n            self.set_view_env_index(self.cfg.env_index)\\n            # set the camera origin to the center of the environment\\n            self.update_view_to_env()\\n        elif self.cfg.origin_type == \"asset_root\" or self.cfg.origin_type == \"asset_body\":\\n            # note: we do not yet update camera for tracking an asset origin, as the asset may not yet be\\n            # in the scene when this is called. Instead, we subscribe to the post update event to update the camera\\n            # at each rendering step.\\n            if self.cfg.asset_name is None:\\n                raise ValueError(f\"No asset name provided for viewer with origin type: \\'{self.cfg.origin_type}\\'.\")\\n            if self.cfg.origin_type == \"asset_body\":\\n                if self.cfg.body_name is None:\\n                    raise ValueError(f\"No body name provided for viewer with origin type: \\'{self.cfg.origin_type}\\'.\")\\n        else:\\n            # set the camera origin to the center of the world\\n            self.update_view_to_world()\\n\\n        # subscribe to post update event so that camera view can be updated at each rendering step\\n        app_interface = omni.kit.app.get_app_interface()\\n        app_event_stream = app_interface.get_post_update_event_stream()\\n        self._viewport_camera_update_handle = app_event_stream.create_subscription_to_pop(\\n            lambda event, obj=weakref.proxy(self): obj._update_tracking_callback(event)\\n        )\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribe from the callback.\"\"\"\\n        # use hasattr to handle case where __init__ has not completed before __del__ is called\\n        if hasattr(self, \"_viewport_camera_update_handle\") and self._viewport_camera_update_handle is not None:\\n            self._viewport_camera_update_handle.unsubscribe()\\n            self._viewport_camera_update_handle = None\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def cfg(self) -> ViewerCfg:\\n        \"\"\"The configuration for the viewer.\"\"\"\\n        return self._cfg\\n\\n    \"\"\"\\n    Public Functions\\n    \"\"\"\\n\\n    def set_view_env_index(self, env_index: int):\\n        \"\"\"Sets the environment index for the camera view.\\n\\n        Args:\\n            env_index: The index of the environment to set the camera view to.\\n\\n        Raises:\\n            ValueError: If the environment index is out of bounds. It should be between 0 and num_envs - 1.\\n        \"\"\"\\n        # check that the env_index is within bounds\\n        if env_index < 0 or env_index >= self._env.num_envs:\\n            raise ValueError(\\n                f\"Out of range value for attribute \\'env_index\\': {env_index}.\"\\n                f\" Expected a value between 0 and {self._env.num_envs - 1} for the current environment.\"\\n            )\\n        # update the environment index\\n        self.cfg.env_index = env_index\\n        # update the camera view if the origin is set to env type (since, the camera view is static)\\n        # note: for assets, the camera view is updated at each rendering step\\n        if self.cfg.origin_type == \"env\":\\n            self.update_view_to_env()\\n\\n    def update_view_to_world(self):\\n        \"\"\"Updates the viewer\\'s origin to the origin of the world which is (0, 0, 0).\"\"\"\\n        # set origin type to world\\n        self.cfg.origin_type = \"world\"\\n        # update the camera origins\\n        self.viewer_origin = torch.zeros(3)\\n        # update the camera view\\n        self.update_view_location()\\n\\n    def update_view_to_env(self):\\n        \"\"\"Updates the viewer\\'s origin to the origin of the selected environment.\"\"\"\\n        # set origin type to world\\n        self.cfg.origin_type = \"env\"\\n        # update the camera origins\\n        self.viewer_origin = self._env.scene.env_origins[self.cfg.env_index]\\n        # update the camera view\\n        self.update_view_location()\\n\\n    def update_view_to_asset_root(self, asset_name: str):\\n        \"\"\"Updates the viewer\\'s origin based upon the root of an asset in the scene.\\n\\n        Args:\\n            asset_name: The name of the asset in the scene. The name should match the name of the\\n                asset in the scene.\\n\\n        Raises:\\n            ValueError: If the asset is not in the scene.\\n        \"\"\"\\n        # check if the asset is in the scene\\n        if self.cfg.asset_name != asset_name:\\n            asset_entities = [*self._env.scene.rigid_objects.keys(), *self._env.scene.articulations.keys()]\\n            if asset_name not in asset_entities:\\n                raise ValueError(f\"Asset \\'{asset_name}\\' is not in the scene. Available entities: {asset_entities}.\")\\n        # update the asset name\\n        self.cfg.asset_name = asset_name\\n        # set origin type to asset_root\\n        self.cfg.origin_type = \"asset_root\"\\n        # update the camera origins\\n        self.viewer_origin = self._env.scene[self.cfg.asset_name].data.root_pos_w[self.cfg.env_index]\\n        # update the camera view\\n        self.update_view_location()\\n\\n    def update_view_to_asset_body(self, asset_name: str, body_name: str):\\n        \"\"\"Updates the viewer\\'s origin based upon the body of an asset in the scene.\\n\\n        Args:\\n            asset_name: The name of the asset in the scene. The name should match the name of the\\n                asset in the scene.\\n            body_name: The name of the body in the asset.\\n\\n        Raises:\\n            ValueError: If the asset is not in the scene or the body is not valid.\\n        \"\"\"\\n        # check if the asset is in the scene\\n        if self.cfg.asset_name != asset_name:\\n            asset_entities = [*self._env.scene.rigid_objects.keys(), *self._env.scene.articulations.keys()]\\n            if asset_name not in asset_entities:\\n                raise ValueError(f\"Asset \\'{asset_name}\\' is not in the scene. Available entities: {asset_entities}.\")\\n        # check if the body is in the asset\\n        asset: Articulation = self._env.scene[asset_name]\\n        if body_name not in asset.body_names:\\n            raise ValueError(\\n                f\"\\'{body_name}\\' is not a body of Asset \\'{asset_name}\\'. Available bodies: {asset.body_names}.\"\\n            )\\n        # get the body index\\n        body_id, _ = asset.find_bodies(body_name)\\n        # update the asset name\\n        self.cfg.asset_name = asset_name\\n        # set origin type to asset_body\\n        self.cfg.origin_type = \"asset_body\"\\n        # update the camera origins\\n        self.viewer_origin = self._env.scene[self.cfg.asset_name].data.body_pos_w[self.cfg.env_index, body_id].view(3)\\n        # update the camera view\\n        self.update_view_location()\\n\\n    def update_view_location(self, eye: Sequence[float] | None = None, lookat: Sequence[float] | None = None):\\n        \"\"\"Updates the camera view pose based on the current viewer origin and the eye and lookat positions.\\n\\n        Args:\\n            eye: The eye position of the camera. If None, the current eye position is used.\\n            lookat: The lookat position of the camera. If None, the current lookat position is used.\\n        \"\"\"\\n        # store the camera view pose for later use\\n        if eye is not None:\\n            self.default_cam_eye = np.asarray(eye)\\n        if lookat is not None:\\n            self.default_cam_lookat = np.asarray(lookat)\\n        # set the camera locations\\n        viewer_origin = self.viewer_origin.detach().cpu().numpy()\\n        cam_eye = viewer_origin + self.default_cam_eye\\n        cam_target = viewer_origin + self.default_cam_lookat\\n\\n        # set the camera view\\n        self._env.sim.set_camera_view(eye=cam_eye, target=cam_target)\\n\\n    \"\"\"\\n    Private Functions\\n    \"\"\"\\n\\n    def _update_tracking_callback(self, event):\\n        \"\"\"Updates the camera view at each rendering step.\"\"\"\\n        # update the camera view if the origin is set to asset_root\\n        # in other cases, the camera view is static and does not need to be updated continuously\\n        if self.cfg.origin_type == \"asset_root\" and self.cfg.asset_name is not None:\\n            self.update_view_to_asset_root(self.cfg.asset_name)\\n        if self.cfg.origin_type == \"asset_body\" and self.cfg.asset_name is not None and self.cfg.body_name is not None:\\n            self.update_view_to_asset_body(self.cfg.asset_name, self.cfg.body_name)'),\n",
       " Document(metadata={}, page_content='def multi_agent_to_single_agent(env: DirectMARLEnv, state_as_observation: bool = False) -> DirectRLEnv:\\n    \"\"\"Convert the multi-agent environment instance to a single-agent environment instance.\\n\\n    The converted environment will be an instance of the single-agent environment interface class (:class:`DirectRLEnv`).\\n    As part of the conversion process, the following operations are carried out:\\n\\n    * The observations of all the agents in the original multi-agent environment are concatenated to compose\\n        the single-agent observation. If the use of the environment state is defined as the observation,\\n        it is returned as is.\\n    * The terminations and time-outs of all the agents in the original multi-agent environment are multiplied\\n        (``AND`` operation) to compose the corresponding single-agent values.\\n    * The rewards of all the agents in the original multi-agent environment are summed to compose the\\n        single-agent reward.\\n    * The action taken by the single-agent is split to compose the actions of each agent in the original\\n        multi-agent environment before stepping it.\\n\\n    Args:\\n        env: The environment to convert to.\\n        state_as_observation: Weather to use the multi-agent environment state as single-agent observation.\\n\\n    Returns:\\n        Single-agent environment instance.\\n\\n    Raises:\\n        AssertionError: If the environment state cannot be used as observation since it was explicitly defined\\n            as unconstructed (:attr:`DirectMARLEnvCfg.state_space`).\\n    \"\"\"\\n\\n    class Env(DirectRLEnv):\\n        def __init__(self, env: DirectMARLEnv) -> None:\\n            self.env: DirectMARLEnv = env.unwrapped\\n\\n            # check if it is possible to use the multi-agent environment state as single-agent observation\\n            self._state_as_observation = state_as_observation\\n            if self._state_as_observation:\\n                assert self.env.cfg.state_space != 0, (\\n                    \"The environment state cannot be used as observation since it was explicitly defined as\"\\n                    \" unconstructed\"\\n                )\\n\\n            # create single-agent properties to expose in the converted environment\\n            self.cfg = self.env.cfg\\n            self.sim = self.env.sim\\n            self.scene = self.env.scene\\n            self.render_mode = self.env.render_mode\\n\\n            self.single_observation_space = gym.spaces.Dict()\\n            if self._state_as_observation:\\n                self.single_observation_space[\"policy\"] = self.env.state_space\\n            else:\\n                self.single_observation_space[\"policy\"] = gym.spaces.flatten_space(\\n                    gym.spaces.Tuple([self.env.observation_spaces[agent] for agent in self.env.possible_agents])\\n                )\\n            self.single_action_space = gym.spaces.flatten_space(\\n                gym.spaces.Tuple([self.env.action_spaces[agent] for agent in self.env.possible_agents])\\n            )\\n\\n            # batch the spaces for vectorized environments\\n            self.observation_space = gym.vector.utils.batch_space(\\n                self.single_observation_space[\"policy\"], self.num_envs\\n            )\\n            self.action_space = gym.vector.utils.batch_space(self.single_action_space, self.num_envs)\\n\\n        def reset(self, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[VecEnvObs, dict]:\\n            obs, extras = self.env.reset(seed, options)\\n\\n            # use environment state as observation\\n            if self._state_as_observation:\\n                obs = {\"policy\": self.env.state()}\\n            # concatenate agents\\' observations\\n            # FIXME: This implementation assumes the spaces are fundamental ones. Fix it to support composite spaces\\n            else:\\n                obs = {\\n                    \"policy\": torch.cat(\\n                        [obs[agent].reshape(self.num_envs, -1) for agent in self.env.possible_agents], dim=-1\\n                    )\\n                }\\n\\n            return obs, extras\\n\\n        def step(self, action: torch.Tensor) -> VecEnvStepReturn:\\n            # split single-agent actions to build the multi-agent ones\\n            # FIXME: This implementation assumes the spaces are fundamental ones. Fix it to support composite spaces\\n            index = 0\\n            _actions = {}\\n            for agent in self.env.possible_agents:\\n                delta = gym.spaces.flatdim(self.env.action_spaces[agent])\\n                _actions[agent] = action[:, index : index + delta]\\n                index += delta\\n\\n            # step the environment\\n            obs, rewards, terminated, time_outs, extras = self.env.step(_actions)\\n\\n            # use environment state as observation\\n            if self._state_as_observation:\\n                obs = {\"policy\": self.env.state()}\\n            # concatenate agents\\' observations\\n            # FIXME: This implementation assumes the spaces are fundamental ones. Fix it to support composite spaces\\n            else:\\n                obs = {\\n                    \"policy\": torch.cat(\\n                        [obs[agent].reshape(self.num_envs, -1) for agent in self.env.possible_agents], dim=-1\\n                    )\\n                }\\n\\n            # process environment outputs to return single-agent data\\n            rewards = sum(rewards.values())\\n            terminated = math.prod(terminated.values()).to(dtype=torch.bool)\\n            time_outs = math.prod(time_outs.values()).to(dtype=torch.bool)\\n\\n            return obs, rewards, terminated, time_outs, extras\\n\\n        def render(self, recompute: bool = False) -> np.ndarray | None:\\n            return self.env.render(recompute)\\n\\n        def close(self) -> None:\\n            self.env.close()\\n\\n    return Env(env)'),\n",
       " Document(metadata={}, page_content='def multi_agent_with_one_agent(env: DirectMARLEnv, state_as_observation: bool = False) -> DirectMARLEnv:\\n    \"\"\"Convert the multi-agent environment instance to a multi-agent environment instance with only one agent.\\n\\n    The converted environment will be an instance of the multi-agent environment interface class\\n    (:class:`DirectMARLEnv`) but with only one agent available (with ID: ``\"single-agent\"``).\\n    As part of the conversion process, the following operations are carried out:\\n\\n    * The observations of all the agents in the original multi-agent environment are concatenated to compose\\n        the agent observation. If the use of the environment state is defined as the observation, it is returned as is.\\n    * The terminations and time-outs of all the agents in the original multi-agent environment are multiplied\\n        (``AND`` operation) to compose the corresponding agent values.\\n    * The rewards of all the agents in the original multi-agent environment are summed to compose the agent reward.\\n    * The action taken by the agent is split to compose the actions of each agent in the original\\n        multi-agent environment before stepping it.\\n\\n    Args:\\n        env: The environment to convert to.\\n        state_as_observation: Weather to use the multi-agent environment state as agent observation.\\n\\n    Returns:\\n        Multi-agent environment instance with only one agent.\\n\\n    Raises:\\n        AssertionError: If the environment state cannot be used as observation since it was explicitly defined\\n            as unconstructed (:attr:`DirectMARLEnvCfg.state_space`).\\n    \"\"\"\\n\\n    class Env(DirectMARLEnv):\\n        def __init__(self, env: DirectMARLEnv) -> None:\\n            self.env: DirectMARLEnv = env.unwrapped\\n\\n            # check if it is possible to use the multi-agent environment state as agent observation\\n            self._state_as_observation = state_as_observation\\n            if self._state_as_observation:\\n                assert self.env.cfg.state_space != 0, (\\n                    \"The environment state cannot be used as observation since it was explicitly defined as\"\\n                    \" unconstructed\"\\n                )\\n\\n            # create agent properties to expose in the converted environment\\n            self._agent_id = \"single-agent\"\\n            self._exported_agents = [self._agent_id]\\n            self._exported_possible_agents = [self._agent_id]\\n            if self._state_as_observation:\\n                self._exported_observation_spaces = {self._agent_id: self.env.state_space}\\n            else:\\n                self._exported_observation_spaces = {\\n                    self._agent_id: gym.spaces.flatten_space(\\n                        gym.spaces.Tuple([self.env.observation_spaces[agent] for agent in self.env.possible_agents])\\n                    )\\n                }\\n            self._exported_action_spaces = {\\n                self._agent_id: gym.spaces.flatten_space(\\n                    gym.spaces.Tuple([self.env.action_spaces[agent] for agent in self.env.possible_agents])\\n                )\\n            }\\n\\n        def __getattr__(self, key: str) -> Any:\\n            return getattr(self.env, key)\\n\\n        @property\\n        def agents(self) -> list[AgentID]:\\n            return self._exported_agents\\n\\n        @property\\n        def possible_agents(self) -> list[AgentID]:\\n            return self._exported_possible_agents\\n\\n        @property\\n        def observation_spaces(self) -> dict[AgentID, gym.Space]:\\n            return self._exported_observation_spaces\\n\\n        @property\\n        def action_spaces(self) -> dict[AgentID, gym.Space]:\\n            return self._exported_action_spaces\\n\\n        def reset(\\n            self, seed: int | None = None, options: dict[str, Any] | None = None\\n        ) -> tuple[dict[AgentID, ObsType], dict[AgentID, dict]]:\\n            obs, extras = self.env.reset(seed, options)\\n\\n            # use environment state as observation\\n            if self._state_as_observation:\\n                obs = {self._agent_id: self.env.state()}\\n            # concatenate agents\\' observations\\n            # FIXME: This implementation assumes the spaces are fundamental ones. Fix it to support composite spaces\\n            else:\\n                obs = {\\n                    self._agent_id: torch.cat(\\n                        [obs[agent].reshape(self.num_envs, -1) for agent in self.env.possible_agents], dim=-1\\n                    )\\n                }\\n\\n            return obs, extras\\n\\n        def step(self, actions: dict[AgentID, ActionType]) -> EnvStepReturn:\\n            # split agent actions to build the multi-agent ones\\n            # FIXME: This implementation assumes the spaces are fundamental ones. Fix it to support composite spaces\\n            index = 0\\n            _actions = {}\\n            for agent in self.env.possible_agents:\\n                delta = gym.spaces.flatdim(self.env.action_spaces[agent])\\n                _actions[agent] = actions[self._agent_id][:, index : index + delta]\\n                index += delta\\n\\n            # step the environment\\n            obs, rewards, terminated, time_outs, extras = self.env.step(_actions)\\n\\n            # use environment state as observation\\n            if self._state_as_observation:\\n                obs = {self._agent_id: self.env.state()}\\n            # concatenate agents\\' observations\\n            # FIXME: This implementation assumes the spaces are fundamental ones. Fix it to support composite spaces\\n            else:\\n                obs = {\\n                    self._agent_id: torch.cat(\\n                        [obs[agent].reshape(self.num_envs, -1) for agent in self.env.possible_agents], dim=-1\\n                    )\\n                }\\n\\n            # process environment outputs to return agent data\\n            rewards = {self._agent_id: sum(rewards.values())}\\n            terminated = {self._agent_id: math.prod(terminated.values()).to(dtype=torch.bool)}\\n            time_outs = {self._agent_id: math.prod(time_outs.values()).to(dtype=torch.bool)}\\n\\n            return obs, rewards, terminated, time_outs, extras\\n\\n        def state(self) -> StateType | None:\\n            return self.env.state()\\n\\n        def render(self, recompute: bool = False) -> np.ndarray | None:\\n            self.env.render(recompute)\\n\\n        def close(self) -> None:\\n            self.env.close()\\n\\n    return Env(env)'),\n",
       " Document(metadata={}, page_content='def spec_to_gym_space(spec: SpaceType) -> gym.spaces.Space:\\n    \"\"\"Generate an appropriate Gymnasium space according to the given space specification.\\n\\n    Args:\\n        spec: Space specification.\\n\\n    Returns:\\n        Gymnasium space.\\n\\n    Raises:\\n        ValueError: If the given space specification is not valid/supported.\\n    \"\"\"\\n    if isinstance(spec, gym.spaces.Space):\\n        return spec\\n    # fundamental spaces\\n    # Box\\n    elif isinstance(spec, int):\\n        return gym.spaces.Box(low=-np.inf, high=np.inf, shape=(spec,))\\n    elif isinstance(spec, list) and all(isinstance(x, int) for x in spec):\\n        return gym.spaces.Box(low=-np.inf, high=np.inf, shape=spec)\\n    # Discrete\\n    elif isinstance(spec, set) and len(spec) == 1:\\n        return gym.spaces.Discrete(n=next(iter(spec)))\\n    # MultiDiscrete\\n    elif isinstance(spec, list) and all(isinstance(x, set) and len(x) == 1 for x in spec):\\n        return gym.spaces.MultiDiscrete(nvec=[next(iter(x)) for x in spec])\\n    # composite spaces\\n    # Tuple\\n    elif isinstance(spec, tuple):\\n        return gym.spaces.Tuple([spec_to_gym_space(x) for x in spec])\\n    # Dict\\n    elif isinstance(spec, dict):\\n        return gym.spaces.Dict({k: spec_to_gym_space(v) for k, v in spec.items()})\\n    raise ValueError(f\"Unsupported space specification: {spec}\")'),\n",
       " Document(metadata={}, page_content='def sample_space(space: gym.spaces.Space, device: str, batch_size: int = -1, fill_value: float | None = None) -> Any:\\n    \"\"\"Sample a Gymnasium space where the data container are PyTorch tensors.\\n\\n    Args:\\n        space: Gymnasium space.\\n        device: The device where the tensor should be created.\\n        batch_size: Batch size. If the specified value is greater than zero, a batched space will be created and sampled from it.\\n        fill_value: The value to fill the created tensors with. If None (default value), tensors will keep their random values.\\n\\n    Returns:\\n        Tensorized sampled space.\\n    \"\"\"\\n\\n    def tensorize(s, x):\\n        if isinstance(s, gym.spaces.Box):\\n            tensor = torch.tensor(x, device=device, dtype=torch.float32).reshape(batch_size, *s.shape)\\n            if fill_value is not None:\\n                tensor.fill_(fill_value)\\n            return tensor\\n        elif isinstance(s, gym.spaces.Discrete):\\n            if isinstance(x, np.ndarray):\\n                tensor = torch.tensor(x, device=device, dtype=torch.int64).reshape(batch_size, 1)\\n                if fill_value is not None:\\n                    tensor.fill_(int(fill_value))\\n                return tensor\\n            elif isinstance(x, np.number) or type(x) in [int, float]:\\n                tensor = torch.tensor([x], device=device, dtype=torch.int64).reshape(batch_size, 1)\\n                if fill_value is not None:\\n                    tensor.fill_(int(fill_value))\\n                return tensor\\n        elif isinstance(s, gym.spaces.MultiDiscrete):\\n            if isinstance(x, np.ndarray):\\n                tensor = torch.tensor(x, device=device, dtype=torch.int64).reshape(batch_size, *s.shape)\\n                if fill_value is not None:\\n                    tensor.fill_(int(fill_value))\\n                return tensor\\n        elif isinstance(s, gym.spaces.Dict):\\n            return {k: tensorize(_s, x[k]) for k, _s in s.items()}\\n        elif isinstance(s, gym.spaces.Tuple):\\n            return tuple([tensorize(_s, v) for _s, v in zip(s, x)])\\n\\n    sample = (gym.vector.utils.batch_space(space, batch_size) if batch_size > 0 else space).sample()\\n    return tensorize(space, sample)'),\n",
       " Document(metadata={}, page_content='def serialize_space(space: SpaceType) -> str:\\n    \"\"\"Serialize a space specification as JSON.\\n\\n    Args:\\n        space: Space specification.\\n\\n    Returns:\\n        Serialized JSON representation.\\n    \"\"\"\\n    # Gymnasium spaces\\n    if isinstance(space, gym.spaces.Discrete):\\n        return json.dumps({\"type\": \"gymnasium\", \"space\": \"Discrete\", \"n\": int(space.n)})\\n    elif isinstance(space, gym.spaces.Box):\\n        return json.dumps({\\n            \"type\": \"gymnasium\",\\n            \"space\": \"Box\",\\n            \"low\": space.low.tolist(),\\n            \"high\": space.high.tolist(),\\n            \"shape\": space.shape,\\n        })\\n    elif isinstance(space, gym.spaces.MultiDiscrete):\\n        return json.dumps({\"type\": \"gymnasium\", \"space\": \"MultiDiscrete\", \"nvec\": space.nvec.tolist()})\\n    elif isinstance(space, gym.spaces.Tuple):\\n        return json.dumps({\"type\": \"gymnasium\", \"space\": \"Tuple\", \"spaces\": tuple(map(serialize_space, space.spaces))})\\n    elif isinstance(space, gym.spaces.Dict):\\n        return json.dumps(\\n            {\"type\": \"gymnasium\", \"space\": \"Dict\", \"spaces\": {k: serialize_space(v) for k, v in space.spaces.items()}}\\n        )\\n    # Python data types\\n    # Box\\n    elif isinstance(space, int) or (isinstance(space, list) and all(isinstance(x, int) for x in space)):\\n        return json.dumps({\"type\": \"python\", \"space\": \"Box\", \"value\": space})\\n    # Discrete\\n    elif isinstance(space, set) and len(space) == 1:\\n        return json.dumps({\"type\": \"python\", \"space\": \"Discrete\", \"value\": next(iter(space))})\\n    # MultiDiscrete\\n    elif isinstance(space, list) and all(isinstance(x, set) and len(x) == 1 for x in space):\\n        return json.dumps({\"type\": \"python\", \"space\": \"MultiDiscrete\", \"value\": [next(iter(x)) for x in space]})\\n    # composite spaces\\n    # Tuple\\n    elif isinstance(space, tuple):\\n        return json.dumps({\"type\": \"python\", \"space\": \"Tuple\", \"value\": [serialize_space(x) for x in space]})\\n    # Dict\\n    elif isinstance(space, dict):\\n        return json.dumps(\\n            {\"type\": \"python\", \"space\": \"Dict\", \"value\": {k: serialize_space(v) for k, v in space.items()}}\\n        )\\n    raise ValueError(f\"Unsupported space ({space})\")'),\n",
       " Document(metadata={}, page_content='def deserialize_space(string: str) -> gym.spaces.Space:\\n    \"\"\"Deserialize a space specification encoded as JSON.\\n\\n    Args:\\n        string: Serialized JSON representation.\\n\\n    Returns:\\n        Space specification.\\n    \"\"\"\\n    obj = json.loads(string)\\n    # Gymnasium spaces\\n    if obj[\"type\"] == \"gymnasium\":\\n        if obj[\"space\"] == \"Discrete\":\\n            return gym.spaces.Discrete(n=obj[\"n\"])\\n        elif obj[\"space\"] == \"Box\":\\n            return gym.spaces.Box(low=np.array(obj[\"low\"]), high=np.array(obj[\"high\"]), shape=obj[\"shape\"])\\n        elif obj[\"space\"] == \"MultiDiscrete\":\\n            return gym.spaces.MultiDiscrete(nvec=np.array(obj[\"nvec\"]))\\n        elif obj[\"space\"] == \"Tuple\":\\n            return gym.spaces.Tuple(spaces=tuple(map(deserialize_space, obj[\"spaces\"])))\\n        elif obj[\"space\"] == \"Dict\":\\n            return gym.spaces.Dict(spaces={k: deserialize_space(v) for k, v in obj[\"spaces\"].items()})\\n        else:\\n            raise ValueError(f\"Unsupported space ({obj[\\'spaces\\']})\")\\n    # Python data types\\n    elif obj[\"type\"] == \"python\":\\n        if obj[\"space\"] == \"Discrete\":\\n            return {obj[\"value\"]}\\n        elif obj[\"space\"] == \"Box\":\\n            return obj[\"value\"]\\n        elif obj[\"space\"] == \"MultiDiscrete\":\\n            return [{x} for x in obj[\"value\"]]\\n        elif obj[\"space\"] == \"Tuple\":\\n            return tuple(map(deserialize_space, obj[\"value\"]))\\n        elif obj[\"space\"] == \"Dict\":\\n            return {k: deserialize_space(v) for k, v in obj[\"value\"].items()}\\n        else:\\n            raise ValueError(f\"Unsupported space ({obj[\\'spaces\\']})\")\\n    else:\\n        raise ValueError(f\"Unsupported type ({obj[\\'type\\']})\")'),\n",
       " Document(metadata={}, page_content='def replace_env_cfg_spaces_with_strings(env_cfg: object) -> object:\\n    \"\"\"Replace spaces objects with their serialized JSON representations in an environment config.\\n\\n    Args:\\n        env_cfg: Environment config instance.\\n\\n    Returns:\\n        Environment config instance with spaces replaced if any.\\n    \"\"\"\\n    for attr in [\"observation_space\", \"action_space\", \"state_space\"]:\\n        if hasattr(env_cfg, attr):\\n            setattr(env_cfg, attr, serialize_space(getattr(env_cfg, attr)))\\n    for attr in [\"observation_spaces\", \"action_spaces\"]:\\n        if hasattr(env_cfg, attr):\\n            setattr(env_cfg, attr, {k: serialize_space(v) for k, v in getattr(env_cfg, attr).items()})\\n    return env_cfg'),\n",
       " Document(metadata={}, page_content='def replace_strings_with_env_cfg_spaces(env_cfg: object) -> object:\\n    \"\"\"Replace spaces objects with their serialized JSON representations in an environment config.\\n\\n    Args:\\n        env_cfg: Environment config instance.\\n\\n    Returns:\\n        Environment config instance with spaces replaced if any.\\n    \"\"\"\\n    for attr in [\"observation_space\", \"action_space\", \"state_space\"]:\\n        if hasattr(env_cfg, attr):\\n            setattr(env_cfg, attr, deserialize_space(getattr(env_cfg, attr)))\\n    for attr in [\"observation_spaces\", \"action_spaces\"]:\\n        if hasattr(env_cfg, attr):\\n            setattr(env_cfg, attr, {k: deserialize_space(v) for k, v in getattr(env_cfg, attr).items()})\\n    return env_cfg'),\n",
       " Document(metadata={}, page_content='class ActionTerm(ManagerTermBase):\\n    \"\"\"Base class for action terms.\\n\\n    The action term is responsible for processing the raw actions sent to the environment\\n    and applying them to the asset managed by the term. The action term is comprised of two\\n    operations:\\n\\n    * Processing of actions: This operation is performed once per **environment step** and\\n      is responsible for pre-processing the raw actions sent to the environment.\\n    * Applying actions: This operation is performed once per **simulation step** and is\\n      responsible for applying the processed actions to the asset managed by the term.\\n    \"\"\"\\n\\n    def __init__(self, cfg: ActionTermCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the action term.\\n\\n        Args:\\n            cfg: The configuration object.\\n            env: The environment instance.\\n        \"\"\"\\n        # call the base class constructor\\n        super().__init__(cfg, env)\\n        # parse config to obtain asset to which the term is applied\\n        self._asset: AssetBase = self._env.scene[self.cfg.asset_name]\\n\\n        # add handle for debug visualization (this is set to a valid handle inside set_debug_vis)\\n        self._debug_vis_handle = None\\n        # set initial state of debug visualization\\n        self.set_debug_vis(self.cfg.debug_vis)\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribe from the callbacks.\"\"\"\\n        if self._debug_vis_handle:\\n            self._debug_vis_handle.unsubscribe()\\n            self._debug_vis_handle = None\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    @abstractmethod\\n    def action_dim(self) -> int:\\n        \"\"\"Dimension of the action term.\"\"\"\\n        raise NotImplementedError\\n\\n    @property\\n    @abstractmethod\\n    def raw_actions(self) -> torch.Tensor:\\n        \"\"\"The input/raw actions sent to the term.\"\"\"\\n        raise NotImplementedError\\n\\n    @property\\n    @abstractmethod\\n    def processed_actions(self) -> torch.Tensor:\\n        \"\"\"The actions computed by the term after applying any processing.\"\"\"\\n        raise NotImplementedError\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the action term has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_debug_vis_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def set_debug_vis(self, debug_vis: bool) -> bool:\\n        \"\"\"Sets whether to visualize the action term data.\\n        Args:\\n            debug_vis: Whether to visualize the action term data.\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the action term does\\n            not support debug visualization.\\n        \"\"\"\\n        # check if debug visualization is supported\\n        if not self.has_debug_vis_implementation:\\n            return False\\n\\n        # toggle debug visualization objects\\n        self._set_debug_vis_impl(debug_vis)\\n        # toggle debug visualization handles\\n        if debug_vis:\\n            # create a subscriber for the post update event if it doesn\\'t exist\\n            if self._debug_vis_handle is None:\\n                app_interface = omni.kit.app.get_app_interface()\\n                self._debug_vis_handle = app_interface.get_post_update_event_stream().create_subscription_to_pop(\\n                    lambda event, obj=weakref.proxy(self): obj._debug_vis_callback(event)\\n                )\\n        else:\\n            # remove the subscriber if it exists\\n            if self._debug_vis_handle is not None:\\n                self._debug_vis_handle.unsubscribe()\\n                self._debug_vis_handle = None\\n        # return success\\n        return True\\n\\n    @abstractmethod\\n    def process_actions(self, actions: torch.Tensor):\\n        \"\"\"Processes the actions sent to the environment.\\n\\n        Note:\\n            This function is called once per environment step by the manager.\\n\\n        Args:\\n            actions: The actions to process.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def apply_actions(self):\\n        \"\"\"Applies the actions to the asset managed by the term.\\n\\n        Note:\\n            This is called at every simulation step by the manager.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set debug visualization into visualization objects.\\n        This function is responsible for creating the visualization objects if they don\\'t exist\\n        and input ``debug_vis`` is True. If the visualization objects exist, the function should\\n        set their visibility into the stage.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")\\n\\n    def _debug_vis_callback(self, event):\\n        \"\"\"Callback for debug visualization.\\n        This function calls the visualization objects and sets the data to visualize into them.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")'),\n",
       " Document(metadata={}, page_content='class ActionManager(ManagerBase):\\n    \"\"\"Manager for processing and applying actions for a given world.\\n\\n    The action manager handles the interpretation and application of user-defined\\n    actions on a given world. It is comprised of different action terms that decide\\n    the dimension of the expected actions.\\n\\n    The action manager performs operations at two stages:\\n\\n    * processing of actions: It splits the input actions to each term and performs any\\n      pre-processing needed. This should be called once at every environment step.\\n    * apply actions: This operation typically sets the processed actions into the assets in the\\n      scene (such as robots). It should be called before every simulation step.\\n    \"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedEnv):\\n        \"\"\"Initialize the action manager.\\n\\n        Args:\\n            cfg: The configuration object or dictionary (``dict[str, ActionTermCfg]``).\\n            env: The environment instance.\\n\\n        Raises:\\n            ValueError: If the configuration is None.\\n        \"\"\"\\n        # check if config is None\\n        if cfg is None:\\n            raise ValueError(\"Action manager configuration is None. Please provide a valid configuration.\")\\n\\n        # call the base class constructor (this prepares the terms)\\n        super().__init__(cfg, env)\\n        # create buffers to store actions\\n        self._action = torch.zeros((self.num_envs, self.total_action_dim), device=self.device)\\n        self._prev_action = torch.zeros_like(self._action)\\n\\n        # check if any term has debug visualization implemented\\n        self.cfg.debug_vis = False\\n        for term in self._terms.values():\\n            self.cfg.debug_vis |= term.cfg.debug_vis\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for action manager.\"\"\"\\n        msg = f\"<ActionManager> contains {len(self._term_names)} active terms.\\\\n\"\\n\\n        # create table for term information\\n        table = PrettyTable()\\n        table.title = f\"Active Action Terms (shape: {self.total_action_dim})\"\\n        table.field_names = [\"Index\", \"Name\", \"Dimension\"]\\n        # set alignment of table columns\\n        table.align[\"Name\"] = \"l\"\\n        table.align[\"Dimension\"] = \"r\"\\n        # add info on each term\\n        for index, (name, term) in enumerate(self._terms.items()):\\n            table.add_row([index, name, term.action_dim])\\n        # convert table to string\\n        msg += table.get_string()\\n        msg += \"\\\\n\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def total_action_dim(self) -> int:\\n        \"\"\"Total dimension of actions.\"\"\"\\n        return sum(self.action_term_dim)\\n\\n    @property\\n    def active_terms(self) -> list[str]:\\n        \"\"\"Name of active action terms.\"\"\"\\n        return self._term_names\\n\\n    @property\\n    def action_term_dim(self) -> list[int]:\\n        \"\"\"Shape of each action term.\"\"\"\\n        return [term.action_dim for term in self._terms.values()]\\n\\n    @property\\n    def action(self) -> torch.Tensor:\\n        \"\"\"The actions sent to the environment. Shape is (num_envs, total_action_dim).\"\"\"\\n        return self._action\\n\\n    @property\\n    def prev_action(self) -> torch.Tensor:\\n        \"\"\"The previous actions sent to the environment. Shape is (num_envs, total_action_dim).\"\"\"\\n        return self._prev_action\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the command terms have debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        has_debug_vis = False\\n        for term in self._terms.values():\\n            has_debug_vis |= term.has_debug_vis_implementation\\n        return has_debug_vis\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def get_active_iterable_terms(self, env_idx: int) -> Sequence[tuple[str, Sequence[float]]]:\\n        \"\"\"Returns the active terms as iterable sequence of tuples.\\n\\n        The first element of the tuple is the name of the term and the second element is the raw value(s) of the term.\\n\\n        Args:\\n            env_idx: The specific environment to pull the active terms from.\\n\\n        Returns:\\n            The active terms.\\n        \"\"\"\\n        terms = []\\n        idx = 0\\n        for name, term in self._terms.items():\\n            term_actions = self._action[env_idx, idx : idx + term.action_dim].cpu()\\n            terms.append((name, term_actions.tolist()))\\n            idx += term.action_dim\\n        return terms\\n\\n    def set_debug_vis(self, debug_vis: bool):\\n        \"\"\"Sets whether to visualize the action data.\\n        Args:\\n            debug_vis: Whether to visualize the action data.\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the action\\n            does not support debug visualization.\\n        \"\"\"\\n        for term in self._terms.values():\\n            term.set_debug_vis(debug_vis)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, torch.Tensor]:\\n        \"\"\"Resets the action history.\\n\\n        Args:\\n            env_ids: The environment ids. Defaults to None, in which case\\n                all environments are considered.\\n\\n        Returns:\\n            An empty dictionary.\\n        \"\"\"\\n        # resolve environment ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset the action history\\n        self._prev_action[env_ids] = 0.0\\n        self._action[env_ids] = 0.0\\n        # reset all action terms\\n        for term in self._terms.values():\\n            term.reset(env_ids=env_ids)\\n        # nothing to log here\\n        return {}\\n\\n    def process_action(self, action: torch.Tensor):\\n        \"\"\"Processes the actions sent to the environment.\\n\\n        Note:\\n            This function should be called once per environment step.\\n\\n        Args:\\n            action: The actions to process.\\n        \"\"\"\\n        # check if action dimension is valid\\n        if self.total_action_dim != action.shape[1]:\\n            raise ValueError(f\"Invalid action shape, expected: {self.total_action_dim}, received: {action.shape[1]}.\")\\n        # store the input actions\\n        self._prev_action[:] = self._action\\n        self._action[:] = action.to(self.device)\\n\\n        # split the actions and apply to each tensor\\n        idx = 0\\n        for term in self._terms.values():\\n            term_actions = action[:, idx : idx + term.action_dim]\\n            term.process_actions(term_actions)\\n            idx += term.action_dim\\n\\n    def apply_action(self) -> None:\\n        \"\"\"Applies the actions to the environment/simulation.\\n\\n        Note:\\n            This should be called at every simulation step.\\n        \"\"\"\\n        for term in self._terms.values():\\n            term.apply_actions()\\n\\n    def get_term(self, name: str) -> ActionTerm:\\n        \"\"\"Returns the action term with the specified name.\\n\\n        Args:\\n            name: The name of the action term.\\n\\n        Returns:\\n            The action term with the specified name.\\n        \"\"\"\\n        return self._terms[name]\\n\\n    def serialize(self) -> dict:\\n        \"\"\"Serialize the action manager configuration.\\n\\n        Returns:\\n            A dictionary of serialized action term configurations.\\n        \"\"\"\\n        return {term_name: term.serialize() for term_name, term in self._terms.items()}\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        # create buffers to parse and store terms\\n        self._term_names: list[str] = list()\\n        self._terms: dict[str, ActionTerm] = dict()\\n\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n        # parse action terms from the config\\n        for term_name, term_cfg in cfg_items:\\n            # check if term config is None\\n            if term_cfg is None:\\n                continue\\n            # check valid type\\n            if not isinstance(term_cfg, ActionTermCfg):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type ActionTermCfg.\"\\n                    f\" Received: \\'{type(term_cfg)}\\'.\"\\n                )\\n            # create the action term\\n            term = term_cfg.class_type(term_cfg, self._env)\\n            # sanity check if term is valid type\\n            if not isinstance(term, ActionTerm):\\n                raise TypeError(f\"Returned object for the term \\'{term_name}\\' is not of type ActionType.\")\\n            # add term name and parameters\\n            self._term_names.append(term_name)\\n            self._terms[term_name] = term'),\n",
       " Document(metadata={}, page_content='class CommandTerm(ManagerTermBase):\\n    \"\"\"The base class for implementing a command term.\\n\\n    A command term is used to generate commands for goal-conditioned tasks. For example,\\n    in the case of a goal-conditioned navigation task, the command term can be used to\\n    generate a target position for the robot to navigate to.\\n\\n    It implements a resampling mechanism that allows the command to be resampled at a fixed\\n    frequency. The resampling frequency can be specified in the configuration object.\\n    Additionally, it is possible to assign a visualization function to the command term\\n    that can be used to visualize the command in the simulator.\\n    \"\"\"\\n\\n    def __init__(self, cfg: CommandTermCfg, env: ManagerBasedRLEnv):\\n        \"\"\"Initialize the command generator class.\\n\\n        Args:\\n            cfg: The configuration parameters for the command generator.\\n            env: The environment object.\\n        \"\"\"\\n        super().__init__(cfg, env)\\n\\n        # create buffers to store the command\\n        # -- metrics that can be used for logging\\n        self.metrics = dict()\\n        # -- time left before resampling\\n        self.time_left = torch.zeros(self.num_envs, device=self.device)\\n        # -- counter for the number of times the command has been resampled within the current episode\\n        self.command_counter = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\\n\\n        # add handle for debug visualization (this is set to a valid handle inside set_debug_vis)\\n        self._debug_vis_handle = None\\n        # set initial state of debug visualization\\n        self.set_debug_vis(self.cfg.debug_vis)\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribe from the callbacks.\"\"\"\\n        if self._debug_vis_handle:\\n            self._debug_vis_handle.unsubscribe()\\n            self._debug_vis_handle = None\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    @abstractmethod\\n    def command(self) -> torch.Tensor:\\n        \"\"\"The command tensor. Shape is (num_envs, command_dim).\"\"\"\\n        raise NotImplementedError\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the command generator has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_debug_vis_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def set_debug_vis(self, debug_vis: bool) -> bool:\\n        \"\"\"Sets whether to visualize the command data.\\n\\n        Args:\\n            debug_vis: Whether to visualize the command data.\\n\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the command\\n            generator does not support debug visualization.\\n        \"\"\"\\n        # check if debug visualization is supported\\n        if not self.has_debug_vis_implementation:\\n            return False\\n        # toggle debug visualization objects\\n        self._set_debug_vis_impl(debug_vis)\\n        # toggle debug visualization handles\\n        if debug_vis:\\n            # create a subscriber for the post update event if it doesn\\'t exist\\n            if self._debug_vis_handle is None:\\n                app_interface = omni.kit.app.get_app_interface()\\n                self._debug_vis_handle = app_interface.get_post_update_event_stream().create_subscription_to_pop(\\n                    lambda event, obj=weakref.proxy(self): obj._debug_vis_callback(event)\\n                )\\n        else:\\n            # remove the subscriber if it exists\\n            if self._debug_vis_handle is not None:\\n                self._debug_vis_handle.unsubscribe()\\n                self._debug_vis_handle = None\\n        # return success\\n        return True\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:\\n        \"\"\"Reset the command generator and log metrics.\\n\\n        This function resets the command counter and resamples the command. It should be called\\n        at the beginning of each episode.\\n\\n        Args:\\n            env_ids: The list of environment IDs to reset. Defaults to None.\\n\\n        Returns:\\n            A dictionary containing the information to log under the \"{name}\" key.\\n        \"\"\"\\n        # resolve the environment IDs\\n        if env_ids is None:\\n            env_ids = slice(None)\\n\\n        # add logging metrics\\n        extras = {}\\n        for metric_name, metric_value in self.metrics.items():\\n            # compute the mean metric value\\n            extras[metric_name] = torch.mean(metric_value[env_ids]).item()\\n            # reset the metric value\\n            metric_value[env_ids] = 0.0\\n\\n        # set the command counter to zero\\n        self.command_counter[env_ids] = 0\\n        # resample the command\\n        self._resample(env_ids)\\n\\n        return extras\\n\\n    def compute(self, dt: float):\\n        \"\"\"Compute the command.\\n\\n        Args:\\n            dt: The time step passed since the last call to compute.\\n        \"\"\"\\n        # update the metrics based on current state\\n        self._update_metrics()\\n        # reduce the time left before resampling\\n        self.time_left -= dt\\n        # resample the command if necessary\\n        resample_env_ids = (self.time_left <= 0.0).nonzero().flatten()\\n        if len(resample_env_ids) > 0:\\n            self._resample(resample_env_ids)\\n        # update the command\\n        self._update_command()\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _resample(self, env_ids: Sequence[int]):\\n        \"\"\"Resample the command.\\n\\n        This function resamples the command and time for which the command is applied for the\\n        specified environment indices.\\n\\n        Args:\\n            env_ids: The list of environment IDs to resample.\\n        \"\"\"\\n        if len(env_ids) != 0:\\n            # resample the time left before resampling\\n            self.time_left[env_ids] = self.time_left[env_ids].uniform_(*self.cfg.resampling_time_range)\\n            # resample the command\\n            self._resample_command(env_ids)\\n            # increment the command counter\\n            self.command_counter[env_ids] += 1\\n\\n    \"\"\"\\n    Implementation specific functions.\\n    \"\"\"\\n\\n    @abstractmethod\\n    def _update_metrics(self):\\n        \"\"\"Update the metrics based on the current state.\"\"\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def _resample_command(self, env_ids: Sequence[int]):\\n        \"\"\"Resample the command for the specified environments.\"\"\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def _update_command(self):\\n        \"\"\"Update the command based on the current state.\"\"\"\\n        raise NotImplementedError\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set debug visualization into visualization objects.\\n\\n        This function is responsible for creating the visualization objects if they don\\'t exist\\n        and input ``debug_vis`` is True. If the visualization objects exist, the function should\\n        set their visibility into the stage.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")\\n\\n    def _debug_vis_callback(self, event):\\n        \"\"\"Callback for debug visualization.\\n\\n        This function calls the visualization objects and sets the data to visualize into them.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")'),\n",
       " Document(metadata={}, page_content='class CommandManager(ManagerBase):\\n    \"\"\"Manager for generating commands.\\n\\n    The command manager is used to generate commands for an agent to execute. It makes it convenient to switch\\n    between different command generation strategies within the same environment. For instance, in an environment\\n    consisting of a quadrupedal robot, the command to it could be a velocity command or position command.\\n    By keeping the command generation logic separate from the environment, it is easy to switch between different\\n    command generation strategies.\\n\\n    The command terms are implemented as classes that inherit from the :class:`CommandTerm` class.\\n    Each command generator term should also have a corresponding configuration class that inherits from the\\n    :class:`CommandTermCfg` class.\\n    \"\"\"\\n\\n    _env: ManagerBasedRLEnv\\n    \"\"\"The environment instance.\"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedRLEnv):\\n        \"\"\"Initialize the command manager.\\n\\n        Args:\\n            cfg: The configuration object or dictionary (``dict[str, CommandTermCfg]``).\\n            env: The environment instance.\\n        \"\"\"\\n        # create buffers to parse and store terms\\n        self._terms: dict[str, CommandTerm] = dict()\\n\\n        # call the base class constructor (this prepares the terms)\\n        super().__init__(cfg, env)\\n        # store the commands\\n        self._commands = dict()\\n        if self.cfg:\\n            self.cfg.debug_vis = False\\n            for term in self._terms.values():\\n                self.cfg.debug_vis |= term.cfg.debug_vis\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for the command manager.\"\"\"\\n        msg = f\"<CommandManager> contains {len(self._terms.values())} active terms.\\\\n\"\\n\\n        # create table for term information\\n        table = PrettyTable()\\n        table.title = \"Active Command Terms\"\\n        table.field_names = [\"Index\", \"Name\", \"Type\"]\\n        # set alignment of table columns\\n        table.align[\"Name\"] = \"l\"\\n        # add info on each term\\n        for index, (name, term) in enumerate(self._terms.items()):\\n            table.add_row([index, name, term.__class__.__name__])\\n        # convert table to string\\n        msg += table.get_string()\\n        msg += \"\\\\n\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def active_terms(self) -> list[str]:\\n        \"\"\"Name of active command terms.\"\"\"\\n        return list(self._terms.keys())\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the command terms have debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        has_debug_vis = False\\n        for term in self._terms.values():\\n            has_debug_vis |= term.has_debug_vis_implementation\\n        return has_debug_vis\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def get_active_iterable_terms(self, env_idx: int) -> Sequence[tuple[str, Sequence[float]]]:\\n        \"\"\"Returns the active terms as iterable sequence of tuples.\\n\\n        The first element of the tuple is the name of the term and the second element is the raw value(s) of the term.\\n\\n        Args:\\n            env_idx: The specific environment to pull the active terms from.\\n\\n        Returns:\\n            The active terms.\\n        \"\"\"\\n\\n        terms = []\\n        idx = 0\\n        for name, term in self._terms.items():\\n            terms.append((name, term.command[env_idx].cpu().tolist()))\\n            idx += term.command.shape[1]\\n        return terms\\n\\n    def set_debug_vis(self, debug_vis: bool):\\n        \"\"\"Sets whether to visualize the command data.\\n\\n        Args:\\n            debug_vis: Whether to visualize the command data.\\n\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the command\\n            generator does not support debug visualization.\\n        \"\"\"\\n        for term in self._terms.values():\\n            term.set_debug_vis(debug_vis)\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, torch.Tensor]:\\n        \"\"\"Reset the command terms and log their metrics.\\n\\n        This function resets the command counter and resamples the command for each term. It should be called\\n        at the beginning of each episode.\\n\\n        Args:\\n            env_ids: The list of environment IDs to reset. Defaults to None.\\n\\n        Returns:\\n            A dictionary containing the information to log under the \"Metrics/{term_name}/{metric_name}\" key.\\n        \"\"\"\\n        # resolve environment ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # store information\\n        extras = {}\\n        for name, term in self._terms.items():\\n            # reset the command term\\n            metrics = term.reset(env_ids=env_ids)\\n            # compute the mean metric value\\n            for metric_name, metric_value in metrics.items():\\n                extras[f\"Metrics/{name}/{metric_name}\"] = metric_value\\n        # return logged information\\n        return extras\\n\\n    def compute(self, dt: float):\\n        \"\"\"Updates the commands.\\n\\n        This function calls each command term managed by the class.\\n\\n        Args:\\n            dt: The time-step interval of the environment.\\n\\n        \"\"\"\\n        # iterate over all the command terms\\n        for term in self._terms.values():\\n            # compute term\\'s value\\n            term.compute(dt)\\n\\n    def get_command(self, name: str) -> torch.Tensor:\\n        \"\"\"Returns the command for the specified command term.\\n\\n        Args:\\n            name: The name of the command term.\\n\\n        Returns:\\n            The command tensor of the specified command term.\\n        \"\"\"\\n        return self._terms[name].command\\n\\n    def get_term(self, name: str) -> CommandTerm:\\n        \"\"\"Returns the command term with the specified name.\\n\\n        Args:\\n            name: The name of the command term.\\n\\n        Returns:\\n            The command term with the specified name.\\n        \"\"\"\\n        return self._terms[name]\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n        # iterate over all the terms\\n        for term_name, term_cfg in cfg_items:\\n            # check for non config\\n            if term_cfg is None:\\n                continue\\n            # check for valid config type\\n            if not isinstance(term_cfg, CommandTermCfg):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type CommandTermCfg.\"\\n                    f\" Received: \\'{type(term_cfg)}\\'.\"\\n                )\\n            # create the action term\\n            term = term_cfg.class_type(term_cfg, self._env)\\n            # sanity check if term is valid type\\n            if not isinstance(term, CommandTerm):\\n                raise TypeError(f\"Returned object for the term \\'{term_name}\\' is not of type CommandType.\")\\n            # add class to dict\\n            self._terms[term_name] = term'),\n",
       " Document(metadata={}, page_content='class CurriculumManager(ManagerBase):\\n    \"\"\"Manager to implement and execute specific curricula.\\n\\n    The curriculum manager updates various quantities of the environment subject to a training curriculum by\\n    calling a list of terms. These help stabilize learning by progressively making the learning tasks harder\\n    as the agent improves.\\n\\n    The curriculum terms are parsed from a config class containing the manager\\'s settings and each term\\'s\\n    parameters. Each curriculum term should instantiate the :class:`CurriculumTermCfg` class.\\n    \"\"\"\\n\\n    _env: ManagerBasedRLEnv\\n    \"\"\"The environment instance.\"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedRLEnv):\\n        \"\"\"Initialize the manager.\\n\\n        Args:\\n            cfg: The configuration object or dictionary (``dict[str, CurriculumTermCfg]``)\\n            env: An environment object.\\n\\n        Raises:\\n            TypeError: If curriculum term is not of type :class:`CurriculumTermCfg`.\\n            ValueError: If curriculum term configuration does not satisfy its function signature.\\n        \"\"\"\\n        # create buffers to parse and store terms\\n        self._term_names: list[str] = list()\\n        self._term_cfgs: list[CurriculumTermCfg] = list()\\n        self._class_term_cfgs: list[CurriculumTermCfg] = list()\\n\\n        # call the base class constructor (this will parse the terms config)\\n        super().__init__(cfg, env)\\n\\n        # prepare logging\\n        self._curriculum_state = dict()\\n        for term_name in self._term_names:\\n            self._curriculum_state[term_name] = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for curriculum manager.\"\"\"\\n        msg = f\"<CurriculumManager> contains {len(self._term_names)} active terms.\\\\n\"\\n\\n        # create table for term information\\n        table = PrettyTable()\\n        table.title = \"Active Curriculum Terms\"\\n        table.field_names = [\"Index\", \"Name\"]\\n        # set alignment of table columns\\n        table.align[\"Name\"] = \"l\"\\n        # add info on each term\\n        for index, name in enumerate(self._term_names):\\n            table.add_row([index, name])\\n        # convert table to string\\n        msg += table.get_string()\\n        msg += \"\\\\n\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def active_terms(self) -> list[str]:\\n        \"\"\"Name of active curriculum terms.\"\"\"\\n        return self._term_names\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:\\n        \"\"\"Returns the current state of individual curriculum terms.\\n\\n        Note:\\n            This function does not use the environment indices :attr:`env_ids`\\n            and logs the state of all the terms. The argument is only present\\n            to maintain consistency with other classes.\\n\\n        Returns:\\n            Dictionary of curriculum terms and their states.\\n        \"\"\"\\n        extras = {}\\n        for term_name, term_state in self._curriculum_state.items():\\n            if term_state is not None:\\n                # deal with dict\\n                if isinstance(term_state, dict):\\n                    # each key is a separate state to log\\n                    for key, value in term_state.items():\\n                        if isinstance(value, torch.Tensor):\\n                            value = value.item()\\n                        extras[f\"Curriculum/{term_name}/{key}\"] = value\\n                else:\\n                    # log directly if not a dict\\n                    if isinstance(term_state, torch.Tensor):\\n                        term_state = term_state.item()\\n                    extras[f\"Curriculum/{term_name}\"] = term_state\\n        # reset all the curriculum terms\\n        for term_cfg in self._class_term_cfgs:\\n            term_cfg.func.reset(env_ids=env_ids)\\n        # return logged information\\n        return extras\\n\\n    def compute(self, env_ids: Sequence[int] | None = None):\\n        \"\"\"Update the curriculum terms.\\n\\n        This function calls each curriculum term managed by the class.\\n\\n        Args:\\n            env_ids: The list of environment IDs to update.\\n                If None, all the environments are updated. Defaults to None.\\n        \"\"\"\\n        # resolve environment indices\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # iterate over all the curriculum terms\\n        for name, term_cfg in zip(self._term_names, self._term_cfgs):\\n            state = term_cfg.func(self._env, env_ids, **term_cfg.params)\\n            self._curriculum_state[name] = state\\n\\n    def get_active_iterable_terms(self, env_idx: int) -> Sequence[tuple[str, Sequence[float]]]:\\n        \"\"\"Returns the active terms as iterable sequence of tuples.\\n\\n        The first element of the tuple is the name of the term and the second element is the raw value(s) of the term.\\n\\n        Args:\\n            env_idx: The specific environment to pull the active terms from.\\n\\n        Returns:\\n            The active terms.\\n        \"\"\"\\n\\n        terms = []\\n\\n        for term_name, term_state in self._curriculum_state.items():\\n            if term_state is not None:\\n                # deal with dict\\n                data = []\\n\\n                if isinstance(term_state, dict):\\n                    # each key is a separate state to log\\n                    for key, value in term_state.items():\\n                        if isinstance(value, torch.Tensor):\\n                            value = value.item()\\n                        terms[term_name].append(value)\\n                else:\\n                    # log directly if not a dict\\n                    if isinstance(term_state, torch.Tensor):\\n                        term_state = term_state.item()\\n                    data.append(term_state)\\n                terms.append((term_name, data))\\n\\n        return terms\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n        # iterate over all the terms\\n        for term_name, term_cfg in cfg_items:\\n            # check for non config\\n            if term_cfg is None:\\n                continue\\n            # check if the term is a valid term config\\n            if not isinstance(term_cfg, CurriculumTermCfg):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type CurriculumTermCfg.\"\\n                    f\" Received: \\'{type(term_cfg)}\\'.\"\\n                )\\n            # resolve common parameters\\n            self._resolve_common_term_cfg(term_name, term_cfg, min_argc=2)\\n            # add name and config to list\\n            self._term_names.append(term_name)\\n            self._term_cfgs.append(term_cfg)\\n            # check if the term is a class\\n            if isinstance(term_cfg.func, ManagerTermBase):\\n                self._class_term_cfgs.append(term_cfg)'),\n",
       " Document(metadata={}, page_content='class EventManager(ManagerBase):\\n    \"\"\"Manager for orchestrating operations based on different simulation events.\\n\\n    The event manager applies operations to the environment based on different simulation events. For example,\\n    changing the masses of objects or their friction coefficients during initialization/ reset, or applying random\\n    pushes to the robot at a fixed interval of steps. The user can specify several modes of events to fine-tune the\\n    behavior based on when to apply the event.\\n\\n    The event terms are parsed from a config class containing the manager\\'s settings and each term\\'s\\n    parameters. Each event term should instantiate the :class:`EventTermCfg` class.\\n\\n    Event terms can be grouped by their mode. The mode is a user-defined string that specifies when\\n    the event term should be applied. This provides the user complete control over when event\\n    terms should be applied.\\n\\n    For a typical training process, you may want to apply events in the following modes:\\n\\n    - \"prestartup\": Event is applied once at the beginning of the training before the simulation starts.\\n      This is used to randomize USD-level properties of the simulation stage.\\n    - \"startup\": Event is applied once at the beginning of the training once simulation is started.\\n    - \"reset\": Event is applied at every reset.\\n    - \"interval\": Event is applied at pre-specified intervals of time.\\n\\n    However, you can also define your own modes and use them in the training process as you see fit.\\n    For this you will need to add the triggering of that mode in the environment implementation as well.\\n\\n    .. note::\\n\\n        The triggering of operations corresponding to the mode ``\"interval\"`` are the only mode that are\\n        directly handled by the manager itself. The other modes are handled by the environment implementation.\\n\\n    \"\"\"\\n\\n    _env: ManagerBasedEnv\\n    \"\"\"The environment instance.\"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedEnv):\\n        \"\"\"Initialize the event manager.\\n\\n        Args:\\n            cfg: A configuration object or dictionary (``dict[str, EventTermCfg]``).\\n            env: An environment object.\\n        \"\"\"\\n        # create buffers to parse and store terms\\n        self._mode_term_names: dict[str, list[str]] = dict()\\n        self._mode_term_cfgs: dict[str, list[EventTermCfg]] = dict()\\n        self._mode_class_term_cfgs: dict[str, list[EventTermCfg]] = dict()\\n\\n        # call the base class (this will parse the terms config)\\n        super().__init__(cfg, env)\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for event manager.\"\"\"\\n        msg = f\"<EventManager> contains {len(self._mode_term_names)} active terms.\\\\n\"\\n\\n        # add info on each mode\\n        for mode in self._mode_term_names:\\n            # create table for term information\\n            table = PrettyTable()\\n            table.title = f\"Active Event Terms in Mode: \\'{mode}\\'\"\\n            # add table headers based on mode\\n            if mode == \"interval\":\\n                table.field_names = [\"Index\", \"Name\", \"Interval time range (s)\"]\\n                table.align[\"Name\"] = \"l\"\\n                for index, (name, cfg) in enumerate(zip(self._mode_term_names[mode], self._mode_term_cfgs[mode])):\\n                    table.add_row([index, name, cfg.interval_range_s])\\n            else:\\n                table.field_names = [\"Index\", \"Name\"]\\n                table.align[\"Name\"] = \"l\"\\n                for index, name in enumerate(self._mode_term_names[mode]):\\n                    table.add_row([index, name])\\n            # convert table to string\\n            msg += table.get_string()\\n            msg += \"\\\\n\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def active_terms(self) -> dict[str, list[str]]:\\n        \"\"\"Name of active event terms.\\n\\n        The keys are the modes of event and the values are the names of the event terms.\\n        \"\"\"\\n        return self._mode_term_names\\n\\n    @property\\n    def available_modes(self) -> list[str]:\\n        \"\"\"Modes of events.\"\"\"\\n        return list(self._mode_term_names.keys())\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:\\n        # call all terms that are classes\\n        for mode_cfg in self._mode_class_term_cfgs.values():\\n            for term_cfg in mode_cfg:\\n                term_cfg.func.reset(env_ids=env_ids)\\n\\n        # resolve number of environments\\n        if env_ids is None:\\n            num_envs = self._env.num_envs\\n        else:\\n            num_envs = len(env_ids)\\n        # if we are doing interval based events then we need to reset the time left\\n        # when the episode starts. otherwise the counter will start from the last time\\n        # for that environment\\n        if \"interval\" in self._mode_term_cfgs:\\n            for index, term_cfg in enumerate(self._mode_class_term_cfgs[\"interval\"]):\\n                # sample a new interval and set that as time left\\n                # note: global time events are based on simulation time and not episode time\\n                #   so we do not reset them\\n                if not term_cfg.is_global_time:\\n                    lower, upper = term_cfg.interval_range_s\\n                    sampled_interval = torch.rand(num_envs, device=self.device) * (upper - lower) + lower\\n                    self._interval_term_time_left[index][env_ids] = sampled_interval\\n\\n        # nothing to log here\\n        return {}\\n\\n    def apply(\\n        self,\\n        mode: str,\\n        env_ids: Sequence[int] | None = None,\\n        dt: float | None = None,\\n        global_env_step_count: int | None = None,\\n    ):\\n        \"\"\"Calls each event term in the specified mode.\\n\\n        This function iterates over all the event terms in the specified mode and calls the function\\n        corresponding to the term. The function is called with the environment instance and the environment\\n        indices to apply the event to.\\n\\n        For the \"interval\" mode, the function is called when the time interval has passed. This requires\\n        specifying the time step of the environment.\\n\\n        For the \"reset\" mode, the function is called when the mode is \"reset\" and the total number of environment\\n        steps that have happened since the last trigger of the function is equal to its configured parameter for\\n        the number of environment steps between resets.\\n\\n        Args:\\n            mode: The mode of event.\\n            env_ids: The indices of the environments to apply the event to.\\n                Defaults to None, in which case the event is applied to all environments when applicable.\\n            dt: The time step of the environment. This is only used for the \"interval\" mode.\\n                Defaults to None to simplify the call for other modes.\\n            global_env_step_count: The total number of environment steps that have happened. This is only used\\n                for the \"reset\" mode. Defaults to None to simplify the call for other modes.\\n\\n        Raises:\\n            ValueError: If the mode is ``\"interval\"`` and the time step is not provided.\\n            ValueError: If the mode is ``\"interval\"`` and the environment indices are provided. This is an undefined\\n                behavior as the environment indices are computed based on the time left for each environment.\\n            ValueError: If the mode is ``\"reset\"`` and the total number of environment steps that have happened\\n                is not provided.\\n        \"\"\"\\n        # check if mode is valid\\n        if mode not in self._mode_term_names:\\n            omni.log.warn(f\"Event mode \\'{mode}\\' is not defined. Skipping event.\")\\n            return\\n\\n        # check if mode is interval and dt is not provided\\n        if mode == \"interval\" and dt is None:\\n            raise ValueError(f\"Event mode \\'{mode}\\' requires the time-step of the environment.\")\\n        if mode == \"interval\" and env_ids is not None:\\n            raise ValueError(\\n                f\"Event mode \\'{mode}\\' does not require environment indices. This is an undefined behavior\"\\n                \" as the environment indices are computed based on the time left for each environment.\"\\n            )\\n        # check if mode is reset and env step count is not provided\\n        if mode == \"reset\" and global_env_step_count is None:\\n            raise ValueError(f\"Event mode \\'{mode}\\' requires the total number of environment steps to be provided.\")\\n\\n        # iterate over all the event terms\\n        for index, term_cfg in enumerate(self._mode_term_cfgs[mode]):\\n            if mode == \"interval\":\\n                # extract time left for this term\\n                time_left = self._interval_term_time_left[index]\\n                # update the time left for each environment\\n                time_left -= dt\\n\\n                # check if the interval has passed and sample a new interval\\n                # note: we compare with a small value to handle floating point errors\\n                if term_cfg.is_global_time:\\n                    if time_left < 1e-6:\\n                        lower, upper = term_cfg.interval_range_s\\n                        sampled_interval = torch.rand(1) * (upper - lower) + lower\\n                        self._interval_term_time_left[index][:] = sampled_interval\\n\\n                        # call the event term (with None for env_ids)\\n                        term_cfg.func(self._env, None, **term_cfg.params)\\n                else:\\n                    valid_env_ids = (time_left < 1e-6).nonzero().flatten()\\n                    if len(valid_env_ids) > 0:\\n                        lower, upper = term_cfg.interval_range_s\\n                        sampled_time = torch.rand(len(valid_env_ids), device=self.device) * (upper - lower) + lower\\n                        self._interval_term_time_left[index][valid_env_ids] = sampled_time\\n\\n                        # call the event term\\n                        term_cfg.func(self._env, valid_env_ids, **term_cfg.params)\\n            elif mode == \"reset\":\\n                # obtain the minimum step count between resets\\n                min_step_count = term_cfg.min_step_count_between_reset\\n                # resolve the environment indices\\n                if env_ids is None:\\n                    env_ids = slice(None)\\n\\n                # We bypass the trigger mechanism if min_step_count is zero, i.e. apply term on every reset call.\\n                # This should avoid the overhead of checking the trigger condition.\\n                if min_step_count == 0:\\n                    self._reset_term_last_triggered_step_id[index][env_ids] = global_env_step_count\\n                    self._reset_term_last_triggered_once[index][env_ids] = True\\n\\n                    # call the event term with the environment indices\\n                    term_cfg.func(self._env, env_ids, **term_cfg.params)\\n                else:\\n                    # extract last reset step for this term\\n                    last_triggered_step = self._reset_term_last_triggered_step_id[index][env_ids]\\n                    triggered_at_least_once = self._reset_term_last_triggered_once[index][env_ids]\\n                    # compute the steps since last reset\\n                    steps_since_triggered = global_env_step_count - last_triggered_step\\n\\n                    # check if the term can be applied after the minimum step count between triggers has passed\\n                    valid_trigger = steps_since_triggered >= min_step_count\\n                    # check if the term has not been triggered yet (in that case, we trigger it at least once)\\n                    # this is usually only needed at the start of the environment\\n                    valid_trigger |= (last_triggered_step == 0) & ~triggered_at_least_once\\n\\n                    # select the valid environment indices based on the trigger\\n                    if env_ids == slice(None):\\n                        valid_env_ids = valid_trigger.nonzero().flatten()\\n                    else:\\n                        valid_env_ids = env_ids[valid_trigger]\\n\\n                    # reset the last reset step for each environment to the current env step count\\n                    if len(valid_env_ids) > 0:\\n                        self._reset_term_last_triggered_once[index][valid_env_ids] = True\\n                        self._reset_term_last_triggered_step_id[index][valid_env_ids] = global_env_step_count\\n\\n                        # call the event term\\n                        term_cfg.func(self._env, valid_env_ids, **term_cfg.params)\\n            else:\\n                # call the event term\\n                term_cfg.func(self._env, env_ids, **term_cfg.params)\\n\\n    \"\"\"\\n    Operations - Term settings.\\n    \"\"\"\\n\\n    def set_term_cfg(self, term_name: str, cfg: EventTermCfg):\\n        \"\"\"Sets the configuration of the specified term into the manager.\\n\\n        The method finds the term by name by searching through all the modes.\\n        It then updates the configuration of the term with the first matching name.\\n\\n        Args:\\n            term_name: The name of the event term.\\n            cfg: The configuration for the event term.\\n\\n        Raises:\\n            ValueError: If the term name is not found.\\n        \"\"\"\\n        term_found = False\\n        for mode, terms in self._mode_term_names.items():\\n            if term_name in terms:\\n                self._mode_term_cfgs[mode][terms.index(term_name)] = cfg\\n                term_found = True\\n                break\\n        if not term_found:\\n            raise ValueError(f\"Event term \\'{term_name}\\' not found.\")\\n\\n    def get_term_cfg(self, term_name: str) -> EventTermCfg:\\n        \"\"\"Gets the configuration for the specified term.\\n\\n        The method finds the term by name by searching through all the modes.\\n        It then returns the configuration of the term with the first matching name.\\n\\n        Args:\\n            term_name: The name of the event term.\\n\\n        Returns:\\n            The configuration of the event term.\\n\\n        Raises:\\n            ValueError: If the term name is not found.\\n        \"\"\"\\n        for mode, terms in self._mode_term_names.items():\\n            if term_name in terms:\\n                return self._mode_term_cfgs[mode][terms.index(term_name)]\\n        raise ValueError(f\"Event term \\'{term_name}\\' not found.\")\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        # buffer to store the time left for \"interval\" mode\\n        # if interval is global, then it is a single value, otherwise it is per environment\\n        self._interval_term_time_left: list[torch.Tensor] = list()\\n        # buffer to store the step count when the term was last triggered for each environment for \"reset\" mode\\n        self._reset_term_last_triggered_step_id: list[torch.Tensor] = list()\\n        self._reset_term_last_triggered_once: list[torch.Tensor] = list()\\n\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n        # iterate over all the terms\\n        for term_name, term_cfg in cfg_items:\\n            # check for non config\\n            if term_cfg is None:\\n                continue\\n            # check for valid config type\\n            if not isinstance(term_cfg, EventTermCfg):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type EventTermCfg.\"\\n                    f\" Received: \\'{type(term_cfg)}\\'.\"\\n                )\\n\\n            if term_cfg.mode != \"reset\" and term_cfg.min_step_count_between_reset != 0:\\n                omni.log.warn(\\n                    f\"Event term \\'{term_name}\\' has \\'min_step_count_between_reset\\' set to a non-zero value\"\\n                    \" but the mode is not \\'reset\\'. Ignoring the \\'min_step_count_between_reset\\' value.\"\\n                )\\n\\n            # resolve common parameters\\n            self._resolve_common_term_cfg(term_name, term_cfg, min_argc=2)\\n\\n            # check if mode is pre-startup and scene replication is enabled\\n            if term_cfg.mode == \"prestartup\" and self._env.scene.cfg.replicate_physics:\\n                raise RuntimeError(\\n                    \"Scene replication is enabled, which may affect USD-level randomization.\"\\n                    \" When assets are replicated, their properties are shared across instances,\"\\n                    \" potentially leading to unintended behavior.\"\\n                    \" For stable USD-level randomization, please disable scene replication\"\\n                    \" by setting \\'replicate_physics\\' to False in \\'InteractiveSceneCfg\\'.\"\\n                )\\n\\n            # for event terms with mode \"prestartup\", we assume a callable class term\\n            # can be initialized before the simulation starts.\\n            # this is done to ensure that the USD-level randomization is possible before the simulation starts.\\n            if inspect.isclass(term_cfg.func) and term_cfg.mode == \"prestartup\":\\n                omni.log.info(f\"Initializing term \\'{term_name}\\' with class \\'{term_cfg.func.__name__}\\'.\")\\n                term_cfg.func = term_cfg.func(cfg=term_cfg, env=self._env)\\n\\n            # check if mode is a new mode\\n            if term_cfg.mode not in self._mode_term_names:\\n                # add new mode\\n                self._mode_term_names[term_cfg.mode] = list()\\n                self._mode_term_cfgs[term_cfg.mode] = list()\\n                self._mode_class_term_cfgs[term_cfg.mode] = list()\\n            # add term name and parameters\\n            self._mode_term_names[term_cfg.mode].append(term_name)\\n            self._mode_term_cfgs[term_cfg.mode].append(term_cfg)\\n\\n            # check if the term is a class\\n            if inspect.isclass(term_cfg.func):\\n                self._mode_class_term_cfgs[term_cfg.mode].append(term_cfg)\\n\\n            # resolve the mode of the events\\n            # -- interval mode\\n            if term_cfg.mode == \"interval\":\\n                if term_cfg.interval_range_s is None:\\n                    raise ValueError(\\n                        f\"Event term \\'{term_name}\\' has mode \\'interval\\' but \\'interval_range_s\\' is not specified.\"\\n                    )\\n\\n                # sample the time left for global\\n                if term_cfg.is_global_time:\\n                    lower, upper = term_cfg.interval_range_s\\n                    time_left = torch.rand(1) * (upper - lower) + lower\\n                    self._interval_term_time_left.append(time_left)\\n                else:\\n                    # sample the time left for each environment\\n                    lower, upper = term_cfg.interval_range_s\\n                    time_left = torch.rand(self.num_envs, device=self.device) * (upper - lower) + lower\\n                    self._interval_term_time_left.append(time_left)\\n            # -- reset mode\\n            elif term_cfg.mode == \"reset\":\\n                if term_cfg.min_step_count_between_reset < 0:\\n                    raise ValueError(\\n                        f\"Event term \\'{term_name}\\' has mode \\'reset\\' but \\'min_step_count_between_reset\\' is\"\\n                        f\" negative: {term_cfg.min_step_count_between_reset}. Please provide a non-negative value.\"\\n                    )\\n\\n                # initialize the current step count for each environment to zero\\n                step_count = torch.zeros(self.num_envs, device=self.device, dtype=torch.int32)\\n                self._reset_term_last_triggered_step_id.append(step_count)\\n                # initialize the trigger flag for each environment to zero\\n                no_trigger = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)\\n                self._reset_term_last_triggered_once.append(no_trigger)'),\n",
       " Document(metadata={}, page_content='class ManagerTermBase(ABC):\\n    \"\"\"Base class for manager terms.\\n\\n    Manager term implementations can be functions or classes. If the term is a class, it should\\n    inherit from this base class and implement the required methods.\\n\\n    Each manager is implemented as a class that inherits from the :class:`ManagerBase` class. Each manager\\n    class should also have a corresponding configuration class that defines the configuration terms for the\\n    manager. Each term should the :class:`ManagerTermBaseCfg` class or its subclass.\\n\\n    Example pseudo-code for creating a manager:\\n\\n    .. code-block:: python\\n\\n        from isaaclab.utils import configclass\\n        from isaaclab.utils.mdp import ManagerBase, ManagerTermBaseCfg\\n\\n        @configclass\\n        class MyManagerCfg:\\n\\n            my_term_1: ManagerTermBaseCfg = ManagerTermBaseCfg(...)\\n            my_term_2: ManagerTermBaseCfg = ManagerTermBaseCfg(...)\\n            my_term_3: ManagerTermBaseCfg = ManagerTermBaseCfg(...)\\n\\n        # define manager instance\\n        my_manager = ManagerBase(cfg=ManagerCfg(), env=env)\\n\\n    \"\"\"\\n\\n    def __init__(self, cfg: ManagerTermBaseCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the manager term.\\n\\n        Args:\\n            cfg: The configuration object.\\n            env: The environment instance.\\n        \"\"\"\\n        # store the inputs\\n        self.cfg = cfg\\n        self._env = env\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_envs(self) -> int:\\n        \"\"\"Number of environments.\"\"\"\\n        return self._env.num_envs\\n\\n    @property\\n    def device(self) -> str:\\n        \"\"\"Device on which to perform computations.\"\"\"\\n        return self._env.device\\n\\n    @property\\n    def __name__(self) -> str:\\n        \"\"\"Return the name of the class or subclass.\"\"\"\\n        return self.__class__.__name__\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> None:\\n        \"\"\"Resets the manager term.\\n\\n        Args:\\n            env_ids: The environment ids. Defaults to None, in which case\\n                all environments are considered.\\n        \"\"\"\\n        pass\\n\\n    def serialize(self) -> dict:\\n        \"\"\"General serialization call. Includes the configuration dict.\"\"\"\\n        return {\"cfg\": class_to_dict(self.cfg)}\\n\\n    def __call__(self, *args) -> Any:\\n        \"\"\"Returns the value of the term required by the manager.\\n\\n        In case of a class implementation, this function is called by the manager\\n        to get the value of the term. The arguments passed to this function are\\n        the ones specified in the term configuration (see :attr:`ManagerTermBaseCfg.params`).\\n\\n        .. attention::\\n            To be consistent with memory-less implementation of terms with functions, it is\\n            recommended to ensure that the returned mutable quantities are cloned before\\n            returning them. For instance, if the term returns a tensor, it is recommended\\n            to ensure that the returned tensor is a clone of the original tensor. This prevents\\n            the manager from storing references to the tensors and altering the original tensors.\\n\\n        Args:\\n            *args: Variable length argument list.\\n\\n        Returns:\\n            The value of the term.\\n        \"\"\"\\n        raise NotImplementedError(\"The method \\'__call__\\' should be implemented by the subclass.\")'),\n",
       " Document(metadata={}, page_content='class ManagerBase(ABC):\\n    \"\"\"Base class for all managers.\"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedEnv):\\n        \"\"\"Initialize the manager.\\n\\n        This function is responsible for parsing the configuration object and creating the terms.\\n\\n        If the simulation is not playing, the scene entities are not resolved immediately.\\n        Instead, the resolution is deferred until the simulation starts. This is done to ensure\\n        that the scene entities are resolved even if the manager is created after the simulation\\n        has already started.\\n\\n        Args:\\n            cfg: The configuration object. If None, the manager is initialized without any terms.\\n            env: The environment instance.\\n        \"\"\"\\n        # store the inputs\\n        self.cfg = copy.deepcopy(cfg)\\n        self._env = env\\n\\n        # flag for whether the scene entities have been resolved\\n        # if sim is playing, we resolve the scene entities directly while preparing the terms\\n        self._is_scene_entities_resolved = self._env.sim.is_playing()\\n\\n        # if the simulation is not playing, we use callbacks to trigger the resolution of the scene\\n        # entities configuration. this is needed for cases where the manager is created after the\\n        # simulation, but before the simulation is playing.\\n        # FIXME: Once Isaac Sim supports storing this information as USD schema, we can remove this\\n        #   callback and resolve the scene entities directly inside `_prepare_terms`.\\n        if not self._env.sim.is_playing():\\n            # note: Use weakref on all callbacks to ensure that this object can be deleted when its destructor\\n            # is called\\n            # The order is set to 20 to allow asset/sensor initialization to complete before the scene entities\\n            # are resolved. Those have the order 10.\\n            timeline_event_stream = omni.timeline.get_timeline_interface().get_timeline_event_stream()\\n            self._resolve_terms_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n                int(omni.timeline.TimelineEventType.PLAY),\\n                lambda event, obj=weakref.proxy(self): obj._resolve_terms_callback(event),\\n                order=20,\\n            )\\n        else:\\n            self._resolve_terms_handle = None\\n\\n        # parse config to create terms information\\n        if self.cfg:\\n            self._prepare_terms()\\n\\n    def __del__(self):\\n        \"\"\"Delete the manager.\"\"\"\\n        if self._resolve_terms_handle:\\n            self._resolve_terms_handle.unsubscribe()\\n            self._resolve_terms_handle = None\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_envs(self) -> int:\\n        \"\"\"Number of environments.\"\"\"\\n        return self._env.num_envs\\n\\n    @property\\n    def device(self) -> str:\\n        \"\"\"Device on which to perform computations.\"\"\"\\n        return self._env.device\\n\\n    @property\\n    @abstractmethod\\n    def active_terms(self) -> list[str] | dict[str, list[str]]:\\n        \"\"\"Name of active terms.\"\"\"\\n        raise NotImplementedError\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:\\n        \"\"\"Resets the manager and returns logging information for the current time-step.\\n\\n        Args:\\n            env_ids: The environment ids for which to log data.\\n                Defaults None, which logs data for all environments.\\n\\n        Returns:\\n            Dictionary containing the logging information.\\n        \"\"\"\\n        return {}\\n\\n    def find_terms(self, name_keys: str | Sequence[str]) -> list[str]:\\n        \"\"\"Find terms in the manager based on the names.\\n\\n        This function searches the manager for terms based on the names. The names can be\\n        specified as regular expressions or a list of regular expressions. The search is\\n        performed on the active terms in the manager.\\n\\n        Please check the :meth:`~isaaclab.utils.string_utils.resolve_matching_names` function for more\\n        information on the name matching.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the term names.\\n\\n        Returns:\\n            A list of term names that match the input keys.\\n        \"\"\"\\n        # resolve search keys\\n        if isinstance(self.active_terms, dict):\\n            list_of_strings = []\\n            for names in self.active_terms.values():\\n                list_of_strings.extend(names)\\n        else:\\n            list_of_strings = self.active_terms\\n\\n        # return the matching names\\n        return string_utils.resolve_matching_names(name_keys, list_of_strings)[1]\\n\\n    def get_active_iterable_terms(self, env_idx: int) -> Sequence[tuple[str, Sequence[float]]]:\\n        \"\"\"Returns the active terms as iterable sequence of tuples.\\n\\n        The first element of the tuple is the name of the term and the second element is the raw value(s) of the term.\\n\\n        Returns:\\n            The active terms.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    \"\"\"\\n    Implementation specific.\\n    \"\"\"\\n\\n    @abstractmethod\\n    def _prepare_terms(self):\\n        \"\"\"Prepare terms information from the configuration object.\"\"\"\\n        raise NotImplementedError\\n\\n    \"\"\"\\n    Internal callbacks.\\n    \"\"\"\\n\\n    def _resolve_terms_callback(self, event):\\n        \"\"\"Resolve configurations of terms once the simulation starts.\\n\\n        Please check the :meth:`_process_term_cfg_at_play` method for more information.\\n        \"\"\"\\n        # check if scene entities have been resolved\\n        if self._is_scene_entities_resolved:\\n            return\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n\\n        # iterate over all the terms\\n        for term_name, term_cfg in cfg_items:\\n            # check for non config\\n            if term_cfg is None:\\n                continue\\n            # process attributes at runtime\\n            # these properties are only resolvable once the simulation starts playing\\n            self._process_term_cfg_at_play(term_name, term_cfg)\\n\\n        # set the flag\\n        self._is_scene_entities_resolved = True\\n\\n    \"\"\"\\n    Internal functions.\\n    \"\"\"\\n\\n    def _resolve_common_term_cfg(self, term_name: str, term_cfg: ManagerTermBaseCfg, min_argc: int = 1):\\n        \"\"\"Resolve common attributes of the term configuration.\\n\\n        Usually, called by the :meth:`_prepare_terms` method to resolve common attributes of the term\\n        configuration. These include:\\n\\n        * Resolving the term function and checking if it is callable.\\n        * Checking if the term function\\'s arguments are matched by the parameters.\\n        * Resolving special attributes of the term configuration like ``asset_cfg``, ``sensor_cfg``, etc.\\n        * Initializing the term if it is a class.\\n\\n        The last two steps are only possible once the simulation starts playing.\\n\\n        By default, all term functions are expected to have at least one argument, which is the\\n        environment object. Some other managers may expect functions to take more arguments, for\\n        instance, the environment indices as the second argument. In such cases, the\\n        ``min_argc`` argument can be used to specify the minimum number of arguments\\n        required by the term function to be called correctly by the manager.\\n\\n        Args:\\n            term_name: The name of the term.\\n            term_cfg: The term configuration.\\n            min_argc: The minimum number of arguments required by the term function to be called correctly\\n                by the manager.\\n\\n        Raises:\\n            TypeError: If the term configuration is not of type :class:`ManagerTermBaseCfg`.\\n            ValueError: If the scene entity defined in the term configuration does not exist.\\n            AttributeError: If the term function is not callable.\\n            ValueError: If the term function\\'s arguments are not matched by the parameters.\\n        \"\"\"\\n        # check if the term is a valid term config\\n        if not isinstance(term_cfg, ManagerTermBaseCfg):\\n            raise TypeError(\\n                f\"Configuration for the term \\'{term_name}\\' is not of type ManagerTermBaseCfg.\"\\n                f\" Received: \\'{type(term_cfg)}\\'.\"\\n            )\\n\\n        # get the corresponding function or functional class\\n        if isinstance(term_cfg.func, str):\\n            term_cfg.func = string_to_callable(term_cfg.func)\\n        # check if function is callable\\n        if not callable(term_cfg.func):\\n            raise AttributeError(f\"The term \\'{term_name}\\' is not callable. Received: {term_cfg.func}\")\\n\\n        # check if the term is a class of valid type\\n        if inspect.isclass(term_cfg.func):\\n            if not issubclass(term_cfg.func, ManagerTermBase):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type ManagerTermBase.\"\\n                    f\" Received: \\'{type(term_cfg.func)}\\'.\"\\n                )\\n            func_static = term_cfg.func.__call__\\n            min_argc += 1  # forward by 1 to account for \\'self\\' argument\\n        else:\\n            func_static = term_cfg.func\\n        # check if function is callable\\n        if not callable(func_static):\\n            raise AttributeError(f\"The term \\'{term_name}\\' is not callable. Received: {term_cfg.func}\")\\n\\n        # check statically if the term\\'s arguments are matched by params\\n        term_params = list(term_cfg.params.keys())\\n        args = inspect.signature(func_static).parameters\\n        args_with_defaults = [arg for arg in args if args[arg].default is not inspect.Parameter.empty]\\n        args_without_defaults = [arg for arg in args if args[arg].default is inspect.Parameter.empty]\\n        args = args_without_defaults + args_with_defaults\\n        # ignore first two arguments for env and env_ids\\n        # Think: Check for cases when kwargs are set inside the function?\\n        if len(args) > min_argc:\\n            if set(args[min_argc:]) != set(term_params + args_with_defaults):\\n                raise ValueError(\\n                    f\"The term \\'{term_name}\\' expects mandatory parameters: {args_without_defaults[min_argc:]}\"\\n                    f\" and optional parameters: {args_with_defaults}, but received: {term_params}.\"\\n                )\\n\\n        # process attributes at runtime\\n        # these properties are only resolvable once the simulation starts playing\\n        if self._env.sim.is_playing():\\n            self._process_term_cfg_at_play(term_name, term_cfg)\\n\\n    def _process_term_cfg_at_play(self, term_name: str, term_cfg: ManagerTermBaseCfg):\\n        \"\"\"Process the term configuration at runtime.\\n\\n        This function is called when the simulation starts playing. It is used to process the term\\n        configuration at runtime. This includes:\\n\\n        * Resolving the scene entity configuration for the term.\\n        * Initializing the term if it is a class.\\n\\n        Since the above steps rely on PhysX to parse over the simulation scene, they are deferred\\n        until the simulation starts playing.\\n\\n        Args:\\n            term_name: The name of the term.\\n            term_cfg: The term configuration.\\n        \"\"\"\\n        for key, value in term_cfg.params.items():\\n            if isinstance(value, SceneEntityCfg):\\n                # load the entity\\n                try:\\n                    value.resolve(self._env.scene)\\n                except ValueError as e:\\n                    raise ValueError(f\"Error while parsing \\'{term_name}:{key}\\'. {e}\")\\n                # log the entity for checking later\\n                msg = f\"[{term_cfg.__class__.__name__}:{term_name}] Found entity \\'{value.name}\\'.\"\\n                if value.joint_ids is not None:\\n                    msg += f\"\\\\n\\\\tJoint names: {value.joint_names} [{value.joint_ids}]\"\\n                if value.body_ids is not None:\\n                    msg += f\"\\\\n\\\\tBody names: {value.body_names} [{value.body_ids}]\"\\n                # print the information\\n                omni.log.info(msg)\\n            # store the entity\\n            term_cfg.params[key] = value\\n\\n        # initialize the term if it is a class\\n        if inspect.isclass(term_cfg.func):\\n            omni.log.info(f\"Initializing term \\'{term_name}\\' with class \\'{term_cfg.func.__name__}\\'.\")\\n            term_cfg.func = term_cfg.func(cfg=term_cfg, env=self._env)'),\n",
       " Document(metadata={}, page_content='class ManagerTermBaseCfg:\\n    \"\"\"Configuration for a manager term.\"\"\"\\n\\n    func: Callable | ManagerTermBase = MISSING\\n    \"\"\"The function or class to be called for the term.\\n\\n    The function must take the environment object as the first argument.\\n    The remaining arguments are specified in the :attr:`params` attribute.\\n\\n    It also supports `callable classes`_, i.e. classes that implement the :meth:`__call__`\\n    method. In this case, the class should inherit from the :class:`ManagerTermBase` class\\n    and implement the required methods.\\n\\n    .. _`callable classes`: https://docs.python.org/3/reference/datamodel.html#object.__call__\\n    \"\"\"\\n\\n    params: dict[str, Any | SceneEntityCfg] = dict()\\n    \"\"\"The parameters to be passed to the function as keyword arguments. Defaults to an empty dict.\\n\\n    .. note::\\n        If the value is a :class:`SceneEntityCfg` object, the manager will query the scene entity\\n        from the :class:`InteractiveScene` and process the entity\\'s joints and bodies as specified\\n        in the :class:`SceneEntityCfg` object.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RecorderTermCfg:\\n    \"\"\"Configuration for an recorder term.\"\"\"\\n\\n    class_type: type[RecorderTerm] = MISSING\\n    \"\"\"The associated recorder term class.\\n\\n    The class should inherit from :class:`isaaclab.managers.action_manager.RecorderTerm`.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ActionTermCfg:\\n    \"\"\"Configuration for an action term.\"\"\"\\n\\n    class_type: type[ActionTerm] = MISSING\\n    \"\"\"The associated action term class.\\n\\n    The class should inherit from :class:`isaaclab.managers.action_manager.ActionTerm`.\\n    \"\"\"\\n\\n    asset_name: str = MISSING\\n    \"\"\"The name of the scene entity.\\n\\n    This is the name defined in the scene configuration file. See the :class:`InteractiveSceneCfg`\\n    class for more details.\\n    \"\"\"\\n\\n    debug_vis: bool = False\\n    \"\"\"Whether to visualize debug information. Defaults to False.\"\"\"\\n\\n    clip: dict[str, tuple] | None = None\\n    \"\"\"Clip range for the action (dict of regex expressions). Defaults to None.\"\"\"'),\n",
       " Document(metadata={}, page_content='class CommandTermCfg:\\n    \"\"\"Configuration for a command generator term.\"\"\"\\n\\n    class_type: type[CommandTerm] = MISSING\\n    \"\"\"The associated command term class to use.\\n\\n    The class should inherit from :class:`isaaclab.managers.command_manager.CommandTerm`.\\n    \"\"\"\\n\\n    resampling_time_range: tuple[float, float] = MISSING\\n    \"\"\"Time before commands are changed [s].\"\"\"\\n    debug_vis: bool = False\\n    \"\"\"Whether to visualize debug information. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='class CurriculumTermCfg(ManagerTermBaseCfg):\\n    \"\"\"Configuration for a curriculum term.\"\"\"\\n\\n    func: Callable[..., float | dict[str, float] | None] = MISSING\\n    \"\"\"The name of the function to be called.\\n\\n    This function should take the environment object, environment indices\\n    and any other parameters as input and return the curriculum state for\\n    logging purposes. If the function returns None, the curriculum state\\n    is not logged.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ObservationTermCfg(ManagerTermBaseCfg):\\n    \"\"\"Configuration for an observation term.\"\"\"\\n\\n    func: Callable[..., torch.Tensor] = MISSING\\n    \"\"\"The name of the function to be called.\\n\\n    This function should take the environment object and any other parameters\\n    as input and return the observation signal as torch float tensors of\\n    shape (num_envs, obs_term_dim).\\n    \"\"\"\\n\\n    modifiers: list[ModifierCfg] | None = None\\n    \"\"\"The list of data modifiers to apply to the observation in order. Defaults to None,\\n    in which case no modifications will be applied.\\n\\n    Modifiers are applied in the order they are specified in the list. They can be stateless\\n    or stateful, and can be used to apply transformations to the observation data. For example,\\n    a modifier can be used to normalize the observation data or to apply a rolling average.\\n\\n    For more information on modifiers, see the :class:`~isaaclab.utils.modifiers.ModifierCfg` class.\\n    \"\"\"\\n\\n    noise: NoiseCfg | None = None\\n    \"\"\"The noise to add to the observation. Defaults to None, in which case no noise is added.\"\"\"\\n\\n    clip: tuple[float, float] | None = None\\n    \"\"\"The clipping range for the observation after adding noise. Defaults to None,\\n    in which case no clipping is applied.\"\"\"\\n\\n    scale: tuple[float, ...] | float | None = None\\n    \"\"\"The scale to apply to the observation after clipping. Defaults to None,\\n    in which case no scaling is applied (same as setting scale to :obj:`1`).\\n\\n    We leverage PyTorch broadcasting to scale the observation tensor with the provided value. If a tuple is provided,\\n    please make sure the length of the tuple matches the dimensions of the tensor outputted from the term.\\n    \"\"\"\\n\\n    history_length: int = 0\\n    \"\"\"Number of past observations to store in the observation buffers. Defaults to 0, meaning no history.\\n\\n    Observation history initializes to empty, but is filled with the first append after reset or initialization. Subsequent history\\n    only adds a single entry to the history buffer. If flatten_history_dim is set to True, the source data of shape\\n    (N, H, D, ...) where N is the batch dimension and H is the history length will be reshaped to a 2D tensor of shape\\n    (N, H*D*...). Otherwise, the data will be returned as is.\\n    \"\"\"\\n\\n    flatten_history_dim: bool = True\\n    \"\"\"Whether or not the observation manager should flatten history-based observation terms to a 2D (N, D) tensor.\\n    Defaults to True.\"\"\"'),\n",
       " Document(metadata={}, page_content='class ObservationGroupCfg:\\n    \"\"\"Configuration for an observation group.\"\"\"\\n\\n    concatenate_terms: bool = True\\n    \"\"\"Whether to concatenate the observation terms in the group. Defaults to True.\\n\\n    If true, the observation terms in the group are concatenated along the dimension specified through :attr:`concatenate_dim`.\\n    Otherwise, they are kept separate and returned as a dictionary.\\n\\n    If the observation group contains terms of different dimensions, it must be set to False.\\n    \"\"\"\\n\\n    concatenate_dim: int = -1\\n    \"\"\"Dimension along to concatenate the different observation terms. Defaults to -1, which\\n    means the last dimension of the observation terms.\\n\\n    If :attr:`concatenate_terms` is True, this parameter specifies the dimension along which the observation terms are concatenated.\\n    The indicated dimension depends on the shape of the observations. For instance, for a 2D RGB image of shape (H, W, C), the dimension\\n    0 means concatenating along the height, 1 along the width, and 2 along the channels. The offset due\\n    to the batched environment is handled automatically.\\n    \"\"\"\\n\\n    enable_corruption: bool = False\\n    \"\"\"Whether to enable corruption for the observation group. Defaults to False.\\n\\n    If true, the observation terms in the group are corrupted by adding noise (if specified).\\n    Otherwise, no corruption is applied.\\n    \"\"\"\\n\\n    history_length: int | None = None\\n    \"\"\"Number of past observation to store in the observation buffers for all observation terms in group.\\n\\n    This parameter will override :attr:`ObservationTermCfg.history_length` if set. Defaults to None. If None, each\\n    terms history will be controlled on a per term basis. See :class:`ObservationTermCfg` for details on history_length\\n    implementation.\\n    \"\"\"\\n\\n    flatten_history_dim: bool = True\\n    \"\"\"Flag to flatten history-based observation terms to a 2D (num_env, D) tensor for all observation terms in group.\\n    Defaults to True.\\n\\n    This parameter will override all :attr:`ObservationTermCfg.flatten_history_dim` in the group if\\n    ObservationGroupCfg.history_length is set.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class EventTermCfg(ManagerTermBaseCfg):\\n    \"\"\"Configuration for a event term.\"\"\"\\n\\n    func: Callable[..., None] = MISSING\\n    \"\"\"The name of the function to be called.\\n\\n    This function should take the environment object, environment indices\\n    and any other parameters as input.\\n    \"\"\"\\n\\n    mode: str = MISSING\\n    \"\"\"The mode in which the event term is applied.\\n\\n    Note:\\n        The mode name ``\"interval\"`` is a special mode that is handled by the\\n        manager Hence, its name is reserved and cannot be used for other modes.\\n    \"\"\"\\n\\n    interval_range_s: tuple[float, float] | None = None\\n    \"\"\"The range of time in seconds at which the term is applied. Defaults to None.\\n\\n    Based on this, the interval is sampled uniformly between the specified\\n    range for each environment instance. The term is applied on the environment\\n    instances where the current time hits the interval time.\\n\\n    Note:\\n        This is only used if the mode is ``\"interval\"``.\\n    \"\"\"\\n\\n    is_global_time: bool = False\\n    \"\"\"Whether randomization should be tracked on a per-environment basis. Defaults to False.\\n\\n    If True, the same interval time is used for all the environment instances.\\n    If False, the interval time is sampled independently for each environment instance\\n    and the term is applied when the current time hits the interval time for that instance.\\n\\n    Note:\\n        This is only used if the mode is ``\"interval\"``.\\n    \"\"\"\\n\\n    min_step_count_between_reset: int = 0\\n    \"\"\"The number of environment steps after which the term is applied since its last application. Defaults to 0.\\n\\n    When the mode is \"reset\", the term is only applied if the number of environment steps since\\n    its last application exceeds this quantity. This helps to avoid calling the term too often,\\n    thereby improving performance.\\n\\n    If the value is zero, the term is applied on every call to the manager with the mode \"reset\".\\n\\n    Note:\\n        This is only used if the mode is ``\"reset\"``.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RewardTermCfg(ManagerTermBaseCfg):\\n    \"\"\"Configuration for a reward term.\"\"\"\\n\\n    func: Callable[..., torch.Tensor] = MISSING\\n    \"\"\"The name of the function to be called.\\n\\n    This function should take the environment object and any other parameters\\n    as input and return the reward signals as torch float tensors of\\n    shape (num_envs,).\\n    \"\"\"\\n\\n    weight: float = MISSING\\n    \"\"\"The weight of the reward term.\\n\\n    This is multiplied with the reward term\\'s value to compute the final\\n    reward.\\n\\n    Note:\\n        If the weight is zero, the reward term is ignored.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class TerminationTermCfg(ManagerTermBaseCfg):\\n    \"\"\"Configuration for a termination term.\"\"\"\\n\\n    func: Callable[..., torch.Tensor] = MISSING\\n    \"\"\"The name of the function to be called.\\n\\n    This function should take the environment object and any other parameters\\n    as input and return the termination signals as torch boolean tensors of\\n    shape (num_envs,).\\n    \"\"\"\\n\\n    time_out: bool = False\\n    \"\"\"Whether the termination term contributes towards episodic timeouts. Defaults to False.\\n\\n    Note:\\n        These usually correspond to tasks that have a fixed time limit.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ObservationManager(ManagerBase):\\n    \"\"\"Manager for computing observation signals for a given world.\\n\\n    Observations are organized into groups based on their intended usage. This allows having different observation\\n    groups for different types of learning such as asymmetric actor-critic and student-teacher training. Each\\n    group contains observation terms which contain information about the observation function to call, the noise\\n    corruption model to use, and the sensor to retrieve data from.\\n\\n    Each observation group should inherit from the :class:`ObservationGroupCfg` class. Within each group, each\\n    observation term should instantiate the :class:`ObservationTermCfg` class. Based on the configuration, the\\n    observations in a group can be concatenated into a single tensor or returned as a dictionary with keys\\n    corresponding to the term\\'s name.\\n\\n    If the observations in a group are concatenated, the shape of the concatenated tensor is computed based on the\\n    shapes of the individual observation terms. This information is stored in the :attr:`group_obs_dim` dictionary\\n    with keys as the group names and values as the shape of the observation tensor. When the terms in a group are not\\n    concatenated, the attribute stores a list of shapes for each term in the group.\\n\\n    .. note::\\n        When the observation terms in a group do not have the same shape, the observation terms cannot be\\n        concatenated. In this case, please set the :attr:`ObservationGroupCfg.concatenate_terms` attribute in the\\n        group configuration to False.\\n\\n    Observations can also have history. This means a running history is updated per sim step. History can be controlled\\n    per :class:`ObservationTermCfg` (See the :attr:`ObservationTermCfg.history_length` and\\n    :attr:`ObservationTermCfg.flatten_history_dim`). History can also be controlled via :class:`ObservationGroupCfg`\\n    where group configuration overwrites per term configuration if set. History follows an oldest to newest ordering.\\n\\n    The observation manager can be used to compute observations for all the groups or for a specific group. The\\n    observations are computed by calling the registered functions for each term in the group. The functions are\\n    called in the order of the terms in the group. The functions are expected to return a tensor with shape\\n    (num_envs, ...).\\n\\n    If a noise model or custom modifier is registered for a term, the function is called to corrupt\\n    the observation. The corruption function is expected to return a tensor with the same shape as the observation.\\n    The observations are clipped and scaled as per the configuration settings.\\n    \"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedEnv):\\n        \"\"\"Initialize observation manager.\\n\\n        Args:\\n            cfg: The configuration object or dictionary (``dict[str, ObservationGroupCfg]``).\\n            env: The environment instance.\\n\\n        Raises:\\n            ValueError: If the configuration is None.\\n            RuntimeError: If the shapes of the observation terms in a group are not compatible for concatenation\\n                and the :attr:`~ObservationGroupCfg.concatenate_terms` attribute is set to True.\\n        \"\"\"\\n        # check that cfg is not None\\n        if cfg is None:\\n            raise ValueError(\"Observation manager configuration is None. Please provide a valid configuration.\")\\n\\n        # call the base class constructor (this will parse the terms config)\\n        super().__init__(cfg, env)\\n\\n        # compute combined vector for obs group\\n        self._group_obs_dim: dict[str, tuple[int, ...] | list[tuple[int, ...]]] = dict()\\n        for group_name, group_term_dims in self._group_obs_term_dim.items():\\n            # if terms are concatenated, compute the combined shape into a single tuple\\n            # otherwise, keep the list of shapes as is\\n            if self._group_obs_concatenate[group_name]:\\n                try:\\n                    term_dims = torch.stack([torch.tensor(dims, device=\"cpu\") for dims in group_term_dims], dim=0)\\n                    if len(term_dims.shape) > 1:\\n                        if self._group_obs_concatenate_dim[group_name] >= 0:\\n                            dim = self._group_obs_concatenate_dim[group_name] - 1  # account for the batch offset\\n                        else:\\n                            dim = self._group_obs_concatenate_dim[group_name]\\n                        dim_sum = torch.sum(term_dims[:, dim], dim=0)\\n                        term_dims[0, dim] = dim_sum\\n                        term_dims = term_dims[0]\\n                    else:\\n                        term_dims = torch.sum(term_dims, dim=0)\\n                    self._group_obs_dim[group_name] = tuple(term_dims.tolist())\\n                except RuntimeError:\\n                    raise RuntimeError(\\n                        f\"Unable to concatenate observation terms in group \\'{group_name}\\'.\"\\n                        f\" The shapes of the terms are: {group_term_dims}.\"\\n                        \" Please ensure that the shapes are compatible for concatenation.\"\\n                        \" Otherwise, set \\'concatenate_terms\\' to False in the group configuration.\"\\n                    )\\n            else:\\n                self._group_obs_dim[group_name] = group_term_dims\\n\\n        # Stores the latest observations.\\n        self._obs_buffer: dict[str, torch.Tensor | dict[str, torch.Tensor]] | None = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for the observation manager.\"\"\"\\n        msg = f\"<ObservationManager> contains {len(self._group_obs_term_names)} groups.\\\\n\"\\n\\n        # add info for each group\\n        for group_name, group_dim in self._group_obs_dim.items():\\n            # create table for term information\\n            table = PrettyTable()\\n            table.title = f\"Active Observation Terms in Group: \\'{group_name}\\'\"\\n            if self._group_obs_concatenate[group_name]:\\n                table.title += f\" (shape: {group_dim})\"\\n            table.field_names = [\"Index\", \"Name\", \"Shape\"]\\n            # set alignment of table columns\\n            table.align[\"Name\"] = \"l\"\\n            # add info for each term\\n            obs_terms = zip(\\n                self._group_obs_term_names[group_name],\\n                self._group_obs_term_dim[group_name],\\n            )\\n            for index, (name, dims) in enumerate(obs_terms):\\n                # resolve inputs to simplify prints\\n                tab_dims = tuple(dims)\\n                # add row\\n                table.add_row([index, name, tab_dims])\\n            # convert table to string\\n            msg += table.get_string()\\n            msg += \"\\\\n\"\\n\\n        return msg\\n\\n    def get_active_iterable_terms(self, env_idx: int) -> Sequence[tuple[str, Sequence[float]]]:\\n        \"\"\"Returns the active terms as iterable sequence of tuples.\\n\\n        The first element of the tuple is the name of the term and the second element is the raw value(s) of the term.\\n\\n        Args:\\n            env_idx: The specific environment to pull the active terms from.\\n\\n        Returns:\\n            The active terms.\\n        \"\"\"\\n        terms = []\\n\\n        if self._obs_buffer is None:\\n            self.compute()\\n        obs_buffer: dict[str, torch.Tensor | dict[str, torch.Tensor]] = self._obs_buffer\\n\\n        for group_name, _ in self._group_obs_dim.items():\\n            if not self.group_obs_concatenate[group_name]:\\n                for name, term in obs_buffer[group_name].items():\\n                    terms.append((group_name + \"-\" + name, term[env_idx].cpu().tolist()))\\n                continue\\n\\n            idx = 0\\n            # add info for each term\\n            data = obs_buffer[group_name]\\n            for name, shape in zip(\\n                self._group_obs_term_names[group_name],\\n                self._group_obs_term_dim[group_name],\\n            ):\\n                data_length = np.prod(shape)\\n                term = data[env_idx, idx : idx + data_length]\\n                terms.append((group_name + \"-\" + name, term.cpu().tolist()))\\n                idx += data_length\\n\\n        return terms\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def active_terms(self) -> dict[str, list[str]]:\\n        \"\"\"Name of active observation terms in each group.\\n\\n        The keys are the group names and the values are the list of observation term names in the group.\\n        \"\"\"\\n        return self._group_obs_term_names\\n\\n    @property\\n    def group_obs_dim(self) -> dict[str, tuple[int, ...] | list[tuple[int, ...]]]:\\n        \"\"\"Shape of computed observations in each group.\\n\\n        The key is the group name and the value is the shape of the observation tensor.\\n        If the terms in the group are concatenated, the value is a single tuple representing the\\n        shape of the concatenated observation tensor. Otherwise, the value is a list of tuples,\\n        where each tuple represents the shape of the observation tensor for a term in the group.\\n        \"\"\"\\n        return self._group_obs_dim\\n\\n    @property\\n    def group_obs_term_dim(self) -> dict[str, list[tuple[int, ...]]]:\\n        \"\"\"Shape of individual observation terms in each group.\\n\\n        The key is the group name and the value is a list of tuples representing the shape of the observation terms\\n        in the group. The order of the tuples corresponds to the order of the terms in the group.\\n        This matches the order of the terms in the :attr:`active_terms`.\\n        \"\"\"\\n        return self._group_obs_term_dim\\n\\n    @property\\n    def group_obs_concatenate(self) -> dict[str, bool]:\\n        \"\"\"Whether the observation terms are concatenated in each group or not.\\n\\n        The key is the group name and the value is a boolean specifying whether the observation terms in the group\\n        are concatenated into a single tensor. If True, the observations are concatenated along the last dimension.\\n\\n        The values are set based on the :attr:`~ObservationGroupCfg.concatenate_terms` attribute in the group\\n        configuration.\\n        \"\"\"\\n        return self._group_obs_concatenate\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:\\n        # call all terms that are classes\\n        for group_name, group_cfg in self._group_obs_class_term_cfgs.items():\\n            for term_cfg in group_cfg:\\n                term_cfg.func.reset(env_ids=env_ids)\\n            # reset terms with history\\n            for term_name in self._group_obs_term_names[group_name]:\\n                if term_name in self._group_obs_term_history_buffer[group_name]:\\n                    self._group_obs_term_history_buffer[group_name][term_name].reset(batch_ids=env_ids)\\n        # call all modifiers that are classes\\n        for mod in self._group_obs_class_modifiers:\\n            mod.reset(env_ids=env_ids)\\n\\n        # nothing to log here\\n        return {}\\n\\n    def compute(self) -> dict[str, torch.Tensor | dict[str, torch.Tensor]]:\\n        \"\"\"Compute the observations per group for all groups.\\n\\n        The method computes the observations for all the groups handled by the observation manager.\\n        Please check the :meth:`compute_group` on the processing of observations per group.\\n\\n        Returns:\\n            A dictionary with keys as the group names and values as the computed observations.\\n            The observations are either concatenated into a single tensor or returned as a dictionary\\n            with keys corresponding to the term\\'s name.\\n        \"\"\"\\n        # create a buffer for storing obs from all the groups\\n        obs_buffer = dict()\\n        # iterate over all the terms in each group\\n        for group_name in self._group_obs_term_names:\\n            obs_buffer[group_name] = self.compute_group(group_name)\\n        # otherwise return a dict with observations of all groups\\n\\n        # Cache the observations.\\n        self._obs_buffer = obs_buffer\\n        return obs_buffer\\n\\n    def compute_group(self, group_name: str) -> torch.Tensor | dict[str, torch.Tensor]:\\n        \"\"\"Computes the observations for a given group.\\n\\n        The observations for a given group are computed by calling the registered functions for each\\n        term in the group. The functions are called in the order of the terms in the group. The functions\\n        are expected to return a tensor with shape (num_envs, ...).\\n\\n        The following steps are performed for each observation term:\\n\\n        1. Compute observation term by calling the function\\n        2. Apply custom modifiers in the order specified in :attr:`ObservationTermCfg.modifiers`\\n        3. Apply corruption/noise model based on :attr:`ObservationTermCfg.noise`\\n        4. Apply clipping based on :attr:`ObservationTermCfg.clip`\\n        5. Apply scaling based on :attr:`ObservationTermCfg.scale`\\n\\n        We apply noise to the computed term first to maintain the integrity of how noise affects the data\\n        as it truly exists in the real world. If the noise is applied after clipping or scaling, the noise\\n        could be artificially constrained or amplified, which might misrepresent how noise naturally occurs\\n        in the data.\\n\\n        Args:\\n            group_name: The name of the group for which to compute the observations. Defaults to None,\\n                in which case observations for all the groups are computed and returned.\\n\\n        Returns:\\n            Depending on the group\\'s configuration, the tensors for individual observation terms are\\n            concatenated along the last dimension into a single tensor. Otherwise, they are returned as\\n            a dictionary with keys corresponding to the term\\'s name.\\n\\n        Raises:\\n            ValueError: If input ``group_name`` is not a valid group handled by the manager.\\n        \"\"\"\\n        # check ig group name is valid\\n        if group_name not in self._group_obs_term_names:\\n            raise ValueError(\\n                f\"Unable to find the group \\'{group_name}\\' in the observation manager.\"\\n                f\" Available groups are: {list(self._group_obs_term_names.keys())}\"\\n            )\\n        # iterate over all the terms in each group\\n        group_term_names = self._group_obs_term_names[group_name]\\n        # buffer to store obs per group\\n        group_obs = dict.fromkeys(group_term_names, None)\\n        # read attributes for each term\\n        obs_terms = zip(group_term_names, self._group_obs_term_cfgs[group_name])\\n\\n        # evaluate terms: compute, add noise, clip, scale, custom modifiers\\n        for term_name, term_cfg in obs_terms:\\n            # compute term\\'s value\\n            obs: torch.Tensor = term_cfg.func(self._env, **term_cfg.params).clone()\\n            # apply post-processing\\n            if term_cfg.modifiers is not None:\\n                for modifier in term_cfg.modifiers:\\n                    obs = modifier.func(obs, **modifier.params)\\n            if term_cfg.noise:\\n                obs = term_cfg.noise.func(obs, term_cfg.noise)\\n            if term_cfg.clip:\\n                obs = obs.clip_(min=term_cfg.clip[0], max=term_cfg.clip[1])\\n            if term_cfg.scale is not None:\\n                obs = obs.mul_(term_cfg.scale)\\n            # Update the history buffer if observation term has history enabled\\n            if term_cfg.history_length > 0:\\n                self._group_obs_term_history_buffer[group_name][term_name].append(obs)\\n                if term_cfg.flatten_history_dim:\\n                    group_obs[term_name] = self._group_obs_term_history_buffer[group_name][term_name].buffer.reshape(\\n                        self._env.num_envs, -1\\n                    )\\n                else:\\n                    group_obs[term_name] = self._group_obs_term_history_buffer[group_name][term_name].buffer\\n            else:\\n                group_obs[term_name] = obs\\n\\n        # concatenate all observations in the group together\\n        if self._group_obs_concatenate[group_name]:\\n            # set the concatenate dimension, account for the batch dimension if positive dimension is given\\n            return torch.cat(list(group_obs.values()), dim=self._group_obs_concatenate_dim[group_name])\\n        else:\\n            return group_obs\\n\\n    def serialize(self) -> dict:\\n        \"\"\"Serialize the observation term configurations for all active groups.\\n\\n        Returns:\\n            A dictionary where each group name maps to its serialized observation term configurations.\\n        \"\"\"\\n        output = {\\n            group_name: {\\n                term_name: (\\n                    term_cfg.func.serialize()\\n                    if isinstance(term_cfg.func, ManagerTermBase)\\n                    else {\"cfg\": class_to_dict(term_cfg)}\\n                )\\n                for term_name, term_cfg in zip(\\n                    self._group_obs_term_names[group_name],\\n                    self._group_obs_term_cfgs[group_name],\\n                )\\n            }\\n            for group_name in self.active_terms.keys()\\n        }\\n\\n        return output\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        \"\"\"Prepares a list of observation terms functions.\"\"\"\\n        # create buffers to store information for each observation group\\n        # TODO: Make this more convenient by using data structures.\\n        self._group_obs_term_names: dict[str, list[str]] = dict()\\n        self._group_obs_term_dim: dict[str, list[tuple[int, ...]]] = dict()\\n        self._group_obs_term_cfgs: dict[str, list[ObservationTermCfg]] = dict()\\n        self._group_obs_class_term_cfgs: dict[str, list[ObservationTermCfg]] = dict()\\n        self._group_obs_concatenate: dict[str, bool] = dict()\\n        self._group_obs_concatenate_dim: dict[str, int] = dict()\\n\\n        self._group_obs_term_history_buffer: dict[str, dict] = dict()\\n        # create a list to store modifiers that are classes\\n        # we store it as a separate list to only call reset on them and prevent unnecessary calls\\n        self._group_obs_class_modifiers: list[modifiers.ModifierBase] = list()\\n\\n        # make sure the simulation is playing since we compute obs dims which needs asset quantities\\n        if not self._env.sim.is_playing():\\n            raise RuntimeError(\\n                \"Simulation is not playing. Observation manager requires the simulation to be playing\"\\n                \" to compute observation dimensions. Please start the simulation before using the\"\\n                \" observation manager.\"\\n            )\\n\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            group_cfg_items = self.cfg.items()\\n        else:\\n            group_cfg_items = self.cfg.__dict__.items()\\n        # iterate over all the groups\\n        for group_name, group_cfg in group_cfg_items:\\n            # check for non config\\n            if group_cfg is None:\\n                continue\\n            # check if the term is a curriculum term\\n            if not isinstance(group_cfg, ObservationGroupCfg):\\n                raise TypeError(\\n                    f\"Observation group \\'{group_name}\\' is not of type \\'ObservationGroupCfg\\'.\"\\n                    f\" Received: \\'{type(group_cfg)}\\'.\"\\n                )\\n            # initialize list for the group settings\\n            self._group_obs_term_names[group_name] = list()\\n            self._group_obs_term_dim[group_name] = list()\\n            self._group_obs_term_cfgs[group_name] = list()\\n            self._group_obs_class_term_cfgs[group_name] = list()\\n            group_entry_history_buffer: dict[str, CircularBuffer] = dict()\\n            # read common config for the group\\n            self._group_obs_concatenate[group_name] = group_cfg.concatenate_terms\\n            self._group_obs_concatenate_dim[group_name] = (\\n                group_cfg.concatenate_dim + 1 if group_cfg.concatenate_dim >= 0 else group_cfg.concatenate_dim\\n            )\\n            # check if config is dict already\\n            if isinstance(group_cfg, dict):\\n                group_cfg_items = group_cfg.items()\\n            else:\\n                group_cfg_items = group_cfg.__dict__.items()\\n            # iterate over all the terms in each group\\n            for term_name, term_cfg in group_cfg_items:\\n                # skip non-obs settings\\n                if term_name in [\\n                    \"enable_corruption\",\\n                    \"concatenate_terms\",\\n                    \"history_length\",\\n                    \"flatten_history_dim\",\\n                    \"concatenate_dim\",\\n                ]:\\n                    continue\\n                # check for non config\\n                if term_cfg is None:\\n                    continue\\n                if not isinstance(term_cfg, ObservationTermCfg):\\n                    raise TypeError(\\n                        f\"Configuration for the term \\'{term_name}\\' is not of type ObservationTermCfg.\"\\n                        f\" Received: \\'{type(term_cfg)}\\'.\"\\n                    )\\n                # resolve common terms in the config\\n                self._resolve_common_term_cfg(f\"{group_name}/{term_name}\", term_cfg, min_argc=1)\\n\\n                # check noise settings\\n                if not group_cfg.enable_corruption:\\n                    term_cfg.noise = None\\n                # check group history params and override terms\\n                if group_cfg.history_length is not None:\\n                    term_cfg.history_length = group_cfg.history_length\\n                    term_cfg.flatten_history_dim = group_cfg.flatten_history_dim\\n                # add term config to list to list\\n                self._group_obs_term_names[group_name].append(term_name)\\n                self._group_obs_term_cfgs[group_name].append(term_cfg)\\n\\n                # call function the first time to fill up dimensions\\n                obs_dims = tuple(term_cfg.func(self._env, **term_cfg.params).shape)\\n\\n                # if scale is set, check if single float or tuple\\n                if term_cfg.scale is not None:\\n                    if not isinstance(term_cfg.scale, (float, int, tuple)):\\n                        raise TypeError(\\n                            f\"Scale for observation term \\'{term_name}\\' in group \\'{group_name}\\'\"\\n                            f\" is not of type float, int or tuple. Received: \\'{type(term_cfg.scale)}\\'.\"\\n                        )\\n                    if isinstance(term_cfg.scale, tuple) and len(term_cfg.scale) != obs_dims[1]:\\n                        raise ValueError(\\n                            f\"Scale for observation term \\'{term_name}\\' in group \\'{group_name}\\'\"\\n                            f\" does not match the dimensions of the observation. Expected: {obs_dims[1]}\"\\n                            f\" but received: {len(term_cfg.scale)}.\"\\n                        )\\n\\n                    # cast the scale into torch tensor\\n                    term_cfg.scale = torch.tensor(term_cfg.scale, dtype=torch.float, device=self._env.device)\\n\\n                # prepare modifiers for each observation\\n                if term_cfg.modifiers is not None:\\n                    # initialize list of modifiers for term\\n                    for mod_cfg in term_cfg.modifiers:\\n                        # check if class modifier and initialize with observation size when adding\\n                        if isinstance(mod_cfg, modifiers.ModifierCfg):\\n                            # to list of modifiers\\n                            if inspect.isclass(mod_cfg.func):\\n                                if not issubclass(mod_cfg.func, modifiers.ModifierBase):\\n                                    raise TypeError(\\n                                        f\"Modifier function \\'{mod_cfg.func}\\' for observation term \\'{term_name}\\'\"\\n                                        f\" is not a subclass of \\'ModifierBase\\'. Received: \\'{type(mod_cfg.func)}\\'.\"\\n                                    )\\n                                mod_cfg.func = mod_cfg.func(cfg=mod_cfg, data_dim=obs_dims, device=self._env.device)\\n\\n                                # add to list of class modifiers\\n                                self._group_obs_class_modifiers.append(mod_cfg.func)\\n                        else:\\n                            raise TypeError(\\n                                f\"Modifier configuration \\'{mod_cfg}\\' of observation term \\'{term_name}\\' is not of\"\\n                                f\" required type ModifierCfg, Received: \\'{type(mod_cfg)}\\'\"\\n                            )\\n\\n                        # check if function is callable\\n                        if not callable(mod_cfg.func):\\n                            raise AttributeError(\\n                                f\"Modifier \\'{mod_cfg}\\' of observation term \\'{term_name}\\' is not callable.\"\\n                                f\" Received: {mod_cfg.func}\"\\n                            )\\n\\n                        # check if term\\'s arguments are matched by params\\n                        term_params = list(mod_cfg.params.keys())\\n                        args = inspect.signature(mod_cfg.func).parameters\\n                        args_with_defaults = [arg for arg in args if args[arg].default is not inspect.Parameter.empty]\\n                        args_without_defaults = [arg for arg in args if args[arg].default is inspect.Parameter.empty]\\n                        args = args_without_defaults + args_with_defaults\\n                        # ignore first two arguments for env and env_ids\\n                        # Think: Check for cases when kwargs are set inside the function?\\n                        if len(args) > 1:\\n                            if set(args[1:]) != set(term_params + args_with_defaults):\\n                                raise ValueError(\\n                                    f\"Modifier \\'{mod_cfg}\\' of observation term \\'{term_name}\\' expects\"\\n                                    f\" mandatory parameters: {args_without_defaults[1:]}\"\\n                                    f\" and optional parameters: {args_with_defaults}, but received: {term_params}.\"\\n                                )\\n\\n                # create history buffers and calculate history term dimensions\\n                if term_cfg.history_length > 0:\\n                    group_entry_history_buffer[term_name] = CircularBuffer(\\n                        max_len=term_cfg.history_length, batch_size=self._env.num_envs, device=self._env.device\\n                    )\\n                    old_dims = list(obs_dims)\\n                    old_dims.insert(1, term_cfg.history_length)\\n                    obs_dims = tuple(old_dims)\\n                    if term_cfg.flatten_history_dim:\\n                        obs_dims = (obs_dims[0], np.prod(obs_dims[1:]))\\n\\n                self._group_obs_term_dim[group_name].append(obs_dims[1:])\\n\\n                # add term in a separate list if term is a class\\n                if isinstance(term_cfg.func, ManagerTermBase):\\n                    self._group_obs_class_term_cfgs[group_name].append(term_cfg)\\n                    # call reset (in-case above call to get obs dims changed the state)\\n                    term_cfg.func.reset()\\n            # add history buffers for each group\\n            self._group_obs_term_history_buffer[group_name] = group_entry_history_buffer'),\n",
       " Document(metadata={}, page_content='class DatasetExportMode(enum.IntEnum):\\n    \"\"\"The mode to handle episode exports.\"\"\"\\n\\n    EXPORT_NONE = 0  # Export none of the episodes\\n    EXPORT_ALL = 1  # Export all episodes to a single dataset file\\n    EXPORT_SUCCEEDED_FAILED_IN_SEPARATE_FILES = 2  # Export succeeded and failed episodes in separate files\\n    EXPORT_SUCCEEDED_ONLY = 3'),\n",
       " Document(metadata={}, page_content='class RecorderManagerBaseCfg:\\n    \"\"\"Base class for configuring recorder manager terms.\"\"\"\\n\\n    dataset_file_handler_class_type: type = HDF5DatasetFileHandler\\n\\n    dataset_export_dir_path: str = \"/tmp/isaaclab/logs\"\\n    \"\"\"The directory path where the recorded datasets are exported.\"\"\"\\n\\n    dataset_filename: str = \"dataset\"\\n    \"\"\"Dataset file name without file extension.\"\"\"\\n\\n    dataset_export_mode: DatasetExportMode = DatasetExportMode.EXPORT_ALL\\n    \"\"\"The mode to handle episode exports.\"\"\"\\n\\n    export_in_record_pre_reset: bool = True\\n    \"\"\"Whether to export episodes in the record_pre_reset call.\"\"\"'),\n",
       " Document(metadata={}, page_content='class RecorderTerm(ManagerTermBase):\\n    \"\"\"Base class for recorder terms.\\n\\n    The recorder term is responsible for recording data at various stages of the environment\\'s lifecycle.\\n    A recorder term is comprised of four user-defined callbacks to record data in the corresponding stages:\\n\\n    * Pre-reset recording: This callback is invoked at the beginning of `env.reset()` before the reset is effective.\\n    * Post-reset recording: This callback is invoked at the end of `env.reset()`.\\n    * Pre-step recording: This callback is invoked at the beginning of `env.step()`, after the step action is processed\\n          and before the action is applied by the action manager.\\n    * Post-step recording: This callback is invoked at the end of `env.step()` when all the managers are processed.\\n    \"\"\"\\n\\n    def __init__(self, cfg: RecorderTermCfg, env: ManagerBasedEnv):\\n        \"\"\"Initialize the recorder term.\\n\\n        Args:\\n            cfg: The configuration object.\\n            env: The environment instance.\\n        \"\"\"\\n        # call the base class constructor\\n        super().__init__(cfg, env)\\n\\n    \"\"\"\\n    User-defined callbacks.\\n    \"\"\"\\n\\n    def record_pre_reset(self, env_ids: Sequence[int] | None) -> tuple[str | None, torch.Tensor | dict | None]:\\n        \"\"\"Record data at the beginning of env.reset() before reset is effective.\\n\\n        Args:\\n            env_ids: The environment ids. All environments should be considered when set to None.\\n\\n        Returns:\\n            A tuple of key and value to be recorded.\\n            The key can contain nested keys separated by \\'/\\'. For example, \"obs/joint_pos\" would add the given\\n            value under [\\'obs\\'][\\'policy\\'] in the underlying dictionary in the recorded episode data.\\n            The value can be a tensor or a nested dictionary of tensors. The shape of a tensor in the value\\n            is (env_ids, ...).\\n        \"\"\"\\n        return None, None\\n\\n    def record_post_reset(self, env_ids: Sequence[int] | None) -> tuple[str | None, torch.Tensor | dict | None]:\\n        \"\"\"Record data at the end of env.reset().\\n\\n        Args:\\n            env_ids: The environment ids. All environments should be considered when set to None.\\n\\n        Returns:\\n            A tuple of key and value to be recorded.\\n            Please refer to the `record_pre_reset` function for more details.\\n        \"\"\"\\n        return None, None\\n\\n    def record_pre_step(self) -> tuple[str | None, torch.Tensor | dict | None]:\\n        \"\"\"Record data in the beginning of env.step() after action is cached/processed in the ActionManager.\\n\\n        Returns:\\n            A tuple of key and value to be recorded.\\n            Please refer to the `record_pre_reset` function for more details.\\n        \"\"\"\\n        return None, None\\n\\n    def record_post_step(self) -> tuple[str | None, torch.Tensor | dict | None]:\\n        \"\"\"Record data at the end of env.step() when all the managers are processed.\\n\\n        Returns:\\n            A tuple of key and value to be recorded.\\n            Please refer to the `record_pre_reset` function for more details.\\n        \"\"\"\\n        return None, None'),\n",
       " Document(metadata={}, page_content='class RecorderManager(ManagerBase):\\n    \"\"\"Manager for recording data from recorder terms.\"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedEnv):\\n        \"\"\"Initialize the recorder manager.\\n\\n        Args:\\n            cfg: The configuration object or dictionary (``dict[str, RecorderTermCfg]``).\\n            env: The environment instance.\\n        \"\"\"\\n        self._term_names: list[str] = list()\\n        self._terms: dict[str, RecorderTerm] = dict()\\n\\n        # Do nothing if cfg is None or an empty dict\\n        if not cfg:\\n            return\\n\\n        super().__init__(cfg, env)\\n\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        if not isinstance(cfg, RecorderManagerBaseCfg):\\n            raise TypeError(\"Configuration for the recorder manager is not of type RecorderManagerBaseCfg.\")\\n\\n        # create episode data buffer indexed by environment id\\n        self._episodes: dict[int, EpisodeData] = dict()\\n        for env_id in range(env.num_envs):\\n            self._episodes[env_id] = EpisodeData()\\n\\n        env_name = getattr(env.cfg, \"env_name\", None)\\n\\n        self._dataset_file_handler = None\\n        if cfg.dataset_export_mode != DatasetExportMode.EXPORT_NONE:\\n            self._dataset_file_handler = cfg.dataset_file_handler_class_type()\\n            self._dataset_file_handler.create(\\n                os.path.join(cfg.dataset_export_dir_path, cfg.dataset_filename), env_name=env_name\\n            )\\n\\n        self._failed_episode_dataset_file_handler = None\\n        if cfg.dataset_export_mode == DatasetExportMode.EXPORT_SUCCEEDED_FAILED_IN_SEPARATE_FILES:\\n            self._failed_episode_dataset_file_handler = cfg.dataset_file_handler_class_type()\\n            self._failed_episode_dataset_file_handler.create(\\n                os.path.join(cfg.dataset_export_dir_path, f\"{cfg.dataset_filename}_failed\"), env_name=env_name\\n            )\\n\\n        self._exported_successful_episode_count = {}\\n        self._exported_failed_episode_count = {}\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for recorder manager.\"\"\"\\n        msg = f\"<RecorderManager> contains {len(self._term_names)} active terms.\\\\n\"\\n        # create table for term information\\n        table = PrettyTable()\\n        table.title = \"Active Recorder Terms\"\\n        table.field_names = [\"Index\", \"Name\"]\\n        # set alignment of table columns\\n        table.align[\"Name\"] = \"l\"\\n        # add info on each term\\n        for index, name in enumerate(self._term_names):\\n            table.add_row([index, name])\\n        # convert table to string\\n        msg += table.get_string()\\n        msg += \"\\\\n\"\\n        return msg\\n\\n    def __del__(self):\\n        \"\"\"Destructor for recorder.\"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        if self._dataset_file_handler is not None:\\n            self._dataset_file_handler.close()\\n\\n        if self._failed_episode_dataset_file_handler is not None:\\n            self._failed_episode_dataset_file_handler.close()\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def active_terms(self) -> list[str]:\\n        \"\"\"Name of active recorder terms.\"\"\"\\n        return self._term_names\\n\\n    @property\\n    def exported_successful_episode_count(self, env_id=None) -> int:\\n        \"\"\"Number of successful episodes.\\n\\n        Args:\\n            env_id: The environment id. Defaults to None, in which case all environments are considered.\\n\\n        Returns:\\n            The number of successful episodes.\\n        \"\"\"\\n        if not hasattr(self, \"_exported_successful_episode_count\"):\\n            return 0\\n        if env_id is not None:\\n            return self._exported_successful_episode_count.get(env_id, 0)\\n        return sum(self._exported_successful_episode_count.values())\\n\\n    @property\\n    def exported_failed_episode_count(self, env_id=None) -> int:\\n        \"\"\"Number of failed episodes.\\n\\n        Args:\\n            env_id: The environment id. Defaults to None, in which case all environments are considered.\\n\\n        Returns:\\n            The number of failed episodes.\\n        \"\"\"\\n        if not hasattr(self, \"_exported_failed_episode_count\"):\\n            return 0\\n        if env_id is not None:\\n            return self._exported_failed_episode_count.get(env_id, 0)\\n        return sum(self._exported_failed_episode_count.values())\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, torch.Tensor]:\\n        \"\"\"Resets the recorder data.\\n\\n        Args:\\n            env_ids: The environment ids. Defaults to None, in which case\\n                all environments are considered.\\n\\n        Returns:\\n            An empty dictionary.\\n        \"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return {}\\n\\n        # resolve environment ids\\n        if env_ids is None:\\n            env_ids = list(range(self._env.num_envs))\\n        if isinstance(env_ids, torch.Tensor):\\n            env_ids = env_ids.tolist()\\n\\n        for term in self._terms.values():\\n            term.reset(env_ids=env_ids)\\n\\n        for env_id in env_ids:\\n            self._episodes[env_id] = EpisodeData()\\n\\n        # nothing to log here\\n        return {}\\n\\n    def get_episode(self, env_id: int) -> EpisodeData:\\n        \"\"\"Returns the episode data for the given environment id.\\n\\n        Args:\\n            env_id: The environment id.\\n\\n        Returns:\\n            The episode data for the given environment id.\\n        \"\"\"\\n        return self._episodes.get(env_id, EpisodeData())\\n\\n    def add_to_episodes(self, key: str, value: torch.Tensor | dict, env_ids: Sequence[int] | None = None):\\n        \"\"\"Adds the given key-value pair to the episodes for the given environment ids.\\n\\n        Args:\\n            key: The key of the given value to be added to the episodes. The key can contain nested keys\\n                separated by \\'/\\'. For example, \"obs/joint_pos\" would add the given value under [\\'obs\\'][\\'policy\\']\\n                in the underlying dictionary in the episode data.\\n            value: The value to be added to the episodes. The value can be a tensor or a nested dictionary of tensors.\\n                The shape of a tensor in the value is (env_ids, ...).\\n            env_ids: The environment ids. Defaults to None, in which case all environments are considered.\\n        \"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        # resolve environment ids\\n        if key is None:\\n            return\\n        if env_ids is None:\\n            env_ids = list(range(self._env.num_envs))\\n        if isinstance(env_ids, torch.Tensor):\\n            env_ids = env_ids.tolist()\\n\\n        if isinstance(value, dict):\\n            for sub_key, sub_value in value.items():\\n                self.add_to_episodes(f\"{key}/{sub_key}\", sub_value, env_ids)\\n            return\\n\\n        for value_index, env_id in enumerate(env_ids):\\n            if env_id not in self._episodes:\\n                self._episodes[env_id] = EpisodeData()\\n                self._episodes[env_id].env_id = env_id\\n            self._episodes[env_id].add(key, value[value_index])\\n\\n    def set_success_to_episodes(self, env_ids: Sequence[int] | None, success_values: torch.Tensor):\\n        \"\"\"Sets the task success values to the episodes for the given environment ids.\\n\\n        Args:\\n            env_ids: The environment ids. Defaults to None, in which case all environments are considered.\\n            success_values: The task success values to be set to the episodes. The shape of the tensor is (env_ids, 1).\\n        \"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        # resolve environment ids\\n        if env_ids is None:\\n            env_ids = list(range(self._env.num_envs))\\n        if isinstance(env_ids, torch.Tensor):\\n            env_ids = env_ids.tolist()\\n\\n        for value_index, env_id in enumerate(env_ids):\\n            self._episodes[env_id].success = success_values[value_index].item()\\n\\n    def record_pre_step(self) -> None:\\n        \"\"\"Trigger recorder terms for pre-step functions.\"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        for term in self._terms.values():\\n            key, value = term.record_pre_step()\\n            self.add_to_episodes(key, value)\\n\\n    def record_post_step(self) -> None:\\n        \"\"\"Trigger recorder terms for post-step functions.\"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        for term in self._terms.values():\\n            key, value = term.record_post_step()\\n            self.add_to_episodes(key, value)\\n\\n    def record_pre_reset(self, env_ids: Sequence[int] | None, force_export_or_skip=None) -> None:\\n        \"\"\"Trigger recorder terms for pre-reset functions.\\n\\n        Args:\\n            env_ids: The environment ids in which a reset is triggered.\\n        \"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        if env_ids is None:\\n            env_ids = list(range(self._env.num_envs))\\n        if isinstance(env_ids, torch.Tensor):\\n            env_ids = env_ids.tolist()\\n\\n        for term in self._terms.values():\\n            key, value = term.record_pre_reset(env_ids)\\n            self.add_to_episodes(key, value, env_ids)\\n\\n        # Set task success values for the relevant episodes\\n        success_results = torch.zeros(len(env_ids), dtype=bool, device=self._env.device)\\n        # Check success indicator from termination terms\\n        if hasattr(self._env, \"termination_manager\"):\\n            if \"success\" in self._env.termination_manager.active_terms:\\n                success_results |= self._env.termination_manager.get_term(\"success\")[env_ids]\\n        self.set_success_to_episodes(env_ids, success_results)\\n\\n        if force_export_or_skip or (force_export_or_skip is None and self.cfg.export_in_record_pre_reset):\\n            self.export_episodes(env_ids)\\n\\n    def record_post_reset(self, env_ids: Sequence[int] | None) -> None:\\n        \"\"\"Trigger recorder terms for post-reset functions.\\n\\n        Args:\\n            env_ids: The environment ids in which a reset is triggered.\\n        \"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        for term in self._terms.values():\\n            key, value = term.record_post_reset(env_ids)\\n            self.add_to_episodes(key, value, env_ids)\\n\\n    def export_episodes(self, env_ids: Sequence[int] | None = None) -> None:\\n        \"\"\"Concludes and exports the episodes for the given environment ids.\\n\\n        Args:\\n            env_ids: The environment ids. Defaults to None, in which case\\n                all environments are considered.\\n        \"\"\"\\n        # Do nothing if no active recorder terms are provided\\n        if len(self.active_terms) == 0:\\n            return\\n\\n        if env_ids is None:\\n            env_ids = list(range(self._env.num_envs))\\n        if isinstance(env_ids, torch.Tensor):\\n            env_ids = env_ids.tolist()\\n\\n        # Export episode data through dataset exporter\\n        need_to_flush = False\\n        for env_id in env_ids:\\n            if env_id in self._episodes and not self._episodes[env_id].is_empty():\\n                episode_succeeded = self._episodes[env_id].success\\n                target_dataset_file_handler = None\\n                if (self.cfg.dataset_export_mode == DatasetExportMode.EXPORT_ALL) or (\\n                    self.cfg.dataset_export_mode == DatasetExportMode.EXPORT_SUCCEEDED_ONLY and episode_succeeded\\n                ):\\n                    target_dataset_file_handler = self._dataset_file_handler\\n                elif self.cfg.dataset_export_mode == DatasetExportMode.EXPORT_SUCCEEDED_FAILED_IN_SEPARATE_FILES:\\n                    if episode_succeeded:\\n                        target_dataset_file_handler = self._dataset_file_handler\\n                    else:\\n                        target_dataset_file_handler = self._failed_episode_dataset_file_handler\\n                if target_dataset_file_handler is not None:\\n                    target_dataset_file_handler.write_episode(self._episodes[env_id])\\n                    need_to_flush = True\\n                # Update episode count\\n                if episode_succeeded:\\n                    self._exported_successful_episode_count[env_id] = (\\n                        self._exported_successful_episode_count.get(env_id, 0) + 1\\n                    )\\n                else:\\n                    self._exported_failed_episode_count[env_id] = self._exported_failed_episode_count.get(env_id, 0) + 1\\n            # Reset the episode buffer for the given environment after export\\n            self._episodes[env_id] = EpisodeData()\\n\\n        if need_to_flush:\\n            if self._dataset_file_handler is not None:\\n                self._dataset_file_handler.flush()\\n            if self._failed_episode_dataset_file_handler is not None:\\n                self._failed_episode_dataset_file_handler.flush()\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        \"\"\"Prepares a list of recorder terms.\"\"\"\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n        for term_name, term_cfg in cfg_items:\\n            # skip non-term settings\\n            if term_name in [\\n                \"dataset_file_handler_class_type\",\\n                \"dataset_filename\",\\n                \"dataset_export_dir_path\",\\n                \"dataset_export_mode\",\\n                \"export_in_record_pre_reset\",\\n            ]:\\n                continue\\n            # check if term config is None\\n            if term_cfg is None:\\n                continue\\n            # check valid type\\n            if not isinstance(term_cfg, RecorderTermCfg):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type RecorderTermCfg.\"\\n                    f\" Received: \\'{type(term_cfg)}\\'.\"\\n                )\\n            # create the recorder term\\n            term = term_cfg.class_type(term_cfg, self._env)\\n            # sanity check if term is valid type\\n            if not isinstance(term, RecorderTerm):\\n                raise TypeError(f\"Returned object for the term \\'{term_name}\\' is not of type RecorderTerm.\")\\n            # add term name and parameters\\n            self._term_names.append(term_name)\\n            self._terms[term_name] = term'),\n",
       " Document(metadata={}, page_content='class RewardManager(ManagerBase):\\n    \"\"\"Manager for computing reward signals for a given world.\\n\\n    The reward manager computes the total reward as a sum of the weighted reward terms. The reward\\n    terms are parsed from a nested config class containing the reward manger\\'s settings and reward\\n    terms configuration.\\n\\n    The reward terms are parsed from a config class containing the manager\\'s settings and each term\\'s\\n    parameters. Each reward term should instantiate the :class:`RewardTermCfg` class.\\n\\n    .. note::\\n\\n        The reward manager multiplies the reward term\\'s ``weight``  with the time-step interval ``dt``\\n        of the environment. This is done to ensure that the computed reward terms are balanced with\\n        respect to the chosen time-step interval in the environment.\\n\\n    \"\"\"\\n\\n    _env: ManagerBasedRLEnv\\n    \"\"\"The environment instance.\"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedRLEnv):\\n        \"\"\"Initialize the reward manager.\\n\\n        Args:\\n            cfg: The configuration object or dictionary (``dict[str, RewardTermCfg]``).\\n            env: The environment instance.\\n        \"\"\"\\n        # create buffers to parse and store terms\\n        self._term_names: list[str] = list()\\n        self._term_cfgs: list[RewardTermCfg] = list()\\n        self._class_term_cfgs: list[RewardTermCfg] = list()\\n\\n        # call the base class constructor (this will parse the terms config)\\n        super().__init__(cfg, env)\\n        # prepare extra info to store individual reward term information\\n        self._episode_sums = dict()\\n        for term_name in self._term_names:\\n            self._episode_sums[term_name] = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)\\n        # create buffer for managing reward per environment\\n        self._reward_buf = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)\\n\\n        # Buffer which stores the current step reward for each term for each environment\\n        self._step_reward = torch.zeros((self.num_envs, len(self._term_names)), dtype=torch.float, device=self.device)\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for reward manager.\"\"\"\\n        msg = f\"<RewardManager> contains {len(self._term_names)} active terms.\\\\n\"\\n\\n        # create table for term information\\n        table = PrettyTable()\\n        table.title = \"Active Reward Terms\"\\n        table.field_names = [\"Index\", \"Name\", \"Weight\"]\\n        # set alignment of table columns\\n        table.align[\"Name\"] = \"l\"\\n        table.align[\"Weight\"] = \"r\"\\n        # add info on each term\\n        for index, (name, term_cfg) in enumerate(zip(self._term_names, self._term_cfgs)):\\n            table.add_row([index, name, term_cfg.weight])\\n        # convert table to string\\n        msg += table.get_string()\\n        msg += \"\\\\n\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def active_terms(self) -> list[str]:\\n        \"\"\"Name of active reward terms.\"\"\"\\n        return self._term_names\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, torch.Tensor]:\\n        \"\"\"Returns the episodic sum of individual reward terms.\\n\\n        Args:\\n            env_ids: The environment ids for which the episodic sum of\\n                individual reward terms is to be returned. Defaults to all the environment ids.\\n\\n        Returns:\\n            Dictionary of episodic sum of individual reward terms.\\n        \"\"\"\\n        # resolve environment ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # store information\\n        extras = {}\\n        for key in self._episode_sums.keys():\\n            # store information\\n            # r_1 + r_2 + ... + r_n\\n            episodic_sum_avg = torch.mean(self._episode_sums[key][env_ids])\\n            extras[\"Episode_Reward/\" + key] = episodic_sum_avg / self._env.max_episode_length_s\\n            # reset episodic sum\\n            self._episode_sums[key][env_ids] = 0.0\\n        # reset all the reward terms\\n        for term_cfg in self._class_term_cfgs:\\n            term_cfg.func.reset(env_ids=env_ids)\\n        # return logged information\\n        return extras\\n\\n    def compute(self, dt: float) -> torch.Tensor:\\n        \"\"\"Computes the reward signal as a weighted sum of individual terms.\\n\\n        This function calls each reward term managed by the class and adds them to compute the net\\n        reward signal. It also updates the episodic sums corresponding to individual reward terms.\\n\\n        Args:\\n            dt: The time-step interval of the environment.\\n\\n        Returns:\\n            The net reward signal of shape (num_envs,).\\n        \"\"\"\\n        # reset computation\\n        self._reward_buf[:] = 0.0\\n        # iterate over all the reward terms\\n        for term_idx, (name, term_cfg) in enumerate(zip(self._term_names, self._term_cfgs)):\\n            # skip if weight is zero (kind of a micro-optimization)\\n            if term_cfg.weight == 0.0:\\n                self._step_reward[:, term_idx] = 0.0\\n                continue\\n            # compute term\\'s value\\n            value = term_cfg.func(self._env, **term_cfg.params) * term_cfg.weight * dt\\n            # update total reward\\n            self._reward_buf += value\\n            # update episodic sum\\n            self._episode_sums[name] += value\\n\\n            # Update current reward for this step.\\n            self._step_reward[:, term_idx] = value / dt\\n\\n        return self._reward_buf\\n\\n    \"\"\"\\n    Operations - Term settings.\\n    \"\"\"\\n\\n    def set_term_cfg(self, term_name: str, cfg: RewardTermCfg):\\n        \"\"\"Sets the configuration of the specified term into the manager.\\n\\n        Args:\\n            term_name: The name of the reward term.\\n            cfg: The configuration for the reward term.\\n\\n        Raises:\\n            ValueError: If the term name is not found.\\n        \"\"\"\\n        if term_name not in self._term_names:\\n            raise ValueError(f\"Reward term \\'{term_name}\\' not found.\")\\n        # set the configuration\\n        self._term_cfgs[self._term_names.index(term_name)] = cfg\\n\\n    def get_term_cfg(self, term_name: str) -> RewardTermCfg:\\n        \"\"\"Gets the configuration for the specified term.\\n\\n        Args:\\n            term_name: The name of the reward term.\\n\\n        Returns:\\n            The configuration of the reward term.\\n\\n        Raises:\\n            ValueError: If the term name is not found.\\n        \"\"\"\\n        if term_name not in self._term_names:\\n            raise ValueError(f\"Reward term \\'{term_name}\\' not found.\")\\n        # return the configuration\\n        return self._term_cfgs[self._term_names.index(term_name)]\\n\\n    def get_active_iterable_terms(self, env_idx: int) -> Sequence[tuple[str, Sequence[float]]]:\\n        \"\"\"Returns the active terms as iterable sequence of tuples.\\n\\n        The first element of the tuple is the name of the term and the second element is the raw value(s) of the term.\\n\\n        Args:\\n            env_idx: The specific environment to pull the active terms from.\\n\\n        Returns:\\n            The active terms.\\n        \"\"\"\\n        terms = []\\n        for idx, name in enumerate(self._term_names):\\n            terms.append((name, [self._step_reward[env_idx, idx].cpu().item()]))\\n        return terms\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n        # iterate over all the terms\\n        for term_name, term_cfg in cfg_items:\\n            # check for non config\\n            if term_cfg is None:\\n                continue\\n            # check for valid config type\\n            if not isinstance(term_cfg, RewardTermCfg):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type RewardTermCfg.\"\\n                    f\" Received: \\'{type(term_cfg)}\\'.\"\\n                )\\n            # check for valid weight type\\n            if not isinstance(term_cfg.weight, (float, int)):\\n                raise TypeError(\\n                    f\"Weight for the term \\'{term_name}\\' is not of type float or int.\"\\n                    f\" Received: \\'{type(term_cfg.weight)}\\'.\"\\n                )\\n            # resolve common parameters\\n            self._resolve_common_term_cfg(term_name, term_cfg, min_argc=1)\\n            # add function to list\\n            self._term_names.append(term_name)\\n            self._term_cfgs.append(term_cfg)\\n            # check if the term is a class\\n            if isinstance(term_cfg.func, ManagerTermBase):\\n                self._class_term_cfgs.append(term_cfg)'),\n",
       " Document(metadata={}, page_content='class SceneEntityCfg:\\n    \"\"\"Configuration for a scene entity that is used by the manager\\'s term.\\n\\n    This class is used to specify the name of the scene entity that is queried from the\\n    :class:`InteractiveScene` and passed to the manager\\'s term function.\\n    \"\"\"\\n\\n    name: str = MISSING\\n    \"\"\"The name of the scene entity.\\n\\n    This is the name defined in the scene configuration file. See the :class:`InteractiveSceneCfg`\\n    class for more details.\\n    \"\"\"\\n\\n    joint_names: str | list[str] | None = None\\n    \"\"\"The names of the joints from the scene entity. Defaults to None.\\n\\n    The names can be either joint names or a regular expression matching the joint names.\\n\\n    These are converted to joint indices on initialization of the manager and passed to the term\\n    function as a list of joint indices under :attr:`joint_ids`.\\n    \"\"\"\\n\\n    joint_ids: list[int] | slice = slice(None)\\n    \"\"\"The indices of the joints from the asset required by the term. Defaults to slice(None), which means\\n    all the joints in the asset (if present).\\n\\n    If :attr:`joint_names` is specified, this is filled in automatically on initialization of the\\n    manager.\\n    \"\"\"\\n\\n    fixed_tendon_names: str | list[str] | None = None\\n    \"\"\"The names of the fixed tendons from the scene entity. Defaults to None.\\n\\n    The names can be either joint names or a regular expression matching the joint names.\\n\\n    These are converted to fixed tendon indices on initialization of the manager and passed to the term\\n    function as a list of fixed tendon indices under :attr:`fixed_tendon_ids`.\\n    \"\"\"\\n\\n    fixed_tendon_ids: list[int] | slice = slice(None)\\n    \"\"\"The indices of the fixed tendons from the asset required by the term. Defaults to slice(None), which means\\n    all the fixed tendons in the asset (if present).\\n\\n    If :attr:`fixed_tendon_names` is specified, this is filled in automatically on initialization of the\\n    manager.\\n    \"\"\"\\n\\n    body_names: str | list[str] | None = None\\n    \"\"\"The names of the bodies from the asset required by the term. Defaults to None.\\n\\n    The names can be either body names or a regular expression matching the body names.\\n\\n    These are converted to body indices on initialization of the manager and passed to the term\\n    function as a list of body indices under :attr:`body_ids`.\\n    \"\"\"\\n\\n    body_ids: list[int] | slice = slice(None)\\n    \"\"\"The indices of the bodies from the asset required by the term. Defaults to slice(None), which means\\n    all the bodies in the asset.\\n\\n    If :attr:`body_names` is specified, this is filled in automatically on initialization of the\\n    manager.\\n    \"\"\"\\n\\n    object_collection_names: str | list[str] | None = None\\n    \"\"\"The names of the objects in the rigid object collection required by the term. Defaults to None.\\n\\n    The names can be either names or a regular expression matching the object names in the collection.\\n\\n    These are converted to object indices on initialization of the manager and passed to the term\\n    function as a list of object indices under :attr:`object_collection_ids`.\\n    \"\"\"\\n\\n    object_collection_ids: list[int] | slice = slice(None)\\n    \"\"\"The indices of the objects from the rigid object collection required by the term. Defaults to slice(None),\\n    which means all the objects in the collection.\\n\\n    If :attr:`object_collection_names` is specified, this is filled in automatically on initialization of the manager.\\n    \"\"\"\\n\\n    preserve_order: bool = False\\n    \"\"\"Whether to preserve indices ordering to match with that in the specified joint, body, or object collection names.\\n    Defaults to False.\\n\\n    If False, the ordering of the indices are sorted in ascending order (i.e. the ordering in the entity\\'s joints,\\n    bodies, or object in the object collection). Otherwise, the indices are preserved in the order of the specified\\n    joint, body, or object collection names.\\n\\n    For more details, see the :meth:`isaaclab.utils.string.resolve_matching_names` function.\\n\\n    .. note::\\n        This attribute is only used when :attr:`joint_names`, :attr:`body_names`, or :attr:`object_collection_names` are specified.\\n\\n    \"\"\"\\n\\n    def resolve(self, scene: InteractiveScene):\\n        \"\"\"Resolves the scene entity and converts the joint and body names to indices.\\n\\n        This function examines the scene entity from the :class:`InteractiveScene` and resolves the indices\\n        and names of the joints and bodies. It is an expensive operation as it resolves regular expressions\\n        and should be called only once.\\n\\n        Args:\\n            scene: The interactive scene instance.\\n\\n        Raises:\\n            ValueError: If the scene entity is not found.\\n            ValueError: If both ``joint_names`` and ``joint_ids`` are specified and are not consistent.\\n            ValueError: If both ``fixed_tendon_names`` and ``fixed_tendon_ids`` are specified and are not consistent.\\n            ValueError: If both ``body_names`` and ``body_ids`` are specified and are not consistent.\\n            ValueError: If both ``object_collection_names`` and ``object_collection_ids`` are specified and are not consistent.\\n        \"\"\"\\n        # check if the entity is valid\\n        if self.name not in scene.keys():\\n            raise ValueError(f\"The scene entity \\'{self.name}\\' does not exist. Available entities: {scene.keys()}.\")\\n\\n        # convert joint names to indices based on regex\\n        self._resolve_joint_names(scene)\\n\\n        # convert fixed tendon names to indices based on regex\\n        self._resolve_fixed_tendon_names(scene)\\n\\n        # convert body names to indices based on regex\\n        self._resolve_body_names(scene)\\n\\n        # convert object collection names to indices based on regex\\n        self._resolve_object_collection_names(scene)\\n\\n    def _resolve_joint_names(self, scene: InteractiveScene):\\n        # convert joint names to indices based on regex\\n        if self.joint_names is not None or self.joint_ids != slice(None):\\n            entity: Articulation = scene[self.name]\\n            # -- if both are not their default values, check if they are valid\\n            if self.joint_names is not None and self.joint_ids != slice(None):\\n                if isinstance(self.joint_names, str):\\n                    self.joint_names = [self.joint_names]\\n                if isinstance(self.joint_ids, int):\\n                    self.joint_ids = [self.joint_ids]\\n                joint_ids, _ = entity.find_joints(self.joint_names, preserve_order=self.preserve_order)\\n                joint_names = [entity.joint_names[i] for i in self.joint_ids]\\n                if joint_ids != self.joint_ids or joint_names != self.joint_names:\\n                    raise ValueError(\\n                        \"Both \\'joint_names\\' and \\'joint_ids\\' are specified, and are not consistent.\"\\n                        f\"\\\\n\\\\tfrom joint names: {self.joint_names} [{joint_ids}]\"\\n                        f\"\\\\n\\\\tfrom joint ids: {joint_names} [{self.joint_ids}]\"\\n                        \"\\\\nHint: Use either \\'joint_names\\' or \\'joint_ids\\' to avoid confusion.\"\\n                    )\\n            # -- from joint names to joint indices\\n            elif self.joint_names is not None:\\n                if isinstance(self.joint_names, str):\\n                    self.joint_names = [self.joint_names]\\n                self.joint_ids, _ = entity.find_joints(self.joint_names, preserve_order=self.preserve_order)\\n                # performance optimization (slice offers faster indexing than list of indices)\\n                # only all joint in the entity order are selected\\n                if len(self.joint_ids) == entity.num_joints and self.joint_names == entity.joint_names:\\n                    self.joint_ids = slice(None)\\n            # -- from joint indices to joint names\\n            elif self.joint_ids != slice(None):\\n                if isinstance(self.joint_ids, int):\\n                    self.joint_ids = [self.joint_ids]\\n                self.joint_names = [entity.joint_names[i] for i in self.joint_ids]\\n\\n    def _resolve_fixed_tendon_names(self, scene: InteractiveScene):\\n        # convert tendon names to indices based on regex\\n        if self.fixed_tendon_names is not None or self.fixed_tendon_ids != slice(None):\\n            entity: Articulation = scene[self.name]\\n            # -- if both are not their default values, check if they are valid\\n            if self.fixed_tendon_names is not None and self.fixed_tendon_ids != slice(None):\\n                if isinstance(self.fixed_tendon_names, str):\\n                    self.fixed_tendon_names = [self.fixed_tendon_names]\\n                if isinstance(self.fixed_tendon_ids, int):\\n                    self.fixed_tendon_ids = [self.fixed_tendon_ids]\\n                fixed_tendon_ids, _ = entity.find_fixed_tendons(\\n                    self.fixed_tendon_names, preserve_order=self.preserve_order\\n                )\\n                fixed_tendon_names = [entity.fixed_tendon_names[i] for i in self.fixed_tendon_ids]\\n                if fixed_tendon_ids != self.fixed_tendon_ids or fixed_tendon_names != self.fixed_tendon_names:\\n                    raise ValueError(\\n                        \"Both \\'fixed_tendon_names\\' and \\'fixed_tendon_ids\\' are specified, and are not consistent.\"\\n                        f\"\\\\n\\\\tfrom joint names: {self.fixed_tendon_names} [{fixed_tendon_ids}]\"\\n                        f\"\\\\n\\\\tfrom joint ids: {fixed_tendon_names} [{self.fixed_tendon_ids}]\"\\n                        \"\\\\nHint: Use either \\'fixed_tendon_names\\' or \\'fixed_tendon_ids\\' to avoid confusion.\"\\n                    )\\n            # -- from fixed tendon names to fixed tendon indices\\n            elif self.fixed_tendon_names is not None:\\n                if isinstance(self.fixed_tendon_names, str):\\n                    self.fixed_tendon_names = [self.fixed_tendon_names]\\n                self.fixed_tendon_ids, _ = entity.find_fixed_tendons(\\n                    self.fixed_tendon_names, preserve_order=self.preserve_order\\n                )\\n                # performance optimization (slice offers faster indexing than list of indices)\\n                # only all fixed tendon in the entity order are selected\\n                if (\\n                    len(self.fixed_tendon_ids) == entity.num_fixed_tendons\\n                    and self.fixed_tendon_names == entity.fixed_tendon_names\\n                ):\\n                    self.fixed_tendon_ids = slice(None)\\n            # -- from fixed tendon indices to fixed tendon names\\n            elif self.fixed_tendon_ids != slice(None):\\n                if isinstance(self.fixed_tendon_ids, int):\\n                    self.fixed_tendon_ids = [self.fixed_tendon_ids]\\n                self.fixed_tendon_names = [entity.fixed_tendon_names[i] for i in self.fixed_tendon_ids]\\n\\n    def _resolve_body_names(self, scene: InteractiveScene):\\n        # convert body names to indices based on regex\\n        if self.body_names is not None or self.body_ids != slice(None):\\n            entity: RigidObject = scene[self.name]\\n            # -- if both are not their default values, check if they are valid\\n            if self.body_names is not None and self.body_ids != slice(None):\\n                if isinstance(self.body_names, str):\\n                    self.body_names = [self.body_names]\\n                if isinstance(self.body_ids, int):\\n                    self.body_ids = [self.body_ids]\\n                body_ids, _ = entity.find_bodies(self.body_names, preserve_order=self.preserve_order)\\n                body_names = [entity.body_names[i] for i in self.body_ids]\\n                if body_ids != self.body_ids or body_names != self.body_names:\\n                    raise ValueError(\\n                        \"Both \\'body_names\\' and \\'body_ids\\' are specified, and are not consistent.\"\\n                        f\"\\\\n\\\\tfrom body names: {self.body_names} [{body_ids}]\"\\n                        f\"\\\\n\\\\tfrom body ids: {body_names} [{self.body_ids}]\"\\n                        \"\\\\nHint: Use either \\'body_names\\' or \\'body_ids\\' to avoid confusion.\"\\n                    )\\n            # -- from body names to body indices\\n            elif self.body_names is not None:\\n                if isinstance(self.body_names, str):\\n                    self.body_names = [self.body_names]\\n                self.body_ids, _ = entity.find_bodies(self.body_names, preserve_order=self.preserve_order)\\n                # performance optimization (slice offers faster indexing than list of indices)\\n                # only all bodies in the entity order are selected\\n                if len(self.body_ids) == entity.num_bodies and self.body_names == entity.body_names:\\n                    self.body_ids = slice(None)\\n            # -- from body indices to body names\\n            elif self.body_ids != slice(None):\\n                if isinstance(self.body_ids, int):\\n                    self.body_ids = [self.body_ids]\\n                self.body_names = [entity.body_names[i] for i in self.body_ids]\\n\\n    def _resolve_object_collection_names(self, scene: InteractiveScene):\\n        # convert object names to indices based on regex\\n        if self.object_collection_names is not None or self.object_collection_ids != slice(None):\\n            entity: RigidObjectCollection = scene[self.name]\\n            # -- if both are not their default values, check if they are valid\\n            if self.object_collection_names is not None and self.object_collection_ids != slice(None):\\n                if isinstance(self.object_collection_names, str):\\n                    self.object_collection_names = [self.object_collection_names]\\n                if isinstance(self.object_collection_ids, int):\\n                    self.object_collection_ids = [self.object_collection_ids]\\n                object_ids, _ = entity.find_objects(self.object_collection_names, preserve_order=self.preserve_order)\\n                object_names = [entity.object_names[i] for i in self.object_collection_ids]\\n                if object_ids != self.object_collection_ids or object_names != self.object_collection_names:\\n                    raise ValueError(\\n                        \"Both \\'object_collection_names\\' and \\'object_collection_ids\\' are specified, and are not\"\\n                        \" consistent.\\\\n\\\\tfrom object collection names:\"\\n                        f\" {self.object_collection_names} [{object_ids}]\\\\n\\\\tfrom object collection ids:\"\\n                        f\" {object_names} [{self.object_collection_ids}]\\\\nHint: Use either \\'object_collection_names\\' or\"\\n                        \" \\'object_collection_ids\\' to avoid confusion.\"\\n                    )\\n            # -- from object names to object indices\\n            elif self.object_collection_names is not None:\\n                if isinstance(self.object_collection_names, str):\\n                    self.object_collection_names = [self.object_collection_names]\\n                self.object_collection_ids, _ = entity.find_objects(\\n                    self.object_collection_names, preserve_order=self.preserve_order\\n                )\\n            # -- from object indices to object names\\n            elif self.object_collection_ids != slice(None):\\n                if isinstance(self.object_collection_ids, int):\\n                    self.object_collection_ids = [self.object_collection_ids]\\n                self.object_collection_names = [entity.object_names[i] for i in self.object_collection_ids]'),\n",
       " Document(metadata={}, page_content='class TerminationManager(ManagerBase):\\n    \"\"\"Manager for computing done signals for a given world.\\n\\n    The termination manager computes the termination signal (also called dones) as a combination\\n    of termination terms. Each termination term is a function which takes the environment as an\\n    argument and returns a boolean tensor of shape (num_envs,). The termination manager\\n    computes the termination signal as the union (logical or) of all the termination terms.\\n\\n    Following the `Gymnasium API <https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/>`_,\\n    the termination signal is computed as the logical OR of the following signals:\\n\\n    * **Time-out**: This signal is set to true if the environment has ended after an externally defined condition\\n      (that is outside the scope of a MDP). For example, the environment may be terminated if the episode has\\n      timed out (i.e. reached max episode length).\\n    * **Terminated**: This signal is set to true if the environment has reached a terminal state defined by the\\n      environment. This state may correspond to task success, task failure, robot falling, etc.\\n\\n    These signals can be individually accessed using the :attr:`time_outs` and :attr:`terminated` properties.\\n\\n    The termination terms are parsed from a config class containing the manager\\'s settings and each term\\'s\\n    parameters. Each termination term should instantiate the :class:`TerminationTermCfg` class. The term\\'s\\n    configuration :attr:`TerminationTermCfg.time_out` decides whether the term is a timeout or a termination term.\\n    \"\"\"\\n\\n    _env: ManagerBasedRLEnv\\n    \"\"\"The environment instance.\"\"\"\\n\\n    def __init__(self, cfg: object, env: ManagerBasedRLEnv):\\n        \"\"\"Initializes the termination manager.\\n\\n        Args:\\n            cfg: The configuration object or dictionary (``dict[str, TerminationTermCfg]``).\\n            env: An environment object.\\n        \"\"\"\\n        # create buffers to parse and store terms\\n        self._term_names: list[str] = list()\\n        self._term_cfgs: list[TerminationTermCfg] = list()\\n        self._class_term_cfgs: list[TerminationTermCfg] = list()\\n\\n        # call the base class constructor (this will parse the terms config)\\n        super().__init__(cfg, env)\\n        # prepare extra info to store individual termination term information\\n        self._term_dones = dict()\\n        for term_name in self._term_names:\\n            self._term_dones[term_name] = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)\\n        # create buffer for managing termination per environment\\n        self._truncated_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)\\n        self._terminated_buf = torch.zeros_like(self._truncated_buf)\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string representation for termination manager.\"\"\"\\n        msg = f\"<TerminationManager> contains {len(self._term_names)} active terms.\\\\n\"\\n\\n        # create table for term information\\n        table = PrettyTable()\\n        table.title = \"Active Termination Terms\"\\n        table.field_names = [\"Index\", \"Name\", \"Time Out\"]\\n        # set alignment of table columns\\n        table.align[\"Name\"] = \"l\"\\n        # add info on each term\\n        for index, (name, term_cfg) in enumerate(zip(self._term_names, self._term_cfgs)):\\n            table.add_row([index, name, term_cfg.time_out])\\n        # convert table to string\\n        msg += table.get_string()\\n        msg += \"\\\\n\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def active_terms(self) -> list[str]:\\n        \"\"\"Name of active termination terms.\"\"\"\\n        return self._term_names\\n\\n    @property\\n    def dones(self) -> torch.Tensor:\\n        \"\"\"The net termination signal. Shape is (num_envs,).\"\"\"\\n        return self._truncated_buf | self._terminated_buf\\n\\n    @property\\n    def time_outs(self) -> torch.Tensor:\\n        \"\"\"The timeout signal (reaching max episode length). Shape is (num_envs,).\\n\\n        This signal is set to true if the environment has ended after an externally defined condition\\n        (that is outside the scope of a MDP). For example, the environment may be terminated if the episode has\\n        timed out (i.e. reached max episode length).\\n        \"\"\"\\n        return self._truncated_buf\\n\\n    @property\\n    def terminated(self) -> torch.Tensor:\\n        \"\"\"The terminated signal (reaching a terminal state). Shape is (num_envs,).\\n\\n        This signal is set to true if the environment has reached a terminal state defined by the environment.\\n        This state may correspond to task success, task failure, robot falling, etc.\\n        \"\"\"\\n        return self._terminated_buf\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, torch.Tensor]:\\n        \"\"\"Returns the episodic counts of individual termination terms.\\n\\n        Args:\\n            env_ids: The environment ids. Defaults to None, in which case\\n                all environments are considered.\\n\\n        Returns:\\n            Dictionary of episodic sum of individual reward terms.\\n        \"\"\"\\n        # resolve environment ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # add to episode dict\\n        extras = {}\\n        for key in self._term_dones.keys():\\n            # store information\\n            extras[\"Episode_Termination/\" + key] = torch.count_nonzero(self._term_dones[key][env_ids]).item()\\n        # reset all the reward terms\\n        for term_cfg in self._class_term_cfgs:\\n            term_cfg.func.reset(env_ids=env_ids)\\n        # return logged information\\n        return extras\\n\\n    def compute(self) -> torch.Tensor:\\n        \"\"\"Computes the termination signal as union of individual terms.\\n\\n        This function calls each termination term managed by the class and performs a logical OR operation\\n        to compute the net termination signal.\\n\\n        Returns:\\n            The combined termination signal of shape (num_envs,).\\n        \"\"\"\\n        # reset computation\\n        self._truncated_buf[:] = False\\n        self._terminated_buf[:] = False\\n        # iterate over all the termination terms\\n        for name, term_cfg in zip(self._term_names, self._term_cfgs):\\n            value = term_cfg.func(self._env, **term_cfg.params)\\n            # store timeout signal separately\\n            if term_cfg.time_out:\\n                self._truncated_buf |= value\\n            else:\\n                self._terminated_buf |= value\\n            # add to episode dones\\n            self._term_dones[name][:] = value\\n        # return combined termination signal\\n        return self._truncated_buf | self._terminated_buf\\n\\n    def get_term(self, name: str) -> torch.Tensor:\\n        \"\"\"Returns the termination term with the specified name.\\n\\n        Args:\\n            name: The name of the termination term.\\n\\n        Returns:\\n            The corresponding termination term value. Shape is (num_envs,).\\n        \"\"\"\\n        return self._term_dones[name]\\n\\n    def get_active_iterable_terms(self, env_idx: int) -> Sequence[tuple[str, Sequence[float]]]:\\n        \"\"\"Returns the active terms as iterable sequence of tuples.\\n\\n        The first element of the tuple is the name of the term and the second element is the raw value(s) of the term.\\n\\n        Args:\\n            env_idx: The specific environment to pull the active terms from.\\n\\n        Returns:\\n            The active terms.\\n        \"\"\"\\n        terms = []\\n        for key in self._term_dones.keys():\\n            terms.append((key, [self._term_dones[key][env_idx].float().cpu().item()]))\\n        return terms\\n\\n    \"\"\"\\n    Operations - Term settings.\\n    \"\"\"\\n\\n    def set_term_cfg(self, term_name: str, cfg: TerminationTermCfg):\\n        \"\"\"Sets the configuration of the specified term into the manager.\\n\\n        Args:\\n            term_name: The name of the termination term.\\n            cfg: The configuration for the termination term.\\n\\n        Raises:\\n            ValueError: If the term name is not found.\\n        \"\"\"\\n        if term_name not in self._term_names:\\n            raise ValueError(f\"Termination term \\'{term_name}\\' not found.\")\\n        # set the configuration\\n        self._term_cfgs[self._term_names.index(term_name)] = cfg\\n\\n    def get_term_cfg(self, term_name: str) -> TerminationTermCfg:\\n        \"\"\"Gets the configuration for the specified term.\\n\\n        Args:\\n            term_name: The name of the termination term.\\n\\n        Returns:\\n            The configuration of the termination term.\\n\\n        Raises:\\n            ValueError: If the term name is not found.\\n        \"\"\"\\n        if term_name not in self._term_names:\\n            raise ValueError(f\"Termination term \\'{term_name}\\' not found.\")\\n        # return the configuration\\n        return self._term_cfgs[self._term_names.index(term_name)]\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _prepare_terms(self):\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n        # iterate over all the terms\\n        for term_name, term_cfg in cfg_items:\\n            # check for non config\\n            if term_cfg is None:\\n                continue\\n            # check for valid config type\\n            if not isinstance(term_cfg, TerminationTermCfg):\\n                raise TypeError(\\n                    f\"Configuration for the term \\'{term_name}\\' is not of type TerminationTermCfg.\"\\n                    f\" Received: \\'{type(term_cfg)}\\'.\"\\n                )\\n            # resolve common parameters\\n            self._resolve_common_term_cfg(term_name, term_cfg, min_argc=1)\\n            # add function to list\\n            self._term_names.append(term_name)\\n            self._term_cfgs.append(term_cfg)\\n            # check if the term is a class\\n            if isinstance(term_cfg.func, ManagerTermBase):\\n                self._class_term_cfgs.append(term_cfg)'),\n",
       " Document(metadata={}, page_content='class VisualizationMarkersCfg:\\n    \"\"\"A class to configure a :class:`VisualizationMarkers`.\"\"\"\\n\\n    prim_path: str = MISSING\\n    \"\"\"The prim path where the :class:`UsdGeom.PointInstancer` will be created.\"\"\"\\n\\n    markers: dict[str, SpawnerCfg] = MISSING\\n    \"\"\"The dictionary of marker configurations.\\n\\n    The key is the name of the marker, and the value is the configuration of the marker.\\n    The key is used to identify the marker in the class.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class VisualizationMarkers:\\n    \"\"\"A class to coordinate groups of visual markers (loaded from USD).\\n\\n    This class allows visualization of different UI markers in the scene, such as points and frames.\\n    The class wraps around the `UsdGeom.PointInstancer`_ for efficient handling of objects\\n    in the stage via instancing the created marker prototype prims.\\n\\n    A marker prototype prim is a reusable template prim used for defining variations of objects\\n    in the scene. For example, a sphere prim can be used as a marker prototype prim to create\\n    multiple sphere prims in the scene at different locations. Thus, prototype prims are useful\\n    for creating multiple instances of the same prim in the scene.\\n\\n    The class parses the configuration to create different the marker prototypes into the stage. Each marker\\n    prototype prim is created as a child of the :class:`UsdGeom.PointInstancer` prim. The prim path for the\\n    the marker prim is resolved using the key of the marker in the :attr:`VisualizationMarkersCfg.markers`\\n    dictionary. The marker prototypes are created using the :meth:`isaacsim.core.utils.create_prim`\\n    function, and then then instanced using :class:`UsdGeom.PointInstancer` prim to allow creating multiple\\n    instances of the marker prims.\\n\\n    Switching between different marker prototypes is possible by calling the :meth:`visualize` method with\\n    the prototype indices corresponding to the marker prototype. The prototype indices are based on the order\\n    in the :attr:`VisualizationMarkersCfg.markers` dictionary. For example, if the dictionary has two markers,\\n    \"marker1\" and \"marker2\", then their prototype indices are 0 and 1 respectively. The prototype indices\\n    can be passed as a list or array of integers.\\n\\n    Usage:\\n        The following snippet shows how to create 24 sphere markers with a radius of 1.0 at random translations\\n        within the range [-1.0, 1.0]. The first 12 markers will be colored red and the rest will be colored green.\\n\\n        .. code-block:: python\\n\\n            import isaaclab.sim as sim_utils\\n            from isaaclab.markers import VisualizationMarkersCfg, VisualizationMarkers\\n\\n            # Create the markers configuration\\n            # This creates two marker prototypes, \"marker1\" and \"marker2\" which are spheres with a radius of 1.0.\\n            # The color of \"marker1\" is red and the color of \"marker2\" is green.\\n            cfg = VisualizationMarkersCfg(\\n                prim_path=\"/World/Visuals/testMarkers\",\\n                markers={\\n                    \"marker1\": sim_utils.SphereCfg(\\n                        radius=1.0,\\n                        visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0)),\\n                    ),\\n                    \"marker2\": VisualizationMarkersCfg.SphereCfg(\\n                        radius=1.0,\\n                        visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0)),\\n                    ),\\n                }\\n            )\\n            # Create the markers instance\\n            # This will create a UsdGeom.PointInstancer prim at the given path along with the marker prototypes.\\n            marker = VisualizationMarkers(cfg)\\n\\n            # Set position of the marker\\n            # -- randomly sample translations between -1.0 and 1.0\\n            marker_translations = np.random.uniform(-1.0, 1.0, (24, 3))\\n            # -- this will create 24 markers at the given translations\\n            # note: the markers will all be `marker1` since the marker indices are not given\\n            marker.visualize(translations=marker_translations)\\n\\n            # alter the markers based on their prototypes indices\\n            # first 12 markers will be marker1 and the rest will be marker2\\n            # 0 -> marker1, 1 -> marker2\\n            marker_indices = [0] * 12 + [1] * 12\\n            # this will change the marker prototypes at the given indices\\n            # note: the translations of the markers will not be changed from the previous call\\n            #  since the translations are not given.\\n            marker.visualize(marker_indices=marker_indices)\\n\\n            # alter the markers based on their prototypes indices and translations\\n            marker.visualize(marker_indices=marker_indices, translations=marker_translations)\\n\\n    .. _UsdGeom.PointInstancer: https://graphics.pixar.com/usd/dev/api/class_usd_geom_point_instancer.html\\n\\n    \"\"\"\\n\\n    def __init__(self, cfg: VisualizationMarkersCfg):\\n        \"\"\"Initialize the class.\\n\\n        When the class is initialized, the :class:`UsdGeom.PointInstancer` is created into the stage\\n        and the marker prims are registered into it.\\n\\n        .. note::\\n            If a prim already exists at the given path, the function will find the next free path\\n            and create the :class:`UsdGeom.PointInstancer` prim there.\\n\\n        Args:\\n            cfg: The configuration for the markers.\\n\\n        Raises:\\n            ValueError: When no markers are provided in the :obj:`cfg`.\\n        \"\"\"\\n        # get next free path for the prim\\n        prim_path = stage_utils.get_next_free_path(cfg.prim_path)\\n        # create a new prim\\n        stage = stage_utils.get_current_stage()\\n        self._instancer_manager = UsdGeom.PointInstancer.Define(stage, prim_path)\\n        # store inputs\\n        self.prim_path = prim_path\\n        self.cfg = cfg\\n        # check if any markers is provided\\n        if len(self.cfg.markers) == 0:\\n            raise ValueError(f\"The `cfg.markers` cannot be empty. Received: {self.cfg.markers}\")\\n\\n        # create a child prim for the marker\\n        self._add_markers_prototypes(self.cfg.markers)\\n        # Note: We need to do this the first time to initialize the instancer.\\n        #   Otherwise, the instancer will not be \"created\" and the function `GetInstanceIndices()` will fail.\\n        self._instancer_manager.GetProtoIndicesAttr().Set(list(range(self.num_prototypes)))\\n        self._instancer_manager.GetPositionsAttr().Set([Gf.Vec3f(0.0)] * self.num_prototypes)\\n        self._count = self.num_prototypes\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return: A string representation of the class.\"\"\"\\n        msg = f\"VisualizationMarkers(prim_path={self.prim_path})\"\\n        msg += f\"\\\\n\\\\tCount: {self.count}\"\\n        msg += f\"\\\\n\\\\tNumber of prototypes: {self.num_prototypes}\"\\n        msg += \"\\\\n\\\\tMarkers Prototypes:\"\\n        for index, (name, marker) in enumerate(self.cfg.markers.items()):\\n            msg += f\"\\\\n\\\\t\\\\t[Index: {index}]: {name}: {marker.to_dict()}\"\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def num_prototypes(self) -> int:\\n        \"\"\"The number of marker prototypes available.\"\"\"\\n        return len(self.cfg.markers)\\n\\n    @property\\n    def count(self) -> int:\\n        \"\"\"The total number of marker instances.\"\"\"\\n        # TODO: Update this when the USD API is available (Isaac Sim 2023.1)\\n        # return self._instancer_manager.GetInstanceCount()\\n        return self._count\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def set_visibility(self, visible: bool):\\n        \"\"\"Sets the visibility of the markers.\\n\\n        The method does this through the USD API.\\n\\n        Args:\\n            visible: flag to set the visibility.\\n        \"\"\"\\n        imageable = UsdGeom.Imageable(self._instancer_manager)\\n        if visible:\\n            imageable.MakeVisible()\\n        else:\\n            imageable.MakeInvisible()\\n\\n    def is_visible(self) -> bool:\\n        \"\"\"Checks the visibility of the markers.\\n\\n        Returns:\\n            True if the markers are visible, False otherwise.\\n        \"\"\"\\n        return self._instancer_manager.GetVisibilityAttr().Get() != UsdGeom.Tokens.invisible\\n\\n    def visualize(\\n        self,\\n        translations: np.ndarray | torch.Tensor | None = None,\\n        orientations: np.ndarray | torch.Tensor | None = None,\\n        scales: np.ndarray | torch.Tensor | None = None,\\n        marker_indices: list[int] | np.ndarray | torch.Tensor | None = None,\\n    ):\\n        \"\"\"Update markers in the viewport.\\n\\n        .. note::\\n            If the prim `PointInstancer` is hidden in the stage, the function will simply return\\n            without updating the markers. This helps in unnecessary computation when the markers\\n            are not visible.\\n\\n        Whenever updating the markers, the input arrays must have the same number of elements\\n        in the first dimension. If the number of elements is different, the `UsdGeom.PointInstancer`\\n        will raise an error complaining about the mismatch.\\n\\n        Additionally, the function supports dynamic update of the markers. This means that the\\n        number of markers can change between calls. For example, if you have 24 points that you\\n        want to visualize, you can pass 24 translations, orientations, and scales. If you want to\\n        visualize only 12 points, you can pass 12 translations, orientations, and scales. The\\n        function will automatically update the number of markers in the scene.\\n\\n        The function will also update the marker prototypes based on their prototype indices. For instance,\\n        if you have two marker prototypes, and you pass the following marker indices: [0, 1, 0, 1], the function\\n        will update the first and third markers with the first prototype, and the second and fourth markers\\n        with the second prototype. This is useful when you want to visualize different markers in the same\\n        scene. The list of marker indices must have the same number of elements as the translations, orientations,\\n        or scales. If the number of elements is different, the function will raise an error.\\n\\n        .. caution::\\n            This function will update all the markers instanced from the prototypes. That means\\n            if you have 24 markers, you will need to pass 24 translations, orientations, and scales.\\n\\n            If you want to update only a subset of the markers, you will need to handle the indices\\n            yourself and pass the complete arrays to this function.\\n\\n        Args:\\n            translations: Translations w.r.t. parent prim frame. Shape is (M, 3).\\n                Defaults to None, which means left unchanged.\\n            orientations: Quaternion orientations (w, x, y, z) w.r.t. parent prim frame. Shape is (M, 4).\\n                Defaults to None, which means left unchanged.\\n            scales: Scale applied before any rotation is applied. Shape is (M, 3).\\n                Defaults to None, which means left unchanged.\\n            marker_indices: Decides which marker prototype to visualize. Shape is (M).\\n                Defaults to None, which means left unchanged provided that the total number of markers\\n                is the same as the previous call. If the number of markers is different, the function\\n                will update the number of markers in the scene.\\n\\n        Raises:\\n            ValueError: When input arrays do not follow the expected shapes.\\n            ValueError: When the function is called with all None arguments.\\n        \"\"\"\\n        # check if it is visible (if not then let\\'s not waste time)\\n        if not self.is_visible():\\n            return\\n        # check if we have any markers to visualize\\n        num_markers = 0\\n        # resolve inputs\\n        # -- position\\n        if translations is not None:\\n            if isinstance(translations, torch.Tensor):\\n                translations = translations.detach().cpu().numpy()\\n            # check that shape is correct\\n            if translations.shape[1] != 3 or len(translations.shape) != 2:\\n                raise ValueError(f\"Expected `translations` to have shape (M, 3). Received: {translations.shape}.\")\\n            # apply translations\\n            self._instancer_manager.GetPositionsAttr().Set(Vt.Vec3fArray.FromNumpy(translations))\\n            # update number of markers\\n            num_markers = translations.shape[0]\\n        # -- orientation\\n        if orientations is not None:\\n            if isinstance(orientations, torch.Tensor):\\n                orientations = orientations.detach().cpu().numpy()\\n            # check that shape is correct\\n            if orientations.shape[1] != 4 or len(orientations.shape) != 2:\\n                raise ValueError(f\"Expected `orientations` to have shape (M, 4). Received: {orientations.shape}.\")\\n            # roll orientations from (w, x, y, z) to (x, y, z, w)\\n            # internally USD expects (x, y, z, w)\\n            orientations = convert_quat(orientations, to=\"xyzw\")\\n            # apply orientations\\n            self._instancer_manager.GetOrientationsAttr().Set(Vt.QuathArray.FromNumpy(orientations))\\n            # update number of markers\\n            num_markers = orientations.shape[0]\\n        # -- scales\\n        if scales is not None:\\n            if isinstance(scales, torch.Tensor):\\n                scales = scales.detach().cpu().numpy()\\n            # check that shape is correct\\n            if scales.shape[1] != 3 or len(scales.shape) != 2:\\n                raise ValueError(f\"Expected `scales` to have shape (M, 3). Received: {scales.shape}.\")\\n            # apply scales\\n            self._instancer_manager.GetScalesAttr().Set(Vt.Vec3fArray.FromNumpy(scales))\\n            # update number of markers\\n            num_markers = scales.shape[0]\\n        # -- status\\n        if marker_indices is not None or num_markers != self._count:\\n            # apply marker indices\\n            if marker_indices is not None:\\n                if isinstance(marker_indices, torch.Tensor):\\n                    marker_indices = marker_indices.detach().cpu().numpy()\\n                elif isinstance(marker_indices, list):\\n                    marker_indices = np.array(marker_indices)\\n                # check that shape is correct\\n                if len(marker_indices.shape) != 1:\\n                    raise ValueError(f\"Expected `marker_indices` to have shape (M,). Received: {marker_indices.shape}.\")\\n                # apply proto indices\\n                self._instancer_manager.GetProtoIndicesAttr().Set(Vt.IntArray.FromNumpy(marker_indices))\\n                # update number of markers\\n                num_markers = marker_indices.shape[0]\\n            else:\\n                # check that number of markers is not zero\\n                if num_markers == 0:\\n                    raise ValueError(\"Number of markers cannot be zero! Hint: The function was called with no inputs?\")\\n                # set all markers to be the first prototype\\n                self._instancer_manager.GetProtoIndicesAttr().Set([0] * num_markers)\\n        # set number of markers\\n        self._count = num_markers\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _add_markers_prototypes(self, markers_cfg: dict[str, sim_utils.SpawnerCfg]):\\n        \"\"\"Adds markers prototypes to the scene and sets the markers instancer to use them.\"\"\"\\n        # add markers based on config\\n        for name, cfg in markers_cfg.items():\\n            # resolve prim path\\n            marker_prim_path = f\"{self.prim_path}/{name}\"\\n            # create a child prim for the marker\\n            marker_prim = cfg.func(prim_path=marker_prim_path, cfg=cfg)\\n            # make the asset uninstanceable (in case it is)\\n            # point instancer defines its own prototypes so if an asset is already instanced, this doesn\\'t work.\\n            self._process_prototype_prim(marker_prim)\\n            # add child reference to point instancer\\n            self._instancer_manager.GetPrototypesRel().AddTarget(marker_prim_path)\\n        # check that we loaded all the prototypes\\n        prototypes = self._instancer_manager.GetPrototypesRel().GetTargets()\\n        if len(prototypes) != len(markers_cfg):\\n            raise RuntimeError(\\n                f\"Failed to load all the prototypes. Expected: {len(markers_cfg)}. Received: {len(prototypes)}.\"\\n            )\\n\\n    def _process_prototype_prim(self, prim: Usd.Prim):\\n        \"\"\"Process a prim and its descendants to make them suitable for defining prototypes.\\n\\n        Point instancer defines its own prototypes so if an asset is already instanced, this doesn\\'t work.\\n        This function checks if the prim at the specified prim path and its descendants are instanced.\\n        If so, it makes the respective prim uninstanceable by disabling instancing on the prim.\\n\\n        Additionally, it makes the prim invisible to secondary rays. This is useful when we do not want\\n        to see the marker prims on camera images.\\n\\n        Args:\\n            prim_path: The prim path to check.\\n            stage: The stage where the prim exists.\\n                Defaults to None, in which case the current stage is used.\\n        \"\"\"\\n        # check if prim is valid\\n        if not prim.IsValid():\\n            raise ValueError(f\"Prim at path \\'{prim.GetPrimAtPath()}\\' is not valid.\")\\n        # iterate over all prims under prim-path\\n        all_prims = [prim]\\n        while len(all_prims) > 0:\\n            # get current prim\\n            child_prim = all_prims.pop(0)\\n            # check if it is physics body -> if so, remove it\\n            if child_prim.HasAPI(UsdPhysics.ArticulationRootAPI):\\n                child_prim.RemoveAPI(UsdPhysics.ArticulationRootAPI)\\n                child_prim.RemoveAPI(PhysxSchema.PhysxArticulationAPI)\\n            if child_prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n                child_prim.RemoveAPI(UsdPhysics.RigidBodyAPI)\\n                child_prim.RemoveAPI(PhysxSchema.PhysxRigidBodyAPI)\\n            if child_prim.IsA(UsdPhysics.Joint):\\n                child_prim.GetAttribute(\"physics:jointEnabled\").Set(False)\\n            # check if prim is instanced -> if so, make it uninstanceable\\n            if child_prim.IsInstance():\\n                child_prim.SetInstanceable(False)\\n            # check if prim is a mesh -> if so, make it invisible to secondary rays\\n            if child_prim.IsA(UsdGeom.Gprim):\\n                # invisible to secondary rays such as depth images\\n                omni.kit.commands.execute(\\n                    \"ChangePropertyCommand\",\\n                    prop_path=Sdf.Path(f\"{child_prim.GetPrimPath().pathString}.primvars:invisibleToSecondaryRays\"),\\n                    value=True,\\n                    prev=None,\\n                    type_to_create_if_not_exist=Sdf.ValueTypeNames.Bool,\\n                )\\n            # add children to list\\n            all_prims += child_prim.GetChildren()\\n\\n        # remove any physics on the markers because they are only for visualization!\\n        physx_utils.removeRigidBodySubtree(prim)'),\n",
       " Document(metadata={}, page_content='class InteractiveScene:\\n    \"\"\"A scene that contains entities added to the simulation.\\n\\n    The interactive scene parses the :class:`InteractiveSceneCfg` class to create the scene.\\n    Based on the specified number of environments, it clones the entities and groups them into different\\n    categories (e.g., articulations, sensors, etc.).\\n\\n    Cloning can be performed in two ways:\\n\\n    * For tasks where all environments contain the same assets, a more performant cloning paradigm\\n      can be used to allow for faster environment creation. This is specified by the ``replicate_physics`` flag.\\n\\n      .. code-block:: python\\n\\n          scene = InteractiveScene(cfg=InteractiveSceneCfg(replicate_physics=True))\\n\\n    * For tasks that require having separate assets in the environments, ``replicate_physics`` would have to\\n      be set to False, which will add some costs to the overall startup time.\\n\\n      .. code-block:: python\\n\\n          scene = InteractiveScene(cfg=InteractiveSceneCfg(replicate_physics=False))\\n\\n    Each entity is registered to scene based on its name in the configuration class. For example, if the user\\n    specifies a robot in the configuration class as follows:\\n\\n    .. code-block:: python\\n\\n        from isaaclab.scene import InteractiveSceneCfg\\n        from isaaclab.utils import configclass\\n\\n        from isaaclab_assets.robots.anymal import ANYMAL_C_CFG\\n\\n        @configclass\\n        class MySceneCfg(InteractiveSceneCfg):\\n\\n            robot = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\\n\\n    Then the robot can be accessed from the scene as follows:\\n\\n    .. code-block:: python\\n\\n        from isaaclab.scene import InteractiveScene\\n\\n        # create 128 environments\\n        scene = InteractiveScene(cfg=MySceneCfg(num_envs=128))\\n\\n        # access the robot from the scene\\n        robot = scene[\"robot\"]\\n        # access the robot based on its type\\n        robot = scene.articulations[\"robot\"]\\n\\n    If the :class:`InteractiveSceneCfg` class does not include asset entities, the cloning process\\n    can still be triggered if assets were added to the stage outside of the :class:`InteractiveScene` class:\\n\\n    .. code-block:: python\\n\\n        scene = InteractiveScene(cfg=InteractiveSceneCfg(num_envs=128, replicate_physics=True))\\n        scene.clone_environments()\\n\\n    .. note::\\n        It is important to note that the scene only performs common operations on the entities. For example,\\n        resetting the internal buffers, writing the buffers to the simulation and updating the buffers from the\\n        simulation. The scene does not perform any task specific to the entity. For example, it does not apply\\n        actions to the robot or compute observations from the robot. These tasks are handled by different\\n        modules called \"managers\" in the framework. Please refer to the :mod:`isaaclab.managers` sub-package\\n        for more details.\\n    \"\"\"\\n\\n    def __init__(self, cfg: InteractiveSceneCfg):\\n        \"\"\"Initializes the scene.\\n\\n        Args:\\n            cfg: The configuration class for the scene.\\n        \"\"\"\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs\\n        self.cfg = cfg\\n        # initialize scene elements\\n        self._terrain = None\\n        self._articulations = dict()\\n        self._deformable_objects = dict()\\n        self._rigid_objects = dict()\\n        self._rigid_object_collections = dict()\\n        self._sensors = dict()\\n        self._extras = dict()\\n        # obtain the current stage\\n        self.stage = omni.usd.get_context().get_stage()\\n        # physics scene path\\n        self._physics_scene_path = None\\n        # prepare cloner for environment replication\\n        self.cloner = GridCloner(spacing=self.cfg.env_spacing)\\n        self.cloner.define_base_env(self.env_ns)\\n        self.env_prim_paths = self.cloner.generate_paths(f\"{self.env_ns}/env\", self.cfg.num_envs)\\n        # create source prim\\n        self.stage.DefinePrim(self.env_prim_paths[0], \"Xform\")\\n\\n        # when replicate_physics=False, we assume heterogeneous environments and clone the xforms first.\\n        # this triggers per-object level cloning in the spawner.\\n        if not self.cfg.replicate_physics:\\n            # clone the env xform\\n            env_origins = self.cloner.clone(\\n                source_prim_path=self.env_prim_paths[0],\\n                prim_paths=self.env_prim_paths,\\n                replicate_physics=False,\\n                copy_from_source=True,\\n                enable_env_ids=self.cfg.filter_collisions,  # this won\\'t do anything because we are not replicating physics\\n            )\\n            self._default_env_origins = torch.tensor(env_origins, device=self.device, dtype=torch.float32)\\n        else:\\n            # otherwise, environment origins will be initialized during cloning at the end of environment creation\\n            self._default_env_origins = None\\n\\n        self._global_prim_paths = list()\\n        if self._is_scene_setup_from_cfg():\\n            # add entities from config\\n            self._add_entities_from_cfg()\\n            # clone environments on a global scope if environment is homogeneous\\n            if self.cfg.replicate_physics:\\n                self.clone_environments(copy_from_source=False)\\n            # replicate physics if we have more than one environment\\n            # this is done to make scene initialization faster at play time\\n            if self.cfg.replicate_physics and self.cfg.num_envs > 1:\\n                self.cloner.replicate_physics(\\n                    source_prim_path=self.env_prim_paths[0],\\n                    prim_paths=self.env_prim_paths,\\n                    base_env_path=self.env_ns,\\n                    root_path=self.env_regex_ns.replace(\".*\", \"\"),\\n                    enable_env_ids=self.cfg.filter_collisions,\\n                )\\n\\n            # since env_ids is only applicable when replicating physics, we have to fallback to the previous method\\n            # to filter collisions if replicate_physics is not enabled\\n            if not self.cfg.replicate_physics and self.cfg.filter_collisions:\\n                self.filter_collisions(self._global_prim_paths)\\n\\n    def clone_environments(self, copy_from_source: bool = False):\\n        \"\"\"Creates clones of the environment ``/World/envs/env_0``.\\n\\n        Args:\\n            copy_from_source: (bool): If set to False, clones inherit from /World/envs/env_0 and mirror its changes.\\n            If True, clones are independent copies of the source prim and won\\'t reflect its changes (start-up time\\n            may increase). Defaults to False.\\n        \"\"\"\\n        # check if user spawned different assets in individual environments\\n        # this flag will be None if no multi asset is spawned\\n        carb_settings_iface = carb.settings.get_settings()\\n        has_multi_assets = carb_settings_iface.get(\"/isaaclab/spawn/multi_assets\")\\n        if has_multi_assets and self.cfg.replicate_physics:\\n            omni.log.warn(\\n                \"Varying assets might have been spawned under different environments.\"\\n                \" However, the replicate physics flag is enabled in the \\'InteractiveScene\\' configuration.\"\\n                \" This may adversely affect PhysX parsing. We recommend disabling this property.\"\\n            )\\n\\n        # clone the environment\\n        env_origins = self.cloner.clone(\\n            source_prim_path=self.env_prim_paths[0],\\n            prim_paths=self.env_prim_paths,\\n            replicate_physics=self.cfg.replicate_physics,\\n            copy_from_source=copy_from_source,\\n            enable_env_ids=self.cfg.filter_collisions,  # this automatically filters collisions between environments\\n        )\\n\\n        # since env_ids is only applicable when replicating physics, we have to fallback to the previous method\\n        # to filter collisions if replicate_physics is not enabled\\n        if not self.cfg.replicate_physics and self.cfg.filter_collisions:\\n            omni.log.warn(\\n                \"Collision filtering can only be automatically enabled when replicate_physics=True.\"\\n                \" Please call scene.filter_collisions(global_prim_paths) to filter collisions across environments.\"\\n            )\\n\\n        # in case of heterogeneous cloning, the env origins is specified at init\\n        if self._default_env_origins is None:\\n            self._default_env_origins = torch.tensor(env_origins, device=self.device, dtype=torch.float32)\\n\\n    def filter_collisions(self, global_prim_paths: list[str] | None = None):\\n        \"\"\"Filter environments collisions.\\n\\n        Disables collisions between the environments in ``/World/envs/env_.*`` and enables collisions with the prims\\n        in global prim paths (e.g. ground plane).\\n\\n        Args:\\n            global_prim_paths: A list of global prim paths to enable collisions with.\\n                Defaults to None, in which case no global prim paths are considered.\\n        \"\"\"\\n        # validate paths in global prim paths\\n        if global_prim_paths is None:\\n            global_prim_paths = []\\n        else:\\n            # remove duplicates in paths\\n            global_prim_paths = list(set(global_prim_paths))\\n\\n        # set global prim paths list if not previously defined\\n        if len(self._global_prim_paths) < 1:\\n            self._global_prim_paths += global_prim_paths\\n\\n        # filter collisions within each environment instance\\n        self.cloner.filter_collisions(\\n            self.physics_scene_path,\\n            \"/World/collisions\",\\n            self.env_prim_paths,\\n            global_paths=self._global_prim_paths,\\n        )\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns a string representation of the scene.\"\"\"\\n        msg = f\"<class {self.__class__.__name__}>\\\\n\"\\n        msg += f\"\\\\tNumber of environments: {self.cfg.num_envs}\\\\n\"\\n        msg += f\"\\\\tEnvironment spacing   : {self.cfg.env_spacing}\\\\n\"\\n        msg += f\"\\\\tSource prim name      : {self.env_prim_paths[0]}\\\\n\"\\n        msg += f\"\\\\tGlobal prim paths     : {self._global_prim_paths}\\\\n\"\\n        msg += f\"\\\\tReplicate physics     : {self.cfg.replicate_physics}\"\\n        return msg\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def physics_scene_path(self) -> str:\\n        \"\"\"The path to the USD Physics Scene.\"\"\"\\n        if self._physics_scene_path is None:\\n            for prim in self.stage.Traverse():\\n                if prim.HasAPI(PhysxSchema.PhysxSceneAPI):\\n                    self._physics_scene_path = prim.GetPrimPath().pathString\\n                    omni.log.info(f\"Physics scene prim path: {self._physics_scene_path}\")\\n                    break\\n            if self._physics_scene_path is None:\\n                raise RuntimeError(\"No physics scene found! Please make sure one exists.\")\\n        return self._physics_scene_path\\n\\n    @property\\n    def physics_dt(self) -> float:\\n        \"\"\"The physics timestep of the scene.\"\"\"\\n        return sim_utils.SimulationContext.instance().get_physics_dt()  # pyright: ignore [reportOptionalMemberAccess]\\n\\n    @property\\n    def device(self) -> str:\\n        \"\"\"The device on which the scene is created.\"\"\"\\n        return sim_utils.SimulationContext.instance().device  # pyright: ignore [reportOptionalMemberAccess]\\n\\n    @property\\n    def env_ns(self) -> str:\\n        \"\"\"The namespace ``/World/envs`` in which all environments created.\\n\\n        The environments are present w.r.t. this namespace under \"env_{N}\" prim,\\n        where N is a natural number.\\n        \"\"\"\\n        return \"/World/envs\"\\n\\n    @property\\n    def env_regex_ns(self) -> str:\\n        \"\"\"The namespace ``/World/envs/env_.*`` in which all environments created.\"\"\"\\n        return f\"{self.env_ns}/env_.*\"\\n\\n    @property\\n    def num_envs(self) -> int:\\n        \"\"\"The number of environments handled by the scene.\"\"\"\\n        return self.cfg.num_envs\\n\\n    @property\\n    def env_origins(self) -> torch.Tensor:\\n        \"\"\"The origins of the environments in the scene. Shape is (num_envs, 3).\"\"\"\\n        if self._terrain is not None:\\n            return self._terrain.env_origins\\n        else:\\n            return self._default_env_origins\\n\\n    @property\\n    def terrain(self) -> TerrainImporter | None:\\n        \"\"\"The terrain in the scene. If None, then the scene has no terrain.\\n\\n        Note:\\n            We treat terrain separate from :attr:`extras` since terrains define environment origins and are\\n            handled differently from other miscellaneous entities.\\n        \"\"\"\\n        return self._terrain\\n\\n    @property\\n    def articulations(self) -> dict[str, Articulation]:\\n        \"\"\"A dictionary of articulations in the scene.\"\"\"\\n        return self._articulations\\n\\n    @property\\n    def deformable_objects(self) -> dict[str, DeformableObject]:\\n        \"\"\"A dictionary of deformable objects in the scene.\"\"\"\\n        return self._deformable_objects\\n\\n    @property\\n    def rigid_objects(self) -> dict[str, RigidObject]:\\n        \"\"\"A dictionary of rigid objects in the scene.\"\"\"\\n        return self._rigid_objects\\n\\n    @property\\n    def rigid_object_collections(self) -> dict[str, RigidObjectCollection]:\\n        \"\"\"A dictionary of rigid object collections in the scene.\"\"\"\\n        return self._rigid_object_collections\\n\\n    @property\\n    def sensors(self) -> dict[str, SensorBase]:\\n        \"\"\"A dictionary of the sensors in the scene, such as cameras and contact reporters.\"\"\"\\n        return self._sensors\\n\\n    @property\\n    def extras(self) -> dict[str, XFormPrim]:\\n        \"\"\"A dictionary of miscellaneous simulation objects that neither inherit from assets nor sensors.\\n\\n        The keys are the names of the miscellaneous objects, and the values are the `XFormPrim`_\\n        of the corresponding prims.\\n\\n        As an example, lights or other props in the scene that do not have any attributes or properties that you\\n        want to alter at runtime can be added to this dictionary.\\n\\n        Note:\\n            These are not reset or updated by the scene. They are mainly other prims that are not necessarily\\n            handled by the interactive scene, but are useful to be accessed by the user.\\n\\n        .. _XFormPrim: https://docs.omniverse.nvidia.com/py/isaacsim/source/isaacsim.core/docs/index.html#isaacsim.core.prims.XFormPrim\\n\\n        \"\"\"\\n        return self._extras\\n\\n    @property\\n    def state(self) -> dict[str, dict[str, dict[str, torch.Tensor]]]:\\n        \"\"\"A dictionary of the state of the scene entities in the simulation world frame.\\n\\n        Please refer to :meth:`get_state` for the format.\\n        \"\"\"\\n        return self.get_state(is_relative=False)\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        \"\"\"Resets the scene entities.\\n\\n        Args:\\n            env_ids: The indices of the environments to reset.\\n                Defaults to None (all instances).\\n        \"\"\"\\n        # -- assets\\n        for articulation in self._articulations.values():\\n            articulation.reset(env_ids)\\n        for deformable_object in self._deformable_objects.values():\\n            deformable_object.reset(env_ids)\\n        for rigid_object in self._rigid_objects.values():\\n            rigid_object.reset(env_ids)\\n        for rigid_object_collection in self._rigid_object_collections.values():\\n            rigid_object_collection.reset(env_ids)\\n        # -- sensors\\n        for sensor in self._sensors.values():\\n            sensor.reset(env_ids)\\n\\n    def write_data_to_sim(self):\\n        \"\"\"Writes the data of the scene entities to the simulation.\"\"\"\\n        # -- assets\\n        for articulation in self._articulations.values():\\n            articulation.write_data_to_sim()\\n        for deformable_object in self._deformable_objects.values():\\n            deformable_object.write_data_to_sim()\\n        for rigid_object in self._rigid_objects.values():\\n            rigid_object.write_data_to_sim()\\n        for rigid_object_collection in self._rigid_object_collections.values():\\n            rigid_object_collection.write_data_to_sim()\\n\\n    def update(self, dt: float) -> None:\\n        \"\"\"Update the scene entities.\\n\\n        Args:\\n            dt: The amount of time passed from last :meth:`update` call.\\n        \"\"\"\\n        # -- assets\\n        for articulation in self._articulations.values():\\n            articulation.update(dt)\\n        for deformable_object in self._deformable_objects.values():\\n            deformable_object.update(dt)\\n        for rigid_object in self._rigid_objects.values():\\n            rigid_object.update(dt)\\n        for rigid_object_collection in self._rigid_object_collections.values():\\n            rigid_object_collection.update(dt)\\n        # -- sensors\\n        for sensor in self._sensors.values():\\n            sensor.update(dt, force_recompute=not self.cfg.lazy_sensor_update)\\n\\n    \"\"\"\\n    Operations: Scene State.\\n    \"\"\"\\n\\n    def reset_to(\\n        self,\\n        state: dict[str, dict[str, dict[str, torch.Tensor]]],\\n        env_ids: Sequence[int] | None = None,\\n        is_relative: bool = False,\\n    ):\\n        \"\"\"Resets the entities in the scene to the provided state.\\n\\n        Args:\\n            state: The state to reset the scene entities to. Please refer to :meth:`get_state` for the format.\\n            env_ids: The indices of the environments to reset. Defaults to None, in which case\\n                all environment instances are reset.\\n            is_relative: If set to True, the state is considered relative to the environment origins.\\n                Defaults to False.\\n        \"\"\"\\n        # resolve env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # articulations\\n        for asset_name, articulation in self._articulations.items():\\n            asset_state = state[\"articulation\"][asset_name]\\n            # root state\\n            root_pose = asset_state[\"root_pose\"].clone()\\n            if is_relative:\\n                root_pose[:, :3] += self.env_origins[env_ids]\\n            root_velocity = asset_state[\"root_velocity\"].clone()\\n            articulation.write_root_pose_to_sim(root_pose, env_ids=env_ids)\\n            articulation.write_root_velocity_to_sim(root_velocity, env_ids=env_ids)\\n            # joint state\\n            joint_position = asset_state[\"joint_position\"].clone()\\n            joint_velocity = asset_state[\"joint_velocity\"].clone()\\n            articulation.write_joint_state_to_sim(joint_position, joint_velocity, env_ids=env_ids)\\n            # FIXME: This is not generic as it assumes PD control over the joints.\\n            #   This assumption does not hold for effort controlled joints.\\n            articulation.set_joint_position_target(joint_position, env_ids=env_ids)\\n            articulation.set_joint_velocity_target(joint_velocity, env_ids=env_ids)\\n        # deformable objects\\n        for asset_name, deformable_object in self._deformable_objects.items():\\n            asset_state = state[\"deformable_object\"][asset_name]\\n            nodal_position = asset_state[\"nodal_position\"].clone()\\n            if is_relative:\\n                nodal_position[:, :3] += self.env_origins[env_ids]\\n            nodal_velocity = asset_state[\"nodal_velocity\"].clone()\\n            deformable_object.write_nodal_pos_to_sim(nodal_position, env_ids=env_ids)\\n            deformable_object.write_nodal_velocity_to_sim(nodal_velocity, env_ids=env_ids)\\n        # rigid objects\\n        for asset_name, rigid_object in self._rigid_objects.items():\\n            asset_state = state[\"rigid_object\"][asset_name]\\n            root_pose = asset_state[\"root_pose\"].clone()\\n            if is_relative:\\n                root_pose[:, :3] += self.env_origins[env_ids]\\n            root_velocity = asset_state[\"root_velocity\"].clone()\\n            rigid_object.write_root_pose_to_sim(root_pose, env_ids=env_ids)\\n            rigid_object.write_root_velocity_to_sim(root_velocity, env_ids=env_ids)\\n\\n        # write data to simulation to make sure initial state is set\\n        # this propagates the joint targets to the simulation\\n        self.write_data_to_sim()\\n\\n    def get_state(self, is_relative: bool = False) -> dict[str, dict[str, dict[str, torch.Tensor]]]:\\n        \"\"\"Returns the state of the scene entities.\\n\\n        Based on the type of the entity, the state comprises of different components.\\n\\n        * For an articulation, the state comprises of the root pose, root velocity, and joint position and velocity.\\n        * For a deformable object, the state comprises of the nodal position and velocity.\\n        * For a rigid object, the state comprises of the root pose and root velocity.\\n\\n        The returned state is a dictionary with the following format:\\n\\n        .. code-block:: python\\n\\n            {\\n                \"articulation\": {\\n                    \"entity_1_name\": {\\n                        \"root_pose\": torch.Tensor,\\n                        \"root_velocity\": torch.Tensor,\\n                        \"joint_position\": torch.Tensor,\\n                        \"joint_velocity\": torch.Tensor,\\n                    },\\n                    \"entity_2_name\": {\\n                        \"root_pose\": torch.Tensor,\\n                        \"root_velocity\": torch.Tensor,\\n                        \"joint_position\": torch.Tensor,\\n                        \"joint_velocity\": torch.Tensor,\\n                    },\\n                },\\n                \"deformable_object\": {\\n                    \"entity_3_name\": {\\n                        \"nodal_position\": torch.Tensor,\\n                        \"nodal_velocity\": torch.Tensor,\\n                    }\\n                },\\n                \"rigid_object\": {\\n                    \"entity_4_name\": {\\n                        \"root_pose\": torch.Tensor,\\n                        \"root_velocity\": torch.Tensor,\\n                    }\\n                },\\n            }\\n\\n        where ``entity_N_name`` is the name of the entity registered in the scene.\\n\\n        Args:\\n            is_relative: If set to True, the state is considered relative to the environment origins.\\n                Defaults to False.\\n\\n        Returns:\\n            A dictionary of the state of the scene entities.\\n        \"\"\"\\n        state = dict()\\n        # articulations\\n        state[\"articulation\"] = dict()\\n        for asset_name, articulation in self._articulations.items():\\n            asset_state = dict()\\n            asset_state[\"root_pose\"] = articulation.data.root_state_w[:, :7].clone()\\n            if is_relative:\\n                asset_state[\"root_pose\"][:, :3] -= self.env_origins\\n            asset_state[\"root_velocity\"] = articulation.data.root_vel_w.clone()\\n            asset_state[\"joint_position\"] = articulation.data.joint_pos.clone()\\n            asset_state[\"joint_velocity\"] = articulation.data.joint_vel.clone()\\n            state[\"articulation\"][asset_name] = asset_state\\n        # deformable objects\\n        state[\"deformable_object\"] = dict()\\n        for asset_name, deformable_object in self._deformable_objects.items():\\n            asset_state = dict()\\n            asset_state[\"nodal_position\"] = deformable_object.data.nodal_pos_w.clone()\\n            if is_relative:\\n                asset_state[\"nodal_position\"][:, :3] -= self.env_origins\\n            asset_state[\"nodal_velocity\"] = deformable_object.data.nodal_vel_w.clone()\\n            state[\"deformable_object\"][asset_name] = asset_state\\n        # rigid objects\\n        state[\"rigid_object\"] = dict()\\n        for asset_name, rigid_object in self._rigid_objects.items():\\n            asset_state = dict()\\n            asset_state[\"root_pose\"] = rigid_object.data.root_state_w[:, :7].clone()\\n            if is_relative:\\n                asset_state[\"root_pose\"][:, :3] -= self.env_origins\\n            asset_state[\"root_velocity\"] = rigid_object.data.root_vel_w.clone()\\n            state[\"rigid_object\"][asset_name] = asset_state\\n        return state\\n\\n    \"\"\"\\n    Operations: Iteration.\\n    \"\"\"\\n\\n    def keys(self) -> list[str]:\\n        \"\"\"Returns the keys of the scene entities.\\n\\n        Returns:\\n            The keys of the scene entities.\\n        \"\"\"\\n        all_keys = [\"terrain\"]\\n        for asset_family in [\\n            self._articulations,\\n            self._deformable_objects,\\n            self._rigid_objects,\\n            self._rigid_object_collections,\\n            self._sensors,\\n            self._extras,\\n        ]:\\n            all_keys += list(asset_family.keys())\\n        return all_keys\\n\\n    def __getitem__(self, key: str) -> Any:\\n        \"\"\"Returns the scene entity with the given key.\\n\\n        Args:\\n            key: The key of the scene entity.\\n\\n        Returns:\\n            The scene entity.\\n        \"\"\"\\n        # check if it is a terrain\\n        if key == \"terrain\":\\n            return self._terrain\\n\\n        all_keys = [\"terrain\"]\\n        # check if it is in other dictionaries\\n        for asset_family in [\\n            self._articulations,\\n            self._deformable_objects,\\n            self._rigid_objects,\\n            self._rigid_object_collections,\\n            self._sensors,\\n            self._extras,\\n        ]:\\n            out = asset_family.get(key)\\n            # if found, return\\n            if out is not None:\\n                return out\\n            all_keys += list(asset_family.keys())\\n        # if not found, raise error\\n        raise KeyError(f\"Scene entity with key \\'{key}\\' not found. Available Entities: \\'{all_keys}\\'\")\\n\\n    \"\"\"\\n    Internal methods.\\n    \"\"\"\\n\\n    def _is_scene_setup_from_cfg(self) -> bool:\\n        \"\"\"Check if scene entities are setup from the config or not.\\n\\n        Returns:\\n            True if scene entities are setup from the config, False otherwise.\\n        \"\"\"\\n        return any(\\n            not (asset_name in InteractiveSceneCfg.__dataclass_fields__ or asset_cfg is None)\\n            for asset_name, asset_cfg in self.cfg.__dict__.items()\\n        )\\n\\n    def _add_entities_from_cfg(self):\\n        \"\"\"Add scene entities from the config.\"\"\"\\n        # store paths that are in global collision filter\\n        self._global_prim_paths = list()\\n        # parse the entire scene config and resolve regex\\n        for asset_name, asset_cfg in self.cfg.__dict__.items():\\n            # skip keywords\\n            # note: easier than writing a list of keywords: [num_envs, env_spacing, lazy_sensor_update]\\n            if asset_name in InteractiveSceneCfg.__dataclass_fields__ or asset_cfg is None:\\n                continue\\n            # resolve regex\\n            if hasattr(asset_cfg, \"prim_path\"):\\n                asset_cfg.prim_path = asset_cfg.prim_path.format(ENV_REGEX_NS=self.env_regex_ns)\\n            # create asset\\n            if isinstance(asset_cfg, TerrainImporterCfg):\\n                # terrains are special entities since they define environment origins\\n                asset_cfg.num_envs = self.cfg.num_envs\\n                asset_cfg.env_spacing = self.cfg.env_spacing\\n                self._terrain = asset_cfg.class_type(asset_cfg)\\n            elif isinstance(asset_cfg, ArticulationCfg):\\n                self._articulations[asset_name] = asset_cfg.class_type(asset_cfg)\\n            elif isinstance(asset_cfg, DeformableObjectCfg):\\n                self._deformable_objects[asset_name] = asset_cfg.class_type(asset_cfg)\\n            elif isinstance(asset_cfg, RigidObjectCfg):\\n                self._rigid_objects[asset_name] = asset_cfg.class_type(asset_cfg)\\n            elif isinstance(asset_cfg, RigidObjectCollectionCfg):\\n                for rigid_object_cfg in asset_cfg.rigid_objects.values():\\n                    rigid_object_cfg.prim_path = rigid_object_cfg.prim_path.format(ENV_REGEX_NS=self.env_regex_ns)\\n                self._rigid_object_collections[asset_name] = asset_cfg.class_type(asset_cfg)\\n                for rigid_object_cfg in asset_cfg.rigid_objects.values():\\n                    if hasattr(rigid_object_cfg, \"collision_group\") and rigid_object_cfg.collision_group == -1:\\n                        asset_paths = sim_utils.find_matching_prim_paths(rigid_object_cfg.prim_path)\\n                        self._global_prim_paths += asset_paths\\n            elif isinstance(asset_cfg, SensorBaseCfg):\\n                # Update target frame path(s)\\' regex name space for FrameTransformer\\n                if isinstance(asset_cfg, FrameTransformerCfg):\\n                    updated_target_frames = []\\n                    for target_frame in asset_cfg.target_frames:\\n                        target_frame.prim_path = target_frame.prim_path.format(ENV_REGEX_NS=self.env_regex_ns)\\n                        updated_target_frames.append(target_frame)\\n                    asset_cfg.target_frames = updated_target_frames\\n                elif isinstance(asset_cfg, ContactSensorCfg):\\n                    updated_filter_prim_paths_expr = []\\n                    for filter_prim_path in asset_cfg.filter_prim_paths_expr:\\n                        updated_filter_prim_paths_expr.append(filter_prim_path.format(ENV_REGEX_NS=self.env_regex_ns))\\n                    asset_cfg.filter_prim_paths_expr = updated_filter_prim_paths_expr\\n\\n                self._sensors[asset_name] = asset_cfg.class_type(asset_cfg)\\n            elif isinstance(asset_cfg, AssetBaseCfg):\\n                # manually spawn asset\\n                if asset_cfg.spawn is not None:\\n                    asset_cfg.spawn.func(\\n                        asset_cfg.prim_path,\\n                        asset_cfg.spawn,\\n                        translation=asset_cfg.init_state.pos,\\n                        orientation=asset_cfg.init_state.rot,\\n                    )\\n                # store xform prim view corresponding to this asset\\n                # all prims in the scene are Xform prims (i.e. have a transform component)\\n                self._extras[asset_name] = XFormPrim(asset_cfg.prim_path, reset_xform_properties=False)\\n            else:\\n                raise ValueError(f\"Unknown asset config type for {asset_name}: {asset_cfg}\")\\n            # store global collision paths\\n            if hasattr(asset_cfg, \"collision_group\") and asset_cfg.collision_group == -1:\\n                asset_paths = sim_utils.find_matching_prim_paths(asset_cfg.prim_path)\\n                self._global_prim_paths += asset_paths'),\n",
       " Document(metadata={}, page_content='class InteractiveSceneCfg:\\n    \"\"\"Configuration for the interactive scene.\\n\\n    The users can inherit from this class to add entities to their scene. This is then parsed by the\\n    :class:`InteractiveScene` class to create the scene.\\n\\n    .. note::\\n        The adding of entities to the scene is sensitive to the order of the attributes in the configuration.\\n        Please make sure to add the entities in the order you want them to be added to the scene.\\n        The recommended order of specification is terrain, physics-related assets (articulations and rigid bodies),\\n        sensors and non-physics-related assets (lights).\\n\\n    For example, to add a robot to the scene, the user can create a configuration class as follows:\\n\\n    .. code-block:: python\\n\\n        import isaaclab.sim as sim_utils\\n        from isaaclab.assets import AssetBaseCfg\\n        from isaaclab.scene import InteractiveSceneCfg\\n        from isaaclab.sensors.ray_caster import GridPatternCfg, RayCasterCfg\\n        from isaaclab.utils import configclass\\n\\n        from isaaclab_assets.robots.anymal import ANYMAL_C_CFG\\n\\n        @configclass\\n        class MySceneCfg(InteractiveSceneCfg):\\n\\n            # terrain - flat terrain plane\\n            terrain = TerrainImporterCfg(\\n                prim_path=\"/World/ground\",\\n                terrain_type=\"plane\",\\n            )\\n\\n            # articulation - robot 1\\n            robot_1 = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot_1\")\\n            # articulation - robot 2\\n            robot_2 = ANYMAL_C_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot_2\")\\n            robot_2.init_state.pos = (0.0, 1.0, 0.6)\\n\\n            # sensor - ray caster attached to the base of robot 1 that scans the ground\\n            height_scanner = RayCasterCfg(\\n                prim_path=\"{ENV_REGEX_NS}/Robot_1/base\",\\n                offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),\\n                attach_yaw_only=True,\\n                pattern_cfg=GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),\\n                debug_vis=True,\\n                mesh_prim_paths=[\"/World/ground\"],\\n            )\\n\\n            # extras - light\\n            light = AssetBaseCfg(\\n                prim_path=\"/World/light\",\\n                spawn=sim_utils.DistantLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75)),\\n                init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, 500.0)),\\n            )\\n\\n    \"\"\"\\n\\n    num_envs: int = MISSING\\n    \"\"\"Number of environment instances handled by the scene.\"\"\"\\n\\n    env_spacing: float = MISSING\\n    \"\"\"Spacing between environments.\\n\\n    This is the default distance between environment origins in the scene. Used only when the\\n    number of environments is greater than one.\\n    \"\"\"\\n\\n    lazy_sensor_update: bool = True\\n    \"\"\"Whether to update sensors only when they are accessed. Default is True.\\n\\n    If true, the sensor data is only updated when their attribute ``data`` is accessed. Otherwise, the sensor\\n    data is updated every time sensors are updated.\\n    \"\"\"\\n\\n    replicate_physics: bool = True\\n    \"\"\"Enable/disable replication of physics schemas when using the Cloner APIs. Default is True.\\n\\n    If True, the simulation will have the same asset instances (USD prims) in all the cloned environments.\\n    Internally, this ensures optimization in setting up the scene and parsing it via the physics stage parser.\\n\\n    If False, the simulation allows having separate asset instances (USD prims) in each environment.\\n    This flexibility comes at a cost of slowdowns in setting up and parsing the scene.\\n\\n    .. note::\\n        Optimized parsing of certain prim types (such as deformable objects) is not currently supported\\n        by the physics engine. In these cases, this flag needs to be set to False.\\n    \"\"\"\\n\\n    filter_collisions: bool = True\\n    \"\"\"Enable/disable collision filtering between cloned environments. Default is True.\\n\\n    If True, collisions will not occur between cloned environments.\\n\\n    If False, the simulation will generate collisions between environments.\\n\\n    .. note::\\n        Collisions can only be filtered automatically in direct workflows when physics replication is enabled.\\n        If ``replicated_physics=False`` and collision filtering is desired, make sure to call ``scene.filter_collisions()``.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class SensorBase(ABC):\\n    \"\"\"The base class for implementing a sensor.\\n\\n    The implementation is based on lazy evaluation. The sensor data is only updated when the user\\n    tries accessing the data through the :attr:`data` property or sets ``force_compute=True`` in\\n    the :meth:`update` method. This is done to avoid unnecessary computation when the sensor data\\n    is not used.\\n\\n    The sensor is updated at the specified update period. If the update period is zero, then the\\n    sensor is updated at every simulation step.\\n    \"\"\"\\n\\n    def __init__(self, cfg: SensorBaseCfg):\\n        \"\"\"Initialize the sensor class.\\n\\n        Args:\\n            cfg: The configuration parameters for the sensor.\\n        \"\"\"\\n        # check that config is valid\\n        if cfg.history_length < 0:\\n            raise ValueError(f\"History length must be greater than 0! Received: {cfg.history_length}\")\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs\\n        self.cfg = cfg.copy()\\n        # flag for whether the sensor is initialized\\n        self._is_initialized = False\\n        # flag for whether the sensor is in visualization mode\\n        self._is_visualizing = False\\n\\n        # note: Use weakref on callbacks to ensure that this object can be deleted when its destructor is called.\\n        # add callbacks for stage play/stop\\n        # The order is set to 10 which is arbitrary but should be lower priority than the default order of 0\\n        timeline_event_stream = omni.timeline.get_timeline_interface().get_timeline_event_stream()\\n        self._initialize_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n            int(omni.timeline.TimelineEventType.PLAY),\\n            lambda event, obj=weakref.proxy(self): obj._initialize_callback(event),\\n            order=10,\\n        )\\n        self._invalidate_initialize_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n            int(omni.timeline.TimelineEventType.STOP),\\n            lambda event, obj=weakref.proxy(self): obj._invalidate_initialize_callback(event),\\n            order=10,\\n        )\\n        self._prim_deletion_callback_id = SimulationManager.register_callback(\\n            self._on_prim_deletion, event=IsaacEvents.PRIM_DELETION\\n        )\\n        # add handle for debug visualization (this is set to a valid handle inside set_debug_vis)\\n        self._debug_vis_handle = None\\n        # set initial state of debug visualization\\n        self.set_debug_vis(self.cfg.debug_vis)\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribe from the callbacks.\"\"\"\\n        # clear physics events handles\\n        self._clear_callbacks()\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def is_initialized(self) -> bool:\\n        \"\"\"Whether the sensor is initialized.\\n\\n        Returns True if the sensor is initialized, False otherwise.\\n        \"\"\"\\n        return self._is_initialized\\n\\n    @property\\n    def num_instances(self) -> int:\\n        \"\"\"Number of instances of the sensor.\\n\\n        This is equal to the number of sensors per environment multiplied by the number of environments.\\n        \"\"\"\\n        return self._num_envs\\n\\n    @property\\n    def device(self) -> str:\\n        \"\"\"Memory device for computation.\"\"\"\\n        return self._device\\n\\n    @property\\n    @abstractmethod\\n    def data(self) -> Any:\\n        \"\"\"Data from the sensor.\\n\\n        This property is only updated when the user tries to access the data. This is done to avoid\\n        unnecessary computation when the sensor data is not used.\\n\\n        For updating the sensor when this property is accessed, you can use the following\\n        code snippet in your sensor implementation:\\n\\n        .. code-block:: python\\n\\n            # update sensors if needed\\n            self._update_outdated_buffers()\\n            # return the data (where `_data` is the data for the sensor)\\n            return self._data\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the sensor has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_debug_vis_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def set_debug_vis(self, debug_vis: bool) -> bool:\\n        \"\"\"Sets whether to visualize the sensor data.\\n\\n        Args:\\n            debug_vis: Whether to visualize the sensor data.\\n\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the sensor\\n            does not support debug visualization.\\n        \"\"\"\\n        # check if debug visualization is supported\\n        if not self.has_debug_vis_implementation:\\n            return False\\n        # toggle debug visualization objects\\n        self._set_debug_vis_impl(debug_vis)\\n        # toggle debug visualization flag\\n        self._is_visualizing = debug_vis\\n        # toggle debug visualization handles\\n        if debug_vis:\\n            # create a subscriber for the post update event if it doesn\\'t exist\\n            if self._debug_vis_handle is None:\\n                app_interface = omni.kit.app.get_app_interface()\\n                self._debug_vis_handle = app_interface.get_post_update_event_stream().create_subscription_to_pop(\\n                    lambda event, obj=weakref.proxy(self): obj._debug_vis_callback(event)\\n                )\\n        else:\\n            # remove the subscriber if it exists\\n            if self._debug_vis_handle is not None:\\n                self._debug_vis_handle.unsubscribe()\\n                self._debug_vis_handle = None\\n        # return success\\n        return True\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        \"\"\"Resets the sensor internals.\\n\\n        Args:\\n            env_ids: The sensor ids to reset. Defaults to None.\\n        \"\"\"\\n        # Resolve sensor ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # Reset the timestamp for the sensors\\n        self._timestamp[env_ids] = 0.0\\n        self._timestamp_last_update[env_ids] = 0.0\\n        # Set all reset sensors to outdated so that they are updated when data is called the next time.\\n        self._is_outdated[env_ids] = True\\n\\n    def update(self, dt: float, force_recompute: bool = False):\\n        # Update the timestamp for the sensors\\n        self._timestamp += dt\\n        self._is_outdated |= self._timestamp - self._timestamp_last_update + 1e-6 >= self.cfg.update_period\\n        # Update the buffers\\n        # TODO (from @mayank): Why is there a history length here when it doesn\\'t mean anything in the sensor base?!?\\n        #   It is only for the contact sensor but there we should redefine the update function IMO.\\n        if force_recompute or self._is_visualizing or (self.cfg.history_length > 0):\\n            self._update_outdated_buffers()\\n\\n    \"\"\"\\n    Implementation specific.\\n    \"\"\"\\n\\n    @abstractmethod\\n    def _initialize_impl(self):\\n        \"\"\"Initializes the sensor-related handles and internal buffers.\"\"\"\\n        # Obtain Simulation Context\\n        sim = sim_utils.SimulationContext.instance()\\n        if sim is None:\\n            raise RuntimeError(\"Simulation Context is not initialized!\")\\n        # Obtain device and backend\\n        self._device = sim.device\\n        self._backend = sim.backend\\n        self._sim_physics_dt = sim.get_physics_dt()\\n        # Count number of environments\\n        env_prim_path_expr = self.cfg.prim_path.rsplit(\"/\", 1)[0]\\n        self._parent_prims = sim_utils.find_matching_prims(env_prim_path_expr)\\n        self._num_envs = len(self._parent_prims)\\n        # Boolean tensor indicating whether the sensor data has to be refreshed\\n        self._is_outdated = torch.ones(self._num_envs, dtype=torch.bool, device=self._device)\\n        # Current timestamp (in seconds)\\n        self._timestamp = torch.zeros(self._num_envs, device=self._device)\\n        # Timestamp from last update\\n        self._timestamp_last_update = torch.zeros_like(self._timestamp)\\n\\n    @abstractmethod\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        \"\"\"Fills the sensor data for provided environment ids.\\n\\n        This function does not perform any time-based checks and directly fills the data into the\\n        data container.\\n\\n        Args:\\n            env_ids: The indices of the sensors that are ready to capture.\\n        \"\"\"\\n        raise NotImplementedError\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set debug visualization into visualization objects.\\n\\n        This function is responsible for creating the visualization objects if they don\\'t exist\\n        and input ``debug_vis`` is True. If the visualization objects exist, the function should\\n        set their visibility into the stage.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")\\n\\n    def _debug_vis_callback(self, event):\\n        \"\"\"Callback for debug visualization.\\n\\n        This function calls the visualization objects and sets the data to visualize into them.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _initialize_callback(self, event):\\n        \"\"\"Initializes the scene elements.\\n\\n        Note:\\n            PhysX handles are only enabled once the simulator starts playing. Hence, this function needs to be\\n            called whenever the simulator \"plays\" from a \"stop\" state.\\n        \"\"\"\\n        if not self._is_initialized:\\n            try:\\n                self._initialize_impl()\\n            except Exception as e:\\n                if builtins.ISAACLAB_CALLBACK_EXCEPTION is None:\\n                    builtins.ISAACLAB_CALLBACK_EXCEPTION = e\\n            self._is_initialized = True\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        self._is_initialized = False\\n        if self._debug_vis_handle is not None:\\n            self._debug_vis_handle.unsubscribe()\\n            self._debug_vis_handle = None\\n\\n    def _on_prim_deletion(self, prim_path: str) -> None:\\n        \"\"\"Invalidates and deletes the callbacks when the prim is deleted.\\n\\n        Args:\\n            prim_path: The path to the prim that is being deleted.\\n\\n        Note:\\n            This function is called when the prim is deleted.\\n        \"\"\"\\n        if prim_path == \"/\":\\n            self._clear_callbacks()\\n            return\\n        result = re.match(\\n            pattern=\"^\" + \"/\".join(self.cfg.prim_path.split(\"/\")[: prim_path.count(\"/\") + 1]) + \"$\", string=prim_path\\n        )\\n        if result:\\n            self._clear_callbacks()\\n\\n    def _clear_callbacks(self) -> None:\\n        \"\"\"Clears the callbacks.\"\"\"\\n        if self._prim_deletion_callback_id:\\n            SimulationManager.deregister_callback(self._prim_deletion_callback_id)\\n            self._prim_deletion_callback_id = None\\n        if self._initialize_handle:\\n            self._initialize_handle.unsubscribe()\\n            self._initialize_handle = None\\n        if self._invalidate_initialize_handle:\\n            self._invalidate_initialize_handle.unsubscribe()\\n            self._invalidate_initialize_handle = None\\n        # clear debug visualization\\n        if self._debug_vis_handle:\\n            self._debug_vis_handle.unsubscribe()\\n            self._debug_vis_handle = None\\n\\n    \"\"\"\\n    Helper functions.\\n    \"\"\"\\n\\n    def _update_outdated_buffers(self):\\n        \"\"\"Fills the sensor data for the outdated sensors.\"\"\"\\n        outdated_env_ids = self._is_outdated.nonzero().squeeze(-1)\\n        if len(outdated_env_ids) > 0:\\n            # obtain new data\\n            self._update_buffers_impl(outdated_env_ids)\\n            # update the timestamp from last update\\n            self._timestamp_last_update[outdated_env_ids] = self._timestamp[outdated_env_ids]\\n            # set outdated flag to false for the updated sensors\\n            self._is_outdated[outdated_env_ids] = False'),\n",
       " Document(metadata={}, page_content='class SensorBaseCfg:\\n    \"\"\"Configuration parameters for a sensor.\"\"\"\\n\\n    class_type: type[SensorBase] = MISSING\\n    \"\"\"The associated sensor class.\\n\\n    The class should inherit from :class:`isaaclab.sensors.sensor_base.SensorBase`.\\n    \"\"\"\\n\\n    prim_path: str = MISSING\\n    \"\"\"Prim path (or expression) to the sensor.\\n\\n    .. note::\\n        The expression can contain the environment namespace regex ``{ENV_REGEX_NS}`` which\\n        will be replaced with the environment namespace.\\n\\n        Example: ``{ENV_REGEX_NS}/Robot/sensor`` will be replaced with ``/World/envs/env_.*/Robot/sensor``.\\n\\n    \"\"\"\\n\\n    update_period: float = 0.0\\n    \"\"\"Update period of the sensor buffers (in seconds). Defaults to 0.0 (update every step).\"\"\"\\n\\n    history_length: int = 0\\n    \"\"\"Number of past frames to store in the sensor buffers. Defaults to 0, which means that only\\n    the current data is stored (no history).\"\"\"\\n\\n    debug_vis: bool = False\\n    \"\"\"Whether to visualize the sensor. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='class Camera(SensorBase):\\n    r\"\"\"The camera sensor for acquiring visual data.\\n\\n    This class wraps over the `UsdGeom Camera`_ for providing a consistent API for acquiring visual data.\\n    It ensures that the camera follows the ROS convention for the coordinate system.\\n\\n    Summarizing from the `replicator extension`_, the following sensor types are supported:\\n\\n    - ``\"rgb\"``: A 3-channel rendered color image.\\n    - ``\"rgba\"``: A 4-channel rendered color image with alpha channel.\\n    - ``\"distance_to_camera\"``: An image containing the distance to camera optical center.\\n    - ``\"distance_to_image_plane\"``: An image containing distances of 3D points from camera plane along camera\\'s z-axis.\\n    - ``\"depth\"``: The same as ``\"distance_to_image_plane\"``.\\n    - ``\"normals\"``: An image containing the local surface normal vectors at each pixel.\\n    - ``\"motion_vectors\"``: An image containing the motion vector data at each pixel.\\n    - ``\"semantic_segmentation\"``: The semantic segmentation data.\\n    - ``\"instance_segmentation_fast\"``: The instance segmentation data.\\n    - ``\"instance_id_segmentation_fast\"``: The instance id segmentation data.\\n\\n    .. note::\\n        Currently the following sensor types are not supported in a \"view\" format:\\n\\n        - ``\"instance_segmentation\"``: The instance segmentation data. Please use the fast counterparts instead.\\n        - ``\"instance_id_segmentation\"``: The instance id segmentation data. Please use the fast counterparts instead.\\n        - ``\"bounding_box_2d_tight\"``: The tight 2D bounding box data (only contains non-occluded regions).\\n        - ``\"bounding_box_2d_tight_fast\"``: The tight 2D bounding box data (only contains non-occluded regions).\\n        - ``\"bounding_box_2d_loose\"``: The loose 2D bounding box data (contains occluded regions).\\n        - ``\"bounding_box_2d_loose_fast\"``: The loose 2D bounding box data (contains occluded regions).\\n        - ``\"bounding_box_3d\"``: The 3D view space bounding box data.\\n        - ``\"bounding_box_3d_fast\"``: The 3D view space bounding box data.\\n\\n    .. _replicator extension: https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/annotators_details.html#annotator-output\\n    .. _USDGeom Camera: https://graphics.pixar.com/usd/docs/api/class_usd_geom_camera.html\\n\\n    \"\"\"\\n\\n    cfg: CameraCfg\\n    \"\"\"The configuration parameters.\"\"\"\\n\\n    UNSUPPORTED_TYPES: set[str] = {\\n        \"instance_id_segmentation\",\\n        \"instance_segmentation\",\\n        \"bounding_box_2d_tight\",\\n        \"bounding_box_2d_loose\",\\n        \"bounding_box_3d\",\\n        \"bounding_box_2d_tight_fast\",\\n        \"bounding_box_2d_loose_fast\",\\n        \"bounding_box_3d_fast\",\\n    }\\n    \"\"\"The set of sensor types that are not supported by the camera class.\"\"\"\\n\\n    def __init__(self, cfg: CameraCfg):\\n        \"\"\"Initializes the camera sensor.\\n\\n        Args:\\n            cfg: The configuration parameters.\\n\\n        Raises:\\n            RuntimeError: If no camera prim is found at the given path.\\n            ValueError: If the provided data types are not supported by the camera.\\n        \"\"\"\\n        # check if sensor path is valid\\n        # note: currently we do not handle environment indices if there is a regex pattern in the leaf\\n        #   For example, if the prim path is \"/World/Sensor_[1,2]\".\\n        sensor_path = cfg.prim_path.split(\"/\")[-1]\\n        sensor_path_is_regex = re.match(r\"^[a-zA-Z0-9/_]+$\", sensor_path) is None\\n        if sensor_path_is_regex:\\n            raise RuntimeError(\\n                f\"Invalid prim path for the camera sensor: {self.cfg.prim_path}.\"\\n                \"\\\\n\\\\tHint: Please ensure that the prim path does not contain any regex patterns in the leaf.\"\\n            )\\n        # perform check on supported data types\\n        self._check_supported_data_types(cfg)\\n        # initialize base class\\n        super().__init__(cfg)\\n\\n        # toggle rendering of rtx sensors as True\\n        # this flag is read by SimulationContext to determine if rtx sensors should be rendered\\n        carb_settings_iface = carb.settings.get_settings()\\n        carb_settings_iface.set_bool(\"/isaaclab/render/rtx_sensors\", True)\\n\\n        # spawn the asset\\n        if self.cfg.spawn is not None:\\n            # compute the rotation offset\\n            rot = torch.tensor(self.cfg.offset.rot, dtype=torch.float32, device=\"cpu\").unsqueeze(0)\\n            rot_offset = convert_camera_frame_orientation_convention(\\n                rot, origin=self.cfg.offset.convention, target=\"opengl\"\\n            )\\n            rot_offset = rot_offset.squeeze(0).cpu().numpy()\\n            # ensure vertical aperture is set, otherwise replace with default for squared pixels\\n            if self.cfg.spawn.vertical_aperture is None:\\n                self.cfg.spawn.vertical_aperture = self.cfg.spawn.horizontal_aperture * self.cfg.height / self.cfg.width\\n            # spawn the asset\\n            self.cfg.spawn.func(\\n                self.cfg.prim_path, self.cfg.spawn, translation=self.cfg.offset.pos, orientation=rot_offset\\n            )\\n        # check that spawn was successful\\n        matching_prims = sim_utils.find_matching_prims(self.cfg.prim_path)\\n        if len(matching_prims) == 0:\\n            raise RuntimeError(f\"Could not find prim with path {self.cfg.prim_path}.\")\\n\\n        # UsdGeom Camera prim for the sensor\\n        self._sensor_prims: list[UsdGeom.Camera] = list()\\n        # Create empty variables for storing output data\\n        self._data = CameraData()\\n\\n        # HACK: we need to disable instancing for semantic_segmentation and instance_segmentation_fast to work\\n        isaac_sim_version = get_version()\\n        # checks for Isaac Sim v4.5 as this issue exists there\\n        if int(isaac_sim_version[2]) == 4 and int(isaac_sim_version[3]) == 5:\\n            if \"semantic_segmentation\" in self.cfg.data_types or \"instance_segmentation_fast\" in self.cfg.data_types:\\n                omni.log.warn(\\n                    \"Isaac Sim 4.5 introduced a bug in Camera and TiledCamera when outputting instance and semantic\"\\n                    \" segmentation outputs for instanceable assets. As a workaround, the instanceable flag on assets\"\\n                    \" will be disabled in the current workflow and may lead to longer load times and increased memory\"\\n                    \" usage.\"\\n                )\\n                stage = omni.usd.get_context().get_stage()\\n                with Sdf.ChangeBlock():\\n                    for prim in stage.Traverse():\\n                        prim.SetInstanceable(False)\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribes from callbacks and detach from the replicator registry.\"\"\"\\n        # unsubscribe callbacks\\n        super().__del__()\\n        # delete from replicator registry\\n        for _, annotators in self._rep_registry.items():\\n            for annotator, render_product_path in zip(annotators, self._render_product_paths):\\n                annotator.detach([render_product_path])\\n                annotator = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing information about the instance.\"\"\"\\n        # message for class\\n        return (\\n            f\"Camera @ \\'{self.cfg.prim_path}\\': \\\\n\"\\n            f\"\\\\tdata types   : {list(self.data.output.keys())} \\\\n\"\\n            f\"\\\\tsemantic filter : {self.cfg.semantic_filter}\\\\n\"\\n            f\"\\\\tcolorize semantic segm.   : {self.cfg.colorize_semantic_segmentation}\\\\n\"\\n            f\"\\\\tcolorize instance segm.   : {self.cfg.colorize_instance_segmentation}\\\\n\"\\n            f\"\\\\tcolorize instance id segm.: {self.cfg.colorize_instance_id_segmentation}\\\\n\"\\n            f\"\\\\tupdate period (s): {self.cfg.update_period}\\\\n\"\\n            f\"\\\\tshape        : {self.image_shape}\\\\n\"\\n            f\"\\\\tnumber of sensors : {self._view.count}\"\\n        )\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def num_instances(self) -> int:\\n        return self._view.count\\n\\n    @property\\n    def data(self) -> CameraData:\\n        # update sensors if needed\\n        self._update_outdated_buffers()\\n        # return the data\\n        return self._data\\n\\n    @property\\n    def frame(self) -> torch.tensor:\\n        \"\"\"Frame number when the measurement took place.\"\"\"\\n        return self._frame\\n\\n    @property\\n    def render_product_paths(self) -> list[str]:\\n        \"\"\"The path of the render products for the cameras.\\n\\n        This can be used via replicator interfaces to attach to writes or external annotator registry.\\n        \"\"\"\\n        return self._render_product_paths\\n\\n    @property\\n    def image_shape(self) -> tuple[int, int]:\\n        \"\"\"A tuple containing (height, width) of the camera sensor.\"\"\"\\n        return (self.cfg.height, self.cfg.width)\\n\\n    \"\"\"\\n    Configuration\\n    \"\"\"\\n\\n    def set_intrinsic_matrices(\\n        self, matrices: torch.Tensor, focal_length: float | None = None, env_ids: Sequence[int] | None = None\\n    ):\\n        \"\"\"Set parameters of the USD camera from its intrinsic matrix.\\n\\n        The intrinsic matrix is used to set the following parameters to the USD camera:\\n\\n        - ``focal_length``: The focal length of the camera.\\n        - ``horizontal_aperture``: The horizontal aperture of the camera.\\n        - ``vertical_aperture``: The vertical aperture of the camera.\\n        - ``horizontal_aperture_offset``: The horizontal offset of the camera.\\n        - ``vertical_aperture_offset``: The vertical offset of the camera.\\n\\n        .. warning::\\n\\n            Due to limitations of Omniverse camera, we need to assume that the camera is a spherical lens,\\n            i.e. has square pixels, and the optical center is centered at the camera eye. If this assumption\\n            is not true in the input intrinsic matrix, then the camera will not set up correctly.\\n\\n        Args:\\n            matrices: The intrinsic matrices for the camera. Shape is (N, 3, 3).\\n            focal_length: Perspective focal length (in cm) used to calculate pixel size. Defaults to None. If None,\\n                focal_length will be calculated 1 / width.\\n            env_ids: A sensor ids to manipulate. Defaults to None, which means all sensor indices.\\n        \"\"\"\\n        # resolve env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_INDICES\\n        # convert matrices to numpy tensors\\n        if isinstance(matrices, torch.Tensor):\\n            matrices = matrices.cpu().numpy()\\n        else:\\n            matrices = np.asarray(matrices, dtype=float)\\n        # iterate over env_ids\\n        for i, intrinsic_matrix in zip(env_ids, matrices):\\n\\n            height, width = self.image_shape\\n\\n            params = sensor_utils.convert_camera_intrinsics_to_usd(\\n                intrinsic_matrix=intrinsic_matrix.reshape(-1), height=height, width=width, focal_length=focal_length\\n            )\\n\\n            # change data for corresponding camera index\\n            sensor_prim = self._sensor_prims[i]\\n            # set parameters for camera\\n            for param_name, param_value in params.items():\\n                # convert to camel case (CC)\\n                param_name = to_camel_case(param_name, to=\"CC\")\\n                # get attribute from the class\\n                param_attr = getattr(sensor_prim, f\"Get{param_name}Attr\")\\n                # set value\\n                # note: We have to do it this way because the camera might be on a different\\n                #   layer (default cameras are on session layer), and this is the simplest\\n                #   way to set the property on the right layer.\\n                omni.usd.set_prop_val(param_attr(), param_value)\\n        # update the internal buffers\\n        self._update_intrinsic_matrices(env_ids)\\n\\n    \"\"\"\\n    Operations - Set pose.\\n    \"\"\"\\n\\n    def set_world_poses(\\n        self,\\n        positions: torch.Tensor | None = None,\\n        orientations: torch.Tensor | None = None,\\n        env_ids: Sequence[int] | None = None,\\n        convention: Literal[\"opengl\", \"ros\", \"world\"] = \"ros\",\\n    ):\\n        r\"\"\"Set the pose of the camera w.r.t. the world frame using specified convention.\\n\\n        Since different fields use different conventions for camera orientations, the method allows users to\\n        set the camera poses in the specified convention. Possible conventions are:\\n\\n        - :obj:`\"opengl\"` - forward axis: -Z - up axis +Y - Offset is applied in the OpenGL (Usd.Camera) convention\\n        - :obj:`\"ros\"`    - forward axis: +Z - up axis -Y - Offset is applied in the ROS convention\\n        - :obj:`\"world\"`  - forward axis: +X - up axis +Z - Offset is applied in the World Frame convention\\n\\n        See :meth:`isaaclab.sensors.camera.utils.convert_camera_frame_orientation_convention` for more details\\n        on the conventions.\\n\\n        Args:\\n            positions: The cartesian coordinates (in meters). Shape is (N, 3).\\n                Defaults to None, in which case the camera position in not changed.\\n            orientations: The quaternion orientation in (w, x, y, z). Shape is (N, 4).\\n                Defaults to None, in which case the camera orientation in not changed.\\n            env_ids: A sensor ids to manipulate. Defaults to None, which means all sensor indices.\\n            convention: The convention in which the poses are fed. Defaults to \"ros\".\\n\\n        Raises:\\n            RuntimeError: If the camera prim is not set. Need to call :meth:`initialize` method first.\\n        \"\"\"\\n        # resolve env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_INDICES\\n        # convert to backend tensor\\n        if positions is not None:\\n            if isinstance(positions, np.ndarray):\\n                positions = torch.from_numpy(positions).to(device=self._device)\\n            elif not isinstance(positions, torch.Tensor):\\n                positions = torch.tensor(positions, device=self._device)\\n        # convert rotation matrix from input convention to OpenGL\\n        if orientations is not None:\\n            if isinstance(orientations, np.ndarray):\\n                orientations = torch.from_numpy(orientations).to(device=self._device)\\n            elif not isinstance(orientations, torch.Tensor):\\n                orientations = torch.tensor(orientations, device=self._device)\\n            orientations = convert_camera_frame_orientation_convention(orientations, origin=convention, target=\"opengl\")\\n        # set the pose\\n        self._view.set_world_poses(positions, orientations, env_ids)\\n\\n    def set_world_poses_from_view(\\n        self, eyes: torch.Tensor, targets: torch.Tensor, env_ids: Sequence[int] | None = None\\n    ):\\n        \"\"\"Set the poses of the camera from the eye position and look-at target position.\\n\\n        Args:\\n            eyes: The positions of the camera\\'s eye. Shape is (N, 3).\\n            targets: The target locations to look at. Shape is (N, 3).\\n            env_ids: A sensor ids to manipulate. Defaults to None, which means all sensor indices.\\n\\n        Raises:\\n            RuntimeError: If the camera prim is not set. Need to call :meth:`initialize` method first.\\n            NotImplementedError: If the stage up-axis is not \"Y\" or \"Z\".\\n        \"\"\"\\n        # resolve env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_INDICES\\n        # get up axis of current stage\\n        up_axis = stage_utils.get_stage_up_axis()\\n        # set camera poses using the view\\n        orientations = quat_from_matrix(create_rotation_matrix_from_view(eyes, targets, up_axis, device=self._device))\\n        self._view.set_world_poses(eyes, orientations, env_ids)\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        if not self._is_initialized:\\n            raise RuntimeError(\\n                \"Camera could not be initialized. Please ensure --enable_cameras is used to enable rendering.\"\\n            )\\n        # reset the timestamps\\n        super().reset(env_ids)\\n        # resolve None\\n        # note: cannot do smart indexing here since we do a for loop over data.\\n        if env_ids is None:\\n            env_ids = self._ALL_INDICES\\n        # reset the data\\n        # note: this recomputation is useful if one performs events such as randomizations on the camera poses.\\n        self._update_poses(env_ids)\\n        # Reset the frame count\\n        self._frame[env_ids] = 0\\n\\n    \"\"\"\\n    Implementation.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        \"\"\"Initializes the sensor handles and internal buffers.\\n\\n        This function creates handles and registers the provided data types with the replicator registry to\\n        be able to access the data from the sensor. It also initializes the internal buffers to store the data.\\n\\n        Raises:\\n            RuntimeError: If the number of camera prims in the view does not match the number of environments.\\n            RuntimeError: If replicator was not found.\\n        \"\"\"\\n        carb_settings_iface = carb.settings.get_settings()\\n        if not carb_settings_iface.get(\"/isaaclab/cameras_enabled\"):\\n            raise RuntimeError(\\n                \"A camera was spawned without the --enable_cameras flag. Please use --enable_cameras to enable\"\\n                \" rendering.\"\\n            )\\n\\n        import omni.replicator.core as rep\\n        from omni.syntheticdata.scripts.SyntheticData import SyntheticData\\n\\n        # Initialize parent class\\n        super()._initialize_impl()\\n        # Create a view for the sensor\\n        self._view = XFormPrim(self.cfg.prim_path, reset_xform_properties=False)\\n        self._view.initialize()\\n        # Check that sizes are correct\\n        if self._view.count != self._num_envs:\\n            raise RuntimeError(\\n                f\"Number of camera prims in the view ({self._view.count}) does not match\"\\n                f\" the number of environments ({self._num_envs}).\"\\n            )\\n\\n        # Create all env_ids buffer\\n        self._ALL_INDICES = torch.arange(self._view.count, device=self._device, dtype=torch.long)\\n        # Create frame count buffer\\n        self._frame = torch.zeros(self._view.count, device=self._device, dtype=torch.long)\\n\\n        # Attach the sensor data types to render node\\n        self._render_product_paths: list[str] = list()\\n        self._rep_registry: dict[str, list[rep.annotators.Annotator]] = {name: list() for name in self.cfg.data_types}\\n\\n        # Obtain current stage\\n        stage = omni.usd.get_context().get_stage()\\n        # Convert all encapsulated prims to Camera\\n        for cam_prim_path in self._view.prim_paths:\\n            # Get camera prim\\n            cam_prim = stage.GetPrimAtPath(cam_prim_path)\\n            # Check if prim is a camera\\n            if not cam_prim.IsA(UsdGeom.Camera):\\n                raise RuntimeError(f\"Prim at path \\'{cam_prim_path}\\' is not a Camera.\")\\n            # Add to list\\n            sensor_prim = UsdGeom.Camera(cam_prim)\\n            self._sensor_prims.append(sensor_prim)\\n\\n            # Get render product\\n            # From Isaac Sim 2023.1 onwards, render product is a HydraTexture so we need to extract the path\\n            render_prod_path = rep.create.render_product(cam_prim_path, resolution=(self.cfg.width, self.cfg.height))\\n            if not isinstance(render_prod_path, str):\\n                render_prod_path = render_prod_path.path\\n            self._render_product_paths.append(render_prod_path)\\n\\n            # Check if semantic types or semantic filter predicate is provided\\n            if isinstance(self.cfg.semantic_filter, list):\\n                semantic_filter_predicate = \":*; \".join(self.cfg.semantic_filter) + \":*\"\\n            elif isinstance(self.cfg.semantic_filter, str):\\n                semantic_filter_predicate = self.cfg.semantic_filter\\n            else:\\n                raise ValueError(f\"Semantic types must be a list or a string. Received: {self.cfg.semantic_filter}.\")\\n            # set the semantic filter predicate\\n            # copied from rep.scripts.writes_default.basic_writer.py\\n            SyntheticData.Get().set_instance_mapping_semantic_filter(semantic_filter_predicate)\\n\\n            # Iterate over each data type and create annotator\\n            # TODO: This will move out of the loop once Replicator supports multiple render products within a single\\n            #  annotator, i.e.: rep_annotator.attach(self._render_product_paths)\\n            for name in self.cfg.data_types:\\n                # note: we are verbose here to make it easier to understand the code.\\n                #   if colorize is true, the data is mapped to colors and a uint8 4 channel image is returned.\\n                #   if colorize is false, the data is returned as a uint32 image with ids as values.\\n                if name == \"semantic_segmentation\":\\n                    init_params = {\\n                        \"colorize\": self.cfg.colorize_semantic_segmentation,\\n                        \"mapping\": json.dumps(self.cfg.semantic_segmentation_mapping),\\n                    }\\n                elif name == \"instance_segmentation_fast\":\\n                    init_params = {\"colorize\": self.cfg.colorize_instance_segmentation}\\n                elif name == \"instance_id_segmentation_fast\":\\n                    init_params = {\"colorize\": self.cfg.colorize_instance_id_segmentation}\\n                else:\\n                    init_params = None\\n\\n                # Resolve device name\\n                if \"cuda\" in self._device:\\n                    device_name = self._device.split(\":\")[0]\\n                else:\\n                    device_name = \"cpu\"\\n\\n                # Map special cases to their corresponding annotator names\\n                special_cases = {\"rgba\": \"rgb\", \"depth\": \"distance_to_image_plane\"}\\n                # Get the annotator name, falling back to the original name if not a special case\\n                annotator_name = special_cases.get(name, name)\\n                # Create the annotator node\\n                rep_annotator = rep.AnnotatorRegistry.get_annotator(annotator_name, init_params, device=device_name)\\n\\n                # attach annotator to render product\\n                rep_annotator.attach(render_prod_path)\\n                # add to registry\\n                self._rep_registry[name].append(rep_annotator)\\n\\n        # Create internal buffers\\n        self._create_buffers()\\n        self._update_intrinsic_matrices(self._ALL_INDICES)\\n\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        # Increment frame count\\n        self._frame[env_ids] += 1\\n        # -- pose\\n        if self.cfg.update_latest_camera_pose:\\n            self._update_poses(env_ids)\\n        # -- read the data from annotator registry\\n        # check if buffer is called for the first time. If so then, allocate the memory\\n        if len(self._data.output) == 0:\\n            # this is the first time buffer is called\\n            # it allocates memory for all the sensors\\n            self._create_annotator_data()\\n        else:\\n            # iterate over all the data types\\n            for name, annotators in self._rep_registry.items():\\n                # iterate over all the annotators\\n                for index in env_ids:\\n                    # get the output\\n                    output = annotators[index].get_data()\\n                    # process the output\\n                    data, info = self._process_annotator_output(name, output)\\n                    # add data to output\\n                    self._data.output[name][index] = data\\n                    # add info to output\\n                    self._data.info[index][name] = info\\n                # NOTE: The `distance_to_camera` annotator returns the distance to the camera optical center. However,\\n                #       the replicator depth clipping is applied w.r.t. to the image plane which may result in values\\n                #       larger than the clipping range in the output. We apply an additional clipping to ensure values\\n                #       are within the clipping range for all the annotators.\\n                if name == \"distance_to_camera\":\\n                    self._data.output[name][self._data.output[name] > self.cfg.spawn.clipping_range[1]] = torch.inf\\n                # apply defined clipping behavior\\n                if (\\n                    name == \"distance_to_camera\" or name == \"distance_to_image_plane\"\\n                ) and self.cfg.depth_clipping_behavior != \"none\":\\n                    self._data.output[name][torch.isinf(self._data.output[name])] = (\\n                        0.0 if self.cfg.depth_clipping_behavior == \"zero\" else self.cfg.spawn.clipping_range[1]\\n                    )\\n\\n    \"\"\"\\n    Private Helpers\\n    \"\"\"\\n\\n    def _check_supported_data_types(self, cfg: CameraCfg):\\n        \"\"\"Checks if the data types are supported by the ray-caster camera.\"\"\"\\n        # check if there is any intersection in unsupported types\\n        # reason: these use np structured data types which we can\\'t yet convert to torch tensor\\n        common_elements = set(cfg.data_types) & Camera.UNSUPPORTED_TYPES\\n        if common_elements:\\n            # provide alternative fast counterparts\\n            fast_common_elements = []\\n            for item in common_elements:\\n                if \"instance_segmentation\" in item or \"instance_id_segmentation\" in item:\\n                    fast_common_elements.append(item + \"_fast\")\\n            # raise error\\n            raise ValueError(\\n                f\"Camera class does not support the following sensor types: {common_elements}.\"\\n                \"\\\\n\\\\tThis is because these sensor types output numpy structured data types which\"\\n                \"can\\'t be converted to torch tensors easily.\"\\n                \"\\\\n\\\\tHint: If you need to work with these sensor types, we recommend using their fast counterparts.\"\\n                f\"\\\\n\\\\t\\\\tFast counterparts: {fast_common_elements}\"\\n            )\\n\\n    def _create_buffers(self):\\n        \"\"\"Create buffers for storing data.\"\"\"\\n        # create the data object\\n        # -- pose of the cameras\\n        self._data.pos_w = torch.zeros((self._view.count, 3), device=self._device)\\n        self._data.quat_w_world = torch.zeros((self._view.count, 4), device=self._device)\\n        # -- intrinsic matrix\\n        self._data.intrinsic_matrices = torch.zeros((self._view.count, 3, 3), device=self._device)\\n        self._data.image_shape = self.image_shape\\n        # -- output data\\n        # lazy allocation of data dictionary\\n        # since the size of the output data is not known in advance, we leave it as None\\n        # the memory will be allocated when the buffer() function is called for the first time.\\n        self._data.output = {}\\n        self._data.info = [{name: None for name in self.cfg.data_types} for _ in range(self._view.count)]\\n\\n    def _update_intrinsic_matrices(self, env_ids: Sequence[int]):\\n        \"\"\"Compute camera\\'s matrix of intrinsic parameters.\\n\\n        Also called calibration matrix. This matrix works for linear depth images. We assume square pixels.\\n\\n        Note:\\n            The calibration matrix projects points in the 3D scene onto an imaginary screen of the camera.\\n            The coordinates of points on the image plane are in the homogeneous representation.\\n        \"\"\"\\n        # iterate over all cameras\\n        for i in env_ids:\\n            # Get corresponding sensor prim\\n            sensor_prim = self._sensor_prims[i]\\n            # get camera parameters\\n            # currently rendering does not use aperture offsets or vertical aperture\\n            focal_length = sensor_prim.GetFocalLengthAttr().Get()\\n            horiz_aperture = sensor_prim.GetHorizontalApertureAttr().Get()\\n\\n            # get viewport parameters\\n            height, width = self.image_shape\\n            # extract intrinsic parameters\\n            f_x = (width * focal_length) / horiz_aperture\\n            f_y = f_x\\n            c_x = width * 0.5\\n            c_y = height * 0.5\\n            # create intrinsic matrix for depth linear\\n            self._data.intrinsic_matrices[i, 0, 0] = f_x\\n            self._data.intrinsic_matrices[i, 0, 2] = c_x\\n            self._data.intrinsic_matrices[i, 1, 1] = f_y\\n            self._data.intrinsic_matrices[i, 1, 2] = c_y\\n            self._data.intrinsic_matrices[i, 2, 2] = 1\\n\\n    def _update_poses(self, env_ids: Sequence[int]):\\n        \"\"\"Computes the pose of the camera in the world frame with ROS convention.\\n\\n        This methods uses the ROS convention to resolve the input pose. In this convention,\\n        we assume that the camera front-axis is +Z-axis and up-axis is -Y-axis.\\n\\n        Returns:\\n            A tuple of the position (in meters) and quaternion (w, x, y, z).\\n        \"\"\"\\n        # check camera prim exists\\n        if len(self._sensor_prims) == 0:\\n            raise RuntimeError(\"Camera prim is None. Please call \\'sim.play()\\' first.\")\\n\\n        # get the poses from the view\\n        poses, quat = self._view.get_world_poses(env_ids)\\n        self._data.pos_w[env_ids] = poses\\n        self._data.quat_w_world[env_ids] = convert_camera_frame_orientation_convention(\\n            quat, origin=\"opengl\", target=\"world\"\\n        )\\n\\n    def _create_annotator_data(self):\\n        \"\"\"Create the buffers to store the annotator data.\\n\\n        We create a buffer for each annotator and store the data in a dictionary. Since the data\\n        shape is not known beforehand, we create a list of buffers and concatenate them later.\\n\\n        This is an expensive operation and should be called only once.\\n        \"\"\"\\n        # add data from the annotators\\n        for name, annotators in self._rep_registry.items():\\n            # create a list to store the data for each annotator\\n            data_all_cameras = list()\\n            # iterate over all the annotators\\n            for index in self._ALL_INDICES:\\n                # get the output\\n                output = annotators[index].get_data()\\n                # process the output\\n                data, info = self._process_annotator_output(name, output)\\n                # append the data\\n                data_all_cameras.append(data)\\n                # store the info\\n                self._data.info[index][name] = info\\n            # concatenate the data along the batch dimension\\n            self._data.output[name] = torch.stack(data_all_cameras, dim=0)\\n            # NOTE: `distance_to_camera` and `distance_to_image_plane` are not both clipped to the maximum defined\\n            #       in the clipping range. The clipping is applied only to `distance_to_image_plane` and then both\\n            #       outputs are only clipped where the values in `distance_to_image_plane` exceed the threshold. To\\n            #       have a unified behavior between all cameras, we clip both outputs to the maximum value defined.\\n            if name == \"distance_to_camera\":\\n                self._data.output[name][self._data.output[name] > self.cfg.spawn.clipping_range[1]] = torch.inf\\n            # clip the data if needed\\n            if (\\n                name == \"distance_to_camera\" or name == \"distance_to_image_plane\"\\n            ) and self.cfg.depth_clipping_behavior != \"none\":\\n                self._data.output[name][torch.isinf(self._data.output[name])] = (\\n                    0.0 if self.cfg.depth_clipping_behavior == \"zero\" else self.cfg.spawn.clipping_range[1]\\n                )\\n\\n    def _process_annotator_output(self, name: str, output: Any) -> tuple[torch.tensor, dict | None]:\\n        \"\"\"Process the annotator output.\\n\\n        This function is called after the data has been collected from all the cameras.\\n        \"\"\"\\n        # extract info and data from the output\\n        if isinstance(output, dict):\\n            data = output[\"data\"]\\n            info = output[\"info\"]\\n        else:\\n            data = output\\n            info = None\\n        # convert data into torch tensor\\n        data = convert_to_torch(data, device=self.device)\\n\\n        # process data for different segmentation types\\n        # Note: Replicator returns raw buffers of dtype int32 for segmentation types\\n        #   so we need to convert them to uint8 4 channel images for colorized types\\n        height, width = self.image_shape\\n        if name == \"semantic_segmentation\":\\n            if self.cfg.colorize_semantic_segmentation:\\n                data = data.view(torch.uint8).reshape(height, width, -1)\\n            else:\\n                data = data.view(height, width, 1)\\n        elif name == \"instance_segmentation_fast\":\\n            if self.cfg.colorize_instance_segmentation:\\n                data = data.view(torch.uint8).reshape(height, width, -1)\\n            else:\\n                data = data.view(height, width, 1)\\n        elif name == \"instance_id_segmentation_fast\":\\n            if self.cfg.colorize_instance_id_segmentation:\\n                data = data.view(torch.uint8).reshape(height, width, -1)\\n            else:\\n                data = data.view(height, width, 1)\\n        # make sure buffer dimensions are consistent as (H, W, C)\\n        elif name == \"distance_to_camera\" or name == \"distance_to_image_plane\" or name == \"depth\":\\n            data = data.view(height, width, 1)\\n        # we only return the RGB channels from the RGBA output if rgb is required\\n        # normals return (x, y, z) in first 3 channels, 4th channel is unused\\n        elif name == \"rgb\" or name == \"normals\":\\n            data = data[..., :3]\\n        # motion vectors return (x, y) in first 2 channels, 3rd and 4th channels are unused\\n        elif name == \"motion_vectors\":\\n            data = data[..., :2]\\n\\n        # return the data and info\\n        return data, info\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        # set all existing views to None to invalidate them\\n        self._view = None'),\n",
       " Document(metadata={}, page_content='class CameraCfg(SensorBaseCfg):\\n    \"\"\"Configuration for a camera sensor.\"\"\"\\n\\n    @configclass\\n    class OffsetCfg:\\n        \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame.\"\"\"\\n\\n        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Translation w.r.t. the parent frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n\\n        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n        \"\"\"Quaternion rotation (w, x, y, z) w.r.t. the parent frame. Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"\\n\\n        convention: Literal[\"opengl\", \"ros\", \"world\"] = \"ros\"\\n        \"\"\"The convention in which the frame offset is applied. Defaults to \"ros\".\\n\\n        - ``\"opengl\"`` - forward axis: ``-Z`` - up axis: ``+Y`` - Offset is applied in the OpenGL (Usd.Camera) convention.\\n        - ``\"ros\"``    - forward axis: ``+Z`` - up axis: ``-Y`` - Offset is applied in the ROS convention.\\n        - ``\"world\"``  - forward axis: ``+X`` - up axis: ``+Z`` - Offset is applied in the World Frame convention.\\n\\n        \"\"\"\\n\\n    class_type: type = Camera\\n\\n    offset: OffsetCfg = OffsetCfg()\\n    \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame. Defaults to identity.\\n\\n    Note:\\n        The parent frame is the frame the sensor attaches to. For example, the parent frame of a\\n        camera at path ``/World/envs/env_0/Robot/Camera`` is ``/World/envs/env_0/Robot``.\\n    \"\"\"\\n\\n    spawn: PinholeCameraCfg | FisheyeCameraCfg | None = MISSING\\n    \"\"\"Spawn configuration for the asset.\\n\\n    If None, then the prim is not spawned by the asset. Instead, it is assumed that the\\n    asset is already present in the scene.\\n    \"\"\"\\n\\n    depth_clipping_behavior: Literal[\"max\", \"zero\", \"none\"] = \"none\"\\n    \"\"\"Clipping behavior for the camera for values exceed the maximum value. Defaults to \"none\".\\n\\n    - ``\"max\"``: Values are clipped to the maximum value.\\n    - ``\"zero\"``: Values are clipped to zero.\\n    - ``\"none``: No clipping is applied. Values will be returned as ``inf``.\\n    \"\"\"\\n\\n    data_types: list[str] = [\"rgb\"]\\n    \"\"\"List of sensor names/types to enable for the camera. Defaults to [\"rgb\"].\\n\\n    Please refer to the :class:`Camera` class for a list of available data types.\\n    \"\"\"\\n\\n    width: int = MISSING\\n    \"\"\"Width of the image in pixels.\"\"\"\\n\\n    height: int = MISSING\\n    \"\"\"Height of the image in pixels.\"\"\"\\n\\n    update_latest_camera_pose: bool = False\\n    \"\"\"Whether to update the latest camera pose when fetching the camera\\'s data. Defaults to False.\\n\\n    If True, the latest camera pose is updated in the camera\\'s data which will slow down performance\\n    due to the use of :class:`XformPrimView`.\\n    If False, the pose of the camera during initialization is returned.\\n    \"\"\"\\n\\n    semantic_filter: str | list[str] = \"*:*\"\\n    \"\"\"A string or a list specifying a semantic filter predicate. Defaults to ``\"*:*\"``.\\n\\n    If a string, it should be a disjunctive normal form of (semantic type, labels). For examples:\\n\\n    * ``\"typeA : labelA & !labelB | labelC , typeB: labelA ; typeC: labelE\"``:\\n      All prims with semantic type \"typeA\" and label \"labelA\" but not \"labelB\" or with label \"labelC\".\\n      Also, all prims with semantic type \"typeB\" and label \"labelA\", or with semantic type \"typeC\" and label \"labelE\".\\n    * ``\"typeA : * ; * : labelA\"``: All prims with semantic type \"typeA\" or with label \"labelA\"\\n\\n    If a list of strings, each string should be a semantic type. The segmentation for prims with\\n    semantics of the specified types will be retrieved. For example, if the list is [\"class\"], only\\n    the segmentation for prims with semantics of type \"class\" will be retrieved.\\n\\n    .. seealso::\\n\\n        For more information on the semantics filter, see the documentation on `Replicator Semantics Schema Editor`_.\\n\\n    .. _Replicator Semantics Schema Editor: https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/semantics_schema_editor.html#semantics-filtering\\n    \"\"\"\\n\\n    colorize_semantic_segmentation: bool = True\\n    \"\"\"Whether to colorize the semantic segmentation images. Defaults to True.\\n\\n    If True, semantic segmentation is converted to an image where semantic IDs are mapped to colors\\n    and returned as a ``uint8`` 4-channel array. If False, the output is returned as a ``int32`` array.\\n    \"\"\"\\n\\n    colorize_instance_id_segmentation: bool = True\\n    \"\"\"Whether to colorize the instance ID segmentation images. Defaults to True.\\n\\n    If True, instance id segmentation is converted to an image where instance IDs are mapped to colors.\\n    and returned as a ``uint8`` 4-channel array. If False, the output is returned as a ``int32`` array.\\n    \"\"\"\\n\\n    colorize_instance_segmentation: bool = True\\n    \"\"\"Whether to colorize the instance ID segmentation images. Defaults to True.\\n\\n    If True, instance segmentation is converted to an image where instance IDs are mapped to colors.\\n    and returned as a ``uint8`` 4-channel array. If False, the output is returned as a ``int32`` array.\\n    \"\"\"\\n\\n    semantic_segmentation_mapping: dict = {}\\n    \"\"\"Dictionary mapping semantics to specific colours\\n\\n    Eg.\\n\\n    .. code-block:: python\\n\\n        {\\n            \"class:cube_1\": (255, 36, 66, 255),\\n            \"class:cube_2\": (255, 184, 48, 255),\\n            \"class:cube_3\": (55, 255, 139, 255),\\n            \"class:table\": (255, 237, 218, 255),\\n            \"class:ground\": (100, 100, 100, 255),\\n            \"class:robot\": (61, 178, 255, 255),\\n        }\\n\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class CameraData:\\n    \"\"\"Data container for the camera sensor.\"\"\"\\n\\n    ##\\n    # Frame state.\\n    ##\\n\\n    pos_w: torch.Tensor = None\\n    \"\"\"Position of the sensor origin in world frame, following ROS convention.\\n\\n    Shape is (N, 3) where N is the number of sensors.\\n    \"\"\"\\n\\n    quat_w_world: torch.Tensor = None\\n    \"\"\"Quaternion orientation `(w, x, y, z)` of the sensor origin in world frame, following the world coordinate frame\\n\\n    .. note::\\n        World frame convention follows the camera aligned with forward axis +X and up axis +Z.\\n\\n    Shape is (N, 4) where N is the number of sensors.\\n    \"\"\"\\n\\n    ##\\n    # Camera data\\n    ##\\n\\n    image_shape: tuple[int, int] = None\\n    \"\"\"A tuple containing (height, width) of the camera sensor.\"\"\"\\n\\n    intrinsic_matrices: torch.Tensor = None\\n    \"\"\"The intrinsic matrices for the camera.\\n\\n    Shape is (N, 3, 3) where N is the number of sensors.\\n    \"\"\"\\n\\n    output: dict[str, torch.Tensor] = None\\n    \"\"\"The retrieved sensor data with sensor types as key.\\n\\n    The format of the data is available in the `Replicator Documentation`_. For semantic-based data,\\n    this corresponds to the ``\"data\"`` key in the output of the sensor.\\n\\n    .. _Replicator Documentation: https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_replicator/annotators_details.html#annotator-output\\n    \"\"\"\\n\\n    info: list[dict[str, Any]] = None\\n    \"\"\"The retrieved sensor info with sensor types as key.\\n\\n    This contains extra information provided by the sensor such as semantic segmentation label mapping, prim paths.\\n    For semantic-based data, this corresponds to the ``\"info\"`` key in the output of the sensor. For other sensor\\n    types, the info is empty.\\n    \"\"\"\\n\\n    ##\\n    # Additional Frame orientation conventions\\n    ##\\n\\n    @property\\n    def quat_w_ros(self) -> torch.Tensor:\\n        \"\"\"Quaternion orientation `(w, x, y, z)` of the sensor origin in the world frame, following ROS convention.\\n\\n        .. note::\\n            ROS convention follows the camera aligned with forward axis +Z and up axis -Y.\\n\\n        Shape is (N, 4) where N is the number of sensors.\\n        \"\"\"\\n        return convert_camera_frame_orientation_convention(self.quat_w_world, origin=\"world\", target=\"ros\")\\n\\n    @property\\n    def quat_w_opengl(self) -> torch.Tensor:\\n        \"\"\"Quaternion orientation `(w, x, y, z)` of the sensor origin in the world frame, following\\n        Opengl / USD Camera convention.\\n\\n        .. note::\\n            OpenGL convention follows the camera aligned with forward axis -Z and up axis +Y.\\n\\n        Shape is (N, 4) where N is the number of sensors.\\n        \"\"\"\\n        return convert_camera_frame_orientation_convention(self.quat_w_world, origin=\"world\", target=\"opengl\")'),\n",
       " Document(metadata={}, page_content='class TiledCamera(Camera):\\n    r\"\"\"The tiled rendering based camera sensor for acquiring the same data as the Camera class.\\n\\n    This class inherits from the :class:`Camera` class but uses the tiled-rendering API to acquire\\n    the visual data. Tiled-rendering concatenates the rendered images from multiple cameras into a single image.\\n    This allows for rendering multiple cameras in parallel and is useful for rendering large scenes with multiple\\n    cameras efficiently.\\n\\n    The following sensor types are supported:\\n\\n    - ``\"rgb\"``: A 3-channel rendered color image.\\n    - ``\"rgba\"``: A 4-channel rendered color image with alpha channel.\\n    - ``\"distance_to_camera\"``: An image containing the distance to camera optical center.\\n    - ``\"distance_to_image_plane\"``: An image containing distances of 3D points from camera plane along camera\\'s z-axis.\\n    - ``\"depth\"``: Alias for ``\"distance_to_image_plane\"``.\\n    - ``\"normals\"``: An image containing the local surface normal vectors at each pixel.\\n    - ``\"motion_vectors\"``: An image containing the motion vector data at each pixel.\\n    - ``\"semantic_segmentation\"``: The semantic segmentation data.\\n    - ``\"instance_segmentation_fast\"``: The instance segmentation data.\\n    - ``\"instance_id_segmentation_fast\"``: The instance id segmentation data.\\n\\n    .. note::\\n        Currently the following sensor types are not supported in a \"view\" format:\\n\\n        - ``\"instance_segmentation\"``: The instance segmentation data. Please use the fast counterparts instead.\\n        - ``\"instance_id_segmentation\"``: The instance id segmentation data. Please use the fast counterparts instead.\\n        - ``\"bounding_box_2d_tight\"``: The tight 2D bounding box data (only contains non-occluded regions).\\n        - ``\"bounding_box_2d_tight_fast\"``: The tight 2D bounding box data (only contains non-occluded regions).\\n        - ``\"bounding_box_2d_loose\"``: The loose 2D bounding box data (contains occluded regions).\\n        - ``\"bounding_box_2d_loose_fast\"``: The loose 2D bounding box data (contains occluded regions).\\n        - ``\"bounding_box_3d\"``: The 3D view space bounding box data.\\n        - ``\"bounding_box_3d_fast\"``: The 3D view space bounding box data.\\n\\n    .. _replicator extension: https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/annotators_details.html#annotator-output\\n    .. _USDGeom Camera: https://graphics.pixar.com/usd/docs/api/class_usd_geom_camera.html\\n\\n    .. versionadded:: v1.0.0\\n\\n        This feature is available starting from Isaac Sim 4.2. Before this version, the tiled rendering APIs\\n        were not available.\\n\\n    \"\"\"\\n\\n    cfg: TiledCameraCfg\\n    \"\"\"The configuration parameters.\"\"\"\\n\\n    def __init__(self, cfg: TiledCameraCfg):\\n        \"\"\"Initializes the tiled camera sensor.\\n\\n        Args:\\n            cfg: The configuration parameters.\\n\\n        Raises:\\n            RuntimeError: If no camera prim is found at the given path.\\n            RuntimeError: If Isaac Sim version < 4.2\\n            ValueError: If the provided data types are not supported by the camera.\\n        \"\"\"\\n        isaac_sim_version = float(\".\".join(get_version()[2:4]))\\n        if isaac_sim_version < 4.2:\\n            raise RuntimeError(\\n                f\"TiledCamera is only available from Isaac Sim 4.2.0. Current version is {isaac_sim_version}. Please\"\\n                \" update to Isaac Sim 4.2.0\"\\n            )\\n        super().__init__(cfg)\\n\\n    def __del__(self):\\n        \"\"\"Unsubscribes from callbacks and detach from the replicator registry.\"\"\"\\n        # unsubscribe from callbacks\\n        SensorBase.__del__(self)\\n        # detach from the replicator registry\\n        for annotator in self._annotators.values():\\n            annotator.detach(self.render_product_paths)\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing information about the instance.\"\"\"\\n        # message for class\\n        return (\\n            f\"Tiled Camera @ \\'{self.cfg.prim_path}\\': \\\\n\"\\n            f\"\\\\tdata types   : {list(self.data.output.keys())} \\\\n\"\\n            f\"\\\\tsemantic filter : {self.cfg.semantic_filter}\\\\n\"\\n            f\"\\\\tcolorize semantic segm.   : {self.cfg.colorize_semantic_segmentation}\\\\n\"\\n            f\"\\\\tcolorize instance segm.   : {self.cfg.colorize_instance_segmentation}\\\\n\"\\n            f\"\\\\tcolorize instance id segm.: {self.cfg.colorize_instance_id_segmentation}\\\\n\"\\n            f\"\\\\tupdate period (s): {self.cfg.update_period}\\\\n\"\\n            f\"\\\\tshape        : {self.image_shape}\\\\n\"\\n            f\"\\\\tnumber of sensors : {self._view.count}\"\\n        )\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        if not self._is_initialized:\\n            raise RuntimeError(\\n                \"TiledCamera could not be initialized. Please ensure --enable_cameras is used to enable rendering.\"\\n            )\\n        # reset the timestamps\\n        SensorBase.reset(self, env_ids)\\n        # resolve None\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset the frame count\\n        self._frame[env_ids] = 0\\n\\n    \"\"\"\\n    Implementation.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        \"\"\"Initializes the sensor handles and internal buffers.\\n\\n        This function creates handles and registers the provided data types with the replicator registry to\\n        be able to access the data from the sensor. It also initializes the internal buffers to store the data.\\n\\n        Raises:\\n            RuntimeError: If the number of camera prims in the view does not match the number of environments.\\n            RuntimeError: If replicator was not found.\\n        \"\"\"\\n        carb_settings_iface = carb.settings.get_settings()\\n        if not carb_settings_iface.get(\"/isaaclab/cameras_enabled\"):\\n            raise RuntimeError(\\n                \"A camera was spawned without the --enable_cameras flag. Please use --enable_cameras to enable\"\\n                \" rendering.\"\\n            )\\n\\n        import omni.replicator.core as rep\\n\\n        # Initialize parent class\\n        SensorBase._initialize_impl(self)\\n        # Create a view for the sensor\\n        self._view = XFormPrim(self.cfg.prim_path, reset_xform_properties=False)\\n        self._view.initialize()\\n        # Check that sizes are correct\\n        if self._view.count != self._num_envs:\\n            raise RuntimeError(\\n                f\"Number of camera prims in the view ({self._view.count}) does not match\"\\n                f\" the number of environments ({self._num_envs}).\"\\n            )\\n\\n        # Create all env_ids buffer\\n        self._ALL_INDICES = torch.arange(self._view.count, device=self._device, dtype=torch.long)\\n        # Create frame count buffer\\n        self._frame = torch.zeros(self._view.count, device=self._device, dtype=torch.long)\\n\\n        # Obtain current stage\\n        stage = omni.usd.get_context().get_stage()\\n        # Convert all encapsulated prims to Camera\\n        for cam_prim_path in self._view.prim_paths:\\n            # Get camera prim\\n            cam_prim = stage.GetPrimAtPath(cam_prim_path)\\n            # Check if prim is a camera\\n            if not cam_prim.IsA(UsdGeom.Camera):\\n                raise RuntimeError(f\"Prim at path \\'{cam_prim_path}\\' is not a Camera.\")\\n            # Add to list\\n            sensor_prim = UsdGeom.Camera(cam_prim)\\n            self._sensor_prims.append(sensor_prim)\\n\\n        # Create replicator tiled render product\\n        rp = rep.create.render_product_tiled(\\n            cameras=self._view.prim_paths, tile_resolution=(self.cfg.width, self.cfg.height)\\n        )\\n        self._render_product_paths = [rp.path]\\n\\n        # Define the annotators based on requested data types\\n        self._annotators = dict()\\n        for annotator_type in self.cfg.data_types:\\n            if annotator_type == \"rgba\" or annotator_type == \"rgb\":\\n                annotator = rep.AnnotatorRegistry.get_annotator(\"rgb\", device=self.device, do_array_copy=False)\\n                self._annotators[\"rgba\"] = annotator\\n            elif annotator_type == \"depth\" or annotator_type == \"distance_to_image_plane\":\\n                # keep depth for backwards compatibility\\n                annotator = rep.AnnotatorRegistry.get_annotator(\\n                    \"distance_to_image_plane\", device=self.device, do_array_copy=False\\n                )\\n                self._annotators[annotator_type] = annotator\\n            # note: we are verbose here to make it easier to understand the code.\\n            #   if colorize is true, the data is mapped to colors and a uint8 4 channel image is returned.\\n            #   if colorize is false, the data is returned as a uint32 image with ids as values.\\n            else:\\n                init_params = None\\n                if annotator_type == \"semantic_segmentation\":\\n                    init_params = {\\n                        \"colorize\": self.cfg.colorize_semantic_segmentation,\\n                        \"mapping\": json.dumps(self.cfg.semantic_segmentation_mapping),\\n                    }\\n                elif annotator_type == \"instance_segmentation_fast\":\\n                    init_params = {\"colorize\": self.cfg.colorize_instance_segmentation}\\n                elif annotator_type == \"instance_id_segmentation_fast\":\\n                    init_params = {\"colorize\": self.cfg.colorize_instance_id_segmentation}\\n\\n                annotator = rep.AnnotatorRegistry.get_annotator(\\n                    annotator_type, init_params, device=self.device, do_array_copy=False\\n                )\\n                self._annotators[annotator_type] = annotator\\n\\n        # Attach the annotator to the render product\\n        for annotator in self._annotators.values():\\n            annotator.attach(self._render_product_paths)\\n\\n        # Create internal buffers\\n        self._create_buffers()\\n\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        # Increment frame count\\n        self._frame[env_ids] += 1\\n\\n        # update latest camera pose\\n        if self.cfg.update_latest_camera_pose:\\n            self._update_poses(env_ids)\\n\\n        # Extract the flattened image buffer\\n        for data_type, annotator in self._annotators.items():\\n            # check whether returned data is a dict (used for segmentation)\\n            output = annotator.get_data()\\n            if isinstance(output, dict):\\n                tiled_data_buffer = output[\"data\"]\\n                self._data.info[data_type] = output[\"info\"]\\n            else:\\n                tiled_data_buffer = output\\n\\n            # convert data buffer to warp array\\n            if isinstance(tiled_data_buffer, np.ndarray):\\n                tiled_data_buffer = wp.array(tiled_data_buffer, device=self.device, dtype=wp.uint8)\\n            else:\\n                tiled_data_buffer = tiled_data_buffer.to(device=self.device)\\n\\n            # process data for different segmentation types\\n            # Note: Replicator returns raw buffers of dtype uint32 for segmentation types\\n            #   so we need to convert them to uint8 4 channel images for colorized types\\n            if (\\n                (data_type == \"semantic_segmentation\" and self.cfg.colorize_semantic_segmentation)\\n                or (data_type == \"instance_segmentation_fast\" and self.cfg.colorize_instance_segmentation)\\n                or (data_type == \"instance_id_segmentation_fast\" and self.cfg.colorize_instance_id_segmentation)\\n            ):\\n                tiled_data_buffer = wp.array(\\n                    ptr=tiled_data_buffer.ptr, shape=(*tiled_data_buffer.shape, 4), dtype=wp.uint8, device=self.device\\n                )\\n\\n            # For motion vectors, we only require the first two channels of the tiled buffer\\n            # Note: Not doing this breaks the alignment of the data (check: https://github.com/isaac-sim/IsaacLab/issues/2003)\\n            if data_type == \"motion_vectors\":\\n                tiled_data_buffer = tiled_data_buffer[:, :, :2].contiguous()\\n\\n            wp.launch(\\n                kernel=reshape_tiled_image,\\n                dim=(self._view.count, self.cfg.height, self.cfg.width),\\n                inputs=[\\n                    tiled_data_buffer.flatten(),\\n                    wp.from_torch(self._data.output[data_type]),  # zero-copy alias\\n                    *list(self._data.output[data_type].shape[1:]),  # height, width, num_channels\\n                    self._tiling_grid_shape()[0],  # num_tiles_x\\n                ],\\n                device=self.device,\\n            )\\n\\n            # alias rgb as first 3 channels of rgba\\n            if data_type == \"rgba\" and \"rgb\" in self.cfg.data_types:\\n                self._data.output[\"rgb\"] = self._data.output[\"rgba\"][..., :3]\\n\\n            # NOTE: The `distance_to_camera` annotator returns the distance to the camera optical center. However,\\n            #       the replicator depth clipping is applied w.r.t. to the image plane which may result in values\\n            #       larger than the clipping range in the output. We apply an additional clipping to ensure values\\n            #       are within the clipping range for all the annotators.\\n            if data_type == \"distance_to_camera\":\\n                self._data.output[data_type][\\n                    self._data.output[data_type] > self.cfg.spawn.clipping_range[1]\\n                ] = torch.inf\\n            # apply defined clipping behavior\\n            if (\\n                data_type == \"distance_to_camera\" or data_type == \"distance_to_image_plane\" or data_type == \"depth\"\\n            ) and self.cfg.depth_clipping_behavior != \"none\":\\n                self._data.output[data_type][torch.isinf(self._data.output[data_type])] = (\\n                    0.0 if self.cfg.depth_clipping_behavior == \"zero\" else self.cfg.spawn.clipping_range[1]\\n                )\\n\\n    \"\"\"\\n    Private Helpers\\n    \"\"\"\\n\\n    def _check_supported_data_types(self, cfg: TiledCameraCfg):\\n        \"\"\"Checks if the data types are supported by the ray-caster camera.\"\"\"\\n        # check if there is any intersection in unsupported types\\n        # reason: these use np structured data types which we can\\'t yet convert to torch tensor\\n        common_elements = set(cfg.data_types) & Camera.UNSUPPORTED_TYPES\\n        if common_elements:\\n            # provide alternative fast counterparts\\n            fast_common_elements = []\\n            for item in common_elements:\\n                if \"instance_segmentation\" in item or \"instance_id_segmentation\" in item:\\n                    fast_common_elements.append(item + \"_fast\")\\n            # raise error\\n            raise ValueError(\\n                f\"TiledCamera class does not support the following sensor types: {common_elements}.\"\\n                \"\\\\n\\\\tThis is because these sensor types output numpy structured data types which\"\\n                \"can\\'t be converted to torch tensors easily.\"\\n                \"\\\\n\\\\tHint: If you need to work with these sensor types, we recommend using their fast counterparts.\"\\n                f\"\\\\n\\\\t\\\\tFast counterparts: {fast_common_elements}\"\\n            )\\n\\n    def _create_buffers(self):\\n        \"\"\"Create buffers for storing data.\"\"\"\\n        # create the data object\\n        # -- pose of the cameras\\n        self._data.pos_w = torch.zeros((self._view.count, 3), device=self._device)\\n        self._data.quat_w_world = torch.zeros((self._view.count, 4), device=self._device)\\n        self._update_poses(self._ALL_INDICES)\\n        # -- intrinsic matrix\\n        self._data.intrinsic_matrices = torch.zeros((self._view.count, 3, 3), device=self._device)\\n        self._update_intrinsic_matrices(self._ALL_INDICES)\\n        self._data.image_shape = self.image_shape\\n        # -- output data\\n        data_dict = dict()\\n        if \"rgba\" in self.cfg.data_types or \"rgb\" in self.cfg.data_types:\\n            data_dict[\"rgba\"] = torch.zeros(\\n                (self._view.count, self.cfg.height, self.cfg.width, 4), device=self.device, dtype=torch.uint8\\n            ).contiguous()\\n        if \"rgb\" in self.cfg.data_types:\\n            # RGB is the first 3 channels of RGBA\\n            data_dict[\"rgb\"] = data_dict[\"rgba\"][..., :3]\\n        if \"distance_to_image_plane\" in self.cfg.data_types:\\n            data_dict[\"distance_to_image_plane\"] = torch.zeros(\\n                (self._view.count, self.cfg.height, self.cfg.width, 1), device=self.device, dtype=torch.float32\\n            ).contiguous()\\n        if \"depth\" in self.cfg.data_types:\\n            data_dict[\"depth\"] = torch.zeros(\\n                (self._view.count, self.cfg.height, self.cfg.width, 1), device=self.device, dtype=torch.float32\\n            ).contiguous()\\n        if \"distance_to_camera\" in self.cfg.data_types:\\n            data_dict[\"distance_to_camera\"] = torch.zeros(\\n                (self._view.count, self.cfg.height, self.cfg.width, 1), device=self.device, dtype=torch.float32\\n            ).contiguous()\\n        if \"normals\" in self.cfg.data_types:\\n            data_dict[\"normals\"] = torch.zeros(\\n                (self._view.count, self.cfg.height, self.cfg.width, 3), device=self.device, dtype=torch.float32\\n            ).contiguous()\\n        if \"motion_vectors\" in self.cfg.data_types:\\n            data_dict[\"motion_vectors\"] = torch.zeros(\\n                (self._view.count, self.cfg.height, self.cfg.width, 2), device=self.device, dtype=torch.float32\\n            ).contiguous()\\n        if \"semantic_segmentation\" in self.cfg.data_types:\\n            if self.cfg.colorize_semantic_segmentation:\\n                data_dict[\"semantic_segmentation\"] = torch.zeros(\\n                    (self._view.count, self.cfg.height, self.cfg.width, 4), device=self.device, dtype=torch.uint8\\n                ).contiguous()\\n            else:\\n                data_dict[\"semantic_segmentation\"] = torch.zeros(\\n                    (self._view.count, self.cfg.height, self.cfg.width, 1), device=self.device, dtype=torch.int32\\n                ).contiguous()\\n        if \"instance_segmentation_fast\" in self.cfg.data_types:\\n            if self.cfg.colorize_instance_segmentation:\\n                data_dict[\"instance_segmentation_fast\"] = torch.zeros(\\n                    (self._view.count, self.cfg.height, self.cfg.width, 4), device=self.device, dtype=torch.uint8\\n                ).contiguous()\\n            else:\\n                data_dict[\"instance_segmentation_fast\"] = torch.zeros(\\n                    (self._view.count, self.cfg.height, self.cfg.width, 1), device=self.device, dtype=torch.int32\\n                ).contiguous()\\n        if \"instance_id_segmentation_fast\" in self.cfg.data_types:\\n            if self.cfg.colorize_instance_id_segmentation:\\n                data_dict[\"instance_id_segmentation_fast\"] = torch.zeros(\\n                    (self._view.count, self.cfg.height, self.cfg.width, 4), device=self.device, dtype=torch.uint8\\n                ).contiguous()\\n            else:\\n                data_dict[\"instance_id_segmentation_fast\"] = torch.zeros(\\n                    (self._view.count, self.cfg.height, self.cfg.width, 1), device=self.device, dtype=torch.int32\\n                ).contiguous()\\n\\n        self._data.output = data_dict\\n        self._data.info = dict()\\n\\n    def _tiled_image_shape(self) -> tuple[int, int]:\\n        \"\"\"Returns a tuple containing the dimension of the tiled image.\"\"\"\\n        cols, rows = self._tiling_grid_shape()\\n        return (self.cfg.width * cols, self.cfg.height * rows)\\n\\n    def _tiling_grid_shape(self) -> tuple[int, int]:\\n        \"\"\"Returns a tuple containing the tiling grid dimension.\"\"\"\\n        cols = math.ceil(math.sqrt(self._view.count))\\n        rows = math.ceil(self._view.count / cols)\\n        return (cols, rows)\\n\\n    def _create_annotator_data(self):\\n        # we do not need to create annotator data for the tiled camera sensor\\n        raise RuntimeError(\"This function should not be called for the tiled camera sensor.\")\\n\\n    def _process_annotator_output(self, name: str, output: Any) -> tuple[torch.tensor, dict | None]:\\n        # we do not need to process annotator output for the tiled camera sensor\\n        raise RuntimeError(\"This function should not be called for the tiled camera sensor.\")\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        # set all existing views to None to invalidate them\\n        self._view = None'),\n",
       " Document(metadata={}, page_content='class TiledCameraCfg(CameraCfg):\\n    \"\"\"Configuration for a tiled rendering-based camera sensor.\"\"\"\\n\\n    class_type: type = TiledCamera'),\n",
       " Document(metadata={}, page_content='def transform_points(\\n    points: TensorData,\\n    position: Sequence[float] | None = None,\\n    orientation: Sequence[float] | None = None,\\n    device: torch.device | str | None = None,\\n) -> np.ndarray | torch.Tensor:\\n    r\"\"\"Transform input points in a given frame to a target frame.\\n\\n    This function transform points from a source frame to a target frame. The transformation is defined by the\\n    position ``t`` and orientation ``R`` of the target frame in the source frame.\\n\\n    .. math::\\n        p_{target} = R_{target} \\\\times p_{source} + t_{target}\\n\\n    If either the inputs `position` and `orientation` are None, the corresponding transformation is not applied.\\n\\n    Args:\\n        points: a tensor of shape (p, 3) or (n, p, 3) comprising of 3d points in source frame.\\n        position: The position of source frame in target frame. Defaults to None.\\n        orientation: The orientation (w, x, y, z) of source frame in target frame.\\n            Defaults to None.\\n        device: The device for torch where the computation\\n            should be executed. Defaults to None, i.e. takes the device that matches the depth image.\\n\\n    Returns:\\n        A tensor of shape (N, 3) comprising of 3D points in target frame.\\n        If the input is a numpy array, the output is a numpy array. Otherwise, it is a torch tensor.\\n    \"\"\"\\n    # check if numpy\\n    is_numpy = isinstance(points, np.ndarray)\\n    # decide device\\n    if device is None and is_numpy:\\n        device = torch.device(\"cpu\")\\n    # convert to torch\\n    points = convert_to_torch(points, dtype=torch.float32, device=device)\\n    # update the device with the device of the depth image\\n    # note: this is needed since warp does not provide the device directly\\n    device = points.device\\n    # apply rotation\\n    if orientation is not None:\\n        orientation = convert_to_torch(orientation, dtype=torch.float32, device=device)\\n    # apply translation\\n    if position is not None:\\n        position = convert_to_torch(position, dtype=torch.float32, device=device)\\n    # apply transformation\\n    points = math_utils.transform_points(points, position, orientation)\\n\\n    # return everything according to input type\\n    if is_numpy:\\n        return points.detach().cpu().numpy()\\n    else:\\n        return points'),\n",
       " Document(metadata={}, page_content='def create_pointcloud_from_depth(\\n    intrinsic_matrix: np.ndarray | torch.Tensor | wp.array,\\n    depth: np.ndarray | torch.Tensor | wp.array,\\n    keep_invalid: bool = False,\\n    position: Sequence[float] | None = None,\\n    orientation: Sequence[float] | None = None,\\n    device: torch.device | str | None = None,\\n) -> np.ndarray | torch.Tensor:\\n    r\"\"\"Creates pointcloud from input depth image and camera intrinsic matrix.\\n\\n    This function creates a pointcloud from a depth image and camera intrinsic matrix. The pointcloud is\\n    computed using the following equation:\\n\\n    .. math::\\n        p_{camera} = K^{-1} \\\\times [u, v, 1]^T \\\\times d\\n\\n    where :math:`K` is the camera intrinsic matrix, :math:`u` and :math:`v` are the pixel coordinates and\\n    :math:`d` is the depth value at the pixel.\\n\\n    Additionally, the pointcloud can be transformed from the camera frame to a target frame by providing\\n    the position ``t`` and orientation ``R`` of the camera in the target frame:\\n\\n    .. math::\\n        p_{target} = R_{target} \\\\times p_{camera} + t_{target}\\n\\n    Args:\\n        intrinsic_matrix: A (3, 3) array providing camera\\'s calibration matrix.\\n        depth: An array of shape (H, W) with values encoding the depth measurement.\\n        keep_invalid: Whether to keep invalid points in the cloud or not. Invalid points\\n            correspond to pixels with depth values 0.0 or NaN. Defaults to False.\\n        position: The position of the camera in a target frame. Defaults to None.\\n        orientation: The orientation (w, x, y, z) of the camera in a target frame. Defaults to None.\\n        device: The device for torch where the computation should be executed.\\n            Defaults to None, i.e. takes the device that matches the depth image.\\n\\n    Returns:\\n        An array/tensor of shape (N, 3) comprising of 3D coordinates of points.\\n        The returned datatype is torch if input depth is of type torch.tensor or wp.array. Otherwise, a np.ndarray\\n        is returned.\\n    \"\"\"\\n    # We use PyTorch here for matrix multiplication since it is compiled with Intel MKL while numpy\\n    # by default uses OpenBLAS. With PyTorch (CPU), we could process a depth image of size (480, 640)\\n    # in 0.0051 secs, while with numpy it took 0.0292 secs.\\n\\n    # convert to numpy matrix\\n    is_numpy = isinstance(depth, np.ndarray)\\n    # decide device\\n    if device is None and is_numpy:\\n        device = torch.device(\"cpu\")\\n    # convert depth to torch tensor\\n    depth = convert_to_torch(depth, dtype=torch.float32, device=device)\\n    # update the device with the device of the depth image\\n    # note: this is needed since warp does not provide the device directly\\n    device = depth.device\\n    # convert inputs to torch tensors\\n    intrinsic_matrix = convert_to_torch(intrinsic_matrix, dtype=torch.float32, device=device)\\n    if position is not None:\\n        position = convert_to_torch(position, dtype=torch.float32, device=device)\\n    if orientation is not None:\\n        orientation = convert_to_torch(orientation, dtype=torch.float32, device=device)\\n    # compute pointcloud\\n    depth_cloud = math_utils.unproject_depth(depth, intrinsic_matrix)\\n    # convert 3D points to world frame\\n    depth_cloud = math_utils.transform_points(depth_cloud, position, orientation)\\n\\n    # keep only valid entries if flag is set\\n    if not keep_invalid:\\n        pts_idx_to_keep = torch.all(torch.logical_and(~torch.isnan(depth_cloud), ~torch.isinf(depth_cloud)), dim=1)\\n        depth_cloud = depth_cloud[pts_idx_to_keep, ...]\\n\\n    # return everything according to input type\\n    if is_numpy:\\n        return depth_cloud.detach().cpu().numpy()\\n    else:\\n        return depth_cloud'),\n",
       " Document(metadata={}, page_content='def create_pointcloud_from_rgbd(\\n    intrinsic_matrix: torch.Tensor | np.ndarray | wp.array,\\n    depth: torch.Tensor | np.ndarray | wp.array,\\n    rgb: torch.Tensor | wp.array | np.ndarray | tuple[float, float, float] = None,\\n    normalize_rgb: bool = False,\\n    position: Sequence[float] | None = None,\\n    orientation: Sequence[float] | None = None,\\n    device: torch.device | str | None = None,\\n    num_channels: int = 3,\\n) -> tuple[torch.Tensor, torch.Tensor] | tuple[np.ndarray, np.ndarray]:\\n    \"\"\"Creates pointcloud from input depth image and camera transformation matrix.\\n\\n    This function provides the same functionality as :meth:`create_pointcloud_from_depth` but also allows\\n    to provide the RGB values for each point.\\n\\n    The ``rgb`` attribute is used to resolve the corresponding point\\'s color:\\n\\n    - If a ``np.array``/``wp.array``/``torch.tensor`` of shape (H, W, 3), then the corresponding channels encode RGB values.\\n    - If a tuple, then the point cloud has a single color specified by the values (r, g, b).\\n    - If None, then default color is white, i.e. (0, 0, 0).\\n\\n    If the input ``normalize_rgb`` is set to :obj:`True`, then the RGB values are normalized to be in the range [0, 1].\\n\\n    Args:\\n        intrinsic_matrix: A (3, 3) array/tensor providing camera\\'s calibration matrix.\\n        depth: An array/tensor of shape (H, W) with values encoding the depth measurement.\\n        rgb: Color for generated point cloud. Defaults to None.\\n        normalize_rgb: Whether to normalize input rgb. Defaults to False.\\n        position: The position of the camera in a target frame. Defaults to None.\\n        orientation: The orientation `(w, x, y, z)` of the camera in a target frame. Defaults to None.\\n        device: The device for torch where the computation should be executed. Defaults to None, in which case\\n            it takes the device that matches the depth image.\\n        num_channels: Number of channels in RGB pointcloud. Defaults to 3.\\n\\n    Returns:\\n        A tuple of (N, 3) arrays or tensors containing the 3D coordinates of points and their RGB color respectively.\\n        The returned datatype is torch if input depth is of type torch.tensor or wp.array. Otherwise, a np.ndarray\\n        is returned.\\n\\n    Raises:\\n        ValueError:  When rgb image is a numpy array but not of shape (H, W, 3) or (H, W, 4).\\n    \"\"\"\\n    # check valid inputs\\n    if rgb is not None and not isinstance(rgb, tuple):\\n        if len(rgb.shape) == 3:\\n            if rgb.shape[2] not in [3, 4]:\\n                raise ValueError(f\"Input rgb image of invalid shape: {rgb.shape} != (H, W, 3) or (H, W, 4).\")\\n        else:\\n            raise ValueError(f\"Input rgb image not three-dimensional. Received shape: {rgb.shape}.\")\\n    if num_channels not in [3, 4]:\\n        raise ValueError(f\"Invalid number of channels: {num_channels} != 3 or 4.\")\\n\\n    # check if input depth is numpy array\\n    is_numpy = isinstance(depth, np.ndarray)\\n    # decide device\\n    if device is None and is_numpy:\\n        device = torch.device(\"cpu\")\\n    # convert depth to torch tensor\\n    if is_numpy:\\n        depth = torch.from_numpy(depth).to(device=device)\\n    # retrieve XYZ pointcloud\\n    points_xyz = create_pointcloud_from_depth(intrinsic_matrix, depth, True, position, orientation, device=device)\\n\\n    # get image height and width\\n    im_height, im_width = depth.shape[:2]\\n    # total number of points\\n    num_points = im_height * im_width\\n    # extract color value\\n    if rgb is not None:\\n        if isinstance(rgb, (np.ndarray, torch.Tensor, wp.array)):\\n            # copy numpy array to preserve\\n            rgb = convert_to_torch(rgb, device=device, dtype=torch.float32)\\n            rgb = rgb[:, :, :3]\\n            # convert the matrix to (W, H, 3) from (H, W, 3) since depth processing\\n            # is done in the order (u, v) where u: (0, W-1) and v: (0 - H-1)\\n            points_rgb = rgb.permute(1, 0, 2).reshape(-1, 3)\\n        elif isinstance(rgb, (tuple, list)):\\n            # same color for all points\\n            points_rgb = torch.Tensor((rgb,) * num_points, device=device, dtype=torch.uint8)\\n        else:\\n            # default color is white\\n            points_rgb = torch.Tensor(((0, 0, 0),) * num_points, device=device, dtype=torch.uint8)\\n    else:\\n        points_rgb = torch.Tensor(((0, 0, 0),) * num_points, device=device, dtype=torch.uint8)\\n    # normalize color values\\n    if normalize_rgb:\\n        points_rgb = points_rgb.float() / 255\\n\\n    # remove invalid points\\n    pts_idx_to_keep = torch.all(torch.logical_and(~torch.isnan(points_xyz), ~torch.isinf(points_xyz)), dim=1)\\n    points_rgb = points_rgb[pts_idx_to_keep, ...]\\n    points_xyz = points_xyz[pts_idx_to_keep, ...]\\n\\n    # add additional channels if required\\n    if num_channels == 4:\\n        points_rgb = torch.nn.functional.pad(points_rgb, (0, 1), mode=\"constant\", value=1.0)\\n\\n    # return everything according to input type\\n    if is_numpy:\\n        return points_xyz.cpu().numpy(), points_rgb.cpu().numpy()\\n    else:\\n        return points_xyz, points_rgb'),\n",
       " Document(metadata={}, page_content='def save_images_to_file(images: torch.Tensor, file_path: str):\\n    \"\"\"Save images to file.\\n\\n    Args:\\n        images: A tensor of shape (N, H, W, C) containing the images.\\n        file_path: The path to save the images to.\\n    \"\"\"\\n    from torchvision.utils import make_grid, save_image\\n\\n    save_image(\\n        make_grid(torch.swapaxes(images.unsqueeze(1), 1, -1).squeeze(-1), nrow=round(images.shape[0] ** 0.5)), file_path\\n    )'),\n",
       " Document(metadata={}, page_content='class ContactSensor(SensorBase):\\n    \"\"\"A contact reporting sensor.\\n\\n    The contact sensor reports the normal contact forces on a rigid body in the world frame.\\n    It relies on the `PhysX ContactReporter`_ API to be activated on the rigid bodies.\\n\\n    To enable the contact reporter on a rigid body, please make sure to enable the\\n    :attr:`isaaclab.sim.spawner.RigidObjectSpawnerCfg.activate_contact_sensors` on your\\n    asset spawner configuration. This will enable the contact reporter on all the rigid bodies\\n    in the asset.\\n\\n    The sensor can be configured to report the contact forces on a set of bodies with a given\\n    filter pattern using the :attr:`ContactSensorCfg.filter_prim_paths_expr`. This is useful\\n    when you want to report the contact forces between the sensor bodies and a specific set of\\n    bodies in the scene. The data can be accessed using the :attr:`ContactSensorData.force_matrix_w`.\\n    Please check the documentation on `RigidContact`_ for more details.\\n\\n    The reporting of the filtered contact forces is only possible as one-to-many. This means that only one\\n    sensor body in an environment can be filtered against multiple bodies in that environment. If you need to\\n    filter multiple sensor bodies against multiple bodies, you need to create separate sensors for each sensor\\n    body.\\n\\n    As an example, suppose you want to report the contact forces for all the feet of a robot against an object\\n    exclusively. In that case, setting the :attr:`ContactSensorCfg.prim_path` and\\n    :attr:`ContactSensorCfg.filter_prim_paths_expr` with ``{ENV_REGEX_NS}/Robot/.*_FOOT`` and ``{ENV_REGEX_NS}/Object``\\n    respectively will not work. Instead, you need to create a separate sensor for each foot and filter\\n    it against the object.\\n\\n    .. _PhysX ContactReporter: https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/104.2/class_physx_schema_physx_contact_report_a_p_i.html\\n    .. _RigidContact: https://docs.omniverse.nvidia.com/py/isaacsim/source/isaacsim.core/docs/index.html#isaacsim.core.prims.RigidContact\\n    \"\"\"\\n\\n    cfg: ContactSensorCfg\\n    \"\"\"The configuration parameters.\"\"\"\\n\\n    def __init__(self, cfg: ContactSensorCfg):\\n        \"\"\"Initializes the contact sensor object.\\n\\n        Args:\\n            cfg: The configuration parameters.\\n        \"\"\"\\n        # initialize base class\\n        super().__init__(cfg)\\n\\n        # Enable contact processing\\n        carb_settings_iface = carb.settings.get_settings()\\n        carb_settings_iface.set_bool(\"/physics/disableContactProcessing\", False)\\n\\n        # Create empty variables for storing output data\\n        self._data: ContactSensorData = ContactSensorData()\\n        # initialize self._body_physx_view for running in extension mode\\n        self._body_physx_view = None\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing information about the instance.\"\"\"\\n        return (\\n            f\"Contact sensor @ \\'{self.cfg.prim_path}\\': \\\\n\"\\n            f\"\\\\tview type         : {self.body_physx_view.__class__}\\\\n\"\\n            f\"\\\\tupdate period (s) : {self.cfg.update_period}\\\\n\"\\n            f\"\\\\tnumber of bodies  : {self.num_bodies}\\\\n\"\\n            f\"\\\\tbody names        : {self.body_names}\\\\n\"\\n        )\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def num_instances(self) -> int:\\n        return self.body_physx_view.count\\n\\n    @property\\n    def data(self) -> ContactSensorData:\\n        # update sensors if needed\\n        self._update_outdated_buffers()\\n        # return the data\\n        return self._data\\n\\n    @property\\n    def num_bodies(self) -> int:\\n        \"\"\"Number of bodies with contact sensors attached.\"\"\"\\n        return self._num_bodies\\n\\n    @property\\n    def body_names(self) -> list[str]:\\n        \"\"\"Ordered names of bodies with contact sensors attached.\"\"\"\\n        prim_paths = self.body_physx_view.prim_paths[: self.num_bodies]\\n        return [path.split(\"/\")[-1] for path in prim_paths]\\n\\n    @property\\n    def body_physx_view(self) -> physx.RigidBodyView:\\n        \"\"\"View for the rigid bodies captured (PhysX).\\n\\n        Note:\\n            Use this view with caution. It requires handling of tensors in a specific way.\\n        \"\"\"\\n        return self._body_physx_view\\n\\n    @property\\n    def contact_physx_view(self) -> physx.RigidContactView:\\n        \"\"\"Contact reporter view for the bodies (PhysX).\\n\\n        Note:\\n            Use this view with caution. It requires handling of tensors in a specific way.\\n        \"\"\"\\n        return self._contact_physx_view\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # reset the timers and counters\\n        super().reset(env_ids)\\n        # resolve None\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset accumulative data buffers\\n        self._data.net_forces_w[env_ids] = 0.0\\n        self._data.net_forces_w_history[env_ids] = 0.0\\n        if self.cfg.history_length > 0:\\n            self._data.net_forces_w_history[env_ids] = 0.0\\n        # reset force matrix\\n        if len(self.cfg.filter_prim_paths_expr) != 0:\\n            self._data.force_matrix_w[env_ids] = 0.0\\n        # reset the current air time\\n        if self.cfg.track_air_time:\\n            self._data.current_air_time[env_ids] = 0.0\\n            self._data.last_air_time[env_ids] = 0.0\\n            self._data.current_contact_time[env_ids] = 0.0\\n            self._data.last_contact_time[env_ids] = 0.0\\n\\n    def find_bodies(self, name_keys: str | Sequence[str], preserve_order: bool = False) -> tuple[list[int], list[str]]:\\n        \"\"\"Find bodies in the articulation based on the name keys.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the body names.\\n            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.\\n\\n        Returns:\\n            A tuple of lists containing the body indices and names.\\n        \"\"\"\\n        return string_utils.resolve_matching_names(name_keys, self.body_names, preserve_order)\\n\\n    def compute_first_contact(self, dt: float, abs_tol: float = 1.0e-8) -> torch.Tensor:\\n        \"\"\"Checks if bodies that have established contact within the last :attr:`dt` seconds.\\n\\n        This function checks if the bodies have established contact within the last :attr:`dt` seconds\\n        by comparing the current contact time with the given time period. If the contact time is less\\n        than the given time period, then the bodies are considered to be in contact.\\n\\n        Note:\\n            The function assumes that :attr:`dt` is a factor of the sensor update time-step. In other\\n            words :math:`dt / dt_sensor = n`, where :math:`n` is a natural number. This is always true\\n            if the sensor is updated by the physics or the environment stepping time-step and the sensor\\n            is read by the environment stepping time-step.\\n\\n        Args:\\n            dt: The time period since the contact was established.\\n            abs_tol: The absolute tolerance for the comparison.\\n\\n        Returns:\\n            A boolean tensor indicating the bodies that have established contact within the last\\n            :attr:`dt` seconds. Shape is (N, B), where N is the number of sensors and B is the\\n            number of bodies in each sensor.\\n\\n        Raises:\\n            RuntimeError: If the sensor is not configured to track contact time.\\n        \"\"\"\\n        # check if the sensor is configured to track contact time\\n        if not self.cfg.track_air_time:\\n            raise RuntimeError(\\n                \"The contact sensor is not configured to track contact time.\"\\n                \"Please enable the \\'track_air_time\\' in the sensor configuration.\"\\n            )\\n        # check if the bodies are in contact\\n        currently_in_contact = self.data.current_contact_time > 0.0\\n        less_than_dt_in_contact = self.data.current_contact_time < (dt + abs_tol)\\n        return currently_in_contact * less_than_dt_in_contact\\n\\n    def compute_first_air(self, dt: float, abs_tol: float = 1.0e-8) -> torch.Tensor:\\n        \"\"\"Checks if bodies that have broken contact within the last :attr:`dt` seconds.\\n\\n        This function checks if the bodies have broken contact within the last :attr:`dt` seconds\\n        by comparing the current air time with the given time period. If the air time is less\\n        than the given time period, then the bodies are considered to not be in contact.\\n\\n        Note:\\n            It assumes that :attr:`dt` is a factor of the sensor update time-step. In other words,\\n            :math:`dt / dt_sensor = n`, where :math:`n` is a natural number. This is always true if\\n            the sensor is updated by the physics or the environment stepping time-step and the sensor\\n            is read by the environment stepping time-step.\\n\\n        Args:\\n            dt: The time period since the contract is broken.\\n            abs_tol: The absolute tolerance for the comparison.\\n\\n        Returns:\\n            A boolean tensor indicating the bodies that have broken contact within the last :attr:`dt` seconds.\\n            Shape is (N, B), where N is the number of sensors and B is the number of bodies in each sensor.\\n\\n        Raises:\\n            RuntimeError: If the sensor is not configured to track contact time.\\n        \"\"\"\\n        # check if the sensor is configured to track contact time\\n        if not self.cfg.track_air_time:\\n            raise RuntimeError(\\n                \"The contact sensor is not configured to track contact time.\"\\n                \"Please enable the \\'track_air_time\\' in the sensor configuration.\"\\n            )\\n        # check if the sensor is configured to track contact time\\n        currently_detached = self.data.current_air_time > 0.0\\n        less_than_dt_detached = self.data.current_air_time < (dt + abs_tol)\\n        return currently_detached * less_than_dt_detached\\n\\n    \"\"\"\\n    Implementation.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        super()._initialize_impl()\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        # check that only rigid bodies are selected\\n        leaf_pattern = self.cfg.prim_path.rsplit(\"/\", 1)[-1]\\n        template_prim_path = self._parent_prims[0].GetPath().pathString\\n        body_names = list()\\n        for prim in sim_utils.find_matching_prims(template_prim_path + \"/\" + leaf_pattern):\\n            # check if prim has contact reporter API\\n            if prim.HasAPI(PhysxSchema.PhysxContactReportAPI):\\n                prim_path = prim.GetPath().pathString\\n                body_names.append(prim_path.rsplit(\"/\", 1)[-1])\\n        # check that there is at least one body with contact reporter API\\n        if not body_names:\\n            raise RuntimeError(\\n                f\"Sensor at path \\'{self.cfg.prim_path}\\' could not find any bodies with contact reporter API.\"\\n                \"\\\\nHINT: Make sure to enable \\'activate_contact_sensors\\' in the corresponding asset spawn configuration.\"\\n            )\\n\\n        # construct regex expression for the body names\\n        body_names_regex = r\"(\" + \"|\".join(body_names) + r\")\"\\n        body_names_regex = f\"{self.cfg.prim_path.rsplit(\\'/\\', 1)[0]}/{body_names_regex}\"\\n        # convert regex expressions to glob expressions for PhysX\\n        body_names_glob = body_names_regex.replace(\".*\", \"*\")\\n        filter_prim_paths_glob = [expr.replace(\".*\", \"*\") for expr in self.cfg.filter_prim_paths_expr]\\n\\n        # create a rigid prim view for the sensor\\n        self._body_physx_view = self._physics_sim_view.create_rigid_body_view(body_names_glob)\\n        self._contact_physx_view = self._physics_sim_view.create_rigid_contact_view(\\n            body_names_glob, filter_patterns=filter_prim_paths_glob\\n        )\\n        # resolve the true count of bodies\\n        self._num_bodies = self.body_physx_view.count // self._num_envs\\n        # check that contact reporter succeeded\\n        if self._num_bodies != len(body_names):\\n            raise RuntimeError(\\n                \"Failed to initialize contact reporter for specified bodies.\"\\n                f\"\\\\n\\\\tInput prim path    : {self.cfg.prim_path}\"\\n                f\"\\\\n\\\\tResolved prim paths: {body_names_regex}\"\\n            )\\n\\n        # prepare data buffers\\n        self._data.net_forces_w = torch.zeros(self._num_envs, self._num_bodies, 3, device=self._device)\\n        # optional buffers\\n        # -- history of net forces\\n        if self.cfg.history_length > 0:\\n            self._data.net_forces_w_history = torch.zeros(\\n                self._num_envs, self.cfg.history_length, self._num_bodies, 3, device=self._device\\n            )\\n        else:\\n            self._data.net_forces_w_history = self._data.net_forces_w.unsqueeze(1)\\n        # -- pose of sensor origins\\n        if self.cfg.track_pose:\\n            self._data.pos_w = torch.zeros(self._num_envs, self._num_bodies, 3, device=self._device)\\n            self._data.quat_w = torch.zeros(self._num_envs, self._num_bodies, 4, device=self._device)\\n        # -- air/contact time between contacts\\n        if self.cfg.track_air_time:\\n            self._data.last_air_time = torch.zeros(self._num_envs, self._num_bodies, device=self._device)\\n            self._data.current_air_time = torch.zeros(self._num_envs, self._num_bodies, device=self._device)\\n            self._data.last_contact_time = torch.zeros(self._num_envs, self._num_bodies, device=self._device)\\n            self._data.current_contact_time = torch.zeros(self._num_envs, self._num_bodies, device=self._device)\\n        # force matrix: (num_envs, num_bodies, num_filter_shapes, 3)\\n        if len(self.cfg.filter_prim_paths_expr) != 0:\\n            num_filters = self.contact_physx_view.filter_count\\n            self._data.force_matrix_w = torch.zeros(\\n                self._num_envs, self._num_bodies, num_filters, 3, device=self._device\\n            )\\n\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        \"\"\"Fills the buffers of the sensor data.\"\"\"\\n        # default to all sensors\\n        if len(env_ids) == self._num_envs:\\n            env_ids = slice(None)\\n\\n        # obtain the contact forces\\n        # TODO: We are handling the indexing ourself because of the shape; (N, B) vs expected (N * B).\\n        #   This isn\\'t the most efficient way to do this, but it\\'s the easiest to implement.\\n        net_forces_w = self.contact_physx_view.get_net_contact_forces(dt=self._sim_physics_dt)\\n        self._data.net_forces_w[env_ids, :, :] = net_forces_w.view(-1, self._num_bodies, 3)[env_ids]\\n        # update contact force history\\n        if self.cfg.history_length > 0:\\n            self._data.net_forces_w_history[env_ids, 1:] = self._data.net_forces_w_history[env_ids, :-1].clone()\\n            self._data.net_forces_w_history[env_ids, 0] = self._data.net_forces_w[env_ids]\\n\\n        # obtain the contact force matrix\\n        if len(self.cfg.filter_prim_paths_expr) != 0:\\n            # shape of the filtering matrix: (num_envs, num_bodies, num_filter_shapes, 3)\\n            num_filters = self.contact_physx_view.filter_count\\n            # acquire and shape the force matrix\\n            force_matrix_w = self.contact_physx_view.get_contact_force_matrix(dt=self._sim_physics_dt)\\n            force_matrix_w = force_matrix_w.view(-1, self._num_bodies, num_filters, 3)\\n            self._data.force_matrix_w[env_ids] = force_matrix_w[env_ids]\\n\\n        # obtain the pose of the sensor origin\\n        if self.cfg.track_pose:\\n            pose = self.body_physx_view.get_transforms().view(-1, self._num_bodies, 7)[env_ids]\\n            pose[..., 3:] = convert_quat(pose[..., 3:], to=\"wxyz\")\\n            self._data.pos_w[env_ids], self._data.quat_w[env_ids] = pose.split([3, 4], dim=-1)\\n\\n        # obtain the air time\\n        if self.cfg.track_air_time:\\n            # -- time elapsed since last update\\n            # since this function is called every frame, we can use the difference to get the elapsed time\\n            elapsed_time = self._timestamp[env_ids] - self._timestamp_last_update[env_ids]\\n            # -- check contact state of bodies\\n            is_contact = torch.norm(self._data.net_forces_w[env_ids, :, :], dim=-1) > self.cfg.force_threshold\\n            is_first_contact = (self._data.current_air_time[env_ids] > 0) * is_contact\\n            is_first_detached = (self._data.current_contact_time[env_ids] > 0) * ~is_contact\\n            # -- update the last contact time if body has just become in contact\\n            self._data.last_air_time[env_ids] = torch.where(\\n                is_first_contact,\\n                self._data.current_air_time[env_ids] + elapsed_time.unsqueeze(-1),\\n                self._data.last_air_time[env_ids],\\n            )\\n            # -- increment time for bodies that are not in contact\\n            self._data.current_air_time[env_ids] = torch.where(\\n                ~is_contact, self._data.current_air_time[env_ids] + elapsed_time.unsqueeze(-1), 0.0\\n            )\\n            # -- update the last contact time if body has just detached\\n            self._data.last_contact_time[env_ids] = torch.where(\\n                is_first_detached,\\n                self._data.current_contact_time[env_ids] + elapsed_time.unsqueeze(-1),\\n                self._data.last_contact_time[env_ids],\\n            )\\n            # -- increment time for bodies that are in contact\\n            self._data.current_contact_time[env_ids] = torch.where(\\n                is_contact, self._data.current_contact_time[env_ids] + elapsed_time.unsqueeze(-1), 0.0\\n            )\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # set visibility of markers\\n        # note: parent only deals with callbacks. not their visibility\\n        if debug_vis:\\n            # create markers if necessary for the first tome\\n            if not hasattr(self, \"contact_visualizer\"):\\n                self.contact_visualizer = VisualizationMarkers(self.cfg.visualizer_cfg)\\n            # set their visibility to true\\n            self.contact_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"contact_visualizer\"):\\n                self.contact_visualizer.set_visibility(False)\\n\\n    def _debug_vis_callback(self, event):\\n        # safely return if view becomes invalid\\n        # note: this invalidity happens because of isaac sim view callbacks\\n        if self.body_physx_view is None:\\n            return\\n        # marker indices\\n        # 0: contact, 1: no contact\\n        net_contact_force_w = torch.norm(self._data.net_forces_w, dim=-1)\\n        marker_indices = torch.where(net_contact_force_w > self.cfg.force_threshold, 0, 1)\\n        # check if prim is visualized\\n        if self.cfg.track_pose:\\n            frame_origins: torch.Tensor = self._data.pos_w\\n        else:\\n            pose = self.body_physx_view.get_transforms()\\n            frame_origins = pose.view(-1, self._num_bodies, 7)[:, :, :3]\\n        # visualize\\n        self.contact_visualizer.visualize(frame_origins.view(-1, 3), marker_indices=marker_indices.view(-1))\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        # set all existing views to None to invalidate them\\n        self._body_physx_view = None\\n        self._contact_physx_view = None'),\n",
       " Document(metadata={}, page_content='class ContactSensorCfg(SensorBaseCfg):\\n    \"\"\"Configuration for the contact sensor.\"\"\"\\n\\n    class_type: type = ContactSensor\\n\\n    track_pose: bool = False\\n    \"\"\"Whether to track the pose of the sensor\\'s origin. Defaults to False.\"\"\"\\n\\n    track_air_time: bool = False\\n    \"\"\"Whether to track the air/contact time of the bodies (time between contacts). Defaults to False.\"\"\"\\n\\n    force_threshold: float = 1.0\\n    \"\"\"The threshold on the norm of the contact force that determines whether two bodies are in collision or not.\\n\\n    This value is only used for tracking the mode duration (the time in contact or in air),\\n    if :attr:`track_air_time` is True.\\n    \"\"\"\\n\\n    filter_prim_paths_expr: list[str] = list()\\n    \"\"\"The list of primitive paths (or expressions) to filter contacts with. Defaults to an empty list, in which case\\n    no filtering is applied.\\n\\n    The contact sensor allows reporting contacts between the primitive specified with :attr:`prim_path` and\\n    other primitives in the scene. For instance, in a scene containing a robot, a ground plane and an object,\\n    you can obtain individual contact reports of the base of the robot with the ground plane and the object.\\n\\n    .. note::\\n        The expression in the list can contain the environment namespace regex ``{ENV_REGEX_NS}`` which\\n        will be replaced with the environment namespace.\\n\\n        Example: ``{ENV_REGEX_NS}/Object`` will be replaced with ``/World/envs/env_.*/Object``.\\n\\n    .. attention::\\n        The reporting of filtered contacts only works when the sensor primitive :attr:`prim_path` corresponds to a\\n        single primitive in that environment. If the sensor primitive corresponds to multiple primitives, the\\n        filtering will not work as expected. Please check :class:`~isaaclab.sensors.contact_sensor.ContactSensor`\\n        for more details.\\n    \"\"\"\\n\\n    visualizer_cfg: VisualizationMarkersCfg = CONTACT_SENSOR_MARKER_CFG.replace(prim_path=\"/Visuals/ContactSensor\")\\n    \"\"\"The configuration object for the visualization markers. Defaults to CONTACT_SENSOR_MARKER_CFG.\\n\\n    .. note::\\n        This attribute is only used when debug visualization is enabled.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ContactSensorData:\\n    \"\"\"Data container for the contact reporting sensor.\"\"\"\\n\\n    pos_w: torch.Tensor | None = None\\n    \"\"\"Position of the sensor origin in world frame.\\n\\n    Shape is (N, 3), where N is the number of sensors.\\n\\n    Note:\\n        If the :attr:`ContactSensorCfg.track_pose` is False, then this quantity is None.\\n    \"\"\"\\n\\n    quat_w: torch.Tensor | None = None\\n    \"\"\"Orientation of the sensor origin in quaternion (w, x, y, z) in world frame.\\n\\n    Shape is (N, 4), where N is the number of sensors.\\n\\n    Note:\\n        If the :attr:`ContactSensorCfg.track_pose` is False, then this quantity is None.\\n    \"\"\"\\n\\n    net_forces_w: torch.Tensor | None = None\\n    \"\"\"The net normal contact forces in world frame.\\n\\n    Shape is (N, B, 3), where N is the number of sensors and B is the number of bodies in each sensor.\\n\\n    Note:\\n        This quantity is the sum of the normal contact forces acting on the sensor bodies. It must not be confused\\n        with the total contact forces acting on the sensor bodies (which also includes the tangential forces).\\n    \"\"\"\\n\\n    net_forces_w_history: torch.Tensor | None = None\\n    \"\"\"The net normal contact forces in world frame.\\n\\n    Shape is (N, T, B, 3), where N is the number of sensors, T is the configured history length\\n    and B is the number of bodies in each sensor.\\n\\n    In the history dimension, the first index is the most recent and the last index is the oldest.\\n\\n    Note:\\n        This quantity is the sum of the normal contact forces acting on the sensor bodies. It must not be confused\\n        with the total contact forces acting on the sensor bodies (which also includes the tangential forces).\\n    \"\"\"\\n\\n    force_matrix_w: torch.Tensor | None = None\\n    \"\"\"The normal contact forces filtered between the sensor bodies and filtered bodies in world frame.\\n\\n    Shape is (N, B, M, 3), where N is the number of sensors, B is number of bodies in each sensor\\n    and ``M`` is the number of filtered bodies.\\n\\n    Note:\\n        If the :attr:`ContactSensorCfg.filter_prim_paths_expr` is empty, then this quantity is None.\\n    \"\"\"\\n\\n    last_air_time: torch.Tensor | None = None\\n    \"\"\"Time spent (in s) in the air before the last contact.\\n\\n    Shape is (N, B), where N is the number of sensors and B is the number of bodies in each sensor.\\n\\n    Note:\\n        If the :attr:`ContactSensorCfg.track_air_time` is False, then this quantity is None.\\n    \"\"\"\\n\\n    current_air_time: torch.Tensor | None = None\\n    \"\"\"Time spent (in s) in the air since the last detach.\\n\\n    Shape is (N, B), where N is the number of sensors and B is the number of bodies in each sensor.\\n\\n    Note:\\n        If the :attr:`ContactSensorCfg.track_air_time` is False, then this quantity is None.\\n    \"\"\"\\n\\n    last_contact_time: torch.Tensor | None = None\\n    \"\"\"Time spent (in s) in contact before the last detach.\\n\\n    Shape is (N, B), where N is the number of sensors and B is the number of bodies in each sensor.\\n\\n    Note:\\n        If the :attr:`ContactSensorCfg.track_air_time` is False, then this quantity is None.\\n    \"\"\"\\n\\n    current_contact_time: torch.Tensor | None = None\\n    \"\"\"Time spent (in s) in contact since the last contact.\\n\\n    Shape is (N, B), where N is the number of sensors and B is the number of bodies in each sensor.\\n\\n    Note:\\n        If the :attr:`ContactSensorCfg.track_air_time` is False, then this quantity is None.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class FrameTransformer(SensorBase):\\n    \"\"\"A sensor for reporting frame transforms.\\n\\n    This class provides an interface for reporting the transform of one or more frames (target frames)\\n    with respect to another frame (source frame). The source frame is specified by the user as a prim path\\n    (:attr:`FrameTransformerCfg.prim_path`) and the target frames are specified by the user as a list of\\n    prim paths (:attr:`FrameTransformerCfg.target_frames`).\\n\\n    The source frame and target frames are assumed to be rigid bodies. The transform of the target frames\\n    with respect to the source frame is computed by first extracting the transform of the source frame\\n    and target frames from the physics engine and then computing the relative transform between the two.\\n\\n    Additionally, the user can specify an offset for the source frame and each target frame. This is useful\\n    for specifying the transform of the desired frame with respect to the body\\'s center of mass, for instance.\\n\\n    A common example of using this sensor is to track the position and orientation of the end effector of a\\n    robotic manipulator. In this case, the source frame would be the body corresponding to the base frame of the\\n    manipulator, and the target frame would be the body corresponding to the end effector. Since the end-effector is\\n    typically a fictitious body, the user may need to specify an offset from the end-effector to the body of the\\n    manipulator.\\n\\n    \"\"\"\\n\\n    cfg: FrameTransformerCfg\\n    \"\"\"The configuration parameters.\"\"\"\\n\\n    def __init__(self, cfg: FrameTransformerCfg):\\n        \"\"\"Initializes the frame transformer object.\\n\\n        Args:\\n            cfg: The configuration parameters.\\n        \"\"\"\\n        # initialize base class\\n        super().__init__(cfg)\\n        # Create empty variables for storing output data\\n        self._data: FrameTransformerData = FrameTransformerData()\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing information about the instance.\"\"\"\\n        return (\\n            f\"FrameTransformer @ \\'{self.cfg.prim_path}\\': \\\\n\"\\n            f\"\\\\ttracked body frames: {[self._source_frame_body_name] + self._target_frame_body_names} \\\\n\"\\n            f\"\\\\tnumber of envs: {self._num_envs}\\\\n\"\\n            f\"\\\\tsource body frame: {self._source_frame_body_name}\\\\n\"\\n            f\"\\\\ttarget frames (count: {self._target_frame_names}): {len(self._target_frame_names)}\\\\n\"\\n        )\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def data(self) -> FrameTransformerData:\\n        # update sensors if needed\\n        self._update_outdated_buffers()\\n        # return the data\\n        return self._data\\n\\n    @property\\n    def num_bodies(self) -> int:\\n        \"\"\"Returns the number of target bodies being tracked.\\n\\n        Note:\\n            This is an alias used for consistency with other sensors. Otherwise, we recommend using\\n            :attr:`len(data.target_frame_names)` to access the number of target frames.\\n        \"\"\"\\n        return len(self._target_frame_body_names)\\n\\n    @property\\n    def body_names(self) -> list[str]:\\n        \"\"\"Returns the names of the target bodies being tracked.\\n\\n        Note:\\n            This is an alias used for consistency with other sensors. Otherwise, we recommend using\\n            :attr:`data.target_frame_names` to access the target frame names.\\n        \"\"\"\\n        return self._target_frame_body_names\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # reset the timers and counters\\n        super().reset(env_ids)\\n        # resolve None\\n        if env_ids is None:\\n            env_ids = ...\\n\\n    def find_bodies(self, name_keys: str | Sequence[str], preserve_order: bool = False) -> tuple[list[int], list[str]]:\\n        \"\"\"Find bodies in the articulation based on the name keys.\\n\\n        Args:\\n            name_keys: A regular expression or a list of regular expressions to match the body names.\\n            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.\\n\\n        Returns:\\n            A tuple of lists containing the body indices and names.\\n        \"\"\"\\n        return string_utils.resolve_matching_names(name_keys, self._target_frame_names, preserve_order)\\n\\n    \"\"\"\\n    Implementation.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        super()._initialize_impl()\\n\\n        # resolve source frame offset\\n        source_frame_offset_pos = torch.tensor(self.cfg.source_frame_offset.pos, device=self.device)\\n        source_frame_offset_quat = torch.tensor(self.cfg.source_frame_offset.rot, device=self.device)\\n        # Only need to perform offsetting of source frame if the position offsets is non-zero and rotation offset is\\n        # not the identity quaternion for efficiency in _update_buffer_impl\\n        self._apply_source_frame_offset = True\\n        # Handle source frame offsets\\n        if is_identity_pose(source_frame_offset_pos, source_frame_offset_quat):\\n            omni.log.verbose(f\"No offset application needed for source frame as it is identity: {self.cfg.prim_path}\")\\n            self._apply_source_frame_offset = False\\n        else:\\n            omni.log.verbose(f\"Applying offset to source frame as it is not identity: {self.cfg.prim_path}\")\\n            # Store offsets as tensors (duplicating each env\\'s offsets for ease of multiplication later)\\n            self._source_frame_offset_pos = source_frame_offset_pos.unsqueeze(0).repeat(self._num_envs, 1)\\n            self._source_frame_offset_quat = source_frame_offset_quat.unsqueeze(0).repeat(self._num_envs, 1)\\n\\n        # Keep track of mapping from the rigid body name to the desired frames and prim path, as there may be multiple frames\\n        # based upon the same body name and we don\\'t want to create unnecessary views\\n        body_names_to_frames: dict[str, dict[str, set[str] | str]] = {}\\n        # The offsets associated with each target frame\\n        target_offsets: dict[str, dict[str, torch.Tensor]] = {}\\n        # The frames whose offsets are not identity\\n        non_identity_offset_frames: list[str] = []\\n\\n        # Only need to perform offsetting of target frame if any of the position offsets are non-zero or any of the\\n        # rotation offsets are not the identity quaternion for efficiency in _update_buffer_impl\\n        self._apply_target_frame_offset = False\\n\\n        # Need to keep track of whether the source frame is also a target frame\\n        self._source_is_also_target_frame = False\\n\\n        # Collect all target frames, their associated body prim paths and their offsets so that we can extract\\n        # the prim, check that it has the appropriate rigid body API in a single loop.\\n        # First element is None because user can\\'t specify source frame name\\n        frames = [None] + [target_frame.name for target_frame in self.cfg.target_frames]\\n        frame_prim_paths = [self.cfg.prim_path] + [target_frame.prim_path for target_frame in self.cfg.target_frames]\\n        # First element is None because source frame offset is handled separately\\n        frame_offsets = [None] + [target_frame.offset for target_frame in self.cfg.target_frames]\\n        frame_types = [\"source\"] + [\"target\"] * len(self.cfg.target_frames)\\n        for frame, prim_path, offset, frame_type in zip(frames, frame_prim_paths, frame_offsets, frame_types):\\n            # Find correct prim\\n            matching_prims = sim_utils.find_matching_prims(prim_path)\\n            if len(matching_prims) == 0:\\n                raise ValueError(\\n                    f\"Failed to create frame transformer for frame \\'{frame}\\' with path \\'{prim_path}\\'.\"\\n                    \" No matching prims were found.\"\\n                )\\n            for prim in matching_prims:\\n                # Get the prim path of the matching prim\\n                matching_prim_path = prim.GetPath().pathString\\n                # Check if it is a rigid prim\\n                if not prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n                    raise ValueError(\\n                        f\"While resolving expression \\'{prim_path}\\' found a prim \\'{matching_prim_path}\\' which is not a\"\\n                        \" rigid body. The class only supports transformations between rigid bodies.\"\\n                    )\\n\\n                # Get the name of the body\\n                body_name = matching_prim_path.rsplit(\"/\", 1)[-1]\\n                # Use body name if frame isn\\'t specified by user\\n                frame_name = frame if frame is not None else body_name\\n\\n                # Keep track of which frames are associated with which bodies\\n                if body_name in body_names_to_frames:\\n                    body_names_to_frames[body_name][\"frames\"].add(frame_name)\\n\\n                    # This is a corner case where the source frame is also a target frame\\n                    if body_names_to_frames[body_name][\"type\"] == \"source\" and frame_type == \"target\":\\n                        self._source_is_also_target_frame = True\\n\\n                else:\\n                    # Store the first matching prim path and the type of frame\\n                    body_names_to_frames[body_name] = {\\n                        \"frames\": {frame_name},\\n                        \"prim_path\": matching_prim_path,\\n                        \"type\": frame_type,\\n                    }\\n\\n                if offset is not None:\\n                    offset_pos = torch.tensor(offset.pos, device=self.device)\\n                    offset_quat = torch.tensor(offset.rot, device=self.device)\\n                    # Check if we need to apply offsets (optimized code path in _update_buffer_impl)\\n                    if not is_identity_pose(offset_pos, offset_quat):\\n                        non_identity_offset_frames.append(frame_name)\\n                        self._apply_target_frame_offset = True\\n                    target_offsets[frame_name] = {\"pos\": offset_pos, \"quat\": offset_quat}\\n\\n        if not self._apply_target_frame_offset:\\n            omni.log.info(\\n                f\"No offsets application needed from \\'{self.cfg.prim_path}\\' to target frames as all\"\\n                f\" are identity: {frames[1:]}\"\\n            )\\n        else:\\n            omni.log.info(\\n                f\"Offsets application needed from \\'{self.cfg.prim_path}\\' to the following target frames:\"\\n                f\" {non_identity_offset_frames}\"\\n            )\\n\\n        # The names of bodies that RigidPrim will be tracking to later extract transforms from\\n        tracked_prim_paths = [body_names_to_frames[body_name][\"prim_path\"] for body_name in body_names_to_frames.keys()]\\n        tracked_body_names = [body_name for body_name in body_names_to_frames.keys()]\\n\\n        body_names_regex = [tracked_prim_path.replace(\"env_0\", \"env_*\") for tracked_prim_path in tracked_prim_paths]\\n\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        # Create a prim view for all frames and initialize it\\n        # order of transforms coming out of view will be source frame followed by target frame(s)\\n        self._frame_physx_view = self._physics_sim_view.create_rigid_body_view(body_names_regex)\\n\\n        # Determine the order in which regex evaluated body names so we can later index into frame transforms\\n        # by frame name correctly\\n        all_prim_paths = self._frame_physx_view.prim_paths\\n\\n        if \"env_\" in all_prim_paths[0]:\\n\\n            def extract_env_num_and_prim_path(item: str) -> tuple[int, str]:\\n                \"\"\"Separates the environment number and prim_path from the item.\\n\\n                Args:\\n                    item: The item to extract the environment number from. Assumes item is of the form\\n                        `/World/envs/env_1/blah` or `/World/envs/env_11/blah`.\\n                Returns:\\n                    The environment number and the prim_path.\\n                \"\"\"\\n                match = re.search(r\"env_(\\\\d+)(.*)\", item)\\n                return (int(match.group(1)), match.group(2))\\n\\n            # Find the indices that would reorganize output to be per environment. We want `env_1/blah` to come before `env_11/blah`\\n            # and env_1/Robot/base to come before env_1/Robot/foot so we need to use custom key function\\n            self._per_env_indices = [\\n                index\\n                for index, _ in sorted(\\n                    list(enumerate(all_prim_paths)), key=lambda x: extract_env_num_and_prim_path(x[1])\\n                )\\n            ]\\n\\n            # Only need 0th env as the names and their ordering are the same across environments\\n            sorted_prim_paths = [\\n                all_prim_paths[index] for index in self._per_env_indices if \"env_0\" in all_prim_paths[index]\\n            ]\\n\\n        else:\\n            # If no environment is present, then the order of the body names is the same as the order of the prim paths sorted alphabetically\\n            self._per_env_indices = [index for index, _ in sorted(enumerate(all_prim_paths), key=lambda x: x[1])]\\n            sorted_prim_paths = [all_prim_paths[index] for index in self._per_env_indices]\\n\\n        # -- target frames\\n        self._target_frame_body_names = [prim_path.split(\"/\")[-1] for prim_path in sorted_prim_paths]\\n\\n        # -- source frame\\n        self._source_frame_body_name = self.cfg.prim_path.split(\"/\")[-1]\\n        source_frame_index = self._target_frame_body_names.index(self._source_frame_body_name)\\n\\n        # Only remove source frame from tracked bodies if it is not also a target frame\\n        if not self._source_is_also_target_frame:\\n            self._target_frame_body_names.remove(self._source_frame_body_name)\\n\\n        # Determine indices into all tracked body frames for both source and target frames\\n        all_ids = torch.arange(self._num_envs * len(tracked_body_names))\\n        self._source_frame_body_ids = torch.arange(self._num_envs) * len(tracked_body_names) + source_frame_index\\n\\n        # If source frame is also a target frame, then the target frame body ids are the same as the source frame body ids\\n        if self._source_is_also_target_frame:\\n            self._target_frame_body_ids = all_ids\\n        else:\\n            self._target_frame_body_ids = all_ids[~torch.isin(all_ids, self._source_frame_body_ids)]\\n\\n        # The name of each of the target frame(s) - either user specified or defaulted to the body name\\n        self._target_frame_names: list[str] = []\\n        # The position and rotation components of target frame offsets\\n        target_frame_offset_pos = []\\n        target_frame_offset_quat = []\\n        # Stores the indices of bodies that need to be duplicated. For instance, if body \"LF_SHANK\" is needed\\n        # for 2 frames, this list enables us to duplicate the body to both frames when doing the calculations\\n        # when updating sensor in _update_buffers_impl\\n        duplicate_frame_indices = []\\n\\n        # Go through each body name and determine the number of duplicates we need for that frame\\n        # and extract the offsets. This is all done to handle the case where multiple frames\\n        # reference the same body, but have different names and/or offsets\\n        for i, body_name in enumerate(self._target_frame_body_names):\\n            for frame in body_names_to_frames[body_name][\"frames\"]:\\n                # Only need to handle target frames here as source frame is handled separately\\n                if frame in target_offsets:\\n                    target_frame_offset_pos.append(target_offsets[frame][\"pos\"])\\n                    target_frame_offset_quat.append(target_offsets[frame][\"quat\"])\\n                    self._target_frame_names.append(frame)\\n                    duplicate_frame_indices.append(i)\\n\\n        # To handle multiple environments, need to expand so [0, 1, 1, 2] with 2 environments becomes\\n        # [0, 1, 1, 2, 3, 4, 4, 5]. Again, this is a optimization to make _update_buffer_impl more efficient\\n        duplicate_frame_indices = torch.tensor(duplicate_frame_indices, device=self.device)\\n        if self._source_is_also_target_frame:\\n            num_target_body_frames = len(tracked_body_names)\\n        else:\\n            num_target_body_frames = len(tracked_body_names) - 1\\n\\n        self._duplicate_frame_indices = torch.cat(\\n            [duplicate_frame_indices + num_target_body_frames * env_num for env_num in range(self._num_envs)]\\n        )\\n\\n        # Target frame offsets are only applied if at least one of the offsets are non-identity\\n        if self._apply_target_frame_offset:\\n            # Stack up all the frame offsets for shape (num_envs, num_frames, 3) and (num_envs, num_frames, 4)\\n            self._target_frame_offset_pos = torch.stack(target_frame_offset_pos).repeat(self._num_envs, 1)\\n            self._target_frame_offset_quat = torch.stack(target_frame_offset_quat).repeat(self._num_envs, 1)\\n\\n        # fill the data buffer\\n        self._data.target_frame_names = self._target_frame_names\\n        self._data.source_pos_w = torch.zeros(self._num_envs, 3, device=self._device)\\n        self._data.source_quat_w = torch.zeros(self._num_envs, 4, device=self._device)\\n        self._data.target_pos_w = torch.zeros(self._num_envs, len(duplicate_frame_indices), 3, device=self._device)\\n        self._data.target_quat_w = torch.zeros(self._num_envs, len(duplicate_frame_indices), 4, device=self._device)\\n        self._data.target_pos_source = torch.zeros_like(self._data.target_pos_w)\\n        self._data.target_quat_source = torch.zeros_like(self._data.target_quat_w)\\n\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        \"\"\"Fills the buffers of the sensor data.\"\"\"\\n        # default to all sensors\\n        if len(env_ids) == self._num_envs:\\n            env_ids = ...\\n\\n        # Extract transforms from view - shape is:\\n        # (the total number of source and target body frames being tracked * self._num_envs, 7)\\n        transforms = self._frame_physx_view.get_transforms()\\n\\n        # Reorder the transforms to be per environment as is expected of SensorData\\n        transforms = transforms[self._per_env_indices]\\n\\n        # Convert quaternions as PhysX uses xyzw form\\n        transforms[:, 3:] = convert_quat(transforms[:, 3:], to=\"wxyz\")\\n\\n        # Process source frame transform\\n        source_frames = transforms[self._source_frame_body_ids]\\n        # Only apply offset if the offsets will result in a coordinate frame transform\\n        if self._apply_source_frame_offset:\\n            source_pos_w, source_quat_w = combine_frame_transforms(\\n                source_frames[:, :3],\\n                source_frames[:, 3:],\\n                self._source_frame_offset_pos,\\n                self._source_frame_offset_quat,\\n            )\\n        else:\\n            source_pos_w = source_frames[:, :3]\\n            source_quat_w = source_frames[:, 3:]\\n\\n        # Process target frame transforms\\n        target_frames = transforms[self._target_frame_body_ids]\\n        duplicated_target_frame_pos_w = target_frames[self._duplicate_frame_indices, :3]\\n        duplicated_target_frame_quat_w = target_frames[self._duplicate_frame_indices, 3:]\\n\\n        # Only apply offset if the offsets will result in a coordinate frame transform\\n        if self._apply_target_frame_offset:\\n            target_pos_w, target_quat_w = combine_frame_transforms(\\n                duplicated_target_frame_pos_w,\\n                duplicated_target_frame_quat_w,\\n                self._target_frame_offset_pos,\\n                self._target_frame_offset_quat,\\n            )\\n        else:\\n            target_pos_w = duplicated_target_frame_pos_w\\n            target_quat_w = duplicated_target_frame_quat_w\\n\\n        # Compute the transform of the target frame with respect to the source frame\\n        total_num_frames = len(self._target_frame_names)\\n        target_pos_source, target_quat_source = subtract_frame_transforms(\\n            source_pos_w.unsqueeze(1).expand(-1, total_num_frames, -1).reshape(-1, 3),\\n            source_quat_w.unsqueeze(1).expand(-1, total_num_frames, -1).reshape(-1, 4),\\n            target_pos_w,\\n            target_quat_w,\\n        )\\n\\n        # Update buffers\\n        # note: The frame names / ordering don\\'t change so no need to update them after initialization\\n        self._data.source_pos_w[:] = source_pos_w.view(-1, 3)\\n        self._data.source_quat_w[:] = source_quat_w.view(-1, 4)\\n        self._data.target_pos_w[:] = target_pos_w.view(-1, total_num_frames, 3)\\n        self._data.target_quat_w[:] = target_quat_w.view(-1, total_num_frames, 4)\\n        self._data.target_pos_source[:] = target_pos_source.view(-1, total_num_frames, 3)\\n        self._data.target_quat_source[:] = target_quat_source.view(-1, total_num_frames, 4)\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # set visibility of markers\\n        # note: parent only deals with callbacks. not their visibility\\n        if debug_vis:\\n            if not hasattr(self, \"frame_visualizer\"):\\n                self.frame_visualizer = VisualizationMarkers(self.cfg.visualizer_cfg)\\n\\n                try:\\n                    # isaacsim.util is not available in headless mode\\n                    import isaacsim.util.debug_draw._debug_draw as isaac_debug_draw\\n\\n                    self.debug_draw = isaac_debug_draw.acquire_debug_draw_interface()\\n                except ImportError:\\n                    omni.log.info(\"isaacsim.util.debug_draw module not found. Debug visualization will be limited.\")\\n\\n            # set their visibility to true\\n            self.frame_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"frame_visualizer\"):\\n                self.frame_visualizer.set_visibility(False)\\n                # clear the lines\\n                if hasattr(self, \"debug_draw\"):\\n                    self.debug_draw.clear_lines()\\n\\n    def _debug_vis_callback(self, event):\\n        # Update the visualized markers\\n        all_pos = torch.cat([self._data.source_pos_w, self._data.target_pos_w.view(-1, 3)], dim=0)\\n        all_quat = torch.cat([self._data.source_quat_w, self._data.target_quat_w.view(-1, 4)], dim=0)\\n        self.frame_visualizer.visualize(all_pos, all_quat)\\n\\n        if hasattr(self, \"debug_draw\"):\\n            # Draw lines connecting the source frame to the target frames\\n            self.debug_draw.clear_lines()\\n            # make the lines color yellow\\n            source_pos = self._data.source_pos_w.cpu().tolist()\\n            colors = [[1, 1, 0, 1]] * self._num_envs\\n            for frame_index in range(len(self._target_frame_names)):\\n                target_pos = self._data.target_pos_w[:, frame_index].cpu().tolist()\\n                self.debug_draw.draw_lines(source_pos, target_pos, colors, [1.5] * self._num_envs)\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        # set all existing views to None to invalidate them\\n        self._frame_physx_view = None'),\n",
       " Document(metadata={}, page_content='class OffsetCfg:\\n    \"\"\"The offset pose of one frame relative to another frame.\"\"\"\\n\\n    pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n    \"\"\"Translation w.r.t. the parent frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n    rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n    \"\"\"Quaternion rotation (w, x, y, z) w.r.t. the parent frame. Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"'),\n",
       " Document(metadata={}, page_content='class FrameTransformerCfg(SensorBaseCfg):\\n    \"\"\"Configuration for the frame transformer sensor.\"\"\"\\n\\n    @configclass\\n    class FrameCfg:\\n        \"\"\"Information specific to a coordinate frame.\"\"\"\\n\\n        prim_path: str = MISSING\\n        \"\"\"The prim path corresponding to a rigid body.\\n\\n        This can be a regex pattern to match multiple prims. For example, \"/Robot/.*\" will match all prims under \"/Robot\".\\n\\n        This means that if the source :attr:`FrameTransformerCfg.prim_path` is \"/Robot/base\", and the target :attr:`FrameTransformerCfg.FrameCfg.prim_path` is \"/Robot/.*\",\\n        then the frame transformer will track the poses of all the prims under \"/Robot\",\\n        including \"/Robot/base\" (even though this will result in an identity pose w.r.t. the source frame).\\n        \"\"\"\\n\\n        name: str | None = None\\n        \"\"\"User-defined name for the new coordinate frame. Defaults to None.\\n\\n        If None, then the name is extracted from the leaf of the prim path.\\n        \"\"\"\\n\\n        offset: OffsetCfg = OffsetCfg()\\n        \"\"\"The pose offset from the parent prim frame.\"\"\"\\n\\n    class_type: type = FrameTransformer\\n\\n    prim_path: str = MISSING\\n    \"\"\"The prim path of the body to transform from (source frame).\"\"\"\\n\\n    source_frame_offset: OffsetCfg = OffsetCfg()\\n    \"\"\"The pose offset from the source prim frame.\"\"\"\\n\\n    target_frames: list[FrameCfg] = MISSING\\n    \"\"\"A list of the target frames.\\n\\n    This allows a single FrameTransformer to handle multiple target prims. For example, in a quadruped,\\n    we can use a single FrameTransformer to track each foot\\'s position and orientation in the body\\n    frame using four frame offsets.\\n    \"\"\"\\n\\n    visualizer_cfg: VisualizationMarkersCfg = FRAME_MARKER_CFG.replace(prim_path=\"/Visuals/FrameTransformer\")\\n    \"\"\"The configuration object for the visualization markers. Defaults to FRAME_MARKER_CFG.\\n\\n    Note:\\n        This attribute is only used when debug visualization is enabled.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class FrameTransformerData:\\n    \"\"\"Data container for the frame transformer sensor.\"\"\"\\n\\n    target_frame_names: list[str] = None\\n    \"\"\"Target frame names (this denotes the order in which that frame data is ordered).\\n\\n    The frame names are resolved from the :attr:`FrameTransformerCfg.FrameCfg.name` field.\\n    This does not necessarily follow the order in which the frames are defined in the config due to\\n    the regex matching.\\n    \"\"\"\\n\\n    target_pos_source: torch.Tensor = None\\n    \"\"\"Position of the target frame(s) relative to source frame.\\n\\n    Shape is (N, M, 3), where N is the number of environments, and M is the number of target frames.\\n    \"\"\"\\n\\n    target_quat_source: torch.Tensor = None\\n    \"\"\"Orientation of the target frame(s) relative to source frame quaternion (w, x, y, z).\\n\\n    Shape is (N, M, 4), where N is the number of environments, and M is the number of target frames.\\n    \"\"\"\\n\\n    target_pos_w: torch.Tensor = None\\n    \"\"\"Position of the target frame(s) after offset (in world frame).\\n\\n    Shape is (N, M, 3), where N is the number of environments, and M is the number of target frames.\\n    \"\"\"\\n\\n    target_quat_w: torch.Tensor = None\\n    \"\"\"Orientation of the target frame(s) after offset (in world frame) quaternion (w, x, y, z).\\n\\n    Shape is (N, M, 4), where N is the number of environments, and M is the number of target frames.\\n    \"\"\"\\n\\n    source_pos_w: torch.Tensor = None\\n    \"\"\"Position of the source frame after offset (in world frame).\\n\\n    Shape is (N, 3), where N is the number of environments.\\n    \"\"\"\\n\\n    source_quat_w: torch.Tensor = None\\n    \"\"\"Orientation of the source frame after offset (in world frame) quaternion (w, x, y, z).\\n\\n    Shape is (N, 4), where N is the number of environments.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class Imu(SensorBase):\\n    \"\"\"The Inertia Measurement Unit (IMU) sensor.\\n\\n    The sensor can be attached to any :class:`RigidObject` or :class:`Articulation` in the scene. The sensor provides complete state information.\\n    The sensor is primarily used to provide the linear acceleration and angular velocity of the object in the body frame. The sensor also provides\\n    the position and orientation of the object in the world frame and the angular acceleration and linear velocity in the body frame. The extra\\n    data outputs are useful for simulating with or comparing against \"perfect\" state estimation.\\n\\n    .. note::\\n\\n        We are computing the accelerations using numerical differentiation from the velocities. Consequently, the\\n        IMU sensor accuracy depends on the chosen phsyx timestep. For a sufficient accuracy, we recommend to keep the\\n        timestep at least as 200Hz.\\n\\n    .. note::\\n\\n        It is suggested to use the OffsetCfg to define an IMU frame relative to a rigid body prim defined at the root of\\n        a :class:`RigidObject` or  a prim that is defined by a non-fixed joint in an :class:`Articulation` (except for the\\n        root of a fixed based articulation). The use frames with fixed joints and small mass/inertia to emulate a transform\\n        relative to a body frame can result in lower performance and accuracy.\\n\\n    \"\"\"\\n\\n    cfg: ImuCfg\\n    \"\"\"The configuration parameters.\"\"\"\\n\\n    def __init__(self, cfg: ImuCfg):\\n        \"\"\"Initializes the Imu sensor.\\n\\n        Args:\\n            cfg: The configuration parameters.\\n        \"\"\"\\n        # initialize base class\\n        super().__init__(cfg)\\n        # Create empty variables for storing output data\\n        self._data = ImuData()\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing information about the instance.\"\"\"\\n        return (\\n            f\"Imu sensor @ \\'{self.cfg.prim_path}\\': \\\\n\"\\n            f\"\\\\tview type         : {self._view.__class__}\\\\n\"\\n            f\"\\\\tupdate period (s) : {self.cfg.update_period}\\\\n\"\\n            f\"\\\\tnumber of sensors : {self._view.count}\\\\n\"\\n        )\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def data(self) -> ImuData:\\n        # update sensors if needed\\n        self._update_outdated_buffers()\\n        # return the data\\n        return self._data\\n\\n    @property\\n    def num_instances(self) -> int:\\n        return self._view.count\\n\\n    \"\"\"\\n    Operations\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # reset the timestamps\\n        super().reset(env_ids)\\n        # resolve None\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset accumulative data buffers\\n        self._data.quat_w[env_ids] = 0.0\\n        self._data.lin_vel_b[env_ids] = 0.0\\n        self._data.ang_vel_b[env_ids] = 0.0\\n        self._data.lin_acc_b[env_ids] = 0.0\\n        self._data.ang_acc_b[env_ids] = 0.0\\n\\n    def update(self, dt: float, force_recompute: bool = False):\\n        # save timestamp\\n        self._dt = dt\\n        # execute updating\\n        super().update(dt, force_recompute)\\n\\n    \"\"\"\\n    Implementation.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        \"\"\"Initializes the sensor handles and internal buffers.\\n\\n        This function creates handles and registers the provided data types with the replicator registry to\\n        be able to access the data from the sensor. It also initializes the internal buffers to store the data.\\n\\n        Raises:\\n            RuntimeError: If the imu prim is not a RigidBodyPrim\\n        \"\"\"\\n        # Initialize parent class\\n        super()._initialize_impl()\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        # check if the prim at path is a rigid prim\\n        prim = sim_utils.find_first_matching_prim(self.cfg.prim_path)\\n        if prim is None:\\n            raise RuntimeError(f\"Failed to find a prim at path expression: {self.cfg.prim_path}\")\\n        # check if it is a RigidBody Prim\\n        if prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n            self._view = self._physics_sim_view.create_rigid_body_view(self.cfg.prim_path.replace(\".*\", \"*\"))\\n        else:\\n            raise RuntimeError(f\"Failed to find a RigidBodyAPI for the prim paths: {self.cfg.prim_path}\")\\n\\n        # Create internal buffers\\n        self._initialize_buffers_impl()\\n\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        \"\"\"Fills the buffers of the sensor data.\"\"\"\\n        # check if self._dt is set (this is set in the update function)\\n        if not hasattr(self, \"_dt\"):\\n            raise RuntimeError(\\n                \"The update function must be called before the data buffers are accessed the first time.\"\\n            )\\n        # default to all sensors\\n        if len(env_ids) == self._num_envs:\\n            env_ids = slice(None)\\n        # obtain the poses of the sensors\\n        pos_w, quat_w = self._view.get_transforms()[env_ids].split([3, 4], dim=-1)\\n        quat_w = math_utils.convert_quat(quat_w, to=\"wxyz\")\\n\\n        # store the poses\\n        self._data.pos_w[env_ids] = pos_w + math_utils.quat_apply(quat_w, self._offset_pos_b[env_ids])\\n        self._data.quat_w[env_ids] = math_utils.quat_mul(quat_w, self._offset_quat_b[env_ids])\\n\\n        # get the offset from COM to link origin\\n        com_pos_b = self._view.get_coms().to(self.device).split([3, 4], dim=-1)[0]\\n\\n        # obtain the velocities of the link COM\\n        lin_vel_w, ang_vel_w = self._view.get_velocities()[env_ids].split([3, 3], dim=-1)\\n        # if an offset is present or the COM does not agree with the link origin, the linear velocity has to be\\n        # transformed taking the angular velocity into account\\n        lin_vel_w += torch.linalg.cross(\\n            ang_vel_w, math_utils.quat_apply(quat_w, self._offset_pos_b[env_ids] - com_pos_b[env_ids]), dim=-1\\n        )\\n\\n        # numerical derivative\\n        lin_acc_w = (lin_vel_w - self._prev_lin_vel_w[env_ids]) / self._dt + self._gravity_bias_w[env_ids]\\n        ang_acc_w = (ang_vel_w - self._prev_ang_vel_w[env_ids]) / self._dt\\n        # store the velocities\\n        self._data.lin_vel_b[env_ids] = math_utils.quat_apply_inverse(self._data.quat_w[env_ids], lin_vel_w)\\n        self._data.ang_vel_b[env_ids] = math_utils.quat_apply_inverse(self._data.quat_w[env_ids], ang_vel_w)\\n        # store the accelerations\\n        self._data.lin_acc_b[env_ids] = math_utils.quat_apply_inverse(self._data.quat_w[env_ids], lin_acc_w)\\n        self._data.ang_acc_b[env_ids] = math_utils.quat_apply_inverse(self._data.quat_w[env_ids], ang_acc_w)\\n\\n        self._prev_lin_vel_w[env_ids] = lin_vel_w\\n        self._prev_ang_vel_w[env_ids] = ang_vel_w\\n\\n    def _initialize_buffers_impl(self):\\n        \"\"\"Create buffers for storing data.\"\"\"\\n        # data buffers\\n        self._data.pos_w = torch.zeros(self._view.count, 3, device=self._device)\\n        self._data.quat_w = torch.zeros(self._view.count, 4, device=self._device)\\n        self._data.quat_w[:, 0] = 1.0\\n        self._data.lin_vel_b = torch.zeros_like(self._data.pos_w)\\n        self._data.ang_vel_b = torch.zeros_like(self._data.pos_w)\\n        self._data.lin_acc_b = torch.zeros_like(self._data.pos_w)\\n        self._data.ang_acc_b = torch.zeros_like(self._data.pos_w)\\n        self._prev_lin_vel_w = torch.zeros_like(self._data.pos_w)\\n        self._prev_ang_vel_w = torch.zeros_like(self._data.pos_w)\\n\\n        # store sensor offset transformation\\n        self._offset_pos_b = torch.tensor(list(self.cfg.offset.pos), device=self._device).repeat(self._view.count, 1)\\n        self._offset_quat_b = torch.tensor(list(self.cfg.offset.rot), device=self._device).repeat(self._view.count, 1)\\n        # set gravity bias\\n        self._gravity_bias_w = torch.tensor(list(self.cfg.gravity_bias), device=self._device).repeat(\\n            self._view.count, 1\\n        )\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # set visibility of markers\\n        # note: parent only deals with callbacks. not their visibility\\n        if debug_vis:\\n            # create markers if necessary for the first tome\\n            if not hasattr(self, \"acceleration_visualizer\"):\\n                self.acceleration_visualizer = VisualizationMarkers(self.cfg.visualizer_cfg)\\n            # set their visibility to true\\n            self.acceleration_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"acceleration_visualizer\"):\\n                self.acceleration_visualizer.set_visibility(False)\\n\\n    def _debug_vis_callback(self, event):\\n        # safely return if view becomes invalid\\n        # note: this invalidity happens because of isaac sim view callbacks\\n        if self._view is None:\\n            return\\n        # get marker location\\n        # -- base state\\n        base_pos_w = self._data.pos_w.clone()\\n        base_pos_w[:, 2] += 0.5\\n        # -- resolve the scales\\n        default_scale = self.acceleration_visualizer.cfg.markers[\"arrow\"].scale\\n        arrow_scale = torch.tensor(default_scale, device=self.device).repeat(self._data.lin_acc_b.shape[0], 1)\\n        # get up axis of current stage\\n        up_axis = stage_utils.get_stage_up_axis()\\n        # arrow-direction\\n        quat_opengl = math_utils.quat_from_matrix(\\n            math_utils.create_rotation_matrix_from_view(\\n                self._data.pos_w,\\n                self._data.pos_w + math_utils.quat_apply(self._data.quat_w, self._data.lin_acc_b),\\n                up_axis=up_axis,\\n                device=self._device,\\n            )\\n        )\\n        quat_w = math_utils.convert_camera_frame_orientation_convention(quat_opengl, \"opengl\", \"world\")\\n        # display markers\\n        self.acceleration_visualizer.visualize(base_pos_w, quat_w, arrow_scale)'),\n",
       " Document(metadata={}, page_content='class ImuCfg(SensorBaseCfg):\\n    \"\"\"Configuration for an Inertial Measurement Unit (IMU) sensor.\"\"\"\\n\\n    class_type: type = Imu\\n\\n    @configclass\\n    class OffsetCfg:\\n        \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame.\"\"\"\\n\\n        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Translation w.r.t. the parent frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n\\n        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n        \"\"\"Quaternion rotation (w, x, y, z) w.r.t. the parent frame. Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"\\n\\n    offset: OffsetCfg = OffsetCfg()\\n    \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame. Defaults to identity.\"\"\"\\n\\n    visualizer_cfg: VisualizationMarkersCfg = RED_ARROW_X_MARKER_CFG.replace(prim_path=\"/Visuals/Command/velocity_goal\")\\n    \"\"\"The configuration object for the visualization markers. Defaults to RED_ARROW_X_MARKER_CFG.\\n\\n    This attribute is only used when debug visualization is enabled.\\n    \"\"\"\\n    gravity_bias: tuple[float, float, float] = (0.0, 0.0, 9.81)\\n    \"\"\"The linear acceleration bias applied to the linear acceleration in the world frame (x,y,z).\\n\\n    Imu sensors typically output a positive gravity acceleration in opposition to the direction of gravity. This\\n    config parameter allows users to subtract that bias if set to (0.,0.,0.). By default this is set to (0.0,0.0,9.81)\\n    which results in a positive acceleration reading in the world Z.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ImuData:\\n    \"\"\"Data container for the Imu sensor.\"\"\"\\n\\n    pos_w: torch.Tensor = None\\n    \"\"\"Position of the sensor origin in world frame.\\n\\n    Shape is (N, 3), where ``N`` is the number of environments.\\n    \"\"\"\\n\\n    quat_w: torch.Tensor = None\\n    \"\"\"Orientation of the sensor origin in quaternion ``(w, x, y, z)`` in world frame.\\n\\n    Shape is (N, 4), where ``N`` is the number of environments.\\n    \"\"\"\\n\\n    lin_vel_b: torch.Tensor = None\\n    \"\"\"IMU frame angular velocity relative to the world expressed in IMU frame.\\n\\n    Shape is (N, 3), where ``N`` is the number of environments.\\n    \"\"\"\\n\\n    ang_vel_b: torch.Tensor = None\\n    \"\"\"IMU frame angular velocity relative to the world expressed in IMU frame.\\n\\n    Shape is (N, 3), where ``N`` is the number of environments.\\n    \"\"\"\\n\\n    lin_acc_b: torch.Tensor = None\\n    \"\"\"IMU frame linear acceleration relative to the world expressed in IMU frame.\\n\\n    Shape is (N, 3), where ``N`` is the number of environments.\\n    \"\"\"\\n\\n    ang_acc_b: torch.Tensor = None\\n    \"\"\"IMU frame angular acceleration relative to the world expressed in IMU frame.\\n\\n    Shape is (N, 3), where ``N`` is the number of environments.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RayCaster(SensorBase):\\n    \"\"\"A ray-casting sensor.\\n\\n    The ray-caster uses a set of rays to detect collisions with meshes in the scene. The rays are\\n    defined in the sensor\\'s local coordinate frame. The sensor can be configured to ray-cast against\\n    a set of meshes with a given ray pattern.\\n\\n    The meshes are parsed from the list of primitive paths provided in the configuration. These are then\\n    converted to warp meshes and stored in the `warp_meshes` list. The ray-caster then ray-casts against\\n    these warp meshes using the ray pattern provided in the configuration.\\n\\n    .. note::\\n        Currently, only static meshes are supported. Extending the warp mesh to support dynamic meshes\\n        is a work in progress.\\n    \"\"\"\\n\\n    cfg: RayCasterCfg\\n    \"\"\"The configuration parameters.\"\"\"\\n\\n    def __init__(self, cfg: RayCasterCfg):\\n        \"\"\"Initializes the ray-caster object.\\n\\n        Args:\\n            cfg: The configuration parameters.\\n        \"\"\"\\n        # check if sensor path is valid\\n        # note: currently we do not handle environment indices if there is a regex pattern in the leaf\\n        #   For example, if the prim path is \"/World/Sensor_[1,2]\".\\n        sensor_path = cfg.prim_path.split(\"/\")[-1]\\n        sensor_path_is_regex = re.match(r\"^[a-zA-Z0-9/_]+$\", sensor_path) is None\\n        if sensor_path_is_regex:\\n            raise RuntimeError(\\n                f\"Invalid prim path for the ray-caster sensor: {self.cfg.prim_path}.\"\\n                \"\\\\n\\\\tHint: Please ensure that the prim path does not contain any regex patterns in the leaf.\"\\n            )\\n        # Initialize base class\\n        super().__init__(cfg)\\n        # Create empty variables for storing output data\\n        self._data = RayCasterData()\\n        # the warp meshes used for raycasting.\\n        self.meshes: dict[str, wp.Mesh] = {}\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing information about the instance.\"\"\"\\n        return (\\n            f\"Ray-caster @ \\'{self.cfg.prim_path}\\': \\\\n\"\\n            f\"\\\\tview type            : {self._view.__class__}\\\\n\"\\n            f\"\\\\tupdate period (s)    : {self.cfg.update_period}\\\\n\"\\n            f\"\\\\tnumber of meshes     : {len(self.meshes)}\\\\n\"\\n            f\"\\\\tnumber of sensors    : {self._view.count}\\\\n\"\\n            f\"\\\\tnumber of rays/sensor: {self.num_rays}\\\\n\"\\n            f\"\\\\ttotal number of rays : {self.num_rays * self._view.count}\"\\n        )\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def num_instances(self) -> int:\\n        return self._view.count\\n\\n    @property\\n    def data(self) -> RayCasterData:\\n        # update sensors if needed\\n        self._update_outdated_buffers()\\n        # return the data\\n        return self._data\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # reset the timers and counters\\n        super().reset(env_ids)\\n        # resolve None\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # resample the drift\\n        self.drift[env_ids] = self.drift[env_ids].uniform_(*self.cfg.drift_range)\\n\\n    \"\"\"\\n    Implementation.\\n    \"\"\"\\n\\n    def _initialize_impl(self):\\n        super()._initialize_impl()\\n        # obtain global simulation view\\n        self._physics_sim_view = SimulationManager.get_physics_sim_view()\\n        # check if the prim at path is an articulated or rigid prim\\n        # we do this since for physics-based view classes we can access their data directly\\n        # otherwise we need to use the xform view class which is slower\\n        found_supported_prim_class = False\\n        prim = sim_utils.find_first_matching_prim(self.cfg.prim_path)\\n        if prim is None:\\n            raise RuntimeError(f\"Failed to find a prim at path expression: {self.cfg.prim_path}\")\\n        # create view based on the type of prim\\n        if prim.HasAPI(UsdPhysics.ArticulationRootAPI):\\n            self._view = self._physics_sim_view.create_articulation_view(self.cfg.prim_path.replace(\".*\", \"*\"))\\n            found_supported_prim_class = True\\n        elif prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n            self._view = self._physics_sim_view.create_rigid_body_view(self.cfg.prim_path.replace(\".*\", \"*\"))\\n            found_supported_prim_class = True\\n        else:\\n            self._view = XFormPrim(self.cfg.prim_path, reset_xform_properties=False)\\n            found_supported_prim_class = True\\n            omni.log.warn(f\"The prim at path {prim.GetPath().pathString} is not a physics prim! Using XFormPrim.\")\\n        # check if prim view class is found\\n        if not found_supported_prim_class:\\n            raise RuntimeError(f\"Failed to find a valid prim view class for the prim paths: {self.cfg.prim_path}\")\\n\\n        # load the meshes by parsing the stage\\n        self._initialize_warp_meshes()\\n        # initialize the ray start and directions\\n        self._initialize_rays_impl()\\n\\n    def _initialize_warp_meshes(self):\\n        # check number of mesh prims provided\\n        if len(self.cfg.mesh_prim_paths) != 1:\\n            raise NotImplementedError(\\n                f\"RayCaster currently only supports one mesh prim. Received: {len(self.cfg.mesh_prim_paths)}\"\\n            )\\n\\n        # read prims to ray-cast\\n        for mesh_prim_path in self.cfg.mesh_prim_paths:\\n            # check if the prim is a plane - handle PhysX plane as a special case\\n            # if a plane exists then we need to create an infinite mesh that is a plane\\n            mesh_prim = sim_utils.get_first_matching_child_prim(\\n                mesh_prim_path, lambda prim: prim.GetTypeName() == \"Plane\"\\n            )\\n            # if we did not find a plane then we need to read the mesh\\n            if mesh_prim is None:\\n                # obtain the mesh prim\\n                mesh_prim = sim_utils.get_first_matching_child_prim(\\n                    mesh_prim_path, lambda prim: prim.GetTypeName() == \"Mesh\"\\n                )\\n                # check if valid\\n                if mesh_prim is None or not mesh_prim.IsValid():\\n                    raise RuntimeError(f\"Invalid mesh prim path: {mesh_prim_path}\")\\n                # cast into UsdGeomMesh\\n                mesh_prim = UsdGeom.Mesh(mesh_prim)\\n                # read the vertices and faces\\n                points = np.asarray(mesh_prim.GetPointsAttr().Get())\\n                transform_matrix = np.array(omni.usd.get_world_transform_matrix(mesh_prim)).T\\n                points = np.matmul(points, transform_matrix[:3, :3].T)\\n                points += transform_matrix[:3, 3]\\n                indices = np.asarray(mesh_prim.GetFaceVertexIndicesAttr().Get())\\n                wp_mesh = convert_to_warp_mesh(points, indices, device=self.device)\\n                # print info\\n                omni.log.info(\\n                    f\"Read mesh prim: {mesh_prim.GetPath()} with {len(points)} vertices and {len(indices)} faces.\"\\n                )\\n            else:\\n                mesh = make_plane(size=(2e6, 2e6), height=0.0, center_zero=True)\\n                wp_mesh = convert_to_warp_mesh(mesh.vertices, mesh.faces, device=self.device)\\n                # print info\\n                omni.log.info(f\"Created infinite plane mesh prim: {mesh_prim.GetPath()}.\")\\n            # add the warp mesh to the list\\n            self.meshes[mesh_prim_path] = wp_mesh\\n\\n        # throw an error if no meshes are found\\n        if all([mesh_prim_path not in self.meshes for mesh_prim_path in self.cfg.mesh_prim_paths]):\\n            raise RuntimeError(\\n                f\"No meshes found for ray-casting! Please check the mesh prim paths: {self.cfg.mesh_prim_paths}\"\\n            )\\n\\n    def _initialize_rays_impl(self):\\n        # compute ray stars and directions\\n        self.ray_starts, self.ray_directions = self.cfg.pattern_cfg.func(self.cfg.pattern_cfg, self._device)\\n        self.num_rays = len(self.ray_directions)\\n        # apply offset transformation to the rays\\n        offset_pos = torch.tensor(list(self.cfg.offset.pos), device=self._device)\\n        offset_quat = torch.tensor(list(self.cfg.offset.rot), device=self._device)\\n        self.ray_directions = quat_apply(offset_quat.repeat(len(self.ray_directions), 1), self.ray_directions)\\n        self.ray_starts += offset_pos\\n        # repeat the rays for each sensor\\n        self.ray_starts = self.ray_starts.repeat(self._view.count, 1, 1)\\n        self.ray_directions = self.ray_directions.repeat(self._view.count, 1, 1)\\n        # prepare drift\\n        self.drift = torch.zeros(self._view.count, 3, device=self.device)\\n        # fill the data buffer\\n        self._data.pos_w = torch.zeros(self._view.count, 3, device=self._device)\\n        self._data.quat_w = torch.zeros(self._view.count, 4, device=self._device)\\n        self._data.ray_hits_w = torch.zeros(self._view.count, self.num_rays, 3, device=self._device)\\n\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        \"\"\"Fills the buffers of the sensor data.\"\"\"\\n        # obtain the poses of the sensors\\n        if isinstance(self._view, XFormPrim):\\n            pos_w, quat_w = self._view.get_world_poses(env_ids)\\n        elif isinstance(self._view, physx.ArticulationView):\\n            pos_w, quat_w = self._view.get_root_transforms()[env_ids].split([3, 4], dim=-1)\\n            quat_w = convert_quat(quat_w, to=\"wxyz\")\\n        elif isinstance(self._view, physx.RigidBodyView):\\n            pos_w, quat_w = self._view.get_transforms()[env_ids].split([3, 4], dim=-1)\\n            quat_w = convert_quat(quat_w, to=\"wxyz\")\\n        else:\\n            raise RuntimeError(f\"Unsupported view type: {type(self._view)}\")\\n        # note: we clone here because we are read-only operations\\n        pos_w = pos_w.clone()\\n        quat_w = quat_w.clone()\\n        # apply drift\\n        pos_w += self.drift[env_ids]\\n        # store the poses\\n        self._data.pos_w[env_ids] = pos_w\\n        self._data.quat_w[env_ids] = quat_w\\n\\n        # ray cast based on the sensor poses\\n        if self.cfg.attach_yaw_only:\\n            # only yaw orientation is considered and directions are not rotated\\n            ray_starts_w = quat_apply_yaw(quat_w.repeat(1, self.num_rays), self.ray_starts[env_ids])\\n            ray_starts_w += pos_w.unsqueeze(1)\\n            ray_directions_w = self.ray_directions[env_ids]\\n        else:\\n            # full orientation is considered\\n            ray_starts_w = quat_apply(quat_w.repeat(1, self.num_rays), self.ray_starts[env_ids])\\n            ray_starts_w += pos_w.unsqueeze(1)\\n            ray_directions_w = quat_apply(quat_w.repeat(1, self.num_rays), self.ray_directions[env_ids])\\n        # ray cast and store the hits\\n        # TODO: Make this work for multiple meshes?\\n        self._data.ray_hits_w[env_ids] = raycast_mesh(\\n            ray_starts_w,\\n            ray_directions_w,\\n            max_dist=self.cfg.max_distance,\\n            mesh=self.meshes[self.cfg.mesh_prim_paths[0]],\\n        )[0]\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        # set visibility of markers\\n        # note: parent only deals with callbacks. not their visibility\\n        if debug_vis:\\n            if not hasattr(self, \"ray_visualizer\"):\\n                self.ray_visualizer = VisualizationMarkers(self.cfg.visualizer_cfg)\\n            # set their visibility to true\\n            self.ray_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"ray_visualizer\"):\\n                self.ray_visualizer.set_visibility(False)\\n\\n    def _debug_vis_callback(self, event):\\n        # remove possible inf values\\n        viz_points = self._data.ray_hits_w.reshape(-1, 3)\\n        viz_points = viz_points[~torch.any(torch.isinf(viz_points), dim=1)]\\n        # show ray hit positions\\n        self.ray_visualizer.visualize(viz_points)\\n\\n    \"\"\"\\n    Internal simulation callbacks.\\n    \"\"\"\\n\\n    def _invalidate_initialize_callback(self, event):\\n        \"\"\"Invalidates the scene elements.\"\"\"\\n        # call parent\\n        super()._invalidate_initialize_callback(event)\\n        # set all existing views to None to invalidate them\\n        self._view = None'),\n",
       " Document(metadata={}, page_content='class RayCasterCamera(RayCaster):\\n    \"\"\"A ray-casting camera sensor.\\n\\n    The ray-caster camera uses a set of rays to get the distances to meshes in the scene. The rays are\\n    defined in the sensor\\'s local coordinate frame. The sensor has the same interface as the\\n    :class:`isaaclab.sensors.Camera` that implements the camera class through USD camera prims.\\n    However, this class provides a faster image generation. The sensor converts meshes from the list of\\n    primitive paths provided in the configuration to Warp meshes. The camera then ray-casts against these\\n    Warp meshes only.\\n\\n    Currently, only the following annotators are supported:\\n\\n    - ``\"distance_to_camera\"``: An image containing the distance to camera optical center.\\n    - ``\"distance_to_image_plane\"``: An image containing distances of 3D points from camera plane along camera\\'s z-axis.\\n    - ``\"normals\"``: An image containing the local surface normal vectors at each pixel.\\n\\n    .. note::\\n        Currently, only static meshes are supported. Extending the warp mesh to support dynamic meshes\\n        is a work in progress.\\n    \"\"\"\\n\\n    cfg: RayCasterCameraCfg\\n    \"\"\"The configuration parameters.\"\"\"\\n    UNSUPPORTED_TYPES: ClassVar[set[str]] = {\\n        \"rgb\",\\n        \"instance_id_segmentation\",\\n        \"instance_id_segmentation_fast\",\\n        \"instance_segmentation\",\\n        \"instance_segmentation_fast\",\\n        \"semantic_segmentation\",\\n        \"skeleton_data\",\\n        \"motion_vectors\",\\n        \"bounding_box_2d_tight\",\\n        \"bounding_box_2d_tight_fast\",\\n        \"bounding_box_2d_loose\",\\n        \"bounding_box_2d_loose_fast\",\\n        \"bounding_box_3d\",\\n        \"bounding_box_3d_fast\",\\n    }\\n    \"\"\"A set of sensor types that are not supported by the ray-caster camera.\"\"\"\\n\\n    def __init__(self, cfg: RayCasterCameraCfg):\\n        \"\"\"Initializes the camera object.\\n\\n        Args:\\n            cfg: The configuration parameters.\\n\\n        Raises:\\n            ValueError: If the provided data types are not supported by the ray-caster camera.\\n        \"\"\"\\n        # perform check on supported data types\\n        self._check_supported_data_types(cfg)\\n        # initialize base class\\n        super().__init__(cfg)\\n        # create empty variables for storing output data\\n        self._data = CameraData()\\n\\n    def __str__(self) -> str:\\n        \"\"\"Returns: A string containing information about the instance.\"\"\"\\n        return (\\n            f\"Ray-Caster-Camera @ \\'{self.cfg.prim_path}\\': \\\\n\"\\n            f\"\\\\tview type            : {self._view.__class__}\\\\n\"\\n            f\"\\\\tupdate period (s)    : {self.cfg.update_period}\\\\n\"\\n            f\"\\\\tnumber of meshes     : {len(self.meshes)}\\\\n\"\\n            f\"\\\\tnumber of sensors    : {self._view.count}\\\\n\"\\n            f\"\\\\tnumber of rays/sensor: {self.num_rays}\\\\n\"\\n            f\"\\\\ttotal number of rays : {self.num_rays * self._view.count}\\\\n\"\\n            f\"\\\\timage shape          : {self.image_shape}\"\\n        )\\n\\n    \"\"\"\\n    Properties\\n    \"\"\"\\n\\n    @property\\n    def data(self) -> CameraData:\\n        # update sensors if needed\\n        self._update_outdated_buffers()\\n        # return the data\\n        return self._data\\n\\n    @property\\n    def image_shape(self) -> tuple[int, int]:\\n        \"\"\"A tuple containing (height, width) of the camera sensor.\"\"\"\\n        return (self.cfg.pattern_cfg.height, self.cfg.pattern_cfg.width)\\n\\n    @property\\n    def frame(self) -> torch.tensor:\\n        \"\"\"Frame number when the measurement took place.\"\"\"\\n        return self._frame\\n\\n    \"\"\"\\n    Operations.\\n    \"\"\"\\n\\n    def set_intrinsic_matrices(\\n        self, matrices: torch.Tensor, focal_length: float = 1.0, env_ids: Sequence[int] | None = None\\n    ):\\n        \"\"\"Set the intrinsic matrix of the camera.\\n\\n        Args:\\n            matrices: The intrinsic matrices for the camera. Shape is (N, 3, 3).\\n            focal_length: Focal length to use when computing aperture values (in cm). Defaults to 1.0.\\n            env_ids: A sensor ids to manipulate. Defaults to None, which means all sensor indices.\\n        \"\"\"\\n        # resolve env_ids\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # save new intrinsic matrices and focal length\\n        self._data.intrinsic_matrices[env_ids] = matrices.to(self._device)\\n        self._focal_length = focal_length\\n        # recompute ray directions\\n        self.ray_starts[env_ids], self.ray_directions[env_ids] = self.cfg.pattern_cfg.func(\\n            self.cfg.pattern_cfg, self._data.intrinsic_matrices[env_ids], self._device\\n        )\\n\\n    def reset(self, env_ids: Sequence[int] | None = None):\\n        # reset the timestamps\\n        super().reset(env_ids)\\n        # resolve None\\n        if env_ids is None:\\n            env_ids = slice(None)\\n        # reset the data\\n        # note: this recomputation is useful if one performs events such as randomizations on the camera poses.\\n        pos_w, quat_w = self._compute_camera_world_poses(env_ids)\\n        self._data.pos_w[env_ids] = pos_w\\n        self._data.quat_w_world[env_ids] = quat_w\\n        # Reset the frame count\\n        self._frame[env_ids] = 0\\n\\n    def set_world_poses(\\n        self,\\n        positions: torch.Tensor | None = None,\\n        orientations: torch.Tensor | None = None,\\n        env_ids: Sequence[int] | None = None,\\n        convention: Literal[\"opengl\", \"ros\", \"world\"] = \"ros\",\\n    ):\\n        \"\"\"Set the pose of the camera w.r.t. the world frame using specified convention.\\n\\n        Since different fields use different conventions for camera orientations, the method allows users to\\n        set the camera poses in the specified convention. Possible conventions are:\\n\\n        - :obj:`\"opengl\"` - forward axis: -Z - up axis +Y - Offset is applied in the OpenGL (Usd.Camera) convention\\n        - :obj:`\"ros\"`    - forward axis: +Z - up axis -Y - Offset is applied in the ROS convention\\n        - :obj:`\"world\"`  - forward axis: +X - up axis +Z - Offset is applied in the World Frame convention\\n\\n        See :meth:`isaaclab.utils.maths.convert_camera_frame_orientation_convention` for more details\\n        on the conventions.\\n\\n        Args:\\n            positions: The cartesian coordinates (in meters). Shape is (N, 3).\\n                Defaults to None, in which case the camera position in not changed.\\n            orientations: The quaternion orientation in (w, x, y, z). Shape is (N, 4).\\n                Defaults to None, in which case the camera orientation in not changed.\\n            env_ids: A sensor ids to manipulate. Defaults to None, which means all sensor indices.\\n            convention: The convention in which the poses are fed. Defaults to \"ros\".\\n\\n        Raises:\\n            RuntimeError: If the camera prim is not set. Need to call :meth:`initialize` method first.\\n        \"\"\"\\n        # resolve env_ids\\n        if env_ids is None:\\n            env_ids = self._ALL_INDICES\\n\\n        # get current positions\\n        pos_w, quat_w = self._compute_view_world_poses(env_ids)\\n        if positions is not None:\\n            # transform to camera frame\\n            pos_offset_world_frame = positions - pos_w\\n            self._offset_pos[env_ids] = math_utils.quat_apply(math_utils.quat_inv(quat_w), pos_offset_world_frame)\\n        if orientations is not None:\\n            # convert rotation matrix from input convention to world\\n            quat_w_set = math_utils.convert_camera_frame_orientation_convention(\\n                orientations, origin=convention, target=\"world\"\\n            )\\n            self._offset_quat[env_ids] = math_utils.quat_mul(math_utils.quat_inv(quat_w), quat_w_set)\\n\\n        # update the data\\n        pos_w, quat_w = self._compute_camera_world_poses(env_ids)\\n        self._data.pos_w[env_ids] = pos_w\\n        self._data.quat_w_world[env_ids] = quat_w\\n\\n    def set_world_poses_from_view(\\n        self, eyes: torch.Tensor, targets: torch.Tensor, env_ids: Sequence[int] | None = None\\n    ):\\n        \"\"\"Set the poses of the camera from the eye position and look-at target position.\\n\\n        Args:\\n            eyes: The positions of the camera\\'s eye. Shape is N, 3).\\n            targets: The target locations to look at. Shape is (N, 3).\\n            env_ids: A sensor ids to manipulate. Defaults to None, which means all sensor indices.\\n\\n        Raises:\\n            RuntimeError: If the camera prim is not set. Need to call :meth:`initialize` method first.\\n            NotImplementedError: If the stage up-axis is not \"Y\" or \"Z\".\\n        \"\"\"\\n        # get up axis of current stage\\n        up_axis = stage_utils.get_stage_up_axis()\\n        # camera position and rotation in opengl convention\\n        orientations = math_utils.quat_from_matrix(\\n            math_utils.create_rotation_matrix_from_view(eyes, targets, up_axis=up_axis, device=self._device)\\n        )\\n        self.set_world_poses(eyes, orientations, env_ids, convention=\"opengl\")\\n\\n    \"\"\"\\n    Implementation.\\n    \"\"\"\\n\\n    def _initialize_rays_impl(self):\\n        # Create all indices buffer\\n        self._ALL_INDICES = torch.arange(self._view.count, device=self._device, dtype=torch.long)\\n        # Create frame count buffer\\n        self._frame = torch.zeros(self._view.count, device=self._device, dtype=torch.long)\\n        # create buffers\\n        self._create_buffers()\\n        # compute intrinsic matrices\\n        self._compute_intrinsic_matrices()\\n        # compute ray stars and directions\\n        self.ray_starts, self.ray_directions = self.cfg.pattern_cfg.func(\\n            self.cfg.pattern_cfg, self._data.intrinsic_matrices, self._device\\n        )\\n        self.num_rays = self.ray_directions.shape[1]\\n        # create buffer to store ray hits\\n        self.ray_hits_w = torch.zeros(self._view.count, self.num_rays, 3, device=self._device)\\n        # set offsets\\n        quat_w = math_utils.convert_camera_frame_orientation_convention(\\n            torch.tensor([self.cfg.offset.rot], device=self._device), origin=self.cfg.offset.convention, target=\"world\"\\n        )\\n        self._offset_quat = quat_w.repeat(self._view.count, 1)\\n        self._offset_pos = torch.tensor(list(self.cfg.offset.pos), device=self._device).repeat(self._view.count, 1)\\n\\n    def _update_buffers_impl(self, env_ids: Sequence[int]):\\n        \"\"\"Fills the buffers of the sensor data.\"\"\"\\n        # increment frame count\\n        self._frame[env_ids] += 1\\n\\n        # compute poses from current view\\n        pos_w, quat_w = self._compute_camera_world_poses(env_ids)\\n        # update the data\\n        self._data.pos_w[env_ids] = pos_w\\n        self._data.quat_w_world[env_ids] = quat_w\\n\\n        # note: full orientation is considered\\n        ray_starts_w = math_utils.quat_apply(quat_w.repeat(1, self.num_rays), self.ray_starts[env_ids])\\n        ray_starts_w += pos_w.unsqueeze(1)\\n        ray_directions_w = math_utils.quat_apply(quat_w.repeat(1, self.num_rays), self.ray_directions[env_ids])\\n\\n        # ray cast and store the hits\\n        # note: we set max distance to 1e6 during the ray-casting. THis is because we clip the distance\\n        # to the image plane and distance to the camera to the maximum distance afterwards in-order to\\n        # match the USD camera behavior.\\n\\n        # TODO: Make ray-casting work for multiple meshes?\\n        # necessary for regular dictionaries.\\n        self.ray_hits_w, ray_depth, ray_normal, _ = raycast_mesh(\\n            ray_starts_w,\\n            ray_directions_w,\\n            mesh=self.meshes[self.cfg.mesh_prim_paths[0]],\\n            max_dist=1e6,\\n            return_distance=any(\\n                [name in self.cfg.data_types for name in [\"distance_to_image_plane\", \"distance_to_camera\"]]\\n            ),\\n            return_normal=\"normals\" in self.cfg.data_types,\\n        )\\n        # update output buffers\\n        if \"distance_to_image_plane\" in self.cfg.data_types:\\n            # note: data is in camera frame so we only take the first component (z-axis of camera frame)\\n            distance_to_image_plane = (\\n                math_utils.quat_apply(\\n                    math_utils.quat_inv(quat_w).repeat(1, self.num_rays),\\n                    (ray_depth[:, :, None] * ray_directions_w),\\n                )\\n            )[:, :, 0]\\n            # apply the maximum distance after the transformation\\n            if self.cfg.depth_clipping_behavior == \"max\":\\n                distance_to_image_plane = torch.clip(distance_to_image_plane, max=self.cfg.max_distance)\\n                distance_to_image_plane[torch.isnan(distance_to_image_plane)] = self.cfg.max_distance\\n            elif self.cfg.depth_clipping_behavior == \"zero\":\\n                distance_to_image_plane[distance_to_image_plane > self.cfg.max_distance] = 0.0\\n                distance_to_image_plane[torch.isnan(distance_to_image_plane)] = 0.0\\n            self._data.output[\"distance_to_image_plane\"][env_ids] = distance_to_image_plane.view(\\n                -1, *self.image_shape, 1\\n            )\\n\\n        if \"distance_to_camera\" in self.cfg.data_types:\\n            if self.cfg.depth_clipping_behavior == \"max\":\\n                ray_depth = torch.clip(ray_depth, max=self.cfg.max_distance)\\n            elif self.cfg.depth_clipping_behavior == \"zero\":\\n                ray_depth[ray_depth > self.cfg.max_distance] = 0.0\\n            self._data.output[\"distance_to_camera\"][env_ids] = ray_depth.view(-1, *self.image_shape, 1)\\n\\n        if \"normals\" in self.cfg.data_types:\\n            self._data.output[\"normals\"][env_ids] = ray_normal.view(-1, *self.image_shape, 3)\\n\\n    def _debug_vis_callback(self, event):\\n        # in case it crashes be safe\\n        if not hasattr(self, \"ray_hits_w\"):\\n            return\\n        # show ray hit positions\\n        self.ray_visualizer.visualize(self.ray_hits_w.view(-1, 3))\\n\\n    \"\"\"\\n    Private Helpers\\n    \"\"\"\\n\\n    def _check_supported_data_types(self, cfg: RayCasterCameraCfg):\\n        \"\"\"Checks if the data types are supported by the ray-caster camera.\"\"\"\\n        # check if there is any intersection in unsupported types\\n        # reason: we cannot obtain this data from simplified warp-based ray caster\\n        common_elements = set(cfg.data_types) & RayCasterCamera.UNSUPPORTED_TYPES\\n        if common_elements:\\n            raise ValueError(\\n                f\"RayCasterCamera class does not support the following sensor types: {common_elements}.\"\\n                \"\\\\n\\\\tThis is because these sensor types cannot be obtained in a fast way using \\'\\'warp\\'\\'.\"\\n                \"\\\\n\\\\tHint: If you need to work with these sensor types, we recommend using the USD camera\"\\n                \" interface from the isaaclab.sensors.camera module.\"\\n            )\\n\\n    def _create_buffers(self):\\n        \"\"\"Create buffers for storing data.\"\"\"\\n        # prepare drift\\n        self.drift = torch.zeros(self._view.count, 3, device=self.device)\\n        # create the data object\\n        # -- pose of the cameras\\n        self._data.pos_w = torch.zeros((self._view.count, 3), device=self._device)\\n        self._data.quat_w_world = torch.zeros((self._view.count, 4), device=self._device)\\n        # -- intrinsic matrix\\n        self._data.intrinsic_matrices = torch.zeros((self._view.count, 3, 3), device=self._device)\\n        self._data.intrinsic_matrices[:, 2, 2] = 1.0\\n        self._data.image_shape = self.image_shape\\n        # -- output data\\n        # create the buffers to store the annotator data.\\n        self._data.output = {}\\n        self._data.info = [{name: None for name in self.cfg.data_types}] * self._view.count\\n        for name in self.cfg.data_types:\\n            if name in [\"distance_to_image_plane\", \"distance_to_camera\"]:\\n                shape = (self.cfg.pattern_cfg.height, self.cfg.pattern_cfg.width, 1)\\n            elif name in [\"normals\"]:\\n                shape = (self.cfg.pattern_cfg.height, self.cfg.pattern_cfg.width, 3)\\n            else:\\n                raise ValueError(f\"Received unknown data type: {name}. Please check the configuration.\")\\n            # allocate tensor to store the data\\n            self._data.output[name] = torch.zeros((self._view.count, *shape), device=self._device)\\n\\n    def _compute_intrinsic_matrices(self):\\n        \"\"\"Computes the intrinsic matrices for the camera based on the config provided.\"\"\"\\n        # get the sensor properties\\n        pattern_cfg = self.cfg.pattern_cfg\\n\\n        # check if vertical aperture is provided\\n        # if not then it is auto-computed based on the aspect ratio to preserve squared pixels\\n        if pattern_cfg.vertical_aperture is None:\\n            pattern_cfg.vertical_aperture = pattern_cfg.horizontal_aperture * pattern_cfg.height / pattern_cfg.width\\n\\n        # compute the intrinsic matrix\\n        f_x = pattern_cfg.width * pattern_cfg.focal_length / pattern_cfg.horizontal_aperture\\n        f_y = pattern_cfg.height * pattern_cfg.focal_length / pattern_cfg.vertical_aperture\\n        c_x = pattern_cfg.horizontal_aperture_offset * f_x + pattern_cfg.width / 2\\n        c_y = pattern_cfg.vertical_aperture_offset * f_y + pattern_cfg.height / 2\\n        # allocate the intrinsic matrices\\n        self._data.intrinsic_matrices[:, 0, 0] = f_x\\n        self._data.intrinsic_matrices[:, 0, 2] = c_x\\n        self._data.intrinsic_matrices[:, 1, 1] = f_y\\n        self._data.intrinsic_matrices[:, 1, 2] = c_y\\n\\n        # save focal length\\n        self._focal_length = pattern_cfg.focal_length\\n\\n    def _compute_view_world_poses(self, env_ids: Sequence[int]) -> tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"Obtains the pose of the view the camera is attached to in the world frame.\\n\\n        Returns:\\n            A tuple of the position (in meters) and quaternion (w, x, y, z).\\n        \"\"\"\\n        # obtain the poses of the sensors\\n        # note: clone arg doesn\\'t exist for xform prim view so we need to do this manually\\n        if isinstance(self._view, XFormPrim):\\n            pos_w, quat_w = self._view.get_world_poses(env_ids)\\n        elif isinstance(self._view, physx.ArticulationView):\\n            pos_w, quat_w = self._view.get_root_transforms()[env_ids].split([3, 4], dim=-1)\\n            quat_w = math_utils.convert_quat(quat_w, to=\"wxyz\")\\n        elif isinstance(self._view, physx.RigidBodyView):\\n            pos_w, quat_w = self._view.get_transforms()[env_ids].split([3, 4], dim=-1)\\n            quat_w = math_utils.convert_quat(quat_w, to=\"wxyz\")\\n        else:\\n            raise RuntimeError(f\"Unsupported view type: {type(self._view)}\")\\n        # return the pose\\n        return pos_w.clone(), quat_w.clone()\\n\\n    def _compute_camera_world_poses(self, env_ids: Sequence[int]) -> tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"Computes the pose of the camera in the world frame.\\n\\n        This function applies the offset pose to the pose of the view the camera is attached to.\\n\\n        Returns:\\n            A tuple of the position (in meters) and quaternion (w, x, y, z) in \"world\" convention.\\n        \"\"\"\\n        # get the pose of the view the camera is attached to\\n        pos_w, quat_w = self._compute_view_world_poses(env_ids)\\n        # apply offsets\\n        # need to apply quat because offset relative to parent frame\\n        pos_w += math_utils.quat_apply(quat_w, self._offset_pos[env_ids])\\n        quat_w = math_utils.quat_mul(quat_w, self._offset_quat[env_ids])\\n\\n        return pos_w, quat_w'),\n",
       " Document(metadata={}, page_content='class RayCasterCameraCfg(RayCasterCfg):\\n    \"\"\"Configuration for the ray-cast sensor.\"\"\"\\n\\n    @configclass\\n    class OffsetCfg:\\n        \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame.\"\"\"\\n\\n        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Translation w.r.t. the parent frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n\\n        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n        \"\"\"Quaternion rotation (w, x, y, z) w.r.t. the parent frame. Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"\\n\\n        convention: Literal[\"opengl\", \"ros\", \"world\"] = \"ros\"\\n        \"\"\"The convention in which the frame offset is applied. Defaults to \"ros\".\\n\\n        - ``\"opengl\"`` - forward axis: ``-Z`` - up axis: ``+Y`` - Offset is applied in the OpenGL (Usd.Camera) convention.\\n        - ``\"ros\"``    - forward axis: ``+Z`` - up axis: ``-Y`` - Offset is applied in the ROS convention.\\n        - ``\"world\"``  - forward axis: ``+X`` - up axis: ``+Z`` - Offset is applied in the World Frame convention.\\n\\n        \"\"\"\\n\\n    class_type: type = RayCasterCamera\\n\\n    offset: OffsetCfg = OffsetCfg()\\n    \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame. Defaults to identity.\"\"\"\\n\\n    data_types: list[str] = [\"distance_to_image_plane\"]\\n    \"\"\"List of sensor names/types to enable for the camera. Defaults to [\"distance_to_image_plane\"].\"\"\"\\n\\n    depth_clipping_behavior: Literal[\"max\", \"zero\", \"none\"] = \"none\"\\n    \"\"\"Clipping behavior for the camera for values exceed the maximum value. Defaults to \"none\".\\n\\n    - ``\"max\"``: Values are clipped to the maximum value.\\n    - ``\"zero\"``: Values are clipped to zero.\\n    - ``\"none``: No clipping is applied. Values will be returned as ``inf`` for ``distance_to_camera`` and ``nan``\\n      for ``distance_to_image_plane`` data type.\\n    \"\"\"\\n\\n    pattern_cfg: PinholeCameraPatternCfg = MISSING\\n    \"\"\"The pattern that defines the local ray starting positions and directions in a pinhole camera pattern.\"\"\"\\n\\n    def __post_init__(self):\\n        # for cameras, this quantity should be False always.\\n        self.attach_yaw_only = False'),\n",
       " Document(metadata={}, page_content='class RayCasterCfg(SensorBaseCfg):\\n    \"\"\"Configuration for the ray-cast sensor.\"\"\"\\n\\n    @configclass\\n    class OffsetCfg:\\n        \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame.\"\"\"\\n\\n        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n        \"\"\"Translation w.r.t. the parent frame. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n        \"\"\"Quaternion rotation (w, x, y, z) w.r.t. the parent frame. Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"\\n\\n    class_type: type = RayCaster\\n\\n    mesh_prim_paths: list[str] = MISSING\\n    \"\"\"The list of mesh primitive paths to ray cast against.\\n\\n    Note:\\n        Currently, only a single static mesh is supported. We are working on supporting multiple\\n        static meshes and dynamic meshes.\\n    \"\"\"\\n\\n    offset: OffsetCfg = OffsetCfg()\\n    \"\"\"The offset pose of the sensor\\'s frame from the sensor\\'s parent frame. Defaults to identity.\"\"\"\\n\\n    attach_yaw_only: bool = MISSING\\n    \"\"\"Whether the rays\\' starting positions and directions only track the yaw orientation.\\n\\n    This is useful for ray-casting height maps, where only yaw rotation is needed.\\n    \"\"\"\\n\\n    pattern_cfg: PatternBaseCfg = MISSING\\n    \"\"\"The pattern that defines the local ray starting positions and directions.\"\"\"\\n\\n    max_distance: float = 1e6\\n    \"\"\"Maximum distance (in meters) from the sensor to ray cast to. Defaults to 1e6.\"\"\"\\n\\n    drift_range: tuple[float, float] = (0.0, 0.0)\\n    \"\"\"The range of drift (in meters) to add to the ray starting positions (xyz). Defaults to (0.0, 0.0).\\n\\n    For floating base robots, this is useful for simulating drift in the robot\\'s pose estimation.\\n    \"\"\"\\n\\n    visualizer_cfg: VisualizationMarkersCfg = RAY_CASTER_MARKER_CFG.replace(prim_path=\"/Visuals/RayCaster\")\\n    \"\"\"The configuration object for the visualization markers. Defaults to RAY_CASTER_MARKER_CFG.\\n\\n    Note:\\n        This attribute is only used when debug visualization is enabled.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RayCasterData:\\n    \"\"\"Data container for the ray-cast sensor.\"\"\"\\n\\n    pos_w: torch.Tensor = None\\n    \"\"\"Position of the sensor origin in world frame.\\n\\n    Shape is (N, 3), where N is the number of sensors.\\n    \"\"\"\\n    quat_w: torch.Tensor = None\\n    \"\"\"Orientation of the sensor origin in quaternion (w, x, y, z) in world frame.\\n\\n    Shape is (N, 4), where N is the number of sensors.\\n    \"\"\"\\n    ray_hits_w: torch.Tensor = None\\n    \"\"\"The ray hit positions in the world frame.\\n\\n    Shape is (N, B, 3), where N is the number of sensors, B is the number of rays\\n    in the scan pattern per sensor.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='def grid_pattern(cfg: patterns_cfg.GridPatternCfg, device: str) -> tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"A regular grid pattern for ray casting.\\n\\n    The grid pattern is made from rays that are parallel to each other. They span a 2D grid in the sensor\\'s\\n    local coordinates from ``(-length/2, -width/2)`` to ``(length/2, width/2)``, which is defined\\n    by the ``size = (length, width)`` and ``resolution`` parameters in the config.\\n\\n    Args:\\n        cfg: The configuration instance for the pattern.\\n        device: The device to create the pattern on.\\n\\n    Returns:\\n        The starting positions and directions of the rays.\\n\\n    Raises:\\n        ValueError: If the ordering is not \"xy\" or \"yx\".\\n        ValueError: If the resolution is less than or equal to 0.\\n    \"\"\"\\n    # check valid arguments\\n    if cfg.ordering not in [\"xy\", \"yx\"]:\\n        raise ValueError(f\"Ordering must be \\'xy\\' or \\'yx\\'. Received: \\'{cfg.ordering}\\'.\")\\n    if cfg.resolution <= 0:\\n        raise ValueError(f\"Resolution must be greater than 0. Received: \\'{cfg.resolution}\\'.\")\\n\\n    # resolve mesh grid indexing (note: torch meshgrid is different from numpy meshgrid)\\n    # check: https://github.com/pytorch/pytorch/issues/15301\\n    indexing = cfg.ordering if cfg.ordering == \"xy\" else \"ij\"\\n    # define grid pattern\\n    x = torch.arange(start=-cfg.size[0] / 2, end=cfg.size[0] / 2 + 1.0e-9, step=cfg.resolution, device=device)\\n    y = torch.arange(start=-cfg.size[1] / 2, end=cfg.size[1] / 2 + 1.0e-9, step=cfg.resolution, device=device)\\n    grid_x, grid_y = torch.meshgrid(x, y, indexing=indexing)\\n\\n    # store into ray starts\\n    num_rays = grid_x.numel()\\n    ray_starts = torch.zeros(num_rays, 3, device=device)\\n    ray_starts[:, 0] = grid_x.flatten()\\n    ray_starts[:, 1] = grid_y.flatten()\\n\\n    # define ray-cast directions\\n    ray_directions = torch.zeros_like(ray_starts)\\n    ray_directions[..., :] = torch.tensor(list(cfg.direction), device=device)\\n\\n    return ray_starts, ray_directions'),\n",
       " Document(metadata={}, page_content='def pinhole_camera_pattern(\\n    cfg: patterns_cfg.PinholeCameraPatternCfg, intrinsic_matrices: torch.Tensor, device: str\\n) -> tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"The image pattern for ray casting.\\n\\n    .. caution::\\n        This function does not follow the standard pattern interface. It requires the intrinsic matrices\\n        of the cameras to be passed in. This is because we want to be able to randomize the intrinsic\\n        matrices of the cameras, which is not possible with the standard pattern interface.\\n\\n    Args:\\n        cfg: The configuration instance for the pattern.\\n        intrinsic_matrices: The intrinsic matrices of the cameras. Shape is (N, 3, 3).\\n        device: The device to create the pattern on.\\n\\n    Returns:\\n        The starting positions and directions of the rays. The shape of the tensors are\\n        (N, H * W, 3) and (N, H * W, 3) respectively.\\n    \"\"\"\\n    # get image plane mesh grid\\n    grid = torch.meshgrid(\\n        torch.arange(start=0, end=cfg.width, dtype=torch.int32, device=device),\\n        torch.arange(start=0, end=cfg.height, dtype=torch.int32, device=device),\\n        indexing=\"xy\",\\n    )\\n    pixels = torch.vstack(list(map(torch.ravel, grid))).T\\n    # convert to homogeneous coordinate system\\n    pixels = torch.hstack([pixels, torch.ones((len(pixels), 1), device=device)])\\n    # move each pixel coordinate to the center of the pixel\\n    pixels += torch.tensor([[0.5, 0.5, 0]], device=device)\\n    # get pixel coordinates in camera frame\\n    pix_in_cam_frame = torch.matmul(torch.inverse(intrinsic_matrices), pixels.T)\\n\\n    # robotics camera frame is (x forward, y left, z up) from camera frame with (x right, y down, z forward)\\n    # transform to robotics camera frame\\n    transform_vec = torch.tensor([1, -1, -1], device=device).unsqueeze(0).unsqueeze(2)\\n    pix_in_cam_frame = pix_in_cam_frame[:, [2, 0, 1], :] * transform_vec\\n    # normalize ray directions\\n    ray_directions = (pix_in_cam_frame / torch.norm(pix_in_cam_frame, dim=1, keepdim=True)).permute(0, 2, 1)\\n    # for camera, we always ray-cast from the sensor\\'s origin\\n    ray_starts = torch.zeros_like(ray_directions, device=device)\\n\\n    return ray_starts, ray_directions'),\n",
       " Document(metadata={}, page_content='def bpearl_pattern(cfg: patterns_cfg.BpearlPatternCfg, device: str) -> tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"The RS-Bpearl pattern for ray casting.\\n\\n    The `Robosense RS-Bpearl`_ is a short-range LiDAR that has a 360 degrees x 90 degrees super wide\\n    field of view. It is designed for near-field blind-spots detection.\\n\\n    .. _Robosense RS-Bpearl: https://www.roscomponents.com/en/lidar-laser-scanner/267-rs-bpearl.html\\n\\n    Args:\\n        cfg: The configuration instance for the pattern.\\n        device: The device to create the pattern on.\\n\\n    Returns:\\n        The starting positions and directions of the rays.\\n    \"\"\"\\n    h = torch.arange(-cfg.horizontal_fov / 2, cfg.horizontal_fov / 2, cfg.horizontal_res, device=device)\\n    v = torch.tensor(list(cfg.vertical_ray_angles), device=device)\\n\\n    pitch, yaw = torch.meshgrid(v, h, indexing=\"xy\")\\n    pitch, yaw = torch.deg2rad(pitch.reshape(-1)), torch.deg2rad(yaw.reshape(-1))\\n    pitch += torch.pi / 2\\n    x = torch.sin(pitch) * torch.cos(yaw)\\n    y = torch.sin(pitch) * torch.sin(yaw)\\n    z = torch.cos(pitch)\\n\\n    ray_directions = -torch.stack([x, y, z], dim=1)\\n    ray_starts = torch.zeros_like(ray_directions)\\n    return ray_starts, ray_directions'),\n",
       " Document(metadata={}, page_content='def lidar_pattern(cfg: patterns_cfg.LidarPatternCfg, device: str) -> tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"Lidar sensor pattern for ray casting.\\n\\n    Args:\\n        cfg: The configuration instance for the pattern.\\n        device: The device to create the pattern on.\\n\\n    Returns:\\n        The starting positions and directions of the rays.\\n    \"\"\"\\n    # Vertical angles\\n    vertical_angles = torch.linspace(cfg.vertical_fov_range[0], cfg.vertical_fov_range[1], cfg.channels)\\n\\n    # If the horizontal field of view is 360 degrees, exclude the last point to avoid overlap\\n    if abs(abs(cfg.horizontal_fov_range[0] - cfg.horizontal_fov_range[1]) - 360.0) < 1e-6:\\n        up_to = -1\\n    else:\\n        up_to = None\\n\\n    # Horizontal angles\\n    num_horizontal_angles = math.ceil((cfg.horizontal_fov_range[1] - cfg.horizontal_fov_range[0]) / cfg.horizontal_res)\\n    horizontal_angles = torch.linspace(cfg.horizontal_fov_range[0], cfg.horizontal_fov_range[1], num_horizontal_angles)[\\n        :up_to\\n    ]\\n\\n    # Convert degrees to radians\\n    vertical_angles_rad = torch.deg2rad(vertical_angles)\\n    horizontal_angles_rad = torch.deg2rad(horizontal_angles)\\n\\n    # Meshgrid to create a 2D array of angles\\n    v_angles, h_angles = torch.meshgrid(vertical_angles_rad, horizontal_angles_rad, indexing=\"ij\")\\n\\n    # Spherical to Cartesian conversion (assuming Z is up)\\n    x = torch.cos(v_angles) * torch.cos(h_angles)\\n    y = torch.cos(v_angles) * torch.sin(h_angles)\\n    z = torch.sin(v_angles)\\n\\n    # Ray directions\\n    ray_directions = torch.stack([x, y, z], dim=-1).reshape(-1, 3).to(device)\\n\\n    # Ray starts: Assuming all rays originate from (0,0,0)\\n    ray_starts = torch.zeros_like(ray_directions).to(device)\\n\\n    return ray_starts, ray_directions'),\n",
       " Document(metadata={}, page_content='class PatternBaseCfg:\\n    \"\"\"Base configuration for a pattern.\"\"\"\\n\\n    func: Callable[[PatternBaseCfg, str], tuple[torch.Tensor, torch.Tensor]] = MISSING\\n    \"\"\"Function to generate the pattern.\\n\\n    The function should take in the configuration and the device name as arguments. It should return\\n    the pattern\\'s starting positions and directions as a tuple of torch.Tensor.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class GridPatternCfg(PatternBaseCfg):\\n    \"\"\"Configuration for the grid pattern for ray-casting.\\n\\n    Defines a 2D grid of rays in the coordinates of the sensor.\\n\\n    .. attention::\\n        The points are ordered based on the :attr:`ordering` attribute.\\n\\n    \"\"\"\\n\\n    func: Callable = patterns.grid_pattern\\n\\n    resolution: float = MISSING\\n    \"\"\"Grid resolution (in meters).\"\"\"\\n\\n    size: tuple[float, float] = MISSING\\n    \"\"\"Grid size (length, width) (in meters).\"\"\"\\n\\n    direction: tuple[float, float, float] = (0.0, 0.0, -1.0)\\n    \"\"\"Ray direction. Defaults to (0.0, 0.0, -1.0).\"\"\"\\n\\n    ordering: Literal[\"xy\", \"yx\"] = \"xy\"\\n    \"\"\"Specifies the ordering of points in the generated grid. Defaults to ``\"xy\"``.\\n\\n    Consider a grid pattern with points at :math:`(x, y)` where :math:`x` and :math:`y` are the grid indices.\\n    The ordering of the points can be specified as \"xy\" or \"yx\". This determines the inner and outer loop order\\n    when iterating over the grid points.\\n\\n    * If \"xy\" is selected, the points are ordered with inner loop over \"x\" and outer loop over \"y\".\\n    * If \"yx\" is selected, the points are ordered with inner loop over \"y\" and outer loop over \"x\".\\n\\n    For example, the grid pattern points with :math:`X = (0, 1, 2)` and :math:`Y = (3, 4)`:\\n\\n    * \"xy\" ordering: :math:`[(0, 3), (1, 3), (2, 3), (1, 4), (2, 4), (2, 4)]`\\n    * \"yx\" ordering: :math:`[(0, 3), (0, 4), (1, 3), (1, 4), (2, 3), (2, 4)]`\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class PinholeCameraPatternCfg(PatternBaseCfg):\\n    \"\"\"Configuration for a pinhole camera depth image pattern for ray-casting.\\n\\n    .. caution::\\n        Focal length as well as the aperture sizes and offsets are set as a tenth of the world unit. In our case, the\\n        world unit is meters, so all of these values are in cm. For more information, please check:\\n        https://docs.omniverse.nvidia.com/materials-and-rendering/latest/cameras.html\\n    \"\"\"\\n\\n    func: Callable = patterns.pinhole_camera_pattern\\n\\n    focal_length: float = 24.0\\n    \"\"\"Perspective focal length (in cm). Defaults to 24.0cm.\\n\\n    Longer lens lengths narrower FOV, shorter lens lengths wider FOV.\\n    \"\"\"\\n\\n    horizontal_aperture: float = 20.955\\n    \"\"\"Horizontal aperture (in cm). Defaults to 20.955 cm.\\n\\n    Emulates sensor/film width on a camera.\\n\\n    Note:\\n        The default value is the horizontal aperture of a 35 mm spherical projector.\\n    \"\"\"\\n    vertical_aperture: float | None = None\\n    r\"\"\"Vertical aperture (in cm). Defaults to None.\\n\\n    Emulates sensor/film height on a camera. If None, then the vertical aperture is calculated based on the\\n    horizontal aperture and the aspect ratio of the image to maintain squared pixels. In this case, the vertical\\n    aperture is calculated as:\\n\\n    .. math::\\n        \\\\text{vertical aperture} = \\\\text{horizontal aperture} \\\\times \\\\frac{\\\\text{height}}{\\\\text{width}}\\n    \"\"\"\\n\\n    horizontal_aperture_offset: float = 0.0\\n    \"\"\"Offsets Resolution/Film gate horizontally. Defaults to 0.0.\"\"\"\\n\\n    vertical_aperture_offset: float = 0.0\\n    \"\"\"Offsets Resolution/Film gate vertically. Defaults to 0.0.\"\"\"\\n\\n    width: int = MISSING\\n    \"\"\"Width of the image (in pixels).\"\"\"\\n\\n    height: int = MISSING\\n    \"\"\"Height of the image (in pixels).\"\"\"\\n\\n    @classmethod\\n    def from_intrinsic_matrix(\\n        cls,\\n        intrinsic_matrix: list[float],\\n        width: int,\\n        height: int,\\n        focal_length: float = 24.0,\\n    ) -> PinholeCameraPatternCfg:\\n        r\"\"\"Create a :class:`PinholeCameraPatternCfg` class instance from an intrinsic matrix.\\n\\n        The intrinsic matrix is a 3x3 matrix that defines the mapping between the 3D world coordinates and\\n        the 2D image. The matrix is defined as:\\n\\n        .. math::\\n            I_{cam} = \\\\begin{bmatrix}\\n            f_x & 0 & c_x \\\\\\\\\\n            0 & f_y & c_y \\\\\\\\\\n            0 & 0 & 1\\n            \\\\end{bmatrix},\\n\\n        where :math:`f_x` and :math:`f_y` are the focal length along x and y direction, while :math:`c_x` and :math:`c_y` are the\\n        principle point offsets along x and y direction respectively.\\n\\n        Args:\\n            intrinsic_matrix: Intrinsic matrix of the camera in row-major format.\\n                The matrix is defined as [f_x, 0, c_x, 0, f_y, c_y, 0, 0, 1]. Shape is (9,).\\n            width: Width of the image (in pixels).\\n            height: Height of the image (in pixels).\\n            focal_length: Focal length of the camera (in cm). Defaults to 24.0 cm.\\n\\n        Returns:\\n            An instance of the :class:`PinholeCameraPatternCfg` class.\\n        \"\"\"\\n        # extract parameters from matrix\\n        f_x = intrinsic_matrix[0]\\n        c_x = intrinsic_matrix[2]\\n        f_y = intrinsic_matrix[4]\\n        c_y = intrinsic_matrix[5]\\n        # resolve parameters for usd camera\\n        horizontal_aperture = width * focal_length / f_x\\n        vertical_aperture = height * focal_length / f_y\\n        horizontal_aperture_offset = (c_x - width / 2) / f_x\\n        vertical_aperture_offset = (c_y - height / 2) / f_y\\n\\n        return cls(\\n            focal_length=focal_length,\\n            horizontal_aperture=horizontal_aperture,\\n            vertical_aperture=vertical_aperture,\\n            horizontal_aperture_offset=horizontal_aperture_offset,\\n            vertical_aperture_offset=vertical_aperture_offset,\\n            width=width,\\n            height=height,\\n        )'),\n",
       " Document(metadata={}, page_content='class BpearlPatternCfg(PatternBaseCfg):\\n    \"\"\"Configuration for the Bpearl pattern for ray-casting.\"\"\"\\n\\n    func: Callable = patterns.bpearl_pattern\\n\\n    horizontal_fov: float = 360.0\\n    \"\"\"Horizontal field of view (in degrees). Defaults to 360.0.\"\"\"\\n\\n    horizontal_res: float = 10.0\\n    \"\"\"Horizontal resolution (in degrees). Defaults to 10.0.\"\"\"\\n\\n    # fmt: off\\n    vertical_ray_angles: Sequence[float] = [\\n        89.5, 86.6875, 83.875, 81.0625, 78.25, 75.4375, 72.625, 69.8125, 67.0, 64.1875, 61.375,\\n        58.5625, 55.75, 52.9375, 50.125, 47.3125, 44.5, 41.6875, 38.875, 36.0625, 33.25, 30.4375,\\n        27.625, 24.8125, 22, 19.1875, 16.375, 13.5625, 10.75, 7.9375, 5.125, 2.3125\\n    ]\\n    # fmt: on\\n    \"\"\"Vertical ray angles (in degrees). Defaults to a list of 32 angles.\\n\\n    Note:\\n        We manually set the vertical ray angles to match the Bpearl sensor. The ray-angles\\n        are not evenly spaced.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class LidarPatternCfg(PatternBaseCfg):\\n    \"\"\"Configuration for the LiDAR pattern for ray-casting.\"\"\"\\n\\n    func: Callable = patterns.lidar_pattern\\n\\n    channels: int = MISSING\\n    \"\"\"Number of Channels (Beams). Determines the vertical resolution of the LiDAR sensor.\"\"\"\\n\\n    vertical_fov_range: tuple[float, float] = MISSING\\n    \"\"\"Vertical field of view range in degrees.\"\"\"\\n\\n    horizontal_fov_range: tuple[float, float] = MISSING\\n    \"\"\"Horizontal field of view range in degrees.\"\"\"\\n\\n    horizontal_res: float = MISSING\\n    \"\"\"Horizontal resolution (in degrees).\"\"\"'),\n",
       " Document(metadata={}, page_content='class PhysxCfg:\\n    \"\"\"Configuration for PhysX solver-related parameters.\\n\\n    These parameters are used to configure the PhysX solver. For more information, see the `PhysX 5 SDK\\n    documentation`_.\\n\\n    PhysX 5 supports GPU-accelerated physics simulation. This is enabled by default, but can be disabled\\n    by setting the :attr:`~SimulationCfg.device` to ``cpu`` in :class:`SimulationCfg`. Unlike CPU PhysX, the GPU\\n    simulation feature is unable to dynamically grow all the buffers. Therefore, it is necessary to provide\\n    a reasonable estimate of the buffer sizes for GPU features. If insufficient buffer sizes are provided, the\\n    simulation will fail with errors and lead to adverse behaviors. The buffer sizes can be adjusted through the\\n    ``gpu_*`` parameters.\\n\\n    .. _PhysX 5 SDK documentation: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/_api_build/classPxSceneDesc.html\\n\\n    \"\"\"\\n\\n    solver_type: Literal[0, 1] = 1\\n    \"\"\"The type of solver to use.Default is 1 (TGS).\\n\\n    Available solvers:\\n\\n    * :obj:`0`: PGS (Projective Gauss-Seidel)\\n    * :obj:`1`: TGS (Truncated Gauss-Seidel)\\n    \"\"\"\\n\\n    min_position_iteration_count: int = 1\\n    \"\"\"Minimum number of solver position iterations (rigid bodies, cloth, particles etc.). Default is 1.\\n\\n    .. note::\\n\\n        Each physics actor in Omniverse specifies its own solver iteration count. The solver takes\\n        the number of iterations specified by the actor with the highest iteration and clamps it to\\n        the range ``[min_position_iteration_count, max_position_iteration_count]``.\\n    \"\"\"\\n\\n    max_position_iteration_count: int = 255\\n    \"\"\"Maximum number of solver position iterations (rigid bodies, cloth, particles etc.). Default is 255.\\n\\n    .. note::\\n\\n        Each physics actor in Omniverse specifies its own solver iteration count. The solver takes\\n        the number of iterations specified by the actor with the highest iteration and clamps it to\\n        the range ``[min_position_iteration_count, max_position_iteration_count]``.\\n    \"\"\"\\n\\n    min_velocity_iteration_count: int = 0\\n    \"\"\"Minimum number of solver velocity iterations (rigid bodies, cloth, particles etc.). Default is 0.\\n\\n    .. note::\\n\\n        Each physics actor in Omniverse specifies its own solver iteration count. The solver takes\\n        the number of iterations specified by the actor with the highest iteration and clamps it to\\n        the range ``[min_velocity_iteration_count, max_velocity_iteration_count]``.\\n    \"\"\"\\n\\n    max_velocity_iteration_count: int = 255\\n    \"\"\"Maximum number of solver velocity iterations (rigid bodies, cloth, particles etc.). Default is 255.\\n\\n    .. note::\\n\\n        Each physics actor in Omniverse specifies its own solver iteration count. The solver takes\\n        the number of iterations specified by the actor with the highest iteration and clamps it to\\n        the range ``[min_velocity_iteration_count, max_velocity_iteration_count]``.\\n    \"\"\"\\n\\n    enable_ccd: bool = False\\n    \"\"\"Enable a second broad-phase pass that makes it possible to prevent objects from tunneling through each other.\\n    Default is False.\"\"\"\\n\\n    enable_stabilization: bool = True\\n    \"\"\"Enable/disable additional stabilization pass in solver. Default is True.\"\"\"\\n\\n    enable_enhanced_determinism: bool = False\\n    \"\"\"Enable/disable improved determinism at the expense of performance. Defaults to False.\\n\\n    For more information on PhysX determinism, please check `here`_.\\n\\n    .. _here: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/RigidBodyDynamics.html#enhanced-determinism\\n    \"\"\"\\n\\n    bounce_threshold_velocity: float = 0.5\\n    \"\"\"Relative velocity threshold for contacts to bounce (in m/s). Default is 0.5 m/s.\"\"\"\\n\\n    friction_offset_threshold: float = 0.04\\n    \"\"\"Threshold for contact point to experience friction force (in m). Default is 0.04 m.\"\"\"\\n\\n    friction_correlation_distance: float = 0.025\\n    \"\"\"Distance threshold for merging contacts into a single friction anchor point (in m). Default is 0.025 m.\"\"\"\\n\\n    gpu_max_rigid_contact_count: int = 2**23\\n    \"\"\"Size of rigid contact stream buffer allocated in pinned host memory. Default is 2 ** 23.\"\"\"\\n\\n    gpu_max_rigid_patch_count: int = 5 * 2**15\\n    \"\"\"Size of the rigid contact patch stream buffer allocated in pinned host memory. Default is 5 * 2 ** 15.\"\"\"\\n\\n    gpu_found_lost_pairs_capacity: int = 2**21\\n    \"\"\"Capacity of found and lost buffers allocated in GPU global memory. Default is 2 ** 21.\\n\\n    This is used for the found/lost pair reports in the BP.\\n    \"\"\"\\n\\n    gpu_found_lost_aggregate_pairs_capacity: int = 2**25\\n    \"\"\"Capacity of found and lost buffers in aggregate system allocated in GPU global memory.\\n    Default is 2 ** 25.\\n\\n    This is used for the found/lost pair reports in AABB manager.\\n    \"\"\"\\n\\n    gpu_total_aggregate_pairs_capacity: int = 2**21\\n    \"\"\"Capacity of total number of aggregate pairs allocated in GPU global memory. Default is 2 ** 21.\"\"\"\\n\\n    gpu_collision_stack_size: int = 2**26\\n    \"\"\"Size of the collision stack buffer allocated in pinned host memory. Default is 2 ** 26.\"\"\"\\n\\n    gpu_heap_capacity: int = 2**26\\n    \"\"\"Initial capacity of the GPU and pinned host memory heaps. Additional memory will be allocated\\n    if more memory is required. Default is 2 ** 26.\"\"\"\\n\\n    gpu_temp_buffer_capacity: int = 2**24\\n    \"\"\"Capacity of temp buffer allocated in pinned host memory. Default is 2 ** 24.\"\"\"\\n\\n    gpu_max_num_partitions: int = 8\\n    \"\"\"Limitation for the partitions in the GPU dynamics pipeline. Default is 8.\\n\\n    This variable must be power of 2. A value greater than 32 is currently not supported. Range: (1, 32)\\n    \"\"\"\\n\\n    gpu_max_soft_body_contacts: int = 2**20\\n    \"\"\"Size of soft body contacts stream buffer allocated in pinned host memory. Default is 2 ** 20.\"\"\"\\n\\n    gpu_max_particle_contacts: int = 2**20\\n    \"\"\"Size of particle contacts stream buffer allocated in pinned host memory. Default is 2 ** 20.\"\"\"'),\n",
       " Document(metadata={}, page_content='class RenderCfg:\\n    \"\"\"Configuration for Omniverse RTX Renderer.\\n\\n    These parameters are used to configure the Omniverse RTX Renderer. The defaults for IsaacLab are set in the\\n    experience files: `apps/isaaclab.python.rendering.kit` and `apps/isaaclab.python.headless.rendering.kit`. Setting any\\n    value here will override the defaults of the experience files.\\n\\n    For more information, see the `Omniverse RTX Renderer documentation`_.\\n\\n    .. _Omniverse RTX Renderer documentation: https://docs.omniverse.nvidia.com/materials-and-rendering/latest/rtx-renderer.html\\n    \"\"\"\\n\\n    enable_translucency: bool | None = None\\n    \"\"\"Enables translucency for specular transmissive surfaces such as glass at the cost of some performance. Default is False.\\n\\n    Set variable: /rtx/translucency/enabled\\n    \"\"\"\\n\\n    enable_reflections: bool | None = None\\n    \"\"\"Enables reflections at the cost of some performance. Default is False.\\n\\n    Set variable: /rtx/reflections/enabled\\n    \"\"\"\\n\\n    enable_global_illumination: bool | None = None\\n    \"\"\"Enables Diffused Global Illumination at the cost of some performance. Default is False.\\n\\n    Set variable: /rtx/indirectDiffuse/enabled\\n    \"\"\"\\n\\n    antialiasing_mode: Literal[\"Off\", \"FXAA\", \"DLSS\", \"TAA\", \"DLAA\"] | None = None\\n    \"\"\"Selects the anti-aliasing mode to use. Defaults to DLSS.\\n       - DLSS: Boosts performance by using AI to output higher resolution frames from a lower resolution input. DLSS samples multiple lower resolution images and uses motion data and feedback from prior frames to reconstruct native quality images.\\n       - DLAA: Provides higher image quality with an AI-based anti-aliasing technique. DLAA uses the same Super Resolution technology developed for DLSS, reconstructing a native resolution image to maximize image quality.\\n\\n    Set variable: /rtx/post/dlss/execMode\\n    \"\"\"\\n\\n    enable_dlssg: bool | None = None\\n    \"\"\"\"Enables the use of DLSS-G.\\n        DLSS Frame Generation boosts performance by using AI to generate more frames.\\n        DLSS analyzes sequential frames and motion data to create additional high quality frames.\\n        This feature requires an Ada Lovelace architecture GPU.\\n        Enabling this feature also enables additional thread-related activities, which can hurt performance.\\n        Default is False.\\n\\n    Set variable: /rtx-transient/dlssg/enabled\\n    \"\"\"\\n\\n    enable_dl_denoiser: bool | None = None\\n    \"\"\"Enables the use of a DL denoiser.\\n       The DL denoiser can help improve the quality of renders, but comes at a cost of performance.\\n\\n    Set variable: /rtx-transient/dldenoiser/enabled\\n    \"\"\"\\n\\n    dlss_mode: Literal[0, 1, 2, 3] | None = None\\n    \"\"\"For DLSS anti-aliasing, selects the performance/quality tradeoff mode.\\n       Valid values are 0 (Performance), 1 (Balanced), 2 (Quality), or 3 (Auto). Default is 0.\\n\\n    Set variable: /rtx/post/dlss/execMode\\n    \"\"\"\\n\\n    enable_direct_lighting: bool | None = None\\n    \"\"\"Enable direct light contributions from lights.\\n\\n    Set variable: /rtx/directLighting/enabled\\n    \"\"\"\\n\\n    samples_per_pixel: int | None = None\\n    \"\"\"Defines the Direct Lighting samples per pixel.\\n       Higher values increase the direct lighting quality at the cost of performance. Default is 1.\\n\\n    Set variable: /rtx/directLighting/sampledLighting/samplesPerPixel\"\"\"\\n\\n    enable_shadows: bool | None = None\\n    \"\"\"Enables shadows at the cost of performance. When disabled, lights will not cast shadows. Defaults to True.\\n\\n    Set variable: /rtx/shadows/enabled\\n    \"\"\"\\n\\n    enable_ambient_occlusion: bool | None = None\\n    \"\"\"Enables ambient occlusion at the cost of some performance. Default is False.\\n\\n    Set variable: /rtx/ambientOcclusion/enabled\\n    \"\"\"\\n\\n    carb_settings: dict | None = None\\n    \"\"\"Provides a general dictionary for users to supply all carb rendering settings with native names.\\n        - Name strings can be formatted like a carb setting, .kit file setting, or python variable.\\n        - For instance, a key value pair can be\\n            /rtx/translucency/enabled: False # carb\\n             rtx.translucency.enabled: False # .kit\\n             rtx_translucency_enabled: False # python\"\"\"\\n\\n    rendering_mode: Literal[\"performance\", \"balanced\", \"quality\", \"xr\"] | None = None\\n    \"\"\"Sets the rendering mode. Behaves the same as the CLI arg \\'--rendering_mode\\'\"\"\"'),\n",
       " Document(metadata={}, page_content='class SimulationCfg:\\n    \"\"\"Configuration for simulation physics.\"\"\"\\n\\n    physics_prim_path: str = \"/physicsScene\"\\n    \"\"\"The prim path where the USD PhysicsScene is created. Default is \"/physicsScene\".\"\"\"\\n\\n    device: str = \"cuda:0\"\\n    \"\"\"The device to run the simulation on. Default is ``\"cuda:0\"``.\\n\\n    Valid options are:\\n\\n    - ``\"cpu\"``: Use CPU.\\n    - ``\"cuda\"``: Use GPU, where the device ID is inferred from :class:`~isaaclab.app.AppLauncher`\\'s config.\\n    - ``\"cuda:N\"``: Use GPU, where N is the device ID. For example, \"cuda:0\".\\n    \"\"\"\\n\\n    dt: float = 1.0 / 60.0\\n    \"\"\"The physics simulation time-step (in seconds). Default is 0.0167 seconds.\"\"\"\\n\\n    render_interval: int = 1\\n    \"\"\"The number of physics simulation steps per rendering step. Default is 1.\"\"\"\\n\\n    gravity: tuple[float, float, float] = (0.0, 0.0, -9.81)\\n    \"\"\"The gravity vector (in m/s^2). Default is (0.0, 0.0, -9.81).\\n\\n    If set to (0.0, 0.0, 0.0), gravity is disabled.\\n    \"\"\"\\n\\n    enable_scene_query_support: bool = False\\n    \"\"\"Enable/disable scene query support for collision shapes. Default is False.\\n\\n    This flag allows performing collision queries (raycasts, sweeps, and overlaps) on actors and\\n    attached shapes in the scene. This is useful for implementing custom collision detection logic\\n    outside of the physics engine.\\n\\n    If set to False, the physics engine does not create the scene query manager and the scene query\\n    functionality will not be available. However, this provides some performance speed-up.\\n\\n    Note:\\n        This flag is overridden to True inside the :class:`SimulationContext` class when running the simulation\\n        with the GUI enabled. This is to allow certain GUI features to work properly.\\n    \"\"\"\\n\\n    use_fabric: bool = True\\n    \"\"\"Enable/disable reading of physics buffers directly. Default is True.\\n\\n    When running the simulation, updates in the states in the scene is normally synchronized with USD.\\n    This leads to an overhead in reading the data and does not scale well with massive parallelization.\\n    This flag allows disabling the synchronization and reading the data directly from the physics buffers.\\n\\n    It is recommended to set this flag to :obj:`True` when running the simulation with a large number\\n    of primitives in the scene.\\n\\n    Note:\\n        When enabled, the GUI will not update the physics parameters in real-time. To enable real-time\\n        updates, please set this flag to :obj:`False`.\\n    \"\"\"\\n\\n    physx: PhysxCfg = PhysxCfg()\\n    \"\"\"PhysX solver settings. Default is PhysxCfg().\"\"\"\\n\\n    physics_material: RigidBodyMaterialCfg = RigidBodyMaterialCfg()\\n    \"\"\"Default physics material settings for rigid bodies. Default is RigidBodyMaterialCfg().\\n\\n    The physics engine defaults to this physics material for all the rigid body prims that do not have any\\n    physics material specified on them.\\n\\n    The material is created at the path: ``{physics_prim_path}/defaultMaterial``.\\n    \"\"\"\\n\\n    render: RenderCfg = RenderCfg()\\n    \"\"\"Render settings. Default is RenderCfg().\"\"\"'),\n",
       " Document(metadata={}, page_content='class SimulationContext(_SimulationContext):\\n    \"\"\"A class to control simulation-related events such as physics stepping and rendering.\\n\\n    The simulation context helps control various simulation aspects. This includes:\\n\\n    * configure the simulator with different settings such as the physics time-step, the number of physics substeps,\\n      and the physics solver parameters (for more information, see :class:`isaaclab.sim.SimulationCfg`)\\n    * playing, pausing, stepping and stopping the simulation\\n    * adding and removing callbacks to different simulation events such as physics stepping, rendering, etc.\\n\\n    This class inherits from the :class:`isaacsim.core.api.simulation_context.SimulationContext` class and\\n    adds additional functionalities such as setting up the simulation context with a configuration object,\\n    exposing other commonly used simulator-related functions, and performing version checks of Isaac Sim\\n    to ensure compatibility between releases.\\n\\n    The simulation context is a singleton object. This means that there can only be one instance\\n    of the simulation context at any given time. This is enforced by the parent class. Therefore, it is\\n    not possible to create multiple instances of the simulation context. Instead, the simulation context\\n    can be accessed using the ``instance()`` method.\\n\\n    .. attention::\\n        Since we only support the `PyTorch <https://pytorch.org/>`_ backend for simulation, the\\n        simulation context is configured to use the ``torch`` backend by default. This means that\\n        all the data structures used in the simulation are ``torch.Tensor`` objects.\\n\\n    The simulation context can be used in two different modes of operations:\\n\\n    1. **Standalone python script**: In this mode, the user has full control over the simulation and\\n       can trigger stepping events synchronously (i.e. as a blocking call). In this case the user\\n       has to manually call :meth:`step` step the physics simulation and :meth:`render` to\\n       render the scene.\\n    2. **Omniverse extension**: In this mode, the user has limited control over the simulation stepping\\n       and all the simulation events are triggered asynchronously (i.e. as a non-blocking call). In this\\n       case, the user can only trigger the simulation to start, pause, and stop. The simulation takes\\n       care of stepping the physics simulation and rendering the scene.\\n\\n    Based on above, for most functions in this class there is an equivalent function that is suffixed\\n    with ``_async``. The ``_async`` functions are used in the Omniverse extension mode and\\n    the non-``_async`` functions are used in the standalone python script mode.\\n    \"\"\"\\n\\n    class RenderMode(enum.IntEnum):\\n        \"\"\"Different rendering modes for the simulation.\\n\\n        Render modes correspond to how the viewport and other UI elements (such as listeners to keyboard or mouse\\n        events) are updated. There are three main components that can be updated when the simulation is rendered:\\n\\n        1. **UI elements and other extensions**: These are UI elements (such as buttons, sliders, etc.) and other\\n           extensions that are running in the background that need to be updated when the simulation is running.\\n        2. **Cameras**: These are typically based on Hydra textures and are used to render the scene from different\\n           viewpoints. They can be attached to a viewport or be used independently to render the scene.\\n        3. **Viewports**: These are windows where you can see the rendered scene.\\n\\n        Updating each of the above components has a different overhead. For example, updating the viewports is\\n        computationally expensive compared to updating the UI elements. Therefore, it is useful to be able to\\n        control what is updated when the simulation is rendered. This is where the render mode comes in. There are\\n        four different render modes:\\n\\n        * :attr:`NO_GUI_OR_RENDERING`: The simulation is running without a GUI and off-screen rendering flag is disabled,\\n          so none of the above are updated.\\n        * :attr:`NO_RENDERING`: No rendering, where only 1 is updated at a lower rate.\\n        * :attr:`PARTIAL_RENDERING`: Partial rendering, where only 1 and 2 are updated.\\n        * :attr:`FULL_RENDERING`: Full rendering, where everything (1, 2, 3) is updated.\\n\\n        .. _Viewports: https://docs.omniverse.nvidia.com/extensions/latest/ext_viewport.html\\n        \"\"\"\\n\\n        NO_GUI_OR_RENDERING = -1\\n        \"\"\"The simulation is running without a GUI and off-screen rendering is disabled.\"\"\"\\n        NO_RENDERING = 0\\n        \"\"\"No rendering, where only other UI elements are updated at a lower rate.\"\"\"\\n        PARTIAL_RENDERING = 1\\n        \"\"\"Partial rendering, where the simulation cameras and UI elements are updated.\"\"\"\\n        FULL_RENDERING = 2\\n        \"\"\"Full rendering, where all the simulation viewports, cameras and UI elements are updated.\"\"\"\\n\\n    def __init__(self, cfg: SimulationCfg | None = None):\\n        \"\"\"Creates a simulation context to control the simulator.\\n\\n        Args:\\n            cfg: The configuration of the simulation. Defaults to None,\\n                in which case the default configuration is used.\\n        \"\"\"\\n        # store input\\n        if cfg is None:\\n            cfg = SimulationCfg()\\n        # check that the config is valid\\n        cfg.validate()\\n        self.cfg = cfg\\n        # check that simulation is running\\n        if stage_utils.get_current_stage() is None:\\n            raise RuntimeError(\"The stage has not been created. Did you run the simulator?\")\\n\\n        # acquire settings interface\\n        self.carb_settings = carb.settings.get_settings()\\n\\n        # apply carb physics settings\\n        self._apply_physics_settings()\\n\\n        # note: we read this once since it is not expected to change during runtime\\n        # read flag for whether a local GUI is enabled\\n        self._local_gui = self.carb_settings.get(\"/app/window/enabled\")\\n        # read flag for whether livestreaming GUI is enabled\\n        self._livestream_gui = self.carb_settings.get(\"/app/livestream/enabled\")\\n        # read flag for whether XR GUI is enabled\\n        self._xr_gui = self.carb_settings.get(\"/app/xr/enabled\")\\n\\n        # read flag for whether the Isaac Lab viewport capture pipeline will be used,\\n        # casting None to False if the flag doesn\\'t exist\\n        # this flag is set from the AppLauncher class\\n        self._offscreen_render = bool(self.carb_settings.get(\"/isaaclab/render/offscreen\"))\\n        # read flag for whether the default viewport should be enabled\\n        self._render_viewport = bool(self.carb_settings.get(\"/isaaclab/render/active_viewport\"))\\n        # flag for whether any GUI will be rendered (local, livestreamed or viewport)\\n        self._has_gui = self._local_gui or self._livestream_gui or self._xr_gui\\n\\n        # apply render settings from render config\\n        self._apply_render_settings_from_cfg()\\n\\n        # store the default render mode\\n        if not self._has_gui and not self._offscreen_render:\\n            # set default render mode\\n            # note: this is the terminal state: cannot exit from this render mode\\n            self.render_mode = self.RenderMode.NO_GUI_OR_RENDERING\\n            # set viewport context to None\\n            self._viewport_context = None\\n            self._viewport_window = None\\n        elif not self._has_gui and self._offscreen_render:\\n            # set default render mode\\n            # note: this is the terminal state: cannot exit from this render mode\\n            self.render_mode = self.RenderMode.PARTIAL_RENDERING\\n            # set viewport context to None\\n            self._viewport_context = None\\n            self._viewport_window = None\\n        else:\\n            # note: need to import here in case the UI is not available (ex. headless mode)\\n            import omni.ui as ui\\n            from omni.kit.viewport.utility import get_active_viewport\\n\\n            # set default render mode\\n            # note: this can be changed by calling the `set_render_mode` function\\n            self.render_mode = self.RenderMode.FULL_RENDERING\\n            # acquire viewport context\\n            self._viewport_context = get_active_viewport()\\n            self._viewport_context.updates_enabled = True  # pyright: ignore [reportOptionalMemberAccess]\\n            # acquire viewport window\\n            # TODO @mayank: Why not just use get_active_viewport_and_window() directly?\\n            self._viewport_window = ui.Workspace.get_window(\"Viewport\")\\n            # counter for periodic rendering\\n            self._render_throttle_counter = 0\\n            # rendering frequency in terms of number of render calls\\n            self._render_throttle_period = 5\\n\\n        # check the case where we don\\'t need to render the viewport\\n        # since render_viewport can only be False in headless mode, we only need to check for offscreen_render\\n        if not self._render_viewport and self._offscreen_render:\\n            # disable the viewport if offscreen_render is enabled\\n            from omni.kit.viewport.utility import get_active_viewport\\n\\n            get_active_viewport().updates_enabled = False\\n\\n        # override enable scene querying if rendering is enabled\\n        # this is needed for some GUI features\\n        if self._has_gui:\\n            self.cfg.enable_scene_query_support = True\\n        # set up flatcache/fabric interface (default is None)\\n        # this is needed to flush the flatcache data into Hydra manually when calling `render()`\\n        # ref: https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_physics.html\\n        # note: need to do this here because super().__init__ calls render and this variable is needed\\n        self._fabric_iface = None\\n        # read isaac sim version (this includes build tag, release tag etc.)\\n        # note: we do it once here because it reads the VERSION file from disk and is not expected to change.\\n        self._isaacsim_version = get_version()\\n\\n        # create a tensor for gravity\\n        # note: this line is needed to create a \"tensor\" in the device to avoid issues with torch 2.1 onwards.\\n        #   the issue is with some heap memory corruption when torch tensor is created inside the asset class.\\n        #   you can reproduce the issue by commenting out this line and running the test `test_articulation.py`.\\n        self._gravity_tensor = torch.tensor(self.cfg.gravity, dtype=torch.float32, device=self.cfg.device)\\n\\n        # define a global variable to store the exceptions raised in the callback stack\\n        builtins.ISAACLAB_CALLBACK_EXCEPTION = None\\n\\n        # add callback to deal the simulation app when simulation is stopped.\\n        # this is needed because physics views go invalid once we stop the simulation\\n        if not builtins.ISAAC_LAUNCHED_FROM_TERMINAL:\\n            timeline_event_stream = omni.timeline.get_timeline_interface().get_timeline_event_stream()\\n            self._app_control_on_stop_handle = timeline_event_stream.create_subscription_to_pop_by_type(\\n                int(omni.timeline.TimelineEventType.STOP),\\n                lambda *args, obj=weakref.proxy(self): obj._app_control_on_stop_handle_fn(*args),\\n                order=15,\\n            )\\n        else:\\n            self._app_control_on_stop_handle = None\\n        self._disable_app_control_on_stop_handle = False\\n\\n        # flatten out the simulation dictionary\\n        sim_params = self.cfg.to_dict()\\n        if sim_params is not None:\\n            if \"physx\" in sim_params:\\n                physx_params = sim_params.pop(\"physx\")\\n                sim_params.update(physx_params)\\n        # create a simulation context to control the simulator\\n        super().__init__(\\n            stage_units_in_meters=1.0,\\n            physics_dt=self.cfg.dt,\\n            rendering_dt=self.cfg.dt * self.cfg.render_interval,\\n            backend=\"torch\",\\n            sim_params=sim_params,\\n            physics_prim_path=self.cfg.physics_prim_path,\\n            device=self.cfg.device,\\n        )\\n\\n    def _apply_physics_settings(self):\\n        \"\"\"Sets various carb physics settings.\"\"\"\\n        # enable hydra scene-graph instancing\\n        # note: this allows rendering of instanceable assets on the GUI\\n        set_carb_setting(self.carb_settings, \"/persistent/omnihydra/useSceneGraphInstancing\", True)\\n        # change dispatcher to use the default dispatcher in PhysX SDK instead of carb tasking\\n        # note: dispatcher handles how threads are launched for multi-threaded physics\\n        set_carb_setting(self.carb_settings, \"/physics/physxDispatcher\", True)\\n        # disable contact processing in omni.physx\\n        # note: we disable it by default to avoid the overhead of contact processing when it isn\\'t needed.\\n        #   The physics flag gets enabled when a contact sensor is created.\\n        if hasattr(self.cfg, \"disable_contact_processing\"):\\n            omni.log.warn(\\n                \"The `disable_contact_processing` attribute is deprecated and always set to True\"\\n                \" to avoid unnecessary overhead. Contact processing is automatically enabled when\"\\n                \" a contact sensor is created, so manual configuration is no longer required.\"\\n            )\\n        # FIXME: From investigation, it seems this flag only affects CPU physics. For GPU physics, contacts\\n        #  are always processed. The issue is reported to the PhysX team by @mmittal.\\n        set_carb_setting(self.carb_settings, \"/physics/disableContactProcessing\", True)\\n        # disable custom geometry for cylinder and cone collision shapes to allow contact reporting for them\\n        # reason: cylinders and cones aren\\'t natively supported by PhysX so we need to use custom geometry flags\\n        # reference: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/Geometry.html?highlight=capsule#geometry\\n        set_carb_setting(self.carb_settings, \"/physics/collisionConeCustomGeometry\", False)\\n        set_carb_setting(self.carb_settings, \"/physics/collisionCylinderCustomGeometry\", False)\\n        # hide the Simulation Settings window\\n        set_carb_setting(self.carb_settings, \"/physics/autoPopupSimulationOutputWindow\", False)\\n\\n    def _apply_render_settings_from_cfg(self):\\n        \"\"\"Sets rtx settings specified in the RenderCfg.\"\"\"\\n\\n        # define mapping of user-friendly RenderCfg names to native carb names\\n        rendering_setting_name_mapping = {\\n            \"enable_translucency\": \"/rtx/translucency/enabled\",\\n            \"enable_reflections\": \"/rtx/reflections/enabled\",\\n            \"enable_global_illumination\": \"/rtx/indirectDiffuse/enabled\",\\n            \"enable_dlssg\": \"/rtx-transient/dlssg/enabled\",\\n            \"enable_dl_denoiser\": \"/rtx-transient/dldenoiser/enabled\",\\n            \"dlss_mode\": \"/rtx/post/dlss/execMode\",\\n            \"enable_direct_lighting\": \"/rtx/directLighting/enabled\",\\n            \"samples_per_pixel\": \"/rtx/directLighting/sampledLighting/samplesPerPixel\",\\n            \"enable_shadows\": \"/rtx/shadows/enabled\",\\n            \"enable_ambient_occlusion\": \"/rtx/ambientOcclusion/enabled\",\\n        }\\n\\n        not_carb_settings = [\"rendering_mode\", \"carb_settings\", \"antialiasing_mode\"]\\n\\n        # set preset settings (same behavior as the CLI arg --rendering_mode)\\n        rendering_mode = self.cfg.render.rendering_mode\\n        if rendering_mode is not None:\\n            # check if preset is supported\\n            supported_rendering_modes = [\"performance\", \"balanced\", \"quality\", \"xr\"]\\n            if rendering_mode not in supported_rendering_modes:\\n                raise ValueError(\\n                    f\"RenderCfg rendering mode \\'{rendering_mode}\\' not in supported modes {supported_rendering_modes}.\"\\n                )\\n\\n            # parse preset file\\n            repo_path = os.path.join(carb.tokens.get_tokens_interface().resolve(\"${app}\"), \"..\")\\n            preset_filename = os.path.join(repo_path, f\"apps/rendering_modes/{rendering_mode}.kit\")\\n            with open(preset_filename) as file:\\n                preset_dict = toml.load(file)\\n            preset_dict = dict(flatdict.FlatDict(preset_dict, delimiter=\".\"))\\n\\n            # set presets\\n            for key, value in preset_dict.items():\\n                key = \"/\" + key.replace(\".\", \"/\")  # convert to carb setting format\\n                set_carb_setting(self.carb_settings, key, value)\\n\\n        # set user-friendly named settings\\n        for key, value in vars(self.cfg.render).items():\\n            if value is None or key in not_carb_settings:\\n                # skip unset settings and non-carb settings\\n                continue\\n            if key not in rendering_setting_name_mapping:\\n                raise ValueError(\\n                    f\"\\'{key}\\' in RenderCfg not found. Note: internal \\'rendering_setting_name_mapping\\' dictionary might\"\\n                    \" need to be updated.\"\\n                )\\n            key = rendering_setting_name_mapping[key]\\n            set_carb_setting(self.carb_settings, key, value)\\n\\n        # set general carb settings\\n        carb_settings = self.cfg.render.carb_settings\\n        if carb_settings is not None:\\n            for key, value in carb_settings.items():\\n                if \"_\" in key:\\n                    key = \"/\" + key.replace(\"_\", \"/\")  # convert from python variable style string\\n                elif \".\" in key:\\n                    key = \"/\" + key.replace(\".\", \"/\")  # convert from .kit file style string\\n                if get_carb_setting(self.carb_settings, key) is None:\\n                    raise ValueError(f\"\\'{key}\\' in RenderCfg.general_parameters does not map to a carb setting.\")\\n                set_carb_setting(self.carb_settings, key, value)\\n\\n        # set denoiser mode\\n        if self.cfg.render.antialiasing_mode is not None:\\n            try:\\n                import omni.replicator.core as rep\\n\\n                rep.settings.set_render_rtx_realtime(antialiasing=self.cfg.render.antialiasing_mode)\\n            except Exception:\\n                pass\\n\\n        # WAR: Ensure /rtx/renderMode RaytracedLighting is correctly cased.\\n        if get_carb_setting(self.carb_settings, \"/rtx/rendermode\").lower() == \"raytracedlighting\":\\n            set_carb_setting(self.carb_settings, \"/rtx/rendermode\", \"RaytracedLighting\")\\n\\n    \"\"\"\\n    Operations - New.\\n    \"\"\"\\n\\n    def has_gui(self) -> bool:\\n        \"\"\"Returns whether the simulation has a GUI enabled.\\n\\n        True if the simulation has a GUI enabled either locally or live-streamed.\\n        \"\"\"\\n        return self._has_gui\\n\\n    def has_rtx_sensors(self) -> bool:\\n        \"\"\"Returns whether the simulation has any RTX-rendering related sensors.\\n\\n        This function returns the value of the simulation parameter ``\"/isaaclab/render/rtx_sensors\"``.\\n        The parameter is set to True when instances of RTX-related sensors (cameras or LiDARs) are\\n        created using Isaac Lab\\'s sensor classes.\\n\\n        True if the simulation has RTX sensors (such as USD Cameras or LiDARs).\\n\\n        For more information, please check `NVIDIA RTX documentation`_.\\n\\n        .. _NVIDIA RTX documentation: https://developer.nvidia.com/rendering-technologies\\n        \"\"\"\\n        return self._settings.get_as_bool(\"/isaaclab/render/rtx_sensors\")\\n\\n    def is_fabric_enabled(self) -> bool:\\n        \"\"\"Returns whether the fabric interface is enabled.\\n\\n        When fabric interface is enabled, USD read/write operations are disabled. Instead all applications\\n        read and write the simulation state directly from the fabric interface. This reduces a lot of overhead\\n        that occurs during USD read/write operations.\\n\\n        For more information, please check `Fabric documentation`_.\\n\\n        .. _Fabric documentation: https://docs.omniverse.nvidia.com/kit/docs/usdrt/latest/docs/usd_fabric_usdrt.html\\n        \"\"\"\\n        return self._fabric_iface is not None\\n\\n    def get_version(self) -> tuple[int, int, int]:\\n        \"\"\"Returns the version of the simulator.\\n\\n        This is a wrapper around the ``isaacsim.core.version.get_version()`` function.\\n\\n        The returned tuple contains the following information:\\n\\n        * Major version (int): This is the year of the release (e.g. 2022).\\n        * Minor version (int): This is the half-year of the release (e.g. 1 or 2).\\n        * Patch version (int): This is the patch number of the release (e.g. 0).\\n        \"\"\"\\n        return int(self._isaacsim_version[2]), int(self._isaacsim_version[3]), int(self._isaacsim_version[4])\\n\\n    \"\"\"\\n    Operations - New utilities.\\n    \"\"\"\\n\\n    def set_camera_view(\\n        self,\\n        eye: tuple[float, float, float],\\n        target: tuple[float, float, float],\\n        camera_prim_path: str = \"/OmniverseKit_Persp\",\\n    ):\\n        \"\"\"Set the location and target of the viewport camera in the stage.\\n\\n        Note:\\n            This is a wrapper around the :math:`isaacsim.core.utils.viewports.set_camera_view` function.\\n            It is provided here for convenience to reduce the amount of imports needed.\\n\\n        Args:\\n            eye: The location of the camera eye.\\n            target: The location of the camera target.\\n            camera_prim_path: The path to the camera primitive in the stage. Defaults to\\n                \"/OmniverseKit_Persp\".\\n        \"\"\"\\n        # safe call only if we have a GUI or viewport rendering enabled\\n        if self._has_gui or self._offscreen_render or self._render_viewport:\\n            set_camera_view(eye, target, camera_prim_path)\\n\\n    def set_render_mode(self, mode: RenderMode):\\n        \"\"\"Change the current render mode of the simulation.\\n\\n        Please see :class:`RenderMode` for more information on the different render modes.\\n\\n        .. note::\\n            When no GUI is available (locally or livestreamed), we do not need to choose whether the viewport\\n            needs to render or not (since there is no GUI). Thus, in this case, calling the function will not\\n            change the render mode.\\n\\n        Args:\\n            mode (RenderMode): The rendering mode. If different than SimulationContext\\'s rendering mode,\\n            SimulationContext\\'s mode is changed to the new mode.\\n\\n        Raises:\\n            ValueError: If the input mode is not supported.\\n        \"\"\"\\n        # check if mode change is possible -- not possible when no GUI is available\\n        if not self._has_gui:\\n            omni.log.warn(\\n                f\"Cannot change render mode when GUI is disabled. Using the default render mode: {self.render_mode}.\"\\n            )\\n            return\\n        # check if there is a mode change\\n        # note: this is mostly needed for GUI when we want to switch between full rendering and no rendering.\\n        if mode != self.render_mode:\\n            if mode == self.RenderMode.FULL_RENDERING:\\n                # display the viewport and enable updates\\n                self._viewport_context.updates_enabled = True  # pyright: ignore [reportOptionalMemberAccess]\\n                self._viewport_window.visible = True  # pyright: ignore [reportOptionalMemberAccess]\\n            elif mode == self.RenderMode.PARTIAL_RENDERING:\\n                # hide the viewport and disable updates\\n                self._viewport_context.updates_enabled = False  # pyright: ignore [reportOptionalMemberAccess]\\n                self._viewport_window.visible = False  # pyright: ignore [reportOptionalMemberAccess]\\n            elif mode == self.RenderMode.NO_RENDERING:\\n                # hide the viewport and disable updates\\n                if self._viewport_context is not None:\\n                    self._viewport_context.updates_enabled = False  # pyright: ignore [reportOptionalMemberAccess]\\n                    self._viewport_window.visible = False  # pyright: ignore [reportOptionalMemberAccess]\\n                # reset the throttle counter\\n                self._render_throttle_counter = 0\\n            else:\\n                raise ValueError(f\"Unsupported render mode: {mode}! Please check `RenderMode` for details.\")\\n            # update render mode\\n            self.render_mode = mode\\n\\n    def set_setting(self, name: str, value: Any):\\n        \"\"\"Set simulation settings using the Carbonite SDK.\\n\\n        .. note::\\n            If the input setting name does not exist, it will be created. If it does exist, the value will be\\n            overwritten. Please make sure to use the correct setting name.\\n\\n            To understand the settings interface, please refer to the\\n            `Carbonite SDK <https://docs.omniverse.nvidia.com/dev-guide/latest/programmer_ref/settings.html>`_\\n            documentation.\\n\\n        Args:\\n            name: The name of the setting.\\n            value: The value of the setting.\\n        \"\"\"\\n        self._settings.set(name, value)\\n\\n    def get_setting(self, name: str) -> Any:\\n        \"\"\"Read the simulation setting using the Carbonite SDK.\\n\\n        Args:\\n            name: The name of the setting.\\n\\n        Returns:\\n            The value of the setting.\\n        \"\"\"\\n        return self._settings.get(name)\\n\\n    def forward(self) -> None:\\n        \"\"\"Updates articulation kinematics and fabric for rendering.\"\"\"\\n        if self._fabric_iface is not None:\\n            if self.physics_sim_view is not None and self.is_playing():\\n                # Update the articulations\\' link\\'s poses before rendering\\n                self.physics_sim_view.update_articulations_kinematic()\\n            self._update_fabric(0.0, 0.0)\\n\\n    \"\"\"\\n    Operations - Override (standalone)\\n    \"\"\"\\n\\n    def reset(self, soft: bool = False):\\n        self._disable_app_control_on_stop_handle = True\\n        # check if we need to raise an exception that was raised in a callback\\n        if builtins.ISAACLAB_CALLBACK_EXCEPTION is not None:\\n            exception_to_raise = builtins.ISAACLAB_CALLBACK_EXCEPTION\\n            builtins.ISAACLAB_CALLBACK_EXCEPTION = None\\n            raise exception_to_raise\\n        super().reset(soft=soft)\\n        # app.update() may be changing the cuda device in reset, so we force it back to our desired device here\\n        if \"cuda\" in self.device:\\n            torch.cuda.set_device(self.device)\\n        # enable kinematic rendering with fabric\\n        if self.physics_sim_view:\\n            self.physics_sim_view._backend.initialize_kinematic_bodies()\\n        # perform additional rendering steps to warm up replicator buffers\\n        # this is only needed for the first time we set the simulation\\n        if not soft:\\n            for _ in range(2):\\n                self.render()\\n        self._disable_app_control_on_stop_handle = False\\n\\n    def step(self, render: bool = True):\\n        \"\"\"Steps the simulation.\\n\\n        .. note::\\n            This function blocks if the timeline is paused. It only returns when the timeline is playing.\\n\\n        Args:\\n            render: Whether to render the scene after stepping the physics simulation.\\n                    If set to False, the scene is not rendered and only the physics simulation is stepped.\\n        \"\"\"\\n        # check if we need to raise an exception that was raised in a callback\\n        if builtins.ISAACLAB_CALLBACK_EXCEPTION is not None:\\n            exception_to_raise = builtins.ISAACLAB_CALLBACK_EXCEPTION\\n            builtins.ISAACLAB_CALLBACK_EXCEPTION = None\\n            raise exception_to_raise\\n        # check if the simulation timeline is paused. in that case keep stepping until it is playing\\n        if not self.is_playing():\\n            # step the simulator (but not the physics) to have UI still active\\n            while not self.is_playing():\\n                self.render()\\n                # meantime if someone stops, break out of the loop\\n                if self.is_stopped():\\n                    break\\n            # need to do one step to refresh the app\\n            # reason: physics has to parse the scene again and inform other extensions like hydra-delegate.\\n            #   without this the app becomes unresponsive.\\n            # FIXME: This steps physics as well, which we is not good in general.\\n            self.app.update()\\n\\n        # step the simulation\\n        super().step(render=render)\\n\\n        # app.update() may be changing the cuda device in step, so we force it back to our desired device here\\n        if \"cuda\" in self.device:\\n            torch.cuda.set_device(self.device)\\n\\n    def render(self, mode: RenderMode | None = None):\\n        \"\"\"Refreshes the rendering components including UI elements and view-ports depending on the render mode.\\n\\n        This function is used to refresh the rendering components of the simulation. This includes updating the\\n        view-ports, UI elements, and other extensions (besides physics simulation) that are running in the\\n        background. The rendering components are refreshed based on the render mode.\\n\\n        Please see :class:`RenderMode` for more information on the different render modes.\\n\\n        Args:\\n            mode: The rendering mode. Defaults to None, in which case the current rendering mode is used.\\n        \"\"\"\\n        # check if we need to raise an exception that was raised in a callback\\n        if builtins.ISAACLAB_CALLBACK_EXCEPTION is not None:\\n            exception_to_raise = builtins.ISAACLAB_CALLBACK_EXCEPTION\\n            builtins.ISAACLAB_CALLBACK_EXCEPTION = None\\n            raise exception_to_raise\\n        # check if we need to change the render mode\\n        if mode is not None:\\n            self.set_render_mode(mode)\\n        # render based on the render mode\\n        if self.render_mode == self.RenderMode.NO_GUI_OR_RENDERING:\\n            # we never want to render anything here (this is for complete headless mode)\\n            pass\\n        elif self.render_mode == self.RenderMode.NO_RENDERING:\\n            # throttle the rendering frequency to keep the UI responsive\\n            self._render_throttle_counter += 1\\n            if self._render_throttle_counter % self._render_throttle_period == 0:\\n                self._render_throttle_counter = 0\\n                # here we don\\'t render viewport so don\\'t need to flush fabric data\\n                # note: we don\\'t call super().render() anymore because they do flush the fabric data\\n                self.set_setting(\"/app/player/playSimulations\", False)\\n                self._app.update()\\n                self.set_setting(\"/app/player/playSimulations\", True)\\n        else:\\n            # manually flush the fabric data to update Hydra textures\\n            self.forward()\\n            # render the simulation\\n            # note: we don\\'t call super().render() anymore because they do above operation inside\\n            #  and we don\\'t want to do it twice. We may remove it once we drop support for Isaac Sim 2022.2.\\n            self.set_setting(\"/app/player/playSimulations\", False)\\n            self._app.update()\\n            self.set_setting(\"/app/player/playSimulations\", True)\\n\\n        # app.update() may be changing the cuda device, so we force it back to our desired device here\\n        if \"cuda\" in self.device:\\n            torch.cuda.set_device(self.device)\\n\\n    \"\"\"\\n    Operations - Override (extension)\\n    \"\"\"\\n\\n    async def reset_async(self, soft: bool = False):\\n        # need to load all \"physics\" information from the USD file\\n        if not soft:\\n            omni.physx.acquire_physx_interface().force_load_physics_from_usd()\\n        # play the simulation\\n        await super().reset_async(soft=soft)\\n\\n    \"\"\"\\n    Initialization/Destruction - Override.\\n    \"\"\"\\n\\n    def _init_stage(self, *args, **kwargs) -> Usd.Stage:\\n        _ = super()._init_stage(*args, **kwargs)\\n        # a stage update here is needed for the case when physics_dt != rendering_dt, otherwise the app crashes\\n        # when in headless mode\\n        self.set_setting(\"/app/player/playSimulations\", False)\\n        self._app.update()\\n        self.set_setting(\"/app/player/playSimulations\", True)\\n        # set additional physx parameters and bind material\\n        self._set_additional_physx_params()\\n        # load flatcache/fabric interface\\n        self._load_fabric_interface()\\n        # return the stage\\n        return self.stage\\n\\n    async def _initialize_stage_async(self, *args, **kwargs) -> Usd.Stage:\\n        await super()._initialize_stage_async(*args, **kwargs)\\n        # set additional physx parameters and bind material\\n        self._set_additional_physx_params()\\n        # load flatcache/fabric interface\\n        self._load_fabric_interface()\\n        # return the stage\\n        return self.stage\\n\\n    @classmethod\\n    def clear_instance(cls):\\n        # clear the callback\\n        if cls._instance is not None:\\n            if cls._instance._app_control_on_stop_handle is not None:\\n                cls._instance._app_control_on_stop_handle.unsubscribe()\\n                cls._instance._app_control_on_stop_handle = None\\n        # call parent to clear the instance\\n        super().clear_instance()\\n\\n    \"\"\"\\n    Helper Functions\\n    \"\"\"\\n\\n    def _set_additional_physx_params(self):\\n        \"\"\"Sets additional PhysX parameters that are not directly supported by the parent class.\"\"\"\\n        # obtain the physics scene api\\n        physics_scene: UsdPhysics.Scene = self._physics_context._physics_scene\\n        physx_scene_api: PhysxSchema.PhysxSceneAPI = self._physics_context._physx_scene_api\\n        # assert that scene api is not None\\n        if physx_scene_api is None:\\n            raise RuntimeError(\"Physics scene API is None! Please create the scene first.\")\\n        # set parameters not directly supported by the constructor\\n        # -- Continuous Collision Detection (CCD)\\n        # ref: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/AdvancedCollisionDetection.html?highlight=ccd#continuous-collision-detection\\n        self._physics_context.enable_ccd(self.cfg.physx.enable_ccd)\\n        # -- GPU collision stack size\\n        physx_scene_api.CreateGpuCollisionStackSizeAttr(self.cfg.physx.gpu_collision_stack_size)\\n        # -- Improved determinism by PhysX\\n        physx_scene_api.CreateEnableEnhancedDeterminismAttr(self.cfg.physx.enable_enhanced_determinism)\\n\\n        # -- Gravity\\n        # note: Isaac sim only takes the \"up-axis\" as the gravity direction. But physics allows any direction so we\\n        #  need to convert the gravity vector to a direction and magnitude pair explicitly.\\n        gravity = np.asarray(self.cfg.gravity)\\n        gravity_magnitude = np.linalg.norm(gravity)\\n\\n        # Avoid division by zero\\n        if gravity_magnitude != 0.0:\\n            gravity_direction = gravity / gravity_magnitude\\n        else:\\n            gravity_direction = gravity\\n\\n        physics_scene.CreateGravityDirectionAttr(Gf.Vec3f(*gravity_direction))\\n        physics_scene.CreateGravityMagnitudeAttr(gravity_magnitude)\\n\\n        # position iteration count\\n        physx_scene_api.CreateMinPositionIterationCountAttr(self.cfg.physx.min_position_iteration_count)\\n        physx_scene_api.CreateMaxPositionIterationCountAttr(self.cfg.physx.max_position_iteration_count)\\n        # velocity iteration count\\n        physx_scene_api.CreateMinVelocityIterationCountAttr(self.cfg.physx.min_velocity_iteration_count)\\n        physx_scene_api.CreateMaxVelocityIterationCountAttr(self.cfg.physx.max_velocity_iteration_count)\\n\\n        # create the default physics material\\n        # this material is used when no material is specified for a primitive\\n        # check: https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/simulation-control/physics-settings.html#physics-materials\\n        material_path = f\"{self.cfg.physics_prim_path}/defaultMaterial\"\\n        self.cfg.physics_material.func(material_path, self.cfg.physics_material)\\n        # bind the physics material to the scene\\n        bind_physics_material(self.cfg.physics_prim_path, material_path)\\n\\n    def _load_fabric_interface(self):\\n        \"\"\"Loads the fabric interface if enabled.\"\"\"\\n        if self.cfg.use_fabric:\\n            from omni.physxfabric import get_physx_fabric_interface\\n\\n            # acquire fabric interface\\n            self._fabric_iface = get_physx_fabric_interface()\\n            if hasattr(self._fabric_iface, \"force_update\"):\\n                # The update method in the fabric interface only performs an update if a physics step has occurred.\\n                # However, for rendering, we need to force an update since any element of the scene might have been\\n                # modified in a reset (which occurs after the physics step) and we want the renderer to be aware of\\n                # these changes.\\n                self._update_fabric = self._fabric_iface.force_update\\n            else:\\n                # Needed for backward compatibility with older Isaac Sim versions\\n                self._update_fabric = self._fabric_iface.update\\n\\n    \"\"\"\\n    Callbacks.\\n    \"\"\"\\n\\n    def _app_control_on_stop_handle_fn(self, event: carb.events.IEvent):\\n        \"\"\"Callback to deal with the app when the simulation is stopped.\\n\\n        Once the simulation is stopped, the physics handles go invalid. After that, it is not possible to\\n        resume the simulation from the last state. This leaves the app in an inconsistent state, where\\n        two possible actions can be taken:\\n\\n        1. **Keep the app rendering**: In this case, the simulation is kept running and the app is not shutdown.\\n           However, the physics is not updated and the script cannot be resumed from the last state. The\\n           user has to manually close the app to stop the simulation.\\n        2. **Shutdown the app**: This is the default behavior. In this case, the app is shutdown and\\n           the simulation is stopped.\\n\\n        Note:\\n            This callback is used only when running the simulation in a standalone python script. In an extension,\\n            it is expected that the user handles the extension shutdown.\\n        \"\"\"\\n        if not self._disable_app_control_on_stop_handle:\\n            while not omni.timeline.get_timeline_interface().is_playing():\\n                self.render()\\n        return'),\n",
       " Document(metadata={}, page_content='def build_simulation_context(\\n    create_new_stage: bool = True,\\n    gravity_enabled: bool = True,\\n    device: str = \"cuda:0\",\\n    dt: float = 0.01,\\n    sim_cfg: SimulationCfg | None = None,\\n    add_ground_plane: bool = False,\\n    add_lighting: bool = False,\\n    auto_add_lighting: bool = False,\\n) -> Iterator[SimulationContext]:\\n    \"\"\"Context manager to build a simulation context with the provided settings.\\n\\n    This function facilitates the creation of a simulation context and provides flexibility in configuring various\\n    aspects of the simulation, such as time step, gravity, device, and scene elements like ground plane and\\n    lighting.\\n\\n    If :attr:`sim_cfg` is None, then an instance of :class:`SimulationCfg` is created with default settings, with parameters\\n    overwritten based on arguments to the function.\\n\\n    An example usage of the context manager function:\\n\\n    ..  code-block:: python\\n\\n        with build_simulation_context() as sim:\\n             # Design the scene\\n\\n             # Play the simulation\\n             sim.reset()\\n             while sim.is_playing():\\n                 sim.step()\\n\\n    Args:\\n        create_new_stage: Whether to create a new stage. Defaults to True.\\n        gravity_enabled: Whether to enable gravity in the simulation. Defaults to True.\\n        device: Device to run the simulation on. Defaults to \"cuda:0\".\\n        dt: Time step for the simulation: Defaults to 0.01.\\n        sim_cfg: :class:`isaaclab.sim.SimulationCfg` to use for the simulation. Defaults to None.\\n        add_ground_plane: Whether to add a ground plane to the simulation. Defaults to False.\\n        add_lighting: Whether to add a dome light to the simulation. Defaults to False.\\n        auto_add_lighting: Whether to automatically add a dome light to the simulation if the simulation has a GUI.\\n            Defaults to False. This is useful for debugging tests in the GUI.\\n\\n    Yields:\\n        The simulation context to use for the simulation.\\n\\n    \"\"\"\\n    try:\\n        if create_new_stage:\\n            stage_utils.create_new_stage()\\n\\n        if sim_cfg is None:\\n            # Construct one and overwrite the dt, gravity, and device\\n            sim_cfg = SimulationCfg(dt=dt)\\n\\n            # Set up gravity\\n            if gravity_enabled:\\n                sim_cfg.gravity = (0.0, 0.0, -9.81)\\n            else:\\n                sim_cfg.gravity = (0.0, 0.0, 0.0)\\n\\n            # Set device\\n            sim_cfg.device = device\\n\\n        # Construct simulation context\\n        sim = SimulationContext(sim_cfg)\\n\\n        if add_ground_plane:\\n            # Ground-plane\\n            cfg = GroundPlaneCfg()\\n            cfg.func(\"/World/defaultGroundPlane\", cfg)\\n\\n        if add_lighting or (auto_add_lighting and sim.has_gui()):\\n            # Lighting\\n            cfg = DomeLightCfg(\\n                color=(0.1, 0.1, 0.1),\\n                enable_color_temperature=True,\\n                color_temperature=5500,\\n                intensity=10000,\\n            )\\n            # Dome light named specifically to avoid conflicts\\n            cfg.func(prim_path=\"/World/defaultDomeLight\", cfg=cfg, translation=(0.0, 0.0, 10.0))\\n\\n        yield sim\\n\\n    except Exception:\\n        omni.log.error(traceback.format_exc())\\n        raise\\n    finally:\\n        if not sim.has_gui():\\n            # Stop simulation only if we aren\\'t rendering otherwise the app will hang indefinitely\\n            sim.stop()\\n\\n        # Clear the stage\\n        sim.clear_all_callbacks()\\n        sim.clear_instance()\\n        # check if we need to raise an exception that was raised in a callback\\n        if builtins.ISAACLAB_CALLBACK_EXCEPTION is not None:\\n            exception_to_raise = builtins.ISAACLAB_CALLBACK_EXCEPTION\\n            builtins.ISAACLAB_CALLBACK_EXCEPTION = None\\n            raise exception_to_raise'),\n",
       " Document(metadata={}, page_content='def safe_set_attribute_on_usd_schema(schema_api: Usd.APISchemaBase, name: str, value: Any, camel_case: bool):\\n    \"\"\"Set the value of an attribute on its USD schema if it exists.\\n\\n    A USD API schema serves as an interface or API for authoring and extracting a set of attributes.\\n    They typically derive from the :class:`pxr.Usd.SchemaBase` class. This function checks if the\\n    attribute exists on the schema and sets the value of the attribute if it exists.\\n\\n    Args:\\n        schema_api: The USD schema to set the attribute on.\\n        name: The name of the attribute.\\n        value: The value to set the attribute to.\\n        camel_case: Whether to convert the attribute name to camel case.\\n\\n    Raises:\\n        TypeError: When the input attribute name does not exist on the provided schema API.\\n    \"\"\"\\n    # if value is None, do nothing\\n    if value is None:\\n        return\\n    # convert attribute name to camel case\\n    if camel_case:\\n        attr_name = to_camel_case(name, to=\"CC\")\\n    else:\\n        attr_name = name\\n    # retrieve the attribute\\n    # reference: https://openusd.org/dev/api/_usd__page__common_idioms.html#Usd_Create_Or_Get_Property\\n    attr = getattr(schema_api, f\"Create{attr_name}Attr\", None)\\n    # check if attribute exists\\n    if attr is not None:\\n        attr().Set(value)\\n    else:\\n        # think: do we ever need to create the attribute if it doesn\\'t exist?\\n        #   currently, we are not doing this since the schemas are already created with some defaults.\\n        omni.log.error(f\"Attribute \\'{attr_name}\\' does not exist on prim \\'{schema_api.GetPath()}\\'.\")\\n        raise TypeError(f\"Attribute \\'{attr_name}\\' does not exist on prim \\'{schema_api.GetPath()}\\'.\")'),\n",
       " Document(metadata={}, page_content='def safe_set_attribute_on_usd_prim(prim: Usd.Prim, attr_name: str, value: Any, camel_case: bool):\\n    \"\"\"Set the value of a attribute on its USD prim.\\n\\n    The function creates a new attribute if it does not exist on the prim. This is because in some cases (such\\n    as with shaders), their attributes are not exposed as USD prim properties that can be altered. This function\\n    allows us to set the value of the attributes in these cases.\\n\\n    Args:\\n        prim: The USD prim to set the attribute on.\\n        attr_name: The name of the attribute.\\n        value: The value to set the attribute to.\\n        camel_case: Whether to convert the attribute name to camel case.\\n    \"\"\"\\n    # if value is None, do nothing\\n    if value is None:\\n        return\\n    # convert attribute name to camel case\\n    if camel_case:\\n        attr_name = to_camel_case(attr_name, to=\"cC\")\\n    # resolve sdf type based on value\\n    if isinstance(value, bool):\\n        sdf_type = Sdf.ValueTypeNames.Bool\\n    elif isinstance(value, int):\\n        sdf_type = Sdf.ValueTypeNames.Int\\n    elif isinstance(value, float):\\n        sdf_type = Sdf.ValueTypeNames.Float\\n    elif isinstance(value, (tuple, list)) and len(value) == 3 and any(isinstance(v, float) for v in value):\\n        sdf_type = Sdf.ValueTypeNames.Float3\\n    elif isinstance(value, (tuple, list)) and len(value) == 2 and any(isinstance(v, float) for v in value):\\n        sdf_type = Sdf.ValueTypeNames.Float2\\n    else:\\n        raise NotImplementedError(\\n            f\"Cannot set attribute \\'{attr_name}\\' with value \\'{value}\\'. Please modify the code to support this type.\"\\n        )\\n    # change property\\n    omni.kit.commands.execute(\\n        \"ChangePropertyCommand\",\\n        prop_path=Sdf.Path(f\"{prim.GetPath()}.{attr_name}\"),\\n        value=value,\\n        prev=None,\\n        type_to_create_if_not_exist=sdf_type,\\n        usd_context_name=prim.GetStage(),\\n    )'),\n",
       " Document(metadata={}, page_content='def apply_nested(func: Callable) -> Callable:\\n    \"\"\"Decorator to apply a function to all prims under a specified prim-path.\\n\\n    The function iterates over the provided prim path and all its children to apply input function\\n    to all prims under the specified prim path.\\n\\n    If the function succeeds to apply to a prim, it will not look at the children of that prim.\\n    This is based on the physics behavior that nested schemas are not allowed. For example, a parent prim\\n    and its child prim cannot both have a rigid-body schema applied on them, or it is not possible to\\n    have nested articulations.\\n\\n    While traversing the prims under the specified prim path, the function will throw a warning if it\\n    does not succeed to apply the function to any prim. This is because the user may have intended to\\n    apply the function to a prim that does not have valid attributes, or the prim may be an instanced prim.\\n\\n    Args:\\n        func: The function to apply to all prims under a specified prim-path. The function\\n            must take the prim-path and other arguments. It should return a boolean indicating whether\\n            the function succeeded or not.\\n\\n    Returns:\\n        The wrapped function that applies the function to all prims under a specified prim-path.\\n\\n    Raises:\\n        ValueError: If the prim-path does not exist on the stage.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapper(prim_path: str | Sdf.Path, *args, **kwargs):\\n        # map args and kwargs to function signature so we can get the stage\\n        # note: we do this to check if stage is given in arg or kwarg\\n        sig = inspect.signature(func)\\n        bound_args = sig.bind(prim_path, *args, **kwargs)\\n        # get current stage\\n        stage = bound_args.arguments.get(\"stage\")\\n        if stage is None:\\n            stage = stage_utils.get_current_stage()\\n        # get USD prim\\n        prim: Usd.Prim = stage.GetPrimAtPath(prim_path)\\n        # check if prim is valid\\n        if not prim.IsValid():\\n            raise ValueError(f\"Prim at path \\'{prim_path}\\' is not valid.\")\\n        # add iterable to check if property was applied on any of the prims\\n        count_success = 0\\n        instanced_prim_paths = []\\n        # iterate over all prims under prim-path\\n        all_prims = [prim]\\n        while len(all_prims) > 0:\\n            # get current prim\\n            child_prim = all_prims.pop(0)\\n            child_prim_path = child_prim.GetPath().pathString  # type: ignore\\n            # check if prim is a prototype\\n            if child_prim.IsInstance():\\n                instanced_prim_paths.append(child_prim_path)\\n                continue\\n            # set properties\\n            success = func(child_prim_path, *args, **kwargs)\\n            # if successful, do not look at children\\n            # this is based on the physics behavior that nested schemas are not allowed\\n            if not success:\\n                all_prims += child_prim.GetChildren()\\n            else:\\n                count_success += 1\\n        # check if we were successful in applying the function to any prim\\n        if count_success == 0:\\n            omni.log.warn(\\n                f\"Could not perform \\'{func.__name__}\\' on any prims under: \\'{prim_path}\\'.\"\\n                \" This might be because of the following reasons:\"\\n                \"\\\\n\\\\t(1) The desired attribute does not exist on any of the prims.\"\\n                \"\\\\n\\\\t(2) The desired attribute exists on an instanced prim.\"\\n                f\"\\\\n\\\\t\\\\tDiscovered list of instanced prim paths: {instanced_prim_paths}\"\\n            )\\n\\n    return wrapper'),\n",
       " Document(metadata={}, page_content='def clone(func: Callable) -> Callable:\\n    \"\"\"Decorator for cloning a prim based on matching prim paths of the prim\\'s parent.\\n\\n    The decorator checks if the parent prim path matches any prim paths in the stage. If so, it clones the\\n    spawned prim at each matching prim path. For example, if the input prim path is: ``/World/Table_[0-9]/Bottle``,\\n    the decorator will clone the prim at each matching prim path of the parent prim: ``/World/Table_0/Bottle``,\\n    ``/World/Table_1/Bottle``, etc.\\n\\n    Note:\\n        For matching prim paths, the decorator assumes that valid prims exist for all matching prim paths.\\n        In case no matching prim paths are found, the decorator raises a ``RuntimeError``.\\n\\n    Args:\\n        func: The function to decorate.\\n\\n    Returns:\\n        The decorated function that spawns the prim and clones it at each matching prim path.\\n        It returns the spawned source prim, i.e., the first prim in the list of matching prim paths.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapper(prim_path: str | Sdf.Path, cfg: SpawnerCfg, *args, **kwargs):\\n        # cast prim_path to str type in case its an Sdf.Path\\n        prim_path = str(prim_path)\\n        # check prim path is global\\n        if not prim_path.startswith(\"/\"):\\n            raise ValueError(f\"Prim path \\'{prim_path}\\' is not global. It must start with \\'/\\'.\")\\n        # resolve: {SPAWN_NS}/AssetName\\n        # note: this assumes that the spawn namespace already exists in the stage\\n        root_path, asset_path = prim_path.rsplit(\"/\", 1)\\n        # check if input is a regex expression\\n        # note: a valid prim path can only contain alphanumeric characters, underscores, and forward slashes\\n        is_regex_expression = re.match(r\"^[a-zA-Z0-9/_]+$\", root_path) is None\\n\\n        # resolve matching prims for source prim path expression\\n        if is_regex_expression and root_path != \"\":\\n            source_prim_paths = find_matching_prim_paths(root_path)\\n            # if no matching prims are found, raise an error\\n            if len(source_prim_paths) == 0:\\n                raise RuntimeError(\\n                    f\"Unable to find source prim path: \\'{root_path}\\'. Please create the prim before spawning.\"\\n                )\\n        else:\\n            source_prim_paths = [root_path]\\n\\n        # resolve prim paths for spawning and cloning\\n        prim_paths = [f\"{source_prim_path}/{asset_path}\" for source_prim_path in source_prim_paths]\\n        # spawn single instance\\n        prim = func(prim_paths[0], cfg, *args, **kwargs)\\n        # set the prim visibility\\n        if hasattr(cfg, \"visible\"):\\n            imageable = UsdGeom.Imageable(prim)\\n            if cfg.visible:\\n                imageable.MakeVisible()\\n            else:\\n                imageable.MakeInvisible()\\n        # set the semantic annotations\\n        if hasattr(cfg, \"semantic_tags\") and cfg.semantic_tags is not None:\\n            # note: taken from replicator scripts.utils.utils.py\\n            for semantic_type, semantic_value in cfg.semantic_tags:\\n                # deal with spaces by replacing them with underscores\\n                semantic_type_sanitized = semantic_type.replace(\" \", \"_\")\\n                semantic_value_sanitized = semantic_value.replace(\" \", \"_\")\\n                # set the semantic API for the instance\\n                instance_name = f\"{semantic_type_sanitized}_{semantic_value_sanitized}\"\\n                sem = Semantics.SemanticsAPI.Apply(prim, instance_name)\\n                # create semantic type and data attributes\\n                sem.CreateSemanticTypeAttr()\\n                sem.CreateSemanticDataAttr()\\n                sem.GetSemanticTypeAttr().Set(semantic_type)\\n                sem.GetSemanticDataAttr().Set(semantic_value)\\n        # activate rigid body contact sensors\\n        if hasattr(cfg, \"activate_contact_sensors\") and cfg.activate_contact_sensors:\\n            schemas.activate_contact_sensors(prim_paths[0], cfg.activate_contact_sensors)\\n        # clone asset using cloner API\\n        if len(prim_paths) > 1:\\n            cloner = Cloner()\\n            # clone the prim\\n            cloner.clone(prim_paths[0], prim_paths[1:], replicate_physics=False, copy_from_source=cfg.copy_from_source)\\n        # return the source prim\\n        return prim\\n\\n    return wrapper'),\n",
       " Document(metadata={}, page_content='def bind_visual_material(\\n    prim_path: str | Sdf.Path,\\n    material_path: str | Sdf.Path,\\n    stage: Usd.Stage | None = None,\\n    stronger_than_descendants: bool = True,\\n):\\n    \"\"\"Bind a visual material to a prim.\\n\\n    This function is a wrapper around the USD command `BindMaterialCommand`_.\\n\\n    .. note::\\n        The function is decorated with :meth:`apply_nested` to allow applying the function to a prim path\\n        and all its descendants.\\n\\n    .. _BindMaterialCommand: https://docs.omniverse.nvidia.com/kit/docs/omni.usd/latest/omni.usd.commands/omni.usd.commands.BindMaterialCommand.html\\n\\n    Args:\\n        prim_path: The prim path where to apply the material.\\n        material_path: The prim path of the material to apply.\\n        stage: The stage where the prim and material exist.\\n            Defaults to None, in which case the current stage is used.\\n        stronger_than_descendants: Whether the material should override the material of its descendants.\\n            Defaults to True.\\n\\n    Raises:\\n        ValueError: If the provided prim paths do not exist on stage.\\n    \"\"\"\\n    # resolve stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # check if prim and material exists\\n    if not stage.GetPrimAtPath(prim_path).IsValid():\\n        raise ValueError(f\"Target prim \\'{material_path}\\' does not exist.\")\\n    if not stage.GetPrimAtPath(material_path).IsValid():\\n        raise ValueError(f\"Visual material \\'{material_path}\\' does not exist.\")\\n\\n    # resolve token for weaker than descendants\\n    if stronger_than_descendants:\\n        binding_strength = \"strongerThanDescendants\"\\n    else:\\n        binding_strength = \"weakerThanDescendants\"\\n    # obtain material binding API\\n    # note: we prefer using the command here as it is more robust than the USD API\\n    success, _ = omni.kit.commands.execute(\\n        \"BindMaterialCommand\",\\n        prim_path=prim_path,\\n        material_path=material_path,\\n        strength=binding_strength,\\n        stage=stage,\\n    )\\n    # return success\\n    return success'),\n",
       " Document(metadata={}, page_content='def bind_physics_material(\\n    prim_path: str | Sdf.Path,\\n    material_path: str | Sdf.Path,\\n    stage: Usd.Stage | None = None,\\n    stronger_than_descendants: bool = True,\\n):\\n    \"\"\"Bind a physics material to a prim.\\n\\n    `Physics material`_ can be applied only to a prim with physics-enabled on them. This includes having\\n    collision APIs, or deformable body APIs, or being a particle system. In case the prim does not have\\n    any of these APIs, the function will not apply the material and return False.\\n\\n    .. note::\\n        The function is decorated with :meth:`apply_nested` to allow applying the function to a prim path\\n        and all its descendants.\\n\\n    .. _Physics material: https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/simulation-control/physics-settings.html#physics-materials\\n\\n    Args:\\n        prim_path: The prim path where to apply the material.\\n        material_path: The prim path of the material to apply.\\n        stage: The stage where the prim and material exist.\\n            Defaults to None, in which case the current stage is used.\\n        stronger_than_descendants: Whether the material should override the material of its descendants.\\n            Defaults to True.\\n\\n    Raises:\\n        ValueError: If the provided prim paths do not exist on stage.\\n    \"\"\"\\n    # resolve stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # check if prim and material exists\\n    if not stage.GetPrimAtPath(prim_path).IsValid():\\n        raise ValueError(f\"Target prim \\'{material_path}\\' does not exist.\")\\n    if not stage.GetPrimAtPath(material_path).IsValid():\\n        raise ValueError(f\"Physics material \\'{material_path}\\' does not exist.\")\\n    # get USD prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim has collision applied on it\\n    has_physics_scene_api = prim.HasAPI(PhysxSchema.PhysxSceneAPI)\\n    has_collider = prim.HasAPI(UsdPhysics.CollisionAPI)\\n    has_deformable_body = prim.HasAPI(PhysxSchema.PhysxDeformableBodyAPI)\\n    has_particle_system = prim.IsA(PhysxSchema.PhysxParticleSystem)\\n    if not (has_physics_scene_api or has_collider or has_deformable_body or has_particle_system):\\n        omni.log.verbose(\\n            f\"Cannot apply physics material \\'{material_path}\\' on prim \\'{prim_path}\\'. It is neither a\"\\n            \" PhysX scene, collider, a deformable body, nor a particle system.\"\\n        )\\n        return False\\n\\n    # obtain material binding API\\n    if prim.HasAPI(UsdShade.MaterialBindingAPI):\\n        material_binding_api = UsdShade.MaterialBindingAPI(prim)\\n    else:\\n        material_binding_api = UsdShade.MaterialBindingAPI.Apply(prim)\\n    # obtain the material prim\\n    material = UsdShade.Material(stage.GetPrimAtPath(material_path))\\n    # resolve token for weaker than descendants\\n    if stronger_than_descendants:\\n        binding_strength = UsdShade.Tokens.strongerThanDescendants\\n    else:\\n        binding_strength = UsdShade.Tokens.weakerThanDescendants\\n    # apply the material\\n    material_binding_api.Bind(material, bindingStrength=binding_strength, materialPurpose=\"physics\")  # type: ignore\\n    # return success\\n    return True'),\n",
       " Document(metadata={}, page_content='def export_prim_to_file(\\n    path: str | Sdf.Path,\\n    source_prim_path: str | Sdf.Path,\\n    target_prim_path: str | Sdf.Path | None = None,\\n    stage: Usd.Stage | None = None,\\n):\\n    \"\"\"Exports a prim from a given stage to a USD file.\\n\\n    The function creates a new layer at the provided path and copies the prim to the layer.\\n    It sets the copied prim as the default prim in the target layer. Additionally, it updates\\n    the stage up-axis and meters-per-unit to match the current stage.\\n\\n    Args:\\n        path: The filepath path to export the prim to.\\n        source_prim_path: The prim path to export.\\n        target_prim_path: The prim path to set as the default prim in the target layer.\\n            Defaults to None, in which case the source prim path is used.\\n        stage: The stage where the prim exists. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Raises:\\n        ValueError: If the prim paths are not global (i.e: do not start with \\'/\\').\\n    \"\"\"\\n    # automatically casting to str in case args\\n    # are path types\\n    path = str(path)\\n    source_prim_path = str(source_prim_path)\\n    if target_prim_path is not None:\\n        target_prim_path = str(target_prim_path)\\n\\n    if not source_prim_path.startswith(\"/\"):\\n        raise ValueError(f\"Source prim path \\'{source_prim_path}\\' is not global. It must start with \\'/\\'.\")\\n    if target_prim_path is not None and not target_prim_path.startswith(\"/\"):\\n        raise ValueError(f\"Target prim path \\'{target_prim_path}\\' is not global. It must start with \\'/\\'.\")\\n    # get current stage\\n    if stage is None:\\n        stage: Usd.Stage = omni.usd.get_context().get_stage()\\n    # get root layer\\n    source_layer = stage.GetRootLayer()\\n\\n    # only create a new layer if it doesn\\'t exist already\\n    target_layer = Sdf.Find(path)\\n    if target_layer is None:\\n        target_layer = Sdf.Layer.CreateNew(path)\\n    # open the target stage\\n    target_stage = Usd.Stage.Open(target_layer)\\n\\n    # update stage data\\n    UsdGeom.SetStageUpAxis(target_stage, UsdGeom.GetStageUpAxis(stage))\\n    UsdGeom.SetStageMetersPerUnit(target_stage, UsdGeom.GetStageMetersPerUnit(stage))\\n\\n    # specify the prim to copy\\n    source_prim_path = Sdf.Path(source_prim_path)\\n    if target_prim_path is None:\\n        target_prim_path = source_prim_path\\n\\n    # copy the prim\\n    Sdf.CreatePrimInLayer(target_layer, target_prim_path)\\n    Sdf.CopySpec(source_layer, source_prim_path, target_layer, target_prim_path)\\n    # set the default prim\\n    target_layer.defaultPrim = Sdf.Path(target_prim_path).name\\n    # resolve all paths relative to layer path\\n    omni.usd.resolve_paths(source_layer.identifier, target_layer.identifier)\\n    # save the stage\\n    target_layer.Save()'),\n",
       " Document(metadata={}, page_content='def make_uninstanceable(prim_path: str | Sdf.Path, stage: Usd.Stage | None = None):\\n    \"\"\"Check if a prim and its descendants are instanced and make them uninstanceable.\\n\\n    This function checks if the prim at the specified prim path and its descendants are instanced.\\n    If so, it makes the respective prim uninstanceable by disabling instancing on the prim.\\n\\n    This is useful when we want to modify the properties of a prim that is instanced. For example, if we\\n    want to apply a different material on an instanced prim, we need to make the prim uninstanceable first.\\n\\n    Args:\\n        prim_path: The prim path to check.\\n        stage: The stage where the prim exists. Defaults to None, in which case the current stage is used.\\n\\n    Raises:\\n        ValueError: If the prim path is not global (i.e: does not start with \\'/\\').\\n    \"\"\"\\n    # make paths str type if they aren\\'t already\\n    prim_path = str(prim_path)\\n    # check if prim path is global\\n    if not prim_path.startswith(\"/\"):\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not global. It must start with \\'/\\'.\")\\n    # get current stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get prim\\n    prim: Usd.Prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim at path \\'{prim_path}\\' is not valid.\")\\n    # iterate over all prims under prim-path\\n    all_prims = [prim]\\n    while len(all_prims) > 0:\\n        # get current prim\\n        child_prim = all_prims.pop(0)\\n        # check if prim is instanced\\n        if child_prim.IsInstance():\\n            # make the prim uninstanceable\\n            child_prim.SetInstanceable(False)\\n        # add children to list\\n        all_prims += child_prim.GetChildren()'),\n",
       " Document(metadata={}, page_content='def get_first_matching_child_prim(\\n    prim_path: str | Sdf.Path, predicate: Callable[[Usd.Prim], bool], stage: Usd.Stage | None = None\\n) -> Usd.Prim | None:\\n    \"\"\"Recursively get the first USD Prim at the path string that passes the predicate function\\n\\n    Args:\\n        prim_path: The path of the prim in the stage.\\n        predicate: The function to test the prims against. It takes a prim as input and returns a boolean.\\n        stage: The stage where the prim exists. Defaults to None, in which case the current stage is used.\\n\\n    Returns:\\n        The first prim on the path that passes the predicate. If no prim passes the predicate, it returns None.\\n\\n    Raises:\\n        ValueError: If the prim path is not global (i.e: does not start with \\'/\\').\\n    \"\"\"\\n    # make paths str type if they aren\\'t already\\n    prim_path = str(prim_path)\\n    # check if prim path is global\\n    if not prim_path.startswith(\"/\"):\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not global. It must start with \\'/\\'.\")\\n    # get current stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim at path \\'{prim_path}\\' is not valid.\")\\n    # iterate over all prims under prim-path\\n    all_prims = [prim]\\n    while len(all_prims) > 0:\\n        # get current prim\\n        child_prim = all_prims.pop(0)\\n        # check if prim passes predicate\\n        if predicate(child_prim):\\n            return child_prim\\n        # add children to list\\n        all_prims += child_prim.GetChildren()\\n    return None'),\n",
       " Document(metadata={}, page_content='def get_all_matching_child_prims(\\n    prim_path: str | Sdf.Path,\\n    predicate: Callable[[Usd.Prim], bool] = lambda _: True,\\n    depth: int | None = None,\\n    stage: Usd.Stage | None = None,\\n) -> list[Usd.Prim]:\\n    \"\"\"Performs a search starting from the root and returns all the prims matching the predicate.\\n\\n    Args:\\n        prim_path: The root prim path to start the search from.\\n        predicate: The predicate that checks if the prim matches the desired criteria. It takes a prim as input\\n            and returns a boolean. Defaults to a function that always returns True.\\n        depth: The maximum depth for traversal, should be bigger than zero if specified.\\n            Defaults to None (i.e: traversal happens till the end of the tree).\\n        stage: The stage where the prim exists. Defaults to None, in which case the current stage is used.\\n\\n    Returns:\\n        A list containing all the prims matching the predicate.\\n\\n    Raises:\\n        ValueError: If the prim path is not global (i.e: does not start with \\'/\\').\\n    \"\"\"\\n    # make paths str type if they aren\\'t already\\n    prim_path = str(prim_path)\\n    # check if prim path is global\\n    if not prim_path.startswith(\"/\"):\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not global. It must start with \\'/\\'.\")\\n    # get current stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim at path \\'{prim_path}\\' is not valid.\")\\n    # check if depth is valid\\n    if depth is not None and depth <= 0:\\n        raise ValueError(f\"Depth must be bigger than zero, got {depth}.\")\\n\\n    # iterate over all prims under prim-path\\n    # list of tuples (prim, current_depth)\\n    all_prims_queue = [(prim, 0)]\\n    output_prims = []\\n    while len(all_prims_queue) > 0:\\n        # get current prim\\n        child_prim, current_depth = all_prims_queue.pop(0)\\n        # check if prim passes predicate\\n        if predicate(child_prim):\\n            output_prims.append(child_prim)\\n        # add children to list\\n        if depth is None or current_depth < depth:\\n            all_prims_queue += [(child, current_depth + 1) for child in child_prim.GetChildren()]\\n\\n    return output_prims'),\n",
       " Document(metadata={}, page_content='def find_first_matching_prim(prim_path_regex: str, stage: Usd.Stage | None = None) -> Usd.Prim | None:\\n    \"\"\"Find the first matching prim in the stage based on input regex expression.\\n\\n    Args:\\n        prim_path_regex: The regex expression for prim path.\\n        stage: The stage where the prim exists. Defaults to None, in which case the current stage is used.\\n\\n    Returns:\\n        The first prim that matches input expression. If no prim matches, returns None.\\n\\n    Raises:\\n        ValueError: If the prim path is not global (i.e: does not start with \\'/\\').\\n    \"\"\"\\n    # check prim path is global\\n    if not prim_path_regex.startswith(\"/\"):\\n        raise ValueError(f\"Prim path \\'{prim_path_regex}\\' is not global. It must start with \\'/\\'.\")\\n    # get current stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # need to wrap the token patterns in \\'^\\' and \\'$\\' to prevent matching anywhere in the string\\n    pattern = f\"^{prim_path_regex}$\"\\n    compiled_pattern = re.compile(pattern)\\n    # obtain matching prim (depth-first search)\\n    for prim in stage.Traverse():\\n        # check if prim passes predicate\\n        if compiled_pattern.match(prim.GetPath().pathString) is not None:\\n            return prim\\n    return None'),\n",
       " Document(metadata={}, page_content='def find_matching_prims(prim_path_regex: str, stage: Usd.Stage | None = None) -> list[Usd.Prim]:\\n    \"\"\"Find all the matching prims in the stage based on input regex expression.\\n\\n    Args:\\n        prim_path_regex: The regex expression for prim path.\\n        stage: The stage where the prim exists. Defaults to None, in which case the current stage is used.\\n\\n    Returns:\\n        A list of prims that match input expression.\\n\\n    Raises:\\n        ValueError: If the prim path is not global (i.e: does not start with \\'/\\').\\n    \"\"\"\\n    # check prim path is global\\n    if not prim_path_regex.startswith(\"/\"):\\n        raise ValueError(f\"Prim path \\'{prim_path_regex}\\' is not global. It must start with \\'/\\'.\")\\n    # get current stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # need to wrap the token patterns in \\'^\\' and \\'$\\' to prevent matching anywhere in the string\\n    tokens = prim_path_regex.split(\"/\")[1:]\\n    tokens = [f\"^{token}$\" for token in tokens]\\n    # iterate over all prims in stage (breath-first search)\\n    all_prims = [stage.GetPseudoRoot()]\\n    output_prims = []\\n    for index, token in enumerate(tokens):\\n        token_compiled = re.compile(token)\\n        for prim in all_prims:\\n            for child in prim.GetAllChildren():\\n                if token_compiled.match(child.GetName()) is not None:\\n                    output_prims.append(child)\\n        if index < len(tokens) - 1:\\n            all_prims = output_prims\\n            output_prims = []\\n    return output_prims'),\n",
       " Document(metadata={}, page_content='def find_matching_prim_paths(prim_path_regex: str, stage: Usd.Stage | None = None) -> list[str]:\\n    \"\"\"Find all the matching prim paths in the stage based on input regex expression.\\n\\n    Args:\\n        prim_path_regex: The regex expression for prim path.\\n        stage: The stage where the prim exists. Defaults to None, in which case the current stage is used.\\n\\n    Returns:\\n        A list of prim paths that match input expression.\\n\\n    Raises:\\n        ValueError: If the prim path is not global (i.e: does not start with \\'/\\').\\n    \"\"\"\\n    # obtain matching prims\\n    output_prims = find_matching_prims(prim_path_regex, stage)\\n    # convert prims to prim paths\\n    output_prim_paths = []\\n    for prim in output_prims:\\n        output_prim_paths.append(prim.GetPath().pathString)\\n    return output_prim_paths'),\n",
       " Document(metadata={}, page_content='def find_global_fixed_joint_prim(\\n    prim_path: str | Sdf.Path, check_enabled_only: bool = False, stage: Usd.Stage | None = None\\n) -> UsdPhysics.Joint | None:\\n    \"\"\"Find the fixed joint prim under the specified prim path that connects the target to the simulation world.\\n\\n    A joint is a connection between two bodies. A fixed joint is a joint that does not allow relative motion\\n    between the two bodies. When a fixed joint has only one target body, it is considered to attach the body\\n    to the simulation world.\\n\\n    This function finds the fixed joint prim that has only one target under the specified prim path. If no such\\n    fixed joint prim exists, it returns None.\\n\\n    Args:\\n        prim_path: The prim path to search for the fixed joint prim.\\n        check_enabled_only: Whether to consider only enabled fixed joints. Defaults to False.\\n            If False, then all joints (enabled or disabled) are considered.\\n        stage: The stage where the prim exists. Defaults to None, in which case the current stage is used.\\n\\n    Returns:\\n        The fixed joint prim that has only one target. If no such fixed joint prim exists, it returns None.\\n\\n    Raises:\\n        ValueError: If the prim path is not global (i.e: does not start with \\'/\\').\\n        ValueError: If the prim path does not exist on the stage.\\n    \"\"\"\\n    # check prim path is global\\n    if not prim_path.startswith(\"/\"):\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not global. It must start with \\'/\\'.\")\\n    # get current stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n\\n    # check if prim exists\\n    prim = stage.GetPrimAtPath(prim_path)\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim at path \\'{prim_path}\\' is not valid.\")\\n\\n    fixed_joint_prim = None\\n    # we check all joints under the root prim and classify the asset as fixed base if there exists\\n    # a fixed joint that has only one target (i.e. the root link).\\n    for prim in Usd.PrimRange(prim):\\n        # note: ideally checking if it is FixedJoint would have been enough, but some assets use \"Joint\" as the\\n        # schema name which makes it difficult to distinguish between the two.\\n        joint_prim = UsdPhysics.Joint(prim)\\n        if joint_prim:\\n            # if check_enabled_only is True, we only consider enabled joints\\n            if check_enabled_only and not joint_prim.GetJointEnabledAttr().Get():\\n                continue\\n            # check body 0 and body 1 exist\\n            body_0_exist = joint_prim.GetBody0Rel().GetTargets() != []\\n            body_1_exist = joint_prim.GetBody1Rel().GetTargets() != []\\n            # if either body 0 or body 1 does not exist, we have a fixed joint that connects to the world\\n            if not (body_0_exist and body_1_exist):\\n                fixed_joint_prim = joint_prim\\n                break\\n\\n    return fixed_joint_prim'),\n",
       " Document(metadata={}, page_content='def select_usd_variants(prim_path: str, variants: object | dict[str, str], stage: Usd.Stage | None = None):\\n    \"\"\"Sets the variant selections from the specified variant sets on a USD prim.\\n\\n    `USD Variants`_ are a very powerful tool in USD composition that allows prims to have different options on\\n    a single asset. This can be done by modifying variations of the same prim parameters per variant option in a set.\\n    This function acts as a script-based utility to set the variant selections for the specified variant sets on a\\n    USD prim.\\n\\n    The function takes a dictionary or a config class mapping variant set names to variant selections. For instance,\\n    if we have a prim at ``\"/World/Table\"`` with two variant sets: \"color\" and \"size\", we can set the variant\\n    selections as follows:\\n\\n    .. code-block:: python\\n\\n        select_usd_variants(\\n            prim_path=\"/World/Table\",\\n            variants={\\n                \"color\": \"red\",\\n                \"size\": \"large\",\\n            },\\n        )\\n\\n    Alternatively, we can use a config class to define the variant selections:\\n\\n    .. code-block:: python\\n\\n        @configclass\\n        class TableVariants:\\n            color: Literal[\"blue\", \"red\"] = \"red\"\\n            size: Literal[\"small\", \"large\"] = \"large\"\\n\\n        select_usd_variants(\\n            prim_path=\"/World/Table\",\\n            variants=TableVariants(),\\n        )\\n\\n    Args:\\n        prim_path: The path of the USD prim.\\n        variants: A dictionary or config class mapping variant set names to variant selections.\\n        stage: The USD stage. Defaults to None, in which case, the current stage is used.\\n\\n    Raises:\\n        ValueError: If the prim at the specified path is not valid.\\n\\n    .. _USD Variants: https://graphics.pixar.com/usd/docs/USD-Glossary.html#USDGlossary-Variant\\n    \"\"\"\\n    # Resolve stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # Obtain prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim at path \\'{prim_path}\\' is not valid.\")\\n    # Convert to dict if we have a configclass object.\\n    if not isinstance(variants, dict):\\n        variants = variants.to_dict()\\n\\n    existing_variant_sets = prim.GetVariantSets()\\n    for variant_set_name, variant_selection in variants.items():\\n        # Check if the variant set exists on the prim.\\n        if not existing_variant_sets.HasVariantSet(variant_set_name):\\n            omni.log.warn(f\"Variant set \\'{variant_set_name}\\' does not exist on prim \\'{prim_path}\\'.\")\\n            continue\\n\\n        variant_set = existing_variant_sets.GetVariantSet(variant_set_name)\\n        # Only set the variant selection if it is different from the current selection.\\n        if variant_set.GetVariantSelection() != variant_selection:\\n            variant_set.SetVariantSelection(variant_selection)\\n            omni.log.info(\\n                f\"Setting variant selection \\'{variant_selection}\\' for variant set \\'{variant_set_name}\\' on\"\\n                f\" prim \\'{prim_path}\\'.\"\\n            )'),\n",
       " Document(metadata={}, page_content='class AssetConverterBase(abc.ABC):\\n    \"\"\"Base class for converting an asset file from different formats into USD format.\\n\\n    This class provides a common interface for converting an asset file into USD. It does not\\n    provide any implementation for the conversion. The derived classes must implement the\\n    :meth:`_convert_asset` method to provide the actual conversion.\\n\\n    The file conversion is lazy if the output directory (:obj:`AssetConverterBaseCfg.usd_dir`) is provided.\\n    In the lazy conversion, the USD file is re-generated only if:\\n\\n    * The asset file is modified.\\n    * The configuration parameters are modified.\\n    * The USD file does not exist.\\n\\n    To override this behavior to force conversion, the flag :obj:`AssetConverterBaseCfg.force_usd_conversion`\\n    can be set to True.\\n\\n    When no output directory is defined, lazy conversion is deactivated and the generated USD file is\\n    stored in folder ``/tmp/IsaacLab/usd_{date}_{time}_{random}``, where the parameters in braces are generated\\n    at runtime. The random identifiers help avoid a race condition where two simultaneously triggered conversions\\n    try to use the same directory for reading/writing the generated files.\\n\\n    .. note::\\n        Changes to the parameters :obj:`AssetConverterBaseCfg.asset_path`, :obj:`AssetConverterBaseCfg.usd_dir`, and\\n        :obj:`AssetConverterBaseCfg.usd_file_name` are not considered as modifications in the configuration instance that\\n        trigger USD file re-generation.\\n\\n    \"\"\"\\n\\n    def __init__(self, cfg: AssetConverterBaseCfg):\\n        \"\"\"Initializes the class.\\n\\n        Args:\\n            cfg: The configuration instance for converting an asset file to USD format.\\n\\n        Raises:\\n            ValueError: When provided asset file does not exist.\\n        \"\"\"\\n        # check that the config is valid\\n        cfg.validate()\\n        # check if the asset file exists\\n        if not check_file_path(cfg.asset_path):\\n            raise ValueError(f\"The asset path does not exist: {cfg.asset_path}\")\\n        # save the inputs\\n        self.cfg = cfg\\n\\n        # resolve USD directory name\\n        if cfg.usd_dir is None:\\n            # a folder in \"/tmp/IsaacLab\" by the name: usd_{date}_{time}_{random}\\n            time_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\\n            self._usd_dir = f\"/tmp/IsaacLab/usd_{time_tag}_{random.randrange(10000)}\"\\n        else:\\n            self._usd_dir = cfg.usd_dir\\n\\n        # resolve the file name from asset file name if not provided\\n        if cfg.usd_file_name is None:\\n            usd_file_name = pathlib.PurePath(cfg.asset_path).stem\\n        else:\\n            usd_file_name = cfg.usd_file_name\\n        # add USD extension if not provided\\n        if not (usd_file_name.endswith(\".usd\") or usd_file_name.endswith(\".usda\")):\\n            self._usd_file_name = usd_file_name + \".usd\"\\n        else:\\n            self._usd_file_name = usd_file_name\\n\\n        # create the USD directory\\n        os.makedirs(self.usd_dir, exist_ok=True)\\n        # check if usd files exist\\n        self._usd_file_exists = os.path.isfile(self.usd_path)\\n        # path to read/write asset hash file\\n        self._dest_hash_path = os.path.join(self.usd_dir, \".asset_hash\")\\n        # create asset hash to check if the asset has changed\\n        self._asset_hash = self._config_to_hash(cfg)\\n        # read the saved hash\\n        try:\\n            with open(self._dest_hash_path) as f:\\n                existing_asset_hash = f.readline()\\n                self._is_same_asset = existing_asset_hash == self._asset_hash\\n        except FileNotFoundError:\\n            self._is_same_asset = False\\n\\n        # convert the asset to USD if the hash is different or USD file does not exist\\n        if cfg.force_usd_conversion or not self._usd_file_exists or not self._is_same_asset:\\n            # write the updated hash\\n            with open(self._dest_hash_path, \"w\") as f:\\n                f.write(self._asset_hash)\\n            # convert the asset to USD\\n            self._convert_asset(cfg)\\n            # dump the configuration to a file\\n            dump_yaml(os.path.join(self.usd_dir, \"config.yaml\"), cfg.to_dict())\\n            # add comment to top of the saved config file with information about the converter\\n            current_date = datetime.now().strftime(\"%Y-%m-%d\")\\n            current_time = datetime.now().strftime(\"%H:%M:%S\")\\n            generation_comment = (\\n                f\"##\\\\n# Generated by {self.__class__.__name__} on {current_date} at {current_time}.\\\\n##\\\\n\"\\n            )\\n            with open(os.path.join(self.usd_dir, \"config.yaml\"), \"a\") as f:\\n                f.write(generation_comment)\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def usd_dir(self) -> str:\\n        \"\"\"The absolute path to the directory where the generated USD files are stored.\"\"\"\\n        return self._usd_dir\\n\\n    @property\\n    def usd_file_name(self) -> str:\\n        \"\"\"The file name of the generated USD file.\"\"\"\\n        return self._usd_file_name\\n\\n    @property\\n    def usd_path(self) -> str:\\n        \"\"\"The absolute path to the generated USD file.\"\"\"\\n        return os.path.join(self.usd_dir, self.usd_file_name)\\n\\n    @property\\n    def usd_instanceable_meshes_path(self) -> str:\\n        \"\"\"The relative path to the USD file with meshes.\\n\\n        The path is with respect to the USD directory :attr:`usd_dir`. This is to ensure that the\\n        mesh references in the generated USD file are resolved relatively. Otherwise, it becomes\\n        difficult to move the USD asset to a different location.\\n        \"\"\"\\n        return os.path.join(\".\", \"Props\", \"instanceable_meshes.usd\")\\n\\n    \"\"\"\\n    Implementation specifics.\\n    \"\"\"\\n\\n    @abc.abstractmethod\\n    def _convert_asset(self, cfg: AssetConverterBaseCfg):\\n        \"\"\"Converts the asset file to USD.\\n\\n        Args:\\n            cfg: The configuration instance for the input asset to USD conversion.\\n        \"\"\"\\n        raise NotImplementedError()\\n\\n    \"\"\"\\n    Private helpers.\\n    \"\"\"\\n\\n    @staticmethod\\n    def _config_to_hash(cfg: AssetConverterBaseCfg) -> str:\\n        \"\"\"Converts the configuration object and asset file to an MD5 hash of a string.\\n\\n        .. warning::\\n            It only checks the main asset file (:attr:`cfg.asset_path`).\\n\\n        Args:\\n            config : The asset converter configuration object.\\n\\n        Returns:\\n            An MD5 hash of a string.\\n        \"\"\"\\n\\n        # convert to dict and remove path related info\\n        config_dic = cfg.to_dict()\\n        _ = config_dic.pop(\"asset_path\")\\n        _ = config_dic.pop(\"usd_dir\")\\n        _ = config_dic.pop(\"usd_file_name\")\\n        # convert config dic to bytes\\n        config_bytes = json.dumps(config_dic).encode()\\n        # hash config\\n        md5 = hashlib.md5()\\n        md5.update(config_bytes)\\n\\n        # read the asset file to observe changes\\n        with open(cfg.asset_path, \"rb\") as f:\\n            while True:\\n                # read 64kb chunks to avoid memory issues for the large files!\\n                data = f.read(65536)\\n                if not data:\\n                    break\\n                md5.update(data)\\n        # return the hash\\n        return md5.hexdigest()'),\n",
       " Document(metadata={}, page_content='class AssetConverterBaseCfg:\\n    \"\"\"The base configuration class for asset converters.\"\"\"\\n\\n    asset_path: str = MISSING\\n    \"\"\"The absolute path to the asset file to convert into USD.\"\"\"\\n\\n    usd_dir: str | None = None\\n    \"\"\"The output directory path to store the generated USD file. Defaults to None.\\n\\n    If None, it is resolved as ``/tmp/IsaacLab/usd_{date}_{time}_{random}``, where\\n    the parameters in braces are runtime generated.\\n    \"\"\"\\n\\n    usd_file_name: str | None = None\\n    \"\"\"The name of the generated usd file. Defaults to None.\\n\\n    If None, it is resolved from the asset file name. For example, if the asset file\\n    name is ``\"my_asset.urdf\"``, then the generated USD file name is ``\"my_asset.usd\"``.\\n\\n    If the providing file name does not end with \".usd\" or \".usda\", then the extension\\n    \".usd\" is appended to the file name.\\n    \"\"\"\\n\\n    force_usd_conversion: bool = False\\n    \"\"\"Force the conversion of the asset file to usd. Defaults to False.\\n\\n    If True, then the USD file is always generated. It will overwrite the existing USD file if it exists.\\n    \"\"\"\\n\\n    make_instanceable: bool = True\\n    \"\"\"Make the generated USD file instanceable. Defaults to True.\\n\\n    Note:\\n        Instancing helps reduce the memory footprint of the asset when multiple copies of the asset are\\n        used in the scene. For more information, please check the USD documentation on\\n        `scene-graph instancing <https://openusd.org/dev/api/_usd__page__scenegraph_instancing.html>`_.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshConverter(AssetConverterBase):\\n    \"\"\"Converter for a mesh file in OBJ / STL / FBX format to a USD file.\\n\\n    This class wraps around the `omni.kit.asset_converter`_ extension to provide a lazy implementation\\n    for mesh to USD conversion. It stores the output USD file in an instanceable format since that is\\n    what is typically used in all learning related applications.\\n\\n    To make the asset instanceable, we must follow a certain structure dictated by how USD scene-graph\\n    instancing and physics work. The rigid body component must be added to each instance and not the\\n    referenced asset (i.e. the prototype prim itself). This is because the rigid body component defines\\n    properties that are specific to each instance and cannot be shared under the referenced asset. For\\n    more information, please check the `documentation <https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/rigid-bodies.html#instancing-rigid-bodies>`_.\\n\\n    Due to the above, we follow the following structure:\\n\\n    * ``{prim_path}`` - The root prim that is an Xform with the rigid body and mass APIs if configured.\\n    * ``{prim_path}/geometry`` - The prim that contains the mesh and optionally the materials if configured.\\n      If instancing is enabled, this prim will be an instanceable reference to the prototype prim.\\n\\n    .. _omni.kit.asset_converter: https://docs.omniverse.nvidia.com/extensions/latest/ext_asset-converter.html\\n\\n    .. caution::\\n        When converting STL files, Z-up convention is assumed, even though this is not the default for many CAD\\n        export programs. Asset orientation convention can either be modified directly in the CAD program\\'s export\\n        process or an offset can be added within the config in Isaac Lab.\\n\\n    \"\"\"\\n\\n    cfg: MeshConverterCfg\\n    \"\"\"The configuration instance for mesh to USD conversion.\"\"\"\\n\\n    def __init__(self, cfg: MeshConverterCfg):\\n        \"\"\"Initializes the class.\\n\\n        Args:\\n            cfg: The configuration instance for mesh to USD conversion.\\n        \"\"\"\\n        super().__init__(cfg=cfg)\\n\\n    \"\"\"\\n    Implementation specific methods.\\n    \"\"\"\\n\\n    def _convert_asset(self, cfg: MeshConverterCfg):\\n        \"\"\"Generate USD from OBJ, STL or FBX.\\n\\n        The USD file has Y-up axis and is scaled to meters.\\n        The asset hierarchy is arranged as follows:\\n\\n        .. code-block:: none\\n            mesh_file_basename (default prim)\\n                |- /geometry/Looks\\n                |- /geometry/mesh\\n\\n        Args:\\n            cfg: The configuration for conversion of mesh to USD.\\n\\n        Raises:\\n            RuntimeError: If the conversion using the Omniverse asset converter fails.\\n        \"\"\"\\n        # resolve mesh name and format\\n        mesh_file_basename, mesh_file_format = os.path.basename(cfg.asset_path).split(\".\")\\n        mesh_file_format = mesh_file_format.lower()\\n\\n        # Check if mesh_file_basename is a valid USD identifier\\n        if not Tf.IsValidIdentifier(mesh_file_basename):\\n            # Correct the name to a valid identifier and update the basename\\n            mesh_file_basename_original = mesh_file_basename\\n            mesh_file_basename = Tf.MakeValidIdentifier(mesh_file_basename)\\n            omni.log.warn(\\n                f\"Input file name \\'{mesh_file_basename_original}\\' is an invalid identifier for the mesh prim path.\"\\n                f\" Renaming it to \\'{mesh_file_basename}\\' for the conversion.\"\\n            )\\n\\n        # Convert USD\\n        asyncio.get_event_loop().run_until_complete(\\n            self._convert_mesh_to_usd(in_file=cfg.asset_path, out_file=self.usd_path)\\n        )\\n        # Create a new stage, set Z up and meters per unit\\n        temp_stage = Usd.Stage.CreateInMemory()\\n        UsdGeom.SetStageUpAxis(temp_stage, UsdGeom.Tokens.z)\\n        UsdGeom.SetStageMetersPerUnit(temp_stage, 1.0)\\n        UsdPhysics.SetStageKilogramsPerUnit(temp_stage, 1.0)\\n        # Add mesh to stage\\n        base_prim = temp_stage.DefinePrim(f\"/{mesh_file_basename}\", \"Xform\")\\n        prim = temp_stage.DefinePrim(f\"/{mesh_file_basename}/geometry\", \"Xform\")\\n        prim.GetReferences().AddReference(self.usd_path)\\n        temp_stage.SetDefaultPrim(base_prim)\\n        temp_stage.Export(self.usd_path)\\n\\n        # Open converted USD stage\\n        stage = Usd.Stage.Open(self.usd_path)\\n        # Need to reload the stage to get the new prim structure, otherwise it can be taken from the cache\\n        stage.Reload()\\n        # Add USD to stage cache\\n        stage_id = UsdUtils.StageCache.Get().Insert(stage)\\n        # Get the default prim (which is the root prim) -- \"/{mesh_file_basename}\"\\n        xform_prim = stage.GetDefaultPrim()\\n        geom_prim = stage.GetPrimAtPath(f\"/{mesh_file_basename}/geometry\")\\n        # Move all meshes to underneath new Xform\\n        for child_mesh_prim in geom_prim.GetChildren():\\n            if child_mesh_prim.GetTypeName() == \"Mesh\":\\n                # Apply collider properties to mesh\\n                if cfg.collision_props is not None:\\n                    # -- Collision approximation to mesh\\n                    # TODO: Move this to a new Schema: https://github.com/isaac-orbit/IsaacLab/issues/163\\n                    mesh_collision_api = UsdPhysics.MeshCollisionAPI.Apply(child_mesh_prim)\\n                    mesh_collision_api.GetApproximationAttr().Set(cfg.collision_approximation)\\n                    # -- Collider properties such as offset, scale, etc.\\n                    schemas.define_collision_properties(\\n                        prim_path=child_mesh_prim.GetPath(), cfg=cfg.collision_props, stage=stage\\n                    )\\n        # Delete the old Xform and make the new Xform the default prim\\n        stage.SetDefaultPrim(xform_prim)\\n        # Apply default Xform rotation to mesh -> enable to set rotation and scale\\n        omni.kit.commands.execute(\\n            \"CreateDefaultXformOnPrimCommand\",\\n            prim_path=xform_prim.GetPath(),\\n            **{\"stage\": stage},\\n        )\\n\\n        # Apply translation, rotation, and scale to the Xform\\n        geom_xform = UsdGeom.Xform(geom_prim)\\n        geom_xform.ClearXformOpOrder()\\n\\n        # Remove any existing rotation attributes\\n        rotate_attr = geom_prim.GetAttribute(\"xformOp:rotateXYZ\")\\n        if rotate_attr:\\n            geom_prim.RemoveProperty(rotate_attr.GetName())\\n\\n        # translation\\n        translate_op = geom_xform.AddTranslateOp(UsdGeom.XformOp.PrecisionDouble)\\n        translate_op.Set(Gf.Vec3d(*cfg.translation))\\n        # rotation\\n        orient_op = geom_xform.AddOrientOp(UsdGeom.XformOp.PrecisionDouble)\\n        orient_op.Set(Gf.Quatd(*cfg.rotation))\\n        # scale\\n        scale_op = geom_xform.AddScaleOp(UsdGeom.XformOp.PrecisionDouble)\\n        scale_op.Set(Gf.Vec3d(*cfg.scale))\\n\\n        # Handle instanceable\\n        # Create a new Xform prim that will be the prototype prim\\n        if cfg.make_instanceable:\\n            # Export Xform to a file so we can reference it from all instances\\n            export_prim_to_file(\\n                path=os.path.join(self.usd_dir, self.usd_instanceable_meshes_path),\\n                source_prim_path=geom_prim.GetPath(),\\n                stage=stage,\\n            )\\n            # Delete the original prim that will now be a reference\\n            geom_prim_path = geom_prim.GetPath().pathString\\n            omni.kit.commands.execute(\"DeletePrims\", paths=[geom_prim_path], stage=stage)\\n            # Update references to exported Xform and make it instanceable\\n            geom_undef_prim = stage.DefinePrim(geom_prim_path)\\n            geom_undef_prim.GetReferences().AddReference(self.usd_instanceable_meshes_path, primPath=geom_prim_path)\\n            geom_undef_prim.SetInstanceable(True)\\n\\n        # Apply mass and rigid body properties after everything else\\n        # Properties are applied to the top level prim to avoid the case where all instances of this\\n        #   asset unintentionally share the same rigid body properties\\n        # apply mass properties\\n        if cfg.mass_props is not None:\\n            schemas.define_mass_properties(prim_path=xform_prim.GetPath(), cfg=cfg.mass_props, stage=stage)\\n        # apply rigid body properties\\n        if cfg.rigid_props is not None:\\n            schemas.define_rigid_body_properties(prim_path=xform_prim.GetPath(), cfg=cfg.rigid_props, stage=stage)\\n\\n        # Save changes to USD stage\\n        stage.Save()\\n        if stage_id is not None:\\n            UsdUtils.StageCache.Get().Erase(stage_id)\\n\\n    \"\"\"\\n    Helper methods.\\n    \"\"\"\\n\\n    @staticmethod\\n    async def _convert_mesh_to_usd(in_file: str, out_file: str, load_materials: bool = True) -> bool:\\n        \"\"\"Convert mesh from supported file types to USD.\\n\\n        This function uses the Omniverse Asset Converter extension to convert a mesh file to USD.\\n        It is an asynchronous function and should be called using `asyncio.get_event_loop().run_until_complete()`.\\n\\n        The converted asset is stored in the USD format in the specified output file.\\n        The USD file has Y-up axis and is scaled to cm.\\n\\n        Args:\\n            in_file: The file to convert.\\n            out_file: The path to store the output file.\\n            load_materials: Set to True to enable attaching materials defined in the input file\\n                to the generated USD mesh. Defaults to True.\\n\\n        Returns:\\n            True if the conversion succeeds.\\n        \"\"\"\\n        enable_extension(\"omni.kit.asset_converter\")\\n\\n        import omni.kit.asset_converter\\n        import omni.usd\\n\\n        # Create converter context\\n        converter_context = omni.kit.asset_converter.AssetConverterContext()\\n        # Set up converter settings\\n        # Don\\'t import/export materials\\n        converter_context.ignore_materials = not load_materials\\n        converter_context.ignore_animations = True\\n        converter_context.ignore_camera = True\\n        converter_context.ignore_light = True\\n        # Merge all meshes into one\\n        converter_context.merge_all_meshes = True\\n        # Sets world units to meters, this will also scale asset if it\\'s centimeters model.\\n        # This does not work right now :(, so we need to scale the mesh manually\\n        converter_context.use_meter_as_world_unit = True\\n        converter_context.baking_scales = True\\n        # Uses double precision for all transform ops.\\n        converter_context.use_double_precision_to_usd_transform_op = True\\n\\n        # Create converter task\\n        instance = omni.kit.asset_converter.get_instance()\\n        task = instance.create_converter_task(in_file, out_file, None, converter_context)\\n        # Start conversion task and wait for it to finish\\n        success = await task.wait_until_finished()\\n        if not success:\\n            raise RuntimeError(f\"Failed to convert {in_file} to USD. Error: {task.get_error_message()}\")\\n        return success'),\n",
       " Document(metadata={}, page_content='class MeshConverterCfg(AssetConverterBaseCfg):\\n    \"\"\"The configuration class for MeshConverter.\"\"\"\\n\\n    mass_props: schemas_cfg.MassPropertiesCfg | None = None\\n    \"\"\"Mass properties to apply to the USD. Defaults to None.\\n\\n    Note:\\n        If None, then no mass properties will be added.\\n    \"\"\"\\n\\n    rigid_props: schemas_cfg.RigidBodyPropertiesCfg | None = None\\n    \"\"\"Rigid body properties to apply to the USD. Defaults to None.\\n\\n    Note:\\n        If None, then no rigid body properties will be added.\\n    \"\"\"\\n\\n    collision_props: schemas_cfg.CollisionPropertiesCfg | None = None\\n    \"\"\"Collision properties to apply to the USD. Defaults to None.\\n\\n    Note:\\n        If None, then no collision properties will be added.\\n    \"\"\"\\n\\n    collision_approximation: str = \"convexDecomposition\"\\n    \"\"\"Collision approximation method to use. Defaults to \"convexDecomposition\".\\n\\n    Valid options are:\\n    \"convexDecomposition\", \"convexHull\", \"boundingCube\",\\n    \"boundingSphere\", \"meshSimplification\", or \"none\"\\n\\n    \"none\" causes no collision mesh to be added.\\n    \"\"\"\\n\\n    translation: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n    \"\"\"The translation of the mesh to the origin. Defaults to (0.0, 0.0, 0.0).\"\"\"\\n\\n    rotation: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)\\n    \"\"\"The rotation of the mesh in quaternion format (w, x, y, z). Defaults to (1.0, 0.0, 0.0, 0.0).\"\"\"\\n\\n    scale: tuple[float, float, float] = (1.0, 1.0, 1.0)\\n    \"\"\"The scale of the mesh. Defaults to (1.0, 1.0, 1.0).\"\"\"'),\n",
       " Document(metadata={}, page_content='class MjcfConverter(AssetConverterBase):\\n    \"\"\"Converter for a MJCF description file to a USD file.\\n\\n    This class wraps around the `isaacsim.asset.importer.mjcf`_ extension to provide a lazy implementation\\n    for MJCF to USD conversion. It stores the output USD file in an instanceable format since that is\\n    what is typically used in all learning related applications.\\n\\n    .. caution::\\n        The current lazy conversion implementation does not automatically trigger USD generation if\\n        only the mesh files used by the MJCF are modified. To force generation, either set\\n        :obj:`AssetConverterBaseCfg.force_usd_conversion` to True or delete the output directory.\\n\\n    .. note::\\n        From Isaac Sim 4.5 onwards, the extension name changed from ``omni.importer.mjcf`` to\\n        ``isaacsim.asset.importer.mjcf``. This converter class now uses the latest extension from Isaac Sim.\\n\\n    .. _isaacsim.asset.importer.mjcf: https://docs.isaacsim.omniverse.nvidia.com/latest/robot_setup/ext_isaacsim_asset_importer_mjcf.html\\n    \"\"\"\\n\\n    cfg: MjcfConverterCfg\\n    \"\"\"The configuration instance for MJCF to USD conversion.\"\"\"\\n\\n    def __init__(self, cfg: MjcfConverterCfg):\\n        \"\"\"Initializes the class.\\n\\n        Args:\\n            cfg: The configuration instance for URDF to USD conversion.\\n        \"\"\"\\n        super().__init__(cfg=cfg)\\n\\n    \"\"\"\\n    Implementation specific methods.\\n    \"\"\"\\n\\n    def _convert_asset(self, cfg: MjcfConverterCfg):\\n        \"\"\"Calls underlying Omniverse command to convert MJCF to USD.\\n\\n        Args:\\n            cfg: The configuration instance for MJCF to USD conversion.\\n        \"\"\"\\n        import_config = self._get_mjcf_import_config()\\n        file_basename, _ = os.path.basename(cfg.asset_path).split(\".\")\\n        omni.kit.commands.execute(\\n            \"MJCFCreateAsset\",\\n            mjcf_path=cfg.asset_path,\\n            import_config=import_config,\\n            dest_path=self.usd_path,\\n            prim_path=f\"/{file_basename}\",\\n        )\\n\\n    def _get_mjcf_import_config(self) -> isaacsim.asset.importer.mjcf.ImportConfig:\\n        \"\"\"Returns the import configuration for MJCF to USD conversion.\\n\\n        Returns:\\n            The constructed ``ImportConfig`` object containing the desired settings.\\n        \"\"\"\\n\\n        _, import_config = omni.kit.commands.execute(\"MJCFCreateImportConfig\")\\n\\n        # set the unit scaling factor, 1.0 means meters, 100.0 means cm\\n        # import_config.set_distance_scale(1.0)\\n        # set imported robot as default prim\\n        # import_config.set_make_default_prim(True)\\n        # add a physics scene to the stage on import if none exists\\n        # import_config.set_create_physics_scene(False)\\n        # set flag to parse <site> tag\\n        import_config.set_import_sites(True)\\n\\n        # -- instancing settings\\n        # meshes will be placed in a separate usd file\\n        import_config.set_make_instanceable(self.cfg.make_instanceable)\\n        import_config.set_instanceable_usd_path(self.usd_instanceable_meshes_path)\\n\\n        # -- asset settings\\n        # default density used for links, use 0 to auto-compute\\n        import_config.set_density(self.cfg.link_density)\\n        # import inertia tensor from urdf, if it is not specified in urdf it will import as identity\\n        import_config.set_import_inertia_tensor(self.cfg.import_inertia_tensor)\\n\\n        # -- physics settings\\n        # create fix joint for base link\\n        import_config.set_fix_base(self.cfg.fix_base)\\n        # self collisions between links in the articulation\\n        import_config.set_self_collision(self.cfg.self_collision)\\n\\n        return import_config'),\n",
       " Document(metadata={}, page_content='class MjcfConverterCfg(AssetConverterBaseCfg):\\n    \"\"\"The configuration class for MjcfConverter.\"\"\"\\n\\n    link_density = 0.0\\n    \"\"\"Default density used for links. Defaults to 0.\\n\\n    This setting is only effective if ``\"inertial\"`` properties are missing in the MJCF.\\n    \"\"\"\\n\\n    import_inertia_tensor: bool = True\\n    \"\"\"Import the inertia tensor from mjcf. Defaults to True.\\n\\n    If the ``\"inertial\"`` tag is missing, then it is imported as an identity.\\n    \"\"\"\\n\\n    fix_base: bool = MISSING\\n    \"\"\"Create a fix joint to the root/base link. Defaults to True.\"\"\"\\n\\n    import_sites: bool = True\\n    \"\"\"Import the sites from the MJCF. Defaults to True.\"\"\"\\n\\n    self_collision: bool = False\\n    \"\"\"Activate self-collisions between links of the articulation. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='class UrdfConverter(AssetConverterBase):\\n    \"\"\"Converter for a URDF description file to a USD file.\\n\\n    This class wraps around the `isaacsim.asset.importer.urdf`_ extension to provide a lazy implementation\\n    for URDF to USD conversion. It stores the output USD file in an instanceable format since that is\\n    what is typically used in all learning related applications.\\n\\n    .. caution::\\n        The current lazy conversion implementation does not automatically trigger USD generation if\\n        only the mesh files used by the URDF are modified. To force generation, either set\\n        :obj:`AssetConverterBaseCfg.force_usd_conversion` to True or delete the output directory.\\n\\n    .. note::\\n        From Isaac Sim 4.5 onwards, the extension name changed from ``omni.importer.urdf`` to\\n        ``isaacsim.asset.importer.urdf``. This converter class now uses the latest extension from Isaac Sim.\\n\\n    .. _isaacsim.asset.importer.urdf: https://docs.isaacsim.omniverse.nvidia.com/latest/robot_setup/ext_isaacsim_asset_importer_urdf.html\\n    \"\"\"\\n\\n    cfg: UrdfConverterCfg\\n    \"\"\"The configuration instance for URDF to USD conversion.\"\"\"\\n\\n    def __init__(self, cfg: UrdfConverterCfg):\\n        \"\"\"Initializes the class.\\n\\n        Args:\\n            cfg: The configuration instance for URDF to USD conversion.\\n        \"\"\"\\n        manager = omni.kit.app.get_app().get_extension_manager()\\n        if not manager.is_extension_enabled(\"isaacsim.asset.importer.urdf\"):\\n            enable_extension(\"isaacsim.asset.importer.urdf\")\\n        from isaacsim.asset.importer.urdf._urdf import acquire_urdf_interface\\n\\n        self._urdf_interface = acquire_urdf_interface()\\n        super().__init__(cfg=cfg)\\n\\n    \"\"\"\\n    Implementation specific methods.\\n    \"\"\"\\n\\n    def _convert_asset(self, cfg: UrdfConverterCfg):\\n        \"\"\"Calls underlying Omniverse command to convert URDF to USD.\\n\\n        Args:\\n            cfg: The URDF conversion configuration.\\n        \"\"\"\\n\\n        import_config = self._get_urdf_import_config()\\n        # parse URDF file\\n        result, self._robot_model = omni.kit.commands.execute(\\n            \"URDFParseFile\", urdf_path=cfg.asset_path, import_config=import_config\\n        )\\n\\n        if result:\\n            if cfg.joint_drive:\\n                # modify joint parameters\\n                self._update_joint_parameters()\\n\\n            # set root link name\\n            if cfg.root_link_name:\\n                self._robot_model.root_link = cfg.root_link_name\\n\\n            # convert the model to USD\\n            omni.kit.commands.execute(\\n                \"URDFImportRobot\",\\n                urdf_path=cfg.asset_path,\\n                urdf_robot=self._robot_model,\\n                import_config=import_config,\\n                dest_path=self.usd_path,\\n            )\\n        else:\\n            raise ValueError(f\"Failed to parse URDF file: {cfg.asset_path}\")\\n\\n    \"\"\"\\n    Helper methods.\\n    \"\"\"\\n\\n    def _get_urdf_import_config(self) -> isaacsim.asset.importer.urdf.ImportConfig:\\n        \"\"\"Create and fill URDF ImportConfig with desired settings\\n\\n        Returns:\\n            The constructed ``ImportConfig`` object containing the desired settings.\\n        \"\"\"\\n        # create a new import config\\n        _, import_config = omni.kit.commands.execute(\"URDFCreateImportConfig\")\\n\\n        # set the unit scaling factor, 1.0 means meters, 100.0 means cm\\n        import_config.set_distance_scale(1.0)\\n        # set imported robot as default prim\\n        import_config.set_make_default_prim(True)\\n        # add a physics scene to the stage on import if none exists\\n        import_config.set_create_physics_scene(False)\\n\\n        # -- asset settings\\n        # default density used for links, use 0 to auto-compute\\n        import_config.set_density(self.cfg.link_density)\\n        # mesh simplification settings\\n        convex_decomp = self.cfg.collider_type == \"convex_decomposition\"\\n        import_config.set_convex_decomp(convex_decomp)\\n        # create collision geometry from visual geometry\\n        import_config.set_collision_from_visuals(self.cfg.collision_from_visuals)\\n        # consolidating links that are connected by fixed joints\\n        import_config.set_merge_fixed_joints(self.cfg.merge_fixed_joints)\\n        # -- physics settings\\n        # create fix joint for base link\\n        import_config.set_fix_base(self.cfg.fix_base)\\n        # self collisions between links in the articulation\\n        import_config.set_self_collision(self.cfg.self_collision)\\n        # convert mimic joints to normal joints\\n        import_config.set_parse_mimic(self.cfg.convert_mimic_joints_to_normal_joints)\\n        # replace cylinder shapes with capsule shapes\\n        import_config.set_replace_cylinders_with_capsules(self.cfg.replace_cylinders_with_capsules)\\n\\n        return import_config\\n\\n    def _update_joint_parameters(self):\\n        \"\"\"Update the joint parameters based on the configuration.\"\"\"\\n        # set the drive type\\n        self._set_joints_drive_type()\\n        # set the drive target type\\n        self._set_joints_drive_target_type()\\n        # set the drive gains\\n        self._set_joint_drive_gains()\\n\\n    def _set_joints_drive_type(self):\\n        \"\"\"Set the joint drive type for all joints in the URDF model.\"\"\"\\n        from isaacsim.asset.importer.urdf._urdf import UrdfJointDriveType\\n\\n        drive_type_mapping = {\\n            \"force\": UrdfJointDriveType.JOINT_DRIVE_FORCE,\\n            \"acceleration\": UrdfJointDriveType.JOINT_DRIVE_ACCELERATION,\\n        }\\n\\n        if isinstance(self.cfg.joint_drive.drive_type, str):\\n            for joint in self._robot_model.joints.values():\\n                joint.drive.set_drive_type(drive_type_mapping[self.cfg.joint_drive.drive_type])\\n        elif isinstance(self.cfg.joint_drive.drive_type, dict):\\n            for joint_name, drive_type in self.cfg.joint_drive.drive_type.items():\\n                # handle joint name being a regex\\n                matches = [s for s in self._robot_model.joints.keys() if re.search(joint_name, s)]\\n                if not matches:\\n                    raise ValueError(\\n                        f\"The joint name {joint_name} in the drive type config was not found in the URDF file. The\"\\n                        f\" joint names in the URDF are {list(self._robot_model.joints.keys())}\"\\n                    )\\n                for match in matches:\\n                    joint = self._robot_model.joints[match]\\n                    joint.drive.set_drive_type(drive_type_mapping[drive_type])\\n\\n    def _set_joints_drive_target_type(self):\\n        \"\"\"Set the joint drive target type for all joints in the URDF model.\"\"\"\\n        from isaacsim.asset.importer.urdf._urdf import UrdfJointTargetType\\n\\n        target_type_mapping = {\\n            \"none\": UrdfJointTargetType.JOINT_DRIVE_NONE,\\n            \"position\": UrdfJointTargetType.JOINT_DRIVE_POSITION,\\n            \"velocity\": UrdfJointTargetType.JOINT_DRIVE_VELOCITY,\\n        }\\n\\n        if isinstance(self.cfg.joint_drive.target_type, str):\\n            for joint in self._robot_model.joints.values():\\n                joint.drive.set_target_type(target_type_mapping[self.cfg.joint_drive.target_type])\\n        elif isinstance(self.cfg.joint_drive.target_type, dict):\\n            for joint_name, target_type in self.cfg.joint_drive.target_type.items():\\n                # handle joint name being a regex\\n                matches = [s for s in self._robot_model.joints.keys() if re.search(joint_name, s)]\\n                if not matches:\\n                    raise ValueError(\\n                        f\"The joint name {joint_name} in the target type config was not found in the URDF file. The\"\\n                        f\" joint names in the URDF are {list(self._robot_model.joints.keys())}\"\\n                    )\\n                for match in matches:\\n                    joint = self._robot_model.joints[match]\\n                    joint.drive.set_target_type(target_type_mapping[target_type])\\n\\n    def _set_joint_drive_gains(self):\\n        \"\"\"Set the joint drive gains for all joints in the URDF model.\"\"\"\\n\\n        # set the gains directly from stiffness and damping values\\n        if isinstance(self.cfg.joint_drive.gains, UrdfConverterCfg.JointDriveCfg.PDGainsCfg):\\n            # stiffness\\n            if isinstance(self.cfg.joint_drive.gains.stiffness, (float, int)):\\n                for joint in self._robot_model.joints.values():\\n                    self._set_joint_drive_stiffness(joint, self.cfg.joint_drive.gains.stiffness)\\n            elif isinstance(self.cfg.joint_drive.gains.stiffness, dict):\\n                for joint_name, stiffness in self.cfg.joint_drive.gains.stiffness.items():\\n                    # handle joint name being a regex\\n                    matches = [s for s in self._robot_model.joints.keys() if re.search(joint_name, s)]\\n                    if not matches:\\n                        raise ValueError(\\n                            f\"The joint name {joint_name} in the drive stiffness config was not found in the URDF file.\"\\n                            f\" The joint names in the URDF are {list(self._robot_model.joints.keys())}\"\\n                        )\\n                    for match in matches:\\n                        joint = self._robot_model.joints[match]\\n                        self._set_joint_drive_stiffness(joint, stiffness)\\n            # damping\\n            if isinstance(self.cfg.joint_drive.gains.damping, (float, int)):\\n                for joint in self._robot_model.joints.values():\\n                    self._set_joint_drive_damping(joint, self.cfg.joint_drive.gains.damping)\\n            elif isinstance(self.cfg.joint_drive.gains.damping, dict):\\n                for joint_name, damping in self.cfg.joint_drive.gains.damping.items():\\n                    # handle joint name being a regex\\n                    matches = [s for s in self._robot_model.joints.keys() if re.search(joint_name, s)]\\n                    if not matches:\\n                        raise ValueError(\\n                            f\"The joint name {joint_name} in the drive damping config was not found in the URDF file.\"\\n                            f\" The joint names in the URDF are {list(self._robot_model.joints.keys())}\"\\n                        )\\n                    for match in matches:\\n                        joint = self._robot_model.joints[match]\\n                        self._set_joint_drive_damping(joint, damping)\\n\\n        # set the gains from natural frequency and damping ratio\\n        elif isinstance(self.cfg.joint_drive.gains, UrdfConverterCfg.JointDriveCfg.NaturalFrequencyGainsCfg):\\n            # damping ratio\\n            if isinstance(self.cfg.joint_drive.gains.damping_ratio, (float, int)):\\n                for joint in self._robot_model.joints.values():\\n                    joint.drive.damping_ratio = self.cfg.joint_drive.gains.damping_ratio\\n            elif isinstance(self.cfg.joint_drive.gains.damping_ratio, dict):\\n                for joint_name, damping_ratio in self.cfg.joint_drive.gains.damping_ratio.items():\\n                    # handle joint name being a regex\\n                    matches = [s for s in self._robot_model.joints.keys() if re.search(joint_name, s)]\\n                    if not matches:\\n                        raise ValueError(\\n                            f\"The joint name {joint_name} in the damping ratio config was not found in the URDF file.\"\\n                            f\" The joint names in the URDF are {list(self._robot_model.joints.keys())}\"\\n                        )\\n                    for match in matches:\\n                        joint = self._robot_model.joints[match]\\n                        joint.drive.damping_ratio = damping_ratio\\n\\n            # natural frequency (this has to be done after damping ratio is set)\\n            if isinstance(self.cfg.joint_drive.gains.natural_frequency, (float, int)):\\n                for joint in self._robot_model.joints.values():\\n                    joint.drive.natural_frequency = self.cfg.joint_drive.gains.natural_frequency\\n                    self._set_joint_drive_gains_from_natural_frequency(joint)\\n            elif isinstance(self.cfg.joint_drive.gains.natural_frequency, dict):\\n                for joint_name, natural_frequency in self.cfg.joint_drive.gains.natural_frequency.items():\\n                    # handle joint name being a regex\\n                    matches = [s for s in self._robot_model.joints.keys() if re.search(joint_name, s)]\\n                    if not matches:\\n                        raise ValueError(\\n                            f\"The joint name {joint_name} in the natural frequency config was not found in the URDF\"\\n                            f\" file. The joint names in the URDF are {list(self._robot_model.joints.keys())}\"\\n                        )\\n                    for match in matches:\\n                        joint = self._robot_model.joints[match]\\n                        joint.drive.natural_frequency = natural_frequency\\n                        self._set_joint_drive_gains_from_natural_frequency(joint)\\n\\n    def _set_joint_drive_stiffness(self, joint, stiffness: float):\\n        \"\"\"Set the joint drive stiffness.\\n\\n        Args:\\n            joint: The joint from the URDF robot model.\\n            stiffness: The stiffness value.\\n        \"\"\"\\n        from isaacsim.asset.importer.urdf._urdf import UrdfJointType\\n\\n        if joint.type == UrdfJointType.JOINT_PRISMATIC:\\n            joint.drive.set_strength(stiffness)\\n        else:\\n            # we need to convert the stiffness from radians to degrees\\n            joint.drive.set_strength(math.pi / 180 * stiffness)\\n\\n    def _set_joint_drive_damping(self, joint, damping: float):\\n        \"\"\"Set the joint drive damping.\\n\\n        Args:\\n            joint: The joint from the URDF robot model.\\n            damping: The damping value.\\n        \"\"\"\\n        from isaacsim.asset.importer.urdf._urdf import UrdfJointType\\n\\n        if joint.type == UrdfJointType.JOINT_PRISMATIC:\\n            joint.drive.set_damping(damping)\\n        else:\\n            # we need to convert the damping from radians to degrees\\n            joint.drive.set_damping(math.pi / 180 * damping)\\n\\n    def _set_joint_drive_gains_from_natural_frequency(self, joint):\\n        \"\"\"Compute the joint drive gains from the natural frequency and damping ratio.\\n\\n        Args:\\n            joint: The joint from the URDF robot model.\\n        \"\"\"\\n        from isaacsim.asset.importer.urdf._urdf import UrdfJointDriveType, UrdfJointTargetType\\n\\n        strength = self._urdf_interface.compute_natural_stiffness(\\n            self._robot_model,\\n            joint.name,\\n            joint.drive.natural_frequency,\\n        )\\n        self._set_joint_drive_stiffness(joint, strength)\\n\\n        if joint.drive.target_type == UrdfJointTargetType.JOINT_DRIVE_POSITION:\\n            m_eq = 1.0\\n            if joint.drive.drive_type == UrdfJointDriveType.JOINT_DRIVE_FORCE:\\n                m_eq = joint.inertia\\n            damping = 2 * m_eq * joint.drive.natural_frequency * joint.drive.damping_ratio\\n            self._set_joint_drive_damping(joint, damping)'),\n",
       " Document(metadata={}, page_content='class UrdfConverterCfg(AssetConverterBaseCfg):\\n    \"\"\"The configuration class for UrdfConverter.\"\"\"\\n\\n    @configclass\\n    class JointDriveCfg:\\n        \"\"\"Configuration for the joint drive.\"\"\"\\n\\n        @configclass\\n        class PDGainsCfg:\\n            \"\"\"Configuration for the PD gains of the drive.\"\"\"\\n\\n            stiffness: dict[str, float] | float = MISSING\\n            \"\"\"The stiffness of the joint drive in Nm/rad or N/rad.\\n\\n            If None, the stiffness is set to the value parsed from the URDF file.\\n            If :attr:`~UrdfConverterCfg.JointDriveCfg.target_type` is set to ``\"velocity\"``, this value determines\\n            the drive strength in joint velocity space.\\n            \"\"\"\\n\\n            damping: dict[str, float] | float | None = None\\n            \"\"\"The damping of the joint drive in Nm/(rad/s) or N/(rad/s). Defaults to None.\\n\\n            If None, the damping is set to the value parsed from the URDF file or 0.0 if no value is found in the URDF.\\n            If :attr:`~UrdfConverterCfg.JointDriveCfg.target_type` is set to ``\"velocity\"``, this attribute is set to\\n            0.0 and :attr:`stiffness` serves as the drive\\'s strength in joint velocity space.\\n            \"\"\"\\n\\n        @configclass\\n        class NaturalFrequencyGainsCfg:\\n            r\"\"\"Configuration for the natural frequency gains of the drive.\\n\\n            Computes the joint drive stiffness and damping based on the desired natural frequency using the formula:\\n\\n            :math:`P = m \\\\cdot f^2`, :math:`D = 2 \\\\cdot r \\\\cdot f \\\\cdot m`\\n\\n            where :math:`f` is the natural frequency, :math:`r` is the damping ratio, and :math:`m` is the total\\n            equivalent inertia at the joint. The damping ratio is such that:\\n\\n            * :math:`r = 1.0` is a critically damped system,\\n            * :math:`r < 1.0` is underdamped,\\n            * :math:`r > 1.0` is overdamped.\\n            \"\"\"\\n\\n            natural_frequency: dict[str, float] | float = MISSING\\n            \"\"\"The natural frequency of the joint drive.\\n\\n            If :attr:`~UrdfConverterCfg.JointDriveCfg.target_type` is set to ``\"velocity\"``, this value determines the\\n            drive\\'s natural frequency in joint velocity space.\\n            \"\"\"\\n\\n            damping_ratio: dict[str, float] | float = 0.005\\n            \"\"\"The damping ratio of the joint drive. Defaults to 0.005.\\n\\n            If :attr:`~UrdfConverterCfg.JointDriveCfg.target_type` is set to ``\"velocity\"``, this value is ignored and\\n            only :attr:`natural_frequency` is used.\\n            \"\"\"\\n\\n        drive_type: dict[str, Literal[\"acceleration\", \"force\"]] | Literal[\"acceleration\", \"force\"] = \"force\"\\n        \"\"\"The drive type used for the joint. Defaults to ``\"force\"``.\\n\\n        * ``\"acceleration\"``: The joint drive normalizes the inertia before applying the joint effort so it\\'s invariant\\n          to inertia and mass changes (equivalent to ideal damped oscillator).\\n        * ``\"force\"``: Applies effort through forces, so is subject to variations on the body inertia.\\n        \"\"\"\\n\\n        target_type: dict[str, Literal[\"none\", \"position\", \"velocity\"]] | Literal[\"none\", \"position\", \"velocity\"] = (\\n            \"position\"\\n        )\\n        \"\"\"The drive target type used for the joint. Defaults to ``\"position\"``.\\n\\n        If the target type is set to ``\"none\"``, the joint stiffness and damping are set to 0.0.\\n        \"\"\"\\n\\n        gains: PDGainsCfg | NaturalFrequencyGainsCfg = PDGainsCfg()\\n        \"\"\"The drive gains configuration.\"\"\"\\n\\n    fix_base: bool = MISSING\\n    \"\"\"Create a fix joint to the root/base link.\"\"\"\\n\\n    root_link_name: str | None = None\\n    \"\"\"The name of the root link. Defaults to None.\\n\\n    If None, the root link will be set by PhysX.\\n    \"\"\"\\n\\n    link_density: float = 0.0\\n    \"\"\"Default density in ``kg/m^3`` for links whose ``\"inertial\"`` properties are missing in the URDF. Defaults to 0.0.\"\"\"\\n\\n    merge_fixed_joints: bool = True\\n    \"\"\"Consolidate links that are connected by fixed joints. Defaults to True.\"\"\"\\n\\n    convert_mimic_joints_to_normal_joints: bool = False\\n    \"\"\"Convert mimic joints to normal joints. Defaults to False.\"\"\"\\n\\n    joint_drive: JointDriveCfg | None = JointDriveCfg()\\n    \"\"\"The joint drive settings. Defaults to :class:`JointDriveCfg`.\\n\\n    The parameter can be set to ``None`` for URDFs without joints.\\n    \"\"\"\\n\\n    collision_from_visuals = False\\n    \"\"\"Whether to create collision geometry from visual geometry. Defaults to False.\"\"\"\\n\\n    collider_type: Literal[\"convex_hull\", \"convex_decomposition\"] = \"convex_hull\"\\n    \"\"\"The collision shape simplification. Defaults to \"convex_hull\".\\n\\n    Supported values are:\\n\\n    * ``\"convex_hull\"``: The collision shape is simplified to a convex hull.\\n    * ``\"convex_decomposition\"``: The collision shape is decomposed into smaller convex shapes for a closer fit.\\n    \"\"\"\\n\\n    self_collision: bool = False\\n    \"\"\"Activate self-collisions between links of the articulation. Defaults to False.\"\"\"\\n\\n    replace_cylinders_with_capsules: bool = False\\n    \"\"\"Replace cylinder shapes with capsule shapes. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='def define_articulation_root_properties(\\n    prim_path: str, cfg: schemas_cfg.ArticulationRootPropertiesCfg, stage: Usd.Stage | None = None\\n):\\n    \"\"\"Apply the articulation root schema on the input prim and set its properties.\\n\\n    See :func:`modify_articulation_root_properties` for more details on how the properties are set.\\n\\n    Args:\\n        prim_path: The prim path where to apply the articulation root schema.\\n        cfg: The configuration for the articulation root.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Raises:\\n        ValueError: When the prim path is not valid.\\n        TypeError: When the prim already has conflicting API schemas.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get articulation USD prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim path is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not valid.\")\\n    # check if prim has articulation applied on it\\n    if not UsdPhysics.ArticulationRootAPI(prim):\\n        UsdPhysics.ArticulationRootAPI.Apply(prim)\\n    # set articulation root properties\\n    modify_articulation_root_properties(prim_path, cfg, stage)'),\n",
       " Document(metadata={}, page_content='def modify_articulation_root_properties(\\n    prim_path: str, cfg: schemas_cfg.ArticulationRootPropertiesCfg, stage: Usd.Stage | None = None\\n) -> bool:\\n    \"\"\"Modify PhysX parameters for an articulation root prim.\\n\\n    The `articulation root`_ marks the root of an articulation tree. For floating articulations, this should be on\\n    the root body. For fixed articulations, this API can be on a direct or indirect parent of the root joint\\n    which is fixed to the world.\\n\\n    The schema comprises of attributes that belong to the `ArticulationRootAPI`_ and `PhysxArticulationAPI`_.\\n    schemas. The latter contains the PhysX parameters for the articulation root.\\n\\n    The properties are applied to the articulation root prim. The common properties (such as solver position\\n    and velocity iteration counts, sleep threshold, stabilization threshold) take precedence over those specified\\n    in the rigid body schemas for all the rigid bodies in the articulation.\\n\\n    .. caution::\\n        When the attribute :attr:`schemas_cfg.ArticulationRootPropertiesCfg.fix_root_link` is set to True,\\n        a fixed joint is created between the root link and the world frame (if it does not already exist). However,\\n        to deal with physics parser limitations, the articulation root schema needs to be applied to the parent of\\n        the root link.\\n\\n    .. note::\\n        This function is decorated with :func:`apply_nested` that set the properties to all the prims\\n        (that have the schema applied on them) under the input prim path.\\n\\n    .. _articulation root: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/Articulations.html\\n    .. _ArticulationRootAPI: https://openusd.org/dev/api/class_usd_physics_articulation_root_a_p_i.html\\n    .. _PhysxArticulationAPI: https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/104.2/class_physx_schema_physx_articulation_a_p_i.html\\n\\n    Args:\\n        prim_path: The prim path to the articulation root.\\n        cfg: The configuration for the articulation root.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Returns:\\n        True if the properties were successfully set, False otherwise.\\n\\n    Raises:\\n        NotImplementedError: When the root prim is not a rigid body and a fixed joint is to be created.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get articulation USD prim\\n    articulation_prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim has articulation applied on it\\n    if not UsdPhysics.ArticulationRootAPI(articulation_prim):\\n        return False\\n    # retrieve the articulation api\\n    physx_articulation_api = PhysxSchema.PhysxArticulationAPI(articulation_prim)\\n    if not physx_articulation_api:\\n        physx_articulation_api = PhysxSchema.PhysxArticulationAPI.Apply(articulation_prim)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    # extract non-USD properties\\n    fix_root_link = cfg.pop(\"fix_root_link\", None)\\n\\n    # set into physx api\\n    for attr_name, value in cfg.items():\\n        safe_set_attribute_on_usd_schema(physx_articulation_api, attr_name, value, camel_case=True)\\n\\n    # fix root link based on input\\n    # we do the fixed joint processing later to not interfere with setting other properties\\n    if fix_root_link is not None:\\n        # check if a global fixed joint exists under the root prim\\n        existing_fixed_joint_prim = find_global_fixed_joint_prim(prim_path)\\n\\n        # if we found a fixed joint, enable/disable it based on the input\\n        # otherwise, create a fixed joint between the world and the root link\\n        if existing_fixed_joint_prim is not None:\\n            omni.log.info(\\n                f\"Found an existing fixed joint for the articulation: \\'{prim_path}\\'. Setting it to: {fix_root_link}.\"\\n            )\\n            existing_fixed_joint_prim.GetJointEnabledAttr().Set(fix_root_link)\\n        elif fix_root_link:\\n            omni.log.info(f\"Creating a fixed joint for the articulation: \\'{prim_path}\\'.\")\\n\\n            # note: we have to assume that the root prim is a rigid body,\\n            #   i.e. we don\\'t handle the case where the root prim is not a rigid body but has articulation api on it\\n            # Currently, there is no obvious way to get first rigid body link identified by the PhysX parser\\n            if not articulation_prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n                raise NotImplementedError(\\n                    f\"The articulation prim \\'{prim_path}\\' does not have the RigidBodyAPI applied.\"\\n                    \" To create a fixed joint, we need to determine the first rigid body link in\"\\n                    \" the articulation tree. However, this is not implemented yet.\"\\n                )\\n\\n            # create a fixed joint between the root link and the world frame\\n            physx_utils.createJoint(stage=stage, joint_type=\"Fixed\", from_prim=None, to_prim=articulation_prim)\\n\\n            # Having a fixed joint on a rigid body is not treated as \"fixed base articulation\".\\n            # instead, it is treated as a part of the maximal coordinate tree.\\n            # Moving the articulation root to the parent solves this issue. This is a limitation of the PhysX parser.\\n            # get parent prim\\n            parent_prim = articulation_prim.GetParent()\\n            # apply api to parent\\n            UsdPhysics.ArticulationRootAPI.Apply(parent_prim)\\n            PhysxSchema.PhysxArticulationAPI.Apply(parent_prim)\\n\\n            # copy the attributes\\n            # -- usd attributes\\n            usd_articulation_api = UsdPhysics.ArticulationRootAPI(articulation_prim)\\n            for attr_name in usd_articulation_api.GetSchemaAttributeNames():\\n                attr = articulation_prim.GetAttribute(attr_name)\\n                parent_prim.GetAttribute(attr_name).Set(attr.Get())\\n            # -- physx attributes\\n            physx_articulation_api = PhysxSchema.PhysxArticulationAPI(articulation_prim)\\n            for attr_name in physx_articulation_api.GetSchemaAttributeNames():\\n                attr = articulation_prim.GetAttribute(attr_name)\\n                parent_prim.GetAttribute(attr_name).Set(attr.Get())\\n\\n            # remove api from root\\n            articulation_prim.RemoveAPI(UsdPhysics.ArticulationRootAPI)\\n            articulation_prim.RemoveAPI(PhysxSchema.PhysxArticulationAPI)\\n\\n    # success\\n    return True'),\n",
       " Document(metadata={}, page_content='def define_rigid_body_properties(\\n    prim_path: str, cfg: schemas_cfg.RigidBodyPropertiesCfg, stage: Usd.Stage | None = None\\n):\\n    \"\"\"Apply the rigid body schema on the input prim and set its properties.\\n\\n    See :func:`modify_rigid_body_properties` for more details on how the properties are set.\\n\\n    Args:\\n        prim_path: The prim path where to apply the rigid body schema.\\n        cfg: The configuration for the rigid body.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Raises:\\n        ValueError: When the prim path is not valid.\\n        TypeError: When the prim already has conflicting API schemas.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim path is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not valid.\")\\n    # check if prim has rigid body applied on it\\n    if not UsdPhysics.RigidBodyAPI(prim):\\n        UsdPhysics.RigidBodyAPI.Apply(prim)\\n    # set rigid body properties\\n    modify_rigid_body_properties(prim_path, cfg, stage)'),\n",
       " Document(metadata={}, page_content='def modify_rigid_body_properties(\\n    prim_path: str, cfg: schemas_cfg.RigidBodyPropertiesCfg, stage: Usd.Stage | None = None\\n) -> bool:\\n    \"\"\"Modify PhysX parameters for a rigid body prim.\\n\\n    A `rigid body`_ is a single body that can be simulated by PhysX. It can be either dynamic or kinematic.\\n    A dynamic body responds to forces and collisions. A `kinematic body`_ can be moved by the user, but does not\\n    respond to forces. They are similar to having static bodies that can be moved around.\\n\\n    The schema comprises of attributes that belong to the `RigidBodyAPI`_ and `PhysxRigidBodyAPI`_.\\n    schemas. The latter contains the PhysX parameters for the rigid body.\\n\\n    .. note::\\n        This function is decorated with :func:`apply_nested` that sets the properties to all the prims\\n        (that have the schema applied on them) under the input prim path.\\n\\n    .. _rigid body: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/RigidBodyOverview.html\\n    .. _kinematic body: https://openusd.org/release/wp_rigid_body_physics.html#kinematic-bodies\\n    .. _RigidBodyAPI: https://openusd.org/dev/api/class_usd_physics_rigid_body_a_p_i.html\\n    .. _PhysxRigidBodyAPI: https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/104.2/class_physx_schema_physx_rigid_body_a_p_i.html\\n\\n    Args:\\n        prim_path: The prim path to the rigid body.\\n        cfg: The configuration for the rigid body.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Returns:\\n        True if the properties were successfully set, False otherwise.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get rigid-body USD prim\\n    rigid_body_prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim has rigid-body applied on it\\n    if not UsdPhysics.RigidBodyAPI(rigid_body_prim):\\n        return False\\n    # retrieve the USD rigid-body api\\n    usd_rigid_body_api = UsdPhysics.RigidBodyAPI(rigid_body_prim)\\n    # retrieve the physx rigid-body api\\n    physx_rigid_body_api = PhysxSchema.PhysxRigidBodyAPI(rigid_body_prim)\\n    if not physx_rigid_body_api:\\n        physx_rigid_body_api = PhysxSchema.PhysxRigidBodyAPI.Apply(rigid_body_prim)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    # set into USD API\\n    for attr_name in [\"rigid_body_enabled\", \"kinematic_enabled\"]:\\n        value = cfg.pop(attr_name, None)\\n        safe_set_attribute_on_usd_schema(usd_rigid_body_api, attr_name, value, camel_case=True)\\n    # set into PhysX API\\n    for attr_name, value in cfg.items():\\n        safe_set_attribute_on_usd_schema(physx_rigid_body_api, attr_name, value, camel_case=True)\\n    # success\\n    return True'),\n",
       " Document(metadata={}, page_content='def define_collision_properties(\\n    prim_path: str, cfg: schemas_cfg.CollisionPropertiesCfg, stage: Usd.Stage | None = None\\n):\\n    \"\"\"Apply the collision schema on the input prim and set its properties.\\n\\n    See :func:`modify_collision_properties` for more details on how the properties are set.\\n\\n    Args:\\n        prim_path: The prim path where to apply the rigid body schema.\\n        cfg: The configuration for the collider.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Raises:\\n        ValueError: When the prim path is not valid.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim path is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not valid.\")\\n    # check if prim has collision applied on it\\n    if not UsdPhysics.CollisionAPI(prim):\\n        UsdPhysics.CollisionAPI.Apply(prim)\\n    # set collision properties\\n    modify_collision_properties(prim_path, cfg, stage)'),\n",
       " Document(metadata={}, page_content='def modify_collision_properties(\\n    prim_path: str, cfg: schemas_cfg.CollisionPropertiesCfg, stage: Usd.Stage | None = None\\n) -> bool:\\n    \"\"\"Modify PhysX properties of collider prim.\\n\\n    These properties are based on the `UsdPhysics.CollisionAPI`_ and `PhysxSchema.PhysxCollisionAPI`_ schemas.\\n    For more information on the properties, please refer to the official documentation.\\n\\n    Tuning these parameters influence the contact behavior of the rigid body. For more information on\\n    tune them and their effect on the simulation, please refer to the\\n    `PhysX documentation <https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/AdvancedCollisionDetection.html>`__.\\n\\n    .. note::\\n        This function is decorated with :func:`apply_nested` that sets the properties to all the prims\\n        (that have the schema applied on them) under the input prim path.\\n\\n    .. _UsdPhysics.CollisionAPI: https://openusd.org/dev/api/class_usd_physics_collision_a_p_i.html\\n    .. _PhysxSchema.PhysxCollisionAPI: https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/104.2/class_physx_schema_physx_collision_a_p_i.html\\n\\n    Args:\\n        prim_path: The prim path of parent.\\n        cfg: The configuration for the collider.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Returns:\\n        True if the properties were successfully set, False otherwise.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    collider_prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim has collision applied on it\\n    if not UsdPhysics.CollisionAPI(collider_prim):\\n        return False\\n    # retrieve the USD collision api\\n    usd_collision_api = UsdPhysics.CollisionAPI(collider_prim)\\n    # retrieve the collision api\\n    physx_collision_api = PhysxSchema.PhysxCollisionAPI(collider_prim)\\n    if not physx_collision_api:\\n        physx_collision_api = PhysxSchema.PhysxCollisionAPI.Apply(collider_prim)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    # set into USD API\\n    for attr_name in [\"collision_enabled\"]:\\n        value = cfg.pop(attr_name, None)\\n        safe_set_attribute_on_usd_schema(usd_collision_api, attr_name, value, camel_case=True)\\n    # set into PhysX API\\n    for attr_name, value in cfg.items():\\n        safe_set_attribute_on_usd_schema(physx_collision_api, attr_name, value, camel_case=True)\\n    # success\\n    return True'),\n",
       " Document(metadata={}, page_content='def define_mass_properties(prim_path: str, cfg: schemas_cfg.MassPropertiesCfg, stage: Usd.Stage | None = None):\\n    \"\"\"Apply the mass schema on the input prim and set its properties.\\n\\n    See :func:`modify_mass_properties` for more details on how the properties are set.\\n\\n    Args:\\n        prim_path: The prim path where to apply the rigid body schema.\\n        cfg: The configuration for the mass properties.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Raises:\\n        ValueError: When the prim path is not valid.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim path is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not valid.\")\\n    # check if prim has mass applied on it\\n    if not UsdPhysics.MassAPI(prim):\\n        UsdPhysics.MassAPI.Apply(prim)\\n    # set mass properties\\n    modify_mass_properties(prim_path, cfg, stage)'),\n",
       " Document(metadata={}, page_content='def modify_mass_properties(prim_path: str, cfg: schemas_cfg.MassPropertiesCfg, stage: Usd.Stage | None = None) -> bool:\\n    \"\"\"Set properties for the mass of a rigid body prim.\\n\\n    These properties are based on the `UsdPhysics.MassAPI` schema. If the mass is not defined, the density is used\\n    to compute the mass. However, in that case, a collision approximation of the rigid body is used to\\n    compute the density. For more information on the properties, please refer to the\\n    `documentation <https://openusd.org/release/wp_rigid_body_physics.html#body-mass-properties>`__.\\n\\n    .. caution::\\n\\n        The mass of an object can be specified in multiple ways and have several conflicting settings\\n        that are resolved based on precedence. Please make sure to understand the precedence rules\\n        before using this property.\\n\\n    .. note::\\n        This function is decorated with :func:`apply_nested` that sets the properties to all the prims\\n        (that have the schema applied on them) under the input prim path.\\n\\n    .. UsdPhysics.MassAPI: https://openusd.org/dev/api/class_usd_physics_mass_a_p_i.html\\n\\n    Args:\\n        prim_path: The prim path of the rigid body.\\n        cfg: The configuration for the mass properties.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Returns:\\n        True if the properties were successfully set, False otherwise.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    rigid_prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim has mass API applied on it\\n    if not UsdPhysics.MassAPI(rigid_prim):\\n        return False\\n    # retrieve the USD mass api\\n    usd_physics_mass_api = UsdPhysics.MassAPI(rigid_prim)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    # set into USD API\\n    for attr_name in [\"mass\", \"density\"]:\\n        value = cfg.pop(attr_name, None)\\n        safe_set_attribute_on_usd_schema(usd_physics_mass_api, attr_name, value, camel_case=True)\\n    # success\\n    return True'),\n",
       " Document(metadata={}, page_content='def activate_contact_sensors(prim_path: str, threshold: float = 0.0, stage: Usd.Stage = None):\\n    \"\"\"Activate the contact sensor on all rigid bodies under a specified prim path.\\n\\n    This function adds the PhysX contact report API to all rigid bodies under the specified prim path.\\n    It also sets the force threshold beyond which the contact sensor reports the contact. The contact\\n    reporting API can only be added to rigid bodies.\\n\\n    Args:\\n        prim_path: The prim path under which to search and prepare contact sensors.\\n        threshold: The threshold for the contact sensor. Defaults to 0.0.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Raises:\\n        ValueError: If the input prim path is not valid.\\n        ValueError: If there are no rigid bodies under the prim path.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get prim\\n    prim: Usd.Prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not valid.\")\\n    # iterate over all children\\n    num_contact_sensors = 0\\n    all_prims = [prim]\\n    while len(all_prims) > 0:\\n        # get current prim\\n        child_prim = all_prims.pop(0)\\n        # check if prim is a rigid body\\n        # nested rigid bodies are not allowed by SDK so we can safely assume that\\n        # if a prim has a rigid body API, it is a rigid body and we don\\'t need to\\n        # check its children\\n        if child_prim.HasAPI(UsdPhysics.RigidBodyAPI):\\n            # set sleep threshold to zero\\n            rb = PhysxSchema.PhysxRigidBodyAPI.Get(stage, prim.GetPrimPath())\\n            rb.CreateSleepThresholdAttr().Set(0.0)\\n            # add contact report API with threshold of zero\\n            if not child_prim.HasAPI(PhysxSchema.PhysxContactReportAPI):\\n                omni.log.verbose(f\"Adding contact report API to prim: \\'{child_prim.GetPrimPath()}\\'\")\\n                cr_api = PhysxSchema.PhysxContactReportAPI.Apply(child_prim)\\n            else:\\n                omni.log.verbose(f\"Contact report API already exists on prim: \\'{child_prim.GetPrimPath()}\\'\")\\n                cr_api = PhysxSchema.PhysxContactReportAPI.Get(stage, child_prim.GetPrimPath())\\n            # set threshold to zero\\n            cr_api.CreateThresholdAttr().Set(threshold)\\n            # increment number of contact sensors\\n            num_contact_sensors += 1\\n        else:\\n            # add all children to tree\\n            all_prims += child_prim.GetChildren()\\n    # check if no contact sensors were found\\n    if num_contact_sensors == 0:\\n        raise ValueError(\\n            f\"No contact sensors added to the prim: \\'{prim_path}\\'. This means that no rigid bodies\"\\n            \" are present under this prim. Please check the prim path.\"\\n        )\\n    # success\\n    return True'),\n",
       " Document(metadata={}, page_content='def modify_joint_drive_properties(\\n    prim_path: str, cfg: schemas_cfg.JointDrivePropertiesCfg, stage: Usd.Stage | None = None\\n) -> bool:\\n    \"\"\"Modify PhysX parameters for a joint prim.\\n\\n    This function checks if the input prim is a prismatic or revolute joint and applies the joint drive schema\\n    on it. If the joint is a tendon (i.e., it has the `PhysxTendonAxisAPI`_ schema applied on it), then the joint\\n    drive schema is not applied.\\n\\n    Based on the configuration, this method modifies the properties of the joint drive. These properties are\\n    based on the `UsdPhysics.DriveAPI`_ schema. For more information on the properties, please refer to the\\n    official documentation.\\n\\n    .. caution::\\n\\n        We highly recommend modifying joint properties of articulations through the functionalities in the\\n        :mod:`isaaclab.actuators` module. The methods here are for setting simulation low-level\\n        properties only.\\n\\n    .. _UsdPhysics.DriveAPI: https://openusd.org/dev/api/class_usd_physics_drive_a_p_i.html\\n    .. _PhysxTendonAxisAPI: https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/104.2/class_physx_schema_physx_tendon_axis_a_p_i.html\\n\\n    Args:\\n        prim_path: The prim path where to apply the joint drive schema.\\n        cfg: The configuration for the joint drive.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Returns:\\n        True if the properties were successfully set, False otherwise.\\n\\n    Raises:\\n        ValueError: If the input prim path is not valid.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim path is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not valid.\")\\n\\n    # check if prim has joint drive applied on it\\n    if prim.IsA(UsdPhysics.RevoluteJoint):\\n        drive_api_name = \"angular\"\\n    elif prim.IsA(UsdPhysics.PrismaticJoint):\\n        drive_api_name = \"linear\"\\n    else:\\n        return False\\n    # check that prim is not a tendon child prim\\n    # note: root prim is what \"controls\" the tendon so we still want to apply the drive to it\\n    if prim.HasAPI(PhysxSchema.PhysxTendonAxisAPI) and not prim.HasAPI(PhysxSchema.PhysxTendonAxisRootAPI):\\n        return False\\n\\n    # check if prim has joint drive applied on it\\n    usd_drive_api = UsdPhysics.DriveAPI(prim, drive_api_name)\\n    if not usd_drive_api:\\n        usd_drive_api = UsdPhysics.DriveAPI.Apply(prim, drive_api_name)\\n    # check if prim has Physx joint drive applied on it\\n    physx_joint_api = PhysxSchema.PhysxJointAPI(prim)\\n    if not physx_joint_api:\\n        physx_joint_api = PhysxSchema.PhysxJointAPI.Apply(prim)\\n\\n    # mapping from configuration name to USD attribute name\\n    cfg_to_usd_map = {\\n        \"max_velocity\": \"max_joint_velocity\",\\n        \"max_effort\": \"max_force\",\\n        \"drive_type\": \"type\",\\n    }\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n\\n    # check if linear drive\\n    is_linear_drive = prim.IsA(UsdPhysics.PrismaticJoint)\\n    # convert values for angular drives from radians to degrees units\\n    if not is_linear_drive:\\n        if cfg[\"max_velocity\"] is not None:\\n            # rad / s --> deg / s\\n            cfg[\"max_velocity\"] = cfg[\"max_velocity\"] * 180.0 / math.pi\\n        if cfg[\"stiffness\"] is not None:\\n            # N-m/rad --> N-m/deg\\n            cfg[\"stiffness\"] = cfg[\"stiffness\"] * math.pi / 180.0\\n        if cfg[\"damping\"] is not None:\\n            # N-m-s/rad --> N-m-s/deg\\n            cfg[\"damping\"] = cfg[\"damping\"] * math.pi / 180.0\\n\\n    # set into PhysX API\\n    for attr_name in [\"max_velocity\"]:\\n        value = cfg.pop(attr_name, None)\\n        attr_name = cfg_to_usd_map[attr_name]\\n        safe_set_attribute_on_usd_schema(physx_joint_api, attr_name, value, camel_case=True)\\n    # set into USD API\\n    for attr_name, attr_value in cfg.items():\\n        attr_name = cfg_to_usd_map.get(attr_name, attr_name)\\n        safe_set_attribute_on_usd_schema(usd_drive_api, attr_name, attr_value, camel_case=True)\\n\\n    return True'),\n",
       " Document(metadata={}, page_content='def modify_fixed_tendon_properties(\\n    prim_path: str, cfg: schemas_cfg.FixedTendonPropertiesCfg, stage: Usd.Stage | None = None\\n) -> bool:\\n    \"\"\"Modify PhysX parameters for a fixed tendon attachment prim.\\n\\n    A `fixed tendon`_ can be used to link multiple degrees of freedom of articulation joints\\n    through length and limit constraints. For instance, it can be used to set up an equality constraint\\n    between a driven and passive revolute joints.\\n\\n    The schema comprises of attributes that belong to the `PhysxTendonAxisRootAPI`_ schema.\\n\\n    .. note::\\n        This function is decorated with :func:`apply_nested` that sets the properties to all the prims\\n        (that have the schema applied on them) under the input prim path.\\n\\n    .. _fixed tendon: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/_api_build/classPxArticulationFixedTendon.html\\n    .. _PhysxTendonAxisRootAPI: https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/104.2/class_physx_schema_physx_tendon_axis_root_a_p_i.html\\n\\n    Args:\\n        prim_path: The prim path to the tendon attachment.\\n        cfg: The configuration for the tendon attachment.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Returns:\\n        True if the properties were successfully set, False otherwise.\\n\\n    Raises:\\n        ValueError: If the input prim path is not valid.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    tendon_prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim has fixed tendon applied on it\\n    has_root_fixed_tendon = tendon_prim.HasAPI(PhysxSchema.PhysxTendonAxisRootAPI)\\n    if not has_root_fixed_tendon:\\n        return False\\n\\n    # resolve all available instances of the schema since it is multi-instance\\n    for schema_name in tendon_prim.GetAppliedSchemas():\\n        # only consider the fixed tendon schema\\n        if \"PhysxTendonAxisRootAPI\" not in schema_name:\\n            continue\\n        # retrieve the USD tendon api\\n        instance_name = schema_name.split(\":\")[-1]\\n        physx_tendon_axis_api = PhysxSchema.PhysxTendonAxisRootAPI(tendon_prim, instance_name)\\n\\n        # convert to dict\\n        cfg = cfg.to_dict()\\n        # set into PhysX API\\n        for attr_name, value in cfg.items():\\n            safe_set_attribute_on_usd_schema(physx_tendon_axis_api, attr_name, value, camel_case=True)\\n    # success\\n    return True'),\n",
       " Document(metadata={}, page_content='def define_deformable_body_properties(\\n    prim_path: str, cfg: schemas_cfg.DeformableBodyPropertiesCfg, stage: Usd.Stage | None = None\\n):\\n    \"\"\"Apply the deformable body schema on the input prim and set its properties.\\n\\n    See :func:`modify_deformable_body_properties` for more details on how the properties are set.\\n\\n    .. note::\\n        If the input prim is not a mesh, this function will traverse the prim and find the first mesh\\n        under it. If no mesh or multiple meshes are found, an error is raised. This is because the deformable\\n        body schema can only be applied to a single mesh.\\n\\n    Args:\\n        prim_path: The prim path where to apply the deformable body schema.\\n        cfg: The configuration for the deformable body.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Raises:\\n        ValueError: When the prim path is not valid.\\n        ValueError: When the prim has no mesh or multiple meshes.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n    # get USD prim\\n    prim = stage.GetPrimAtPath(prim_path)\\n    # check if prim path is valid\\n    if not prim.IsValid():\\n        raise ValueError(f\"Prim path \\'{prim_path}\\' is not valid.\")\\n\\n    # traverse the prim and get the mesh\\n    matching_prims = get_all_matching_child_prims(prim_path, lambda p: p.GetTypeName() == \"Mesh\")\\n    # check if the mesh is valid\\n    if len(matching_prims) == 0:\\n        raise ValueError(f\"Could not find any mesh in \\'{prim_path}\\'. Please check asset.\")\\n    if len(matching_prims) > 1:\\n        # get list of all meshes found\\n        mesh_paths = [p.GetPrimPath() for p in matching_prims]\\n        raise ValueError(\\n            f\"Found multiple meshes in \\'{prim_path}\\': {mesh_paths}.\"\\n            \" Deformable body schema can only be applied to one mesh.\"\\n        )\\n\\n    # get deformable-body USD prim\\n    mesh_prim = matching_prims[0]\\n    # check if prim has deformable-body applied on it\\n    if not PhysxSchema.PhysxDeformableBodyAPI(mesh_prim):\\n        PhysxSchema.PhysxDeformableBodyAPI.Apply(mesh_prim)\\n    # set deformable body properties\\n    modify_deformable_body_properties(mesh_prim.GetPrimPath(), cfg, stage)'),\n",
       " Document(metadata={}, page_content='def modify_deformable_body_properties(\\n    prim_path: str, cfg: schemas_cfg.DeformableBodyPropertiesCfg, stage: Usd.Stage | None = None\\n):\\n    \"\"\"Modify PhysX parameters for a deformable body prim.\\n\\n    A `deformable body`_ is a single body that can be simulated by PhysX. Unlike rigid bodies, deformable bodies\\n    support relative motion of the nodes in the mesh. Consequently, they can be used to simulate deformations\\n    under applied forces.\\n\\n    PhysX soft body simulation employs Finite Element Analysis (FEA) to simulate the deformations of the mesh.\\n    It uses two tetrahedral meshes to represent the deformable body:\\n\\n    1. **Simulation mesh**: This mesh is used for the simulation and is the one that is deformed by the solver.\\n    2. **Collision mesh**: This mesh only needs to match the surface of the simulation mesh and is used for\\n       collision detection.\\n\\n    For most applications, we assume that the above two meshes are computed from the \"render mesh\" of the deformable\\n    body. The render mesh is the mesh that is visible in the scene and is used for rendering purposes. It is composed\\n    of triangles and is the one that is used to compute the above meshes based on PhysX cookings.\\n\\n    The schema comprises of attributes that belong to the `PhysxDeformableBodyAPI`_. schemas containing the PhysX\\n    parameters for the deformable body.\\n\\n    .. caution::\\n        The deformable body schema is still under development by the Omniverse team. The current implementation\\n        works with the PhysX schemas shipped with Isaac Sim 4.0.0 onwards. It may change in future releases.\\n\\n    .. note::\\n        This function is decorated with :func:`apply_nested` that sets the properties to all the prims\\n        (that have the schema applied on them) under the input prim path.\\n\\n    .. _deformable body: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/SoftBodies.html\\n    .. _PhysxDeformableBodyAPI: https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/104.2/class_physx_schema_physx_deformable_a_p_i.html\\n\\n    Args:\\n        prim_path: The prim path to the deformable body.\\n        cfg: The configuration for the deformable body.\\n        stage: The stage where to find the prim. Defaults to None, in which case the\\n            current stage is used.\\n\\n    Returns:\\n        True if the properties were successfully set, False otherwise.\\n    \"\"\"\\n    # obtain stage\\n    if stage is None:\\n        stage = stage_utils.get_current_stage()\\n\\n    # get deformable-body USD prim\\n    deformable_body_prim = stage.GetPrimAtPath(prim_path)\\n\\n    # check if the prim is valid and has the deformable-body API\\n    if not deformable_body_prim.IsValid() or not PhysxSchema.PhysxDeformableBodyAPI(deformable_body_prim):\\n        return False\\n\\n    # retrieve the physx deformable-body api\\n    physx_deformable_body_api = PhysxSchema.PhysxDeformableBodyAPI(deformable_body_prim)\\n    # retrieve the physx deformable api\\n    physx_deformable_api = PhysxSchema.PhysxDeformableAPI(physx_deformable_body_api)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    # set into deformable body API\\n    attr_kwargs = {\\n        attr_name: cfg.pop(attr_name)\\n        for attr_name in [\\n            \"kinematic_enabled\",\\n            \"collision_simplification\",\\n            \"collision_simplification_remeshing\",\\n            \"collision_simplification_remeshing_resolution\",\\n            \"collision_simplification_target_triangle_count\",\\n            \"collision_simplification_force_conforming\",\\n            \"simulation_hexahedral_resolution\",\\n            \"solver_position_iteration_count\",\\n            \"vertex_velocity_damping\",\\n            \"sleep_damping\",\\n            \"sleep_threshold\",\\n            \"settling_threshold\",\\n            \"self_collision\",\\n            \"self_collision_filter_distance\",\\n        ]\\n    }\\n    status = deformable_utils.add_physx_deformable_body(stage, prim_path=prim_path, **attr_kwargs)\\n    # check if the deformable body was successfully added\\n    if not status:\\n        return False\\n\\n    # obtain the PhysX collision API (this is set when the deformable body is added)\\n    physx_collision_api = PhysxSchema.PhysxCollisionAPI(deformable_body_prim)\\n\\n    # set into PhysX API\\n    for attr_name, value in cfg.items():\\n        if attr_name in [\"rest_offset\", \"contact_offset\"]:\\n            safe_set_attribute_on_usd_schema(physx_collision_api, attr_name, value, camel_case=True)\\n        else:\\n            safe_set_attribute_on_usd_schema(physx_deformable_api, attr_name, value, camel_case=True)\\n\\n    # success\\n    return True'),\n",
       " Document(metadata={}, page_content='class ArticulationRootPropertiesCfg:\\n    \"\"\"Properties to apply to the root of an articulation.\\n\\n    See :meth:`modify_articulation_root_properties` for more information.\\n\\n    .. note::\\n        If the values are None, they are not modified. This is useful when you want to set only a subset of\\n        the properties and leave the rest as-is.\\n    \"\"\"\\n\\n    articulation_enabled: bool | None = None\\n    \"\"\"Whether to enable or disable articulation.\"\"\"\\n\\n    enabled_self_collisions: bool | None = None\\n    \"\"\"Whether to enable or disable self-collisions.\"\"\"\\n\\n    solver_position_iteration_count: int | None = None\\n    \"\"\"Solver position iteration counts for the body.\"\"\"\\n\\n    solver_velocity_iteration_count: int | None = None\\n    \"\"\"Solver velocity iteration counts for the body.\"\"\"\\n\\n    sleep_threshold: float | None = None\\n    \"\"\"Mass-normalized kinetic energy threshold below which an actor may go to sleep.\"\"\"\\n\\n    stabilization_threshold: float | None = None\\n    \"\"\"The mass-normalized kinetic energy threshold below which an articulation may participate in stabilization.\"\"\"\\n\\n    fix_root_link: bool | None = None\\n    \"\"\"Whether to fix the root link of the articulation.\\n\\n    * If set to None, the root link is not modified.\\n    * If the articulation already has a fixed root link, this flag will enable or disable the fixed joint.\\n    * If the articulation does not have a fixed root link, this flag will create a fixed joint between the world\\n      frame and the root link. The joint is created with the name \"FixedJoint\" under the articulation prim.\\n\\n    .. note::\\n        This is a non-USD schema property. It is handled by the :meth:`modify_articulation_root_properties` function.\\n\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RigidBodyPropertiesCfg:\\n    \"\"\"Properties to apply to a rigid body.\\n\\n    See :meth:`modify_rigid_body_properties` for more information.\\n\\n    .. note::\\n        If the values are None, they are not modified. This is useful when you want to set only a subset of\\n        the properties and leave the rest as-is.\\n    \"\"\"\\n\\n    rigid_body_enabled: bool | None = None\\n    \"\"\"Whether to enable or disable the rigid body.\"\"\"\\n\\n    kinematic_enabled: bool | None = None\\n    \"\"\"Determines whether the body is kinematic or not.\\n\\n    A kinematic body is a body that is moved through animated poses or through user defined poses. The simulation\\n    still derives velocities for the kinematic body based on the external motion.\\n\\n    For more information on kinematic bodies, please refer to the `documentation <https://openusd.org/release/wp_rigid_body_physics.html#kinematic-bodies>`_.\\n    \"\"\"\\n\\n    disable_gravity: bool | None = None\\n    \"\"\"Disable gravity for the actor.\"\"\"\\n\\n    linear_damping: float | None = None\\n    \"\"\"Linear damping for the body.\"\"\"\\n\\n    angular_damping: float | None = None\\n    \"\"\"Angular damping for the body.\"\"\"\\n\\n    max_linear_velocity: float | None = None\\n    \"\"\"Maximum linear velocity for rigid bodies (in m/s).\"\"\"\\n\\n    max_angular_velocity: float | None = None\\n    \"\"\"Maximum angular velocity for rigid bodies (in deg/s).\"\"\"\\n\\n    max_depenetration_velocity: float | None = None\\n    \"\"\"Maximum depenetration velocity permitted to be introduced by the solver (in m/s).\"\"\"\\n\\n    max_contact_impulse: float | None = None\\n    \"\"\"The limit on the impulse that may be applied at a contact.\"\"\"\\n\\n    enable_gyroscopic_forces: bool | None = None\\n    \"\"\"Enables computation of gyroscopic forces on the rigid body.\"\"\"\\n\\n    retain_accelerations: bool | None = None\\n    \"\"\"Carries over forces/accelerations over sub-steps.\"\"\"\\n\\n    solver_position_iteration_count: int | None = None\\n    \"\"\"Solver position iteration counts for the body.\"\"\"\\n\\n    solver_velocity_iteration_count: int | None = None\\n    \"\"\"Solver position iteration counts for the body.\"\"\"\\n\\n    sleep_threshold: float | None = None\\n    \"\"\"Mass-normalized kinetic energy threshold below which an actor may go to sleep.\"\"\"\\n\\n    stabilization_threshold: float | None = None\\n    \"\"\"The mass-normalized kinetic energy threshold below which an actor may participate in stabilization.\"\"\"'),\n",
       " Document(metadata={}, page_content='class CollisionPropertiesCfg:\\n    \"\"\"Properties to apply to colliders in a rigid body.\\n\\n    See :meth:`modify_collision_properties` for more information.\\n\\n    .. note::\\n        If the values are None, they are not modified. This is useful when you want to set only a subset of\\n        the properties and leave the rest as-is.\\n    \"\"\"\\n\\n    collision_enabled: bool | None = None\\n    \"\"\"Whether to enable or disable collisions.\"\"\"\\n\\n    contact_offset: float | None = None\\n    \"\"\"Contact offset for the collision shape (in m).\\n\\n    The collision detector generates contact points as soon as two shapes get closer than the sum of their\\n    contact offsets. This quantity should be non-negative which means that contact generation can potentially start\\n    before the shapes actually penetrate.\\n    \"\"\"\\n\\n    rest_offset: float | None = None\\n    \"\"\"Rest offset for the collision shape (in m).\\n\\n    The rest offset quantifies how close a shape gets to others at rest, At rest, the distance between two\\n    vertically stacked objects is the sum of their rest offsets. If a pair of shapes have a positive rest\\n    offset, the shapes will be separated at rest by an air gap.\\n    \"\"\"\\n\\n    torsional_patch_radius: float | None = None\\n    \"\"\"Radius of the contact patch for applying torsional friction (in m).\\n\\n    It is used to approximate rotational friction introduced by the compression of contacting surfaces.\\n    If the radius is zero, no torsional friction is applied.\\n    \"\"\"\\n\\n    min_torsional_patch_radius: float | None = None\\n    \"\"\"Minimum radius of the contact patch for applying torsional friction (in m).\"\"\"'),\n",
       " Document(metadata={}, page_content='class MassPropertiesCfg:\\n    \"\"\"Properties to define explicit mass properties of a rigid body.\\n\\n    See :meth:`modify_mass_properties` for more information.\\n\\n    .. note::\\n        If the values are None, they are not modified. This is useful when you want to set only a subset of\\n        the properties and leave the rest as-is.\\n    \"\"\"\\n\\n    mass: float | None = None\\n    \"\"\"The mass of the rigid body (in kg).\\n\\n    Note:\\n        If non-zero, the mass is ignored and the density is used to compute the mass.\\n    \"\"\"\\n\\n    density: float | None = None\\n    \"\"\"The density of the rigid body (in kg/m^3).\\n\\n    The density indirectly defines the mass of the rigid body. It is generally computed using the collision\\n    approximation of the body.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class JointDrivePropertiesCfg:\\n    \"\"\"Properties to define the drive mechanism of a joint.\\n\\n    See :meth:`modify_joint_drive_properties` for more information.\\n\\n    .. note::\\n        If the values are None, they are not modified. This is useful when you want to set only a subset of\\n        the properties and leave the rest as-is.\\n    \"\"\"\\n\\n    drive_type: Literal[\"force\", \"acceleration\"] | None = None\\n    \"\"\"Joint drive type to apply.\\n\\n    If the drive type is \"force\", then the joint is driven by a force. If the drive type is \"acceleration\",\\n    then the joint is driven by an acceleration (usually used for kinematic joints).\\n    \"\"\"\\n\\n    max_effort: float | None = None\\n    \"\"\"Maximum effort that can be applied to the joint (in kg-m^2/s^2).\"\"\"\\n\\n    max_velocity: float | None = None\\n    \"\"\"Maximum velocity of the joint.\\n\\n    The unit depends on the joint model:\\n\\n    * For linear joints, the unit is m/s.\\n    * For angular joints, the unit is rad/s.\\n    \"\"\"\\n\\n    stiffness: float | None = None\\n    \"\"\"Stiffness of the joint drive.\\n\\n    The unit depends on the joint model:\\n\\n    * For linear joints, the unit is kg-m/s^2 (N/m).\\n    * For angular joints, the unit is kg-m^2/s^2/rad (N-m/rad).\\n    \"\"\"\\n\\n    damping: float | None = None\\n    \"\"\"Damping of the joint drive.\\n\\n    The unit depends on the joint model:\\n\\n    * For linear joints, the unit is kg-m/s (N-s/m).\\n    * For angular joints, the unit is kg-m^2/s/rad (N-m-s/rad).\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class FixedTendonPropertiesCfg:\\n    \"\"\"Properties to define fixed tendons of an articulation.\\n\\n    See :meth:`modify_fixed_tendon_properties` for more information.\\n\\n    .. note::\\n        If the values are None, they are not modified. This is useful when you want to set only a subset of\\n        the properties and leave the rest as-is.\\n    \"\"\"\\n\\n    tendon_enabled: bool | None = None\\n    \"\"\"Whether to enable or disable the tendon.\"\"\"\\n\\n    stiffness: float | None = None\\n    \"\"\"Spring stiffness term acting on the tendon\\'s length.\"\"\"\\n\\n    damping: float | None = None\\n    \"\"\"The damping term acting on both the tendon length and the tendon-length limits.\"\"\"\\n\\n    limit_stiffness: float | None = None\\n    \"\"\"Limit stiffness term acting on the tendon\\'s length limits.\"\"\"\\n\\n    offset: float | None = None\\n    \"\"\"Length offset term for the tendon.\\n\\n    It defines an amount to be added to the accumulated length computed for the tendon. This allows the application\\n    to actuate the tendon by shortening or lengthening it.\\n    \"\"\"\\n\\n    rest_length: float | None = None\\n    \"\"\"Spring rest length of the tendon.\"\"\"'),\n",
       " Document(metadata={}, page_content='class DeformableBodyPropertiesCfg:\\n    \"\"\"Properties to apply to a deformable body.\\n\\n    A deformable body is a body that can deform under forces. The configuration allows users to specify\\n    the properties of the deformable body, such as the solver iteration counts, damping, and self-collision.\\n\\n    An FEM-based deformable body is created by providing a collision mesh and simulation mesh. The collision mesh\\n    is used for collision detection and the simulation mesh is used for simulation. The collision mesh is usually\\n    a simplified version of the simulation mesh.\\n\\n    Based on the above, the PhysX team provides APIs to either set the simulation and collision mesh directly\\n    (by specifying the points) or to simplify the collision mesh based on the simulation mesh. The simplification\\n    process involves remeshing the collision mesh and simplifying it based on the target triangle count.\\n\\n    Since specifying the collision mesh points directly is not a common use case, we only expose the parameters\\n    to simplify the collision mesh based on the simulation mesh. If you want to provide the collision mesh points,\\n    please open an issue on the repository and we can add support for it.\\n\\n    See :meth:`modify_deformable_body_properties` for more information.\\n\\n    .. note::\\n        If the values are :obj:`None`, they are not modified. This is useful when you want to set only a subset of\\n        the properties and leave the rest as-is.\\n    \"\"\"\\n\\n    deformable_enabled: bool | None = None\\n    \"\"\"Enables deformable body.\"\"\"\\n\\n    kinematic_enabled: bool = False\\n    \"\"\"Enables kinematic body. Defaults to False, which means that the body is not kinematic.\\n\\n    Similar to rigid bodies, this allows setting user-driven motion for the deformable body. For more information,\\n    please refer to the `documentation <https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/docs/SoftBodies.html#kinematic-soft-bodies>`__.\\n    \"\"\"\\n\\n    self_collision: bool | None = None\\n    \"\"\"Whether to enable or disable self-collisions for the deformable body based on the rest position distances.\"\"\"\\n\\n    self_collision_filter_distance: float | None = None\\n    \"\"\"Penetration value that needs to get exceeded before contacts for self collision are generated.\\n\\n    This parameter must be greater than of equal to twice the :attr:`rest_offset` value.\\n\\n    This value has an effect only if :attr:`self_collision` is enabled.\\n    \"\"\"\\n\\n    settling_threshold: float | None = None\\n    \"\"\"Threshold vertex velocity (in m/s) under which sleep damping is applied in addition to velocity damping.\"\"\"\\n\\n    sleep_damping: float | None = None\\n    \"\"\"Coefficient for the additional damping term if fertex velocity drops below setting threshold.\"\"\"\\n\\n    sleep_threshold: float | None = None\\n    \"\"\"The velocity threshold (in m/s) under which the vertex becomes a candidate for sleeping in the next step.\"\"\"\\n\\n    solver_position_iteration_count: int | None = None\\n    \"\"\"Number of the solver positional iterations per step. Range is [1,255]\"\"\"\\n\\n    vertex_velocity_damping: float | None = None\\n    \"\"\"Coefficient for artificial damping on the vertex velocity.\\n\\n    This parameter can be used to approximate the effect of air drag on the deformable body.\\n    \"\"\"\\n\\n    simulation_hexahedral_resolution: int = 10\\n    \"\"\"The target resolution for the hexahedral mesh used for simulation. Defaults to 10.\\n\\n    Note:\\n        This value is ignored if the user provides the simulation mesh points directly. However, we assume that\\n        most users will not provide the simulation mesh points directly. If you want to provide the simulation mesh\\n        directly, please set this value to :obj:`None`.\\n    \"\"\"\\n\\n    collision_simplification: bool = True\\n    \"\"\"Whether or not to simplify the collision mesh before creating a soft body out of it. Defaults to True.\\n\\n    Note:\\n        This flag is ignored if the user provides the simulation mesh points directly. However, we assume that\\n        most users will not provide the simulation mesh points directly. Hence, this flag is enabled by default.\\n\\n        If you want to provide the simulation mesh points directly, please set this flag to False.\\n    \"\"\"\\n\\n    collision_simplification_remeshing: bool = True\\n    \"\"\"Whether or not the collision mesh should be remeshed before simplification. Defaults to True.\\n\\n    This parameter is ignored if :attr:`collision_simplification` is False.\\n    \"\"\"\\n\\n    collision_simplification_remeshing_resolution: int = 0\\n    \"\"\"The resolution used for remeshing. Defaults to 0, which means that a heuristic is used to determine the\\n    resolution.\\n\\n    This parameter is ignored if :attr:`collision_simplification_remeshing` is False.\\n    \"\"\"\\n\\n    collision_simplification_target_triangle_count: int = 0\\n    \"\"\"The target triangle count used for the simplification. Defaults to 0, which means that a heuristic based on\\n    the :attr:`simulation_hexahedral_resolution` is used to determine the target count.\\n\\n    This parameter is ignored if :attr:`collision_simplification` is False.\\n    \"\"\"\\n\\n    collision_simplification_force_conforming: bool = True\\n    \"\"\"Whether or not the simplification should force the output mesh to conform to the input mesh. Defaults to True.\\n\\n    The flag indicates that the tretrahedralizer used to generate the collision mesh should produce tetrahedra\\n    that conform to the triangle mesh. If False, the simplifier uses the output from the tretrahedralizer used.\\n\\n    This parameter is ignored if :attr:`collision_simplification` is False.\\n    \"\"\"\\n\\n    contact_offset: float | None = None\\n    \"\"\"Contact offset for the collision shape (in m).\\n\\n    The collision detector generates contact points as soon as two shapes get closer than the sum of their\\n    contact offsets. This quantity should be non-negative which means that contact generation can potentially start\\n    before the shapes actually penetrate.\\n    \"\"\"\\n\\n    rest_offset: float | None = None\\n    \"\"\"Rest offset for the collision shape (in m).\\n\\n    The rest offset quantifies how close a shape gets to others at rest, At rest, the distance between two\\n    vertically stacked objects is the sum of their rest offsets. If a pair of shapes have a positive rest\\n    offset, the shapes will be separated at rest by an air gap.\\n    \"\"\"\\n\\n    max_depenetration_velocity: float | None = None\\n    \"\"\"Maximum depenetration velocity permitted to be introduced by the solver (in m/s).\"\"\"'),\n",
       " Document(metadata={}, page_content='class SpawnerCfg:\\n    \"\"\"Configuration parameters for spawning an asset.\\n\\n    Spawning an asset is done by calling the :attr:`func` function. The function takes in the\\n    prim path to spawn the asset at, the configuration instance and transformation, and returns the\\n    prim path of the spawned asset.\\n\\n    The function is typically decorated with :func:`isaaclab.sim.spawner.utils.clone` decorator\\n    that checks if input prim path is a regex expression and spawns the asset at all matching prims.\\n    For this, the decorator uses the Cloner API from Isaac Sim and handles the :attr:`copy_from_source`\\n    parameter.\\n    \"\"\"\\n\\n    func: Callable[..., Usd.Prim] = MISSING\\n    \"\"\"Function to use for spawning the asset.\\n\\n    The function takes in the prim path (or expression) to spawn the asset at, the configuration instance\\n    and transformation, and returns the source prim spawned.\\n    \"\"\"\\n\\n    visible: bool = True\\n    \"\"\"Whether the spawned asset should be visible. Defaults to True.\"\"\"\\n\\n    semantic_tags: list[tuple[str, str]] | None = None\\n    \"\"\"List of semantic tags to add to the spawned asset. Defaults to None,\\n    which means no semantic tags will be added.\\n\\n    The semantic tags follow the `Replicator Semantic` tagging system. Each tag is a tuple of the\\n    form ``(type, data)``, where ``type`` is the type of the tag and ``data`` is the semantic label\\n    associated with the tag. For example, to annotate a spawned asset in the class avocado, the semantic\\n    tag would be ``[(\"class\", \"avocado\")]``.\\n\\n    You can specify multiple semantic tags by passing in a list of tags. For example, to annotate a\\n    spawned asset in the class avocado and the color green, the semantic tags would be\\n    ``[(\"class\", \"avocado\"), (\"color\", \"green\")]``.\\n\\n    .. seealso::\\n\\n        For more information on the semantics filter, see the documentation for the `semantics schema editor`_.\\n\\n    .. _semantics schema editor: https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/semantics_schema_editor.html#semantics-filtering\\n\\n    \"\"\"\\n\\n    copy_from_source: bool = True\\n    \"\"\"Whether to copy the asset from the source prim or inherit it. Defaults to True.\\n\\n    This parameter is only used when cloning prims. If False, then the asset will be inherited from\\n    the source prim, i.e. all USD changes to the source prim will be reflected in the cloned prims.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class RigidObjectSpawnerCfg(SpawnerCfg):\\n    \"\"\"Configuration parameters for spawning a rigid asset.\\n\\n    Note:\\n        By default, all properties are set to None. This means that no properties will be added or modified\\n        to the prim outside of the properties available by default when spawning the prim.\\n    \"\"\"\\n\\n    mass_props: schemas.MassPropertiesCfg | None = None\\n    \"\"\"Mass properties.\"\"\"\\n\\n    rigid_props: schemas.RigidBodyPropertiesCfg | None = None\\n    \"\"\"Rigid body properties.\\n\\n    For making a rigid object static, set the :attr:`schemas.RigidBodyPropertiesCfg.kinematic_enabled`\\n    as True. This will make the object static and will not be affected by gravity or other forces.\\n    \"\"\"\\n\\n    collision_props: schemas.CollisionPropertiesCfg | None = None\\n    \"\"\"Properties to apply to all collision meshes.\"\"\"\\n\\n    activate_contact_sensors: bool = False\\n    \"\"\"Activate contact reporting on all rigid bodies. Defaults to False.\\n\\n    This adds the PhysxContactReporter API to all the rigid bodies in the given prim path and its children.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class DeformableObjectSpawnerCfg(SpawnerCfg):\\n    \"\"\"Configuration parameters for spawning a deformable asset.\\n\\n    Unlike rigid objects, deformable objects are affected by forces and can deform when subjected to\\n    external forces. This class is used to configure the properties of the deformable object.\\n\\n    Deformable bodies don\\'t have a separate collision mesh. The collision mesh is the same as the visual mesh.\\n    The collision properties such as rest and collision offsets are specified in the :attr:`deformable_props`.\\n\\n    Note:\\n        By default, all properties are set to None. This means that no properties will be added or modified\\n        to the prim outside of the properties available by default when spawning the prim.\\n    \"\"\"\\n\\n    mass_props: schemas.MassPropertiesCfg | None = None\\n    \"\"\"Mass properties.\"\"\"\\n\\n    deformable_props: schemas.DeformableBodyPropertiesCfg | None = None\\n    \"\"\"Deformable body properties.\"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_from_usd(\\n    prim_path: str,\\n    cfg: from_files_cfg.UsdFileCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Spawn an asset from a USD file and override the settings with the given config.\\n\\n    In the case of a USD file, the asset is spawned at the default prim specified in the USD file.\\n    If a default prim is not specified, then the asset is spawned at the root prim.\\n\\n    In case a prim already exists at the given prim path, then the function does not create a new prim\\n    or throw an error that the prim already exists. Instead, it just takes the existing prim and overrides\\n    the settings with the given config.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which\\n            case the translation specified in the USD file is used.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case the orientation specified in the USD file is used.\\n\\n    Returns:\\n        The prim of the spawned asset.\\n\\n    Raises:\\n        FileNotFoundError: If the USD file does not exist at the given path.\\n    \"\"\"\\n    # spawn asset from the given usd file\\n    return _spawn_from_usd_file(prim_path, cfg.usd_path, cfg, translation, orientation)'),\n",
       " Document(metadata={}, page_content='def spawn_from_urdf(\\n    prim_path: str,\\n    cfg: from_files_cfg.UrdfFileCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Spawn an asset from a URDF file and override the settings with the given config.\\n\\n    It uses the :class:`UrdfConverter` class to create a USD file from URDF. This file is then imported\\n    at the specified prim path.\\n\\n    In case a prim already exists at the given prim path, then the function does not create a new prim\\n    or throw an error that the prim already exists. Instead, it just takes the existing prim and overrides\\n    the settings with the given config.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which\\n            case the translation specified in the generated USD file is used.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case the orientation specified in the generated USD file is used.\\n\\n    Returns:\\n        The prim of the spawned asset.\\n\\n    Raises:\\n        FileNotFoundError: If the URDF file does not exist at the given path.\\n    \"\"\"\\n    # urdf loader to convert urdf to usd\\n    urdf_loader = converters.UrdfConverter(cfg)\\n    # spawn asset from the generated usd file\\n    return _spawn_from_usd_file(prim_path, urdf_loader.usd_path, cfg, translation, orientation)'),\n",
       " Document(metadata={}, page_content='def spawn_ground_plane(\\n    prim_path: str,\\n    cfg: from_files_cfg.GroundPlaneCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Spawns a ground plane into the scene.\\n\\n    This function loads the USD file containing the grid plane asset from Isaac Sim. It may\\n    not work with other assets for ground planes. In those cases, please use the `spawn_from_usd`\\n    function.\\n\\n    Note:\\n        This function takes keyword arguments to be compatible with other spawners. However, it does not\\n        use any of the kwargs.\\n\\n    Args:\\n        prim_path: The path to spawn the asset at.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which\\n            case the translation specified in the USD file is used.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case the orientation specified in the USD file is used.\\n\\n    Returns:\\n        The prim of the spawned asset.\\n\\n    Raises:\\n        ValueError: If the prim path already exists.\\n    \"\"\"\\n    # Spawn Ground-plane\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        prim_utils.create_prim(prim_path, usd_path=cfg.usd_path, translation=translation, orientation=orientation)\\n    else:\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\'.\")\\n\\n    # Create physics material\\n    if cfg.physics_material is not None:\\n        cfg.physics_material.func(f\"{prim_path}/physicsMaterial\", cfg.physics_material)\\n        # Apply physics material to ground plane\\n        collision_prim_path = prim_utils.get_prim_path(\\n            prim_utils.get_first_matching_child_prim(\\n                prim_path, predicate=lambda x: prim_utils.get_prim_type_name(x) == \"Plane\"\\n            )\\n        )\\n        bind_physics_material(collision_prim_path, f\"{prim_path}/physicsMaterial\")\\n\\n    # Scale only the mesh\\n    # Warning: This is specific to the default grid plane asset.\\n    if prim_utils.is_prim_path_valid(f\"{prim_path}/Environment\"):\\n        # compute scale from size\\n        scale = (cfg.size[0] / 100.0, cfg.size[1] / 100.0, 1.0)\\n        # apply scale to the mesh\\n        prim_utils.set_prim_property(f\"{prim_path}/Environment\", \"xformOp:scale\", scale)\\n\\n    # Change the color of the plane\\n    # Warning: This is specific to the default grid plane asset.\\n    if cfg.color is not None:\\n        prop_path = f\"{prim_path}/Looks/theGrid/Shader.inputs:diffuse_tint\"\\n        # change the color\\n        omni.kit.commands.execute(\\n            \"ChangePropertyCommand\",\\n            prop_path=Sdf.Path(prop_path),\\n            value=Gf.Vec3f(*cfg.color),\\n            prev=None,\\n            type_to_create_if_not_exist=Sdf.ValueTypeNames.Color3f,\\n        )\\n    # Remove the light from the ground plane\\n    # It isn\\'t bright enough and messes up with the user\\'s lighting settings\\n    omni.kit.commands.execute(\"ToggleVisibilitySelectedPrims\", selected_paths=[f\"{prim_path}/SphereLight\"])\\n\\n    prim = prim_utils.get_prim_at_path(prim_path)\\n    # Apply semantic tags\\n    if hasattr(cfg, \"semantic_tags\") and cfg.semantic_tags is not None:\\n        # note: taken from replicator scripts.utils.utils.py\\n        for semantic_type, semantic_value in cfg.semantic_tags:\\n            # deal with spaces by replacing them with underscores\\n            semantic_type_sanitized = semantic_type.replace(\" \", \"_\")\\n            semantic_value_sanitized = semantic_value.replace(\" \", \"_\")\\n            # set the semantic API for the instance\\n            instance_name = f\"{semantic_type_sanitized}_{semantic_value_sanitized}\"\\n            sem = Semantics.SemanticsAPI.Apply(prim, instance_name)\\n            # create semantic type and data attributes\\n            sem.CreateSemanticTypeAttr().Set(semantic_type)\\n            sem.CreateSemanticDataAttr().Set(semantic_value)\\n    # return the prim\\n    return prim'),\n",
       " Document(metadata={}, page_content='def _spawn_from_usd_file(\\n    prim_path: str,\\n    usd_path: str,\\n    cfg: from_files_cfg.FileCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Spawn an asset from a USD file and override the settings with the given config.\\n\\n    In case a prim already exists at the given prim path, then the function does not create a new prim\\n    or throw an error that the prim already exists. Instead, it just takes the existing prim and overrides\\n    the settings with the given config.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        usd_path: The path to the USD file to spawn the asset from.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which\\n            case the translation specified in the generated USD file is used.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case the orientation specified in the generated USD file is used.\\n\\n    Returns:\\n        The prim of the spawned asset.\\n\\n    Raises:\\n        FileNotFoundError: If the USD file does not exist at the given path.\\n    \"\"\"\\n    # check file path exists\\n    stage: Usd.Stage = stage_utils.get_current_stage()\\n    if not stage.ResolveIdentifierToEditTarget(usd_path):\\n        raise FileNotFoundError(f\"USD file not found at path: \\'{usd_path}\\'.\")\\n    # spawn asset if it doesn\\'t exist.\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        # add prim as reference to stage\\n        prim_utils.create_prim(\\n            prim_path,\\n            usd_path=usd_path,\\n            translation=translation,\\n            orientation=orientation,\\n            scale=cfg.scale,\\n        )\\n    else:\\n        omni.log.warn(f\"A prim already exists at prim path: \\'{prim_path}\\'.\")\\n\\n    # modify variants\\n    if hasattr(cfg, \"variants\") and cfg.variants is not None:\\n        select_usd_variants(prim_path, cfg.variants)\\n\\n    # modify rigid body properties\\n    if cfg.rigid_props is not None:\\n        schemas.modify_rigid_body_properties(prim_path, cfg.rigid_props)\\n    # modify collision properties\\n    if cfg.collision_props is not None:\\n        schemas.modify_collision_properties(prim_path, cfg.collision_props)\\n    # modify mass properties\\n    if cfg.mass_props is not None:\\n        schemas.modify_mass_properties(prim_path, cfg.mass_props)\\n\\n    # modify articulation root properties\\n    if cfg.articulation_props is not None:\\n        schemas.modify_articulation_root_properties(prim_path, cfg.articulation_props)\\n    # modify tendon properties\\n    if cfg.fixed_tendons_props is not None:\\n        schemas.modify_fixed_tendon_properties(prim_path, cfg.fixed_tendons_props)\\n    # define drive API on the joints\\n    # note: these are only for setting low-level simulation properties. all others should be set or are\\n    #  and overridden by the articulation/actuator properties.\\n    if cfg.joint_drive_props is not None:\\n        schemas.modify_joint_drive_properties(prim_path, cfg.joint_drive_props)\\n\\n    # modify deformable body properties\\n    if cfg.deformable_props is not None:\\n        schemas.modify_deformable_body_properties(prim_path, cfg.deformable_props)\\n\\n    # apply visual material\\n    if cfg.visual_material is not None:\\n        if not cfg.visual_material_path.startswith(\"/\"):\\n            material_path = f\"{prim_path}/{cfg.visual_material_path}\"\\n        else:\\n            material_path = cfg.visual_material_path\\n        # create material\\n        cfg.visual_material.func(material_path, cfg.visual_material)\\n        # apply material\\n        bind_visual_material(prim_path, material_path)\\n\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='class FileCfg(RigidObjectSpawnerCfg, DeformableObjectSpawnerCfg):\\n    \"\"\"Configuration parameters for spawning an asset from a file.\\n\\n    This class is a base class for spawning assets from files. It includes the common parameters\\n    for spawning assets from files, such as the path to the file and the function to use for spawning\\n    the asset.\\n\\n    Note:\\n        By default, all properties are set to None. This means that no properties will be added or modified\\n        to the prim outside of the properties available by default when spawning the prim.\\n\\n        If they are set to a value, then the properties are modified on the spawned prim in a nested manner.\\n        This is done by calling the respective function with the specified properties.\\n    \"\"\"\\n\\n    scale: tuple[float, float, float] | None = None\\n    \"\"\"Scale of the asset. Defaults to None, in which case the scale is not modified.\"\"\"\\n\\n    articulation_props: schemas.ArticulationRootPropertiesCfg | None = None\\n    \"\"\"Properties to apply to the articulation root.\"\"\"\\n\\n    fixed_tendons_props: schemas.FixedTendonsPropertiesCfg | None = None\\n    \"\"\"Properties to apply to the fixed tendons (if any).\"\"\"\\n\\n    joint_drive_props: schemas.JointDrivePropertiesCfg | None = None\\n    \"\"\"Properties to apply to a joint.\\n\\n    .. note::\\n        The joint drive properties set the USD attributes of all the joint drives in the asset.\\n        We recommend using this attribute sparingly and only when necessary. Instead, please use the\\n        :attr:`~isaaclab.assets.ArticulationCfg.actuators` parameter to set the joint drive properties\\n        for specific joints in an articulation.\\n    \"\"\"\\n\\n    visual_material_path: str = \"material\"\\n    \"\"\"Path to the visual material to use for the prim. Defaults to \"material\".\\n\\n    If the path is relative, then it will be relative to the prim\\'s path.\\n    This parameter is ignored if `visual_material` is not None.\\n    \"\"\"\\n\\n    visual_material: materials.VisualMaterialCfg | None = None\\n    \"\"\"Visual material properties to override the visual material properties in the URDF file.\\n\\n    Note:\\n        If None, then no visual material will be added.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class UsdFileCfg(FileCfg):\\n    \"\"\"USD file to spawn asset from.\\n\\n    USD files are imported directly into the scene. However, given their complexity, there are various different\\n    operations that can be performed on them. For example, selecting variants, applying materials, or modifying\\n    existing properties.\\n\\n    To prevent the explosion of configuration parameters, the available operations are limited to the most common\\n    ones. These include:\\n\\n    - **Selecting variants**: This is done by specifying the :attr:`variants` parameter.\\n    - **Creating and applying materials**: This is done by specifying the :attr:`visual_material` parameter.\\n    - **Modifying existing properties**: This is done by specifying the respective properties in the configuration\\n      class. For instance, to modify the scale of the imported prim, set the :attr:`scale` parameter.\\n\\n    See :meth:`spawn_from_usd` for more information.\\n\\n    .. note::\\n        The configuration parameters include various properties. If not `None`, these properties\\n        are modified on the spawned prim in a nested manner.\\n\\n        If they are set to a value, then the properties are modified on the spawned prim in a nested manner.\\n        This is done by calling the respective function with the specified properties.\\n    \"\"\"\\n\\n    func: Callable = from_files.spawn_from_usd\\n\\n    usd_path: str = MISSING\\n    \"\"\"Path to the USD file to spawn asset from.\"\"\"\\n\\n    variants: object | dict[str, str] | None = None\\n    \"\"\"Variants to select from in the input USD file. Defaults to None, in which case no variants are applied.\\n\\n    This can either be a configclass object, in which case each attribute is used as a variant set name and its specified value,\\n    or a dictionary mapping between the two. Please check the :meth:`~isaaclab.sim.utils.select_usd_variants` function\\n    for more information.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class UrdfFileCfg(FileCfg, converters.UrdfConverterCfg):\\n    \"\"\"URDF file to spawn asset from.\\n\\n    It uses the :class:`UrdfConverter` class to create a USD file from URDF and spawns the imported\\n    USD file. Similar to the :class:`UsdFileCfg`, the generated USD file can be modified by specifying\\n    the respective properties in the configuration class.\\n\\n    See :meth:`spawn_from_urdf` for more information.\\n\\n    .. note::\\n        The configuration parameters include various properties. If not `None`, these properties\\n        are modified on the spawned prim in a nested manner.\\n\\n        If they are set to a value, then the properties are modified on the spawned prim in a nested manner.\\n        This is done by calling the respective function with the specified properties.\\n\\n    \"\"\"\\n\\n    func: Callable = from_files.spawn_from_urdf'),\n",
       " Document(metadata={}, page_content='class GroundPlaneCfg(SpawnerCfg):\\n    \"\"\"Create a ground plane prim.\\n\\n    This uses the USD for the standard grid-world ground plane from Isaac Sim by default.\\n    \"\"\"\\n\\n    func: Callable = from_files.spawn_ground_plane\\n\\n    usd_path: str = f\"{ISAAC_NUCLEUS_DIR}/Environments/Grid/default_environment.usd\"\\n    \"\"\"Path to the USD file to spawn asset from. Defaults to the grid-world ground plane.\"\"\"\\n\\n    color: tuple[float, float, float] | None = (0.0, 0.0, 0.0)\\n    \"\"\"The color of the ground plane. Defaults to (0.0, 0.0, 0.0).\\n\\n    If None, then the color remains unchanged.\\n    \"\"\"\\n\\n    size: tuple[float, float] = (100.0, 100.0)\\n    \"\"\"The size of the ground plane. Defaults to 100 m x 100 m.\"\"\"\\n\\n    physics_material: materials.RigidBodyMaterialCfg = materials.RigidBodyMaterialCfg()\\n    \"\"\"Physics material properties. Defaults to the default rigid body material.\"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_light(\\n    prim_path: str,\\n    cfg: lights_cfg.LightCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a light prim at the specified prim path with the specified configuration.\\n\\n    The created prim is based on the `USD.LuxLight <https://openusd.org/dev/api/class_usd_lux_light_a_p_i.html>`_ API.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration for the light source.\\n        translation: The translation of the prim. Defaults to None, in which case this is set to the origin.\\n        orientation: The orientation of the prim as (w, x, y, z). Defaults to None, in which case this\\n            is set to identity.\\n\\n    Raises:\\n        ValueError:  When a prim already exists at the specified prim path.\\n    \"\"\"\\n    # check if prim already exists\\n    if prim_utils.is_prim_path_valid(prim_path):\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\'.\")\\n    # create the prim\\n    prim = prim_utils.create_prim(prim_path, prim_type=cfg.prim_type, translation=translation, orientation=orientation)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    # delete spawner func specific parameters\\n    del cfg[\"prim_type\"]\\n    # delete custom attributes in the config that are not USD parameters\\n    non_usd_cfg_param_names = [\"func\", \"copy_from_source\", \"visible\", \"semantic_tags\"]\\n    for param_name in non_usd_cfg_param_names:\\n        del cfg[param_name]\\n    # set into USD API\\n    for attr_name, value in cfg.items():\\n        # special operation for texture properties\\n        # note: this is only used for dome light\\n        if \"texture\" in attr_name:\\n            light_prim = UsdLux.DomeLight(prim)\\n            if attr_name == \"texture_file\":\\n                light_prim.CreateTextureFileAttr(value)\\n            elif attr_name == \"texture_format\":\\n                light_prim.CreateTextureFormatAttr(value)\\n            else:\\n                raise ValueError(f\"Unsupported texture attribute: \\'{attr_name}\\'.\")\\n        else:\\n            if attr_name == \"visible_in_primary_ray\":\\n                prim_prop_name = attr_name\\n            else:\\n                prim_prop_name = f\"inputs:{attr_name}\"\\n            # set the attribute\\n            safe_set_attribute_on_usd_prim(prim, prim_prop_name, value, camel_case=True)\\n    # return the prim\\n    return prim'),\n",
       " Document(metadata={}, page_content='class LightCfg(SpawnerCfg):\\n    \"\"\"Configuration parameters for creating a light in the scene.\\n\\n    Please refer to the documentation on `USD LuxLight <https://openusd.org/dev/api/class_usd_lux_light_a_p_i.html>`_\\n    for more information.\\n\\n    .. note::\\n        The default values for the attributes are those specified in the their official documentation.\\n    \"\"\"\\n\\n    func: Callable = lights.spawn_light\\n\\n    prim_type: str = MISSING\\n    \"\"\"The prim type name for the light prim.\"\"\"\\n\\n    color: tuple[float, float, float] = (1.0, 1.0, 1.0)\\n    \"\"\"The color of emitted light, in energy-linear terms. Defaults to white.\"\"\"\\n\\n    enable_color_temperature: bool = False\\n    \"\"\"Enables color temperature. Defaults to false.\"\"\"\\n\\n    color_temperature: float = 6500.0\\n    \"\"\"Color temperature (in Kelvin) representing the white point. The valid range is [1000, 10000]. Defaults to 6500K.\\n\\n    The `color temperature <https://en.wikipedia.org/wiki/Color_temperature>`_ corresponds to the warmth\\n    or coolness of light. Warmer light has a lower color temperature, while cooler light has a higher\\n    color temperature.\\n\\n    Note:\\n        It only takes effect when :attr:`enable_color_temperature` is true.\\n    \"\"\"\\n\\n    normalize: bool = False\\n    \"\"\"Normalizes power by the surface area of the light. Defaults to false.\\n\\n    This makes it easier to independently adjust the power and shape of the light, by causing the power\\n    to not vary with the area or angular size of the light.\\n    \"\"\"\\n\\n    exposure: float = 0.0\\n    \"\"\"Scales the power of the light exponentially as a power of 2. Defaults to 0.0.\\n\\n    The result is multiplied against the intensity.\\n    \"\"\"\\n\\n    intensity: float = 1.0\\n    \"\"\"Scales the power of the light linearly. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class DiskLightCfg(LightCfg):\\n    \"\"\"Configuration parameters for creating a disk light in the scene.\\n\\n    A disk light is a light source that emits light from a disk. It is useful for simulating\\n    fluorescent lights. For more information, please refer to the documentation on\\n    `USDLux DiskLight <https://openusd.org/dev/api/class_usd_lux_disk_light.html>`_.\\n\\n    .. note::\\n        The default values for the attributes are those specified in the their official documentation.\\n    \"\"\"\\n\\n    prim_type = \"DiskLight\"\\n\\n    radius: float = 0.5\\n    \"\"\"Radius of the disk (in m). Defaults to 0.5m.\"\"\"'),\n",
       " Document(metadata={}, page_content='class DistantLightCfg(LightCfg):\\n    \"\"\"Configuration parameters for creating a distant light in the scene.\\n\\n    A distant light is a light source that is infinitely far away, and emits parallel rays of light.\\n    It is useful for simulating sun/moon light. For more information, please refer to the documentation on\\n    `USDLux DistantLight <https://openusd.org/dev/api/class_usd_lux_distant_light.html>`_.\\n\\n    .. note::\\n        The default values for the attributes are those specified in the their official documentation.\\n    \"\"\"\\n\\n    prim_type = \"DistantLight\"\\n\\n    angle: float = 0.53\\n    \"\"\"Angular size of the light (in degrees). Defaults to 0.53 degrees.\\n\\n    As an example, the Sun is approximately 0.53 degrees as seen from Earth.\\n    Higher values broaden the light and therefore soften shadow edges.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class DomeLightCfg(LightCfg):\\n    \"\"\"Configuration parameters for creating a dome light in the scene.\\n\\n    A dome light is a light source that emits light inwards from all directions. It is also possible to\\n    attach a texture to the dome light, which will be used to emit light. For more information, please refer\\n    to the documentation on `USDLux DomeLight <https://openusd.org/dev/api/class_usd_lux_dome_light.html>`_.\\n\\n    .. note::\\n        The default values for the attributes are those specified in the their official documentation.\\n    \"\"\"\\n\\n    prim_type = \"DomeLight\"\\n\\n    texture_file: str | None = None\\n    \"\"\"A color texture to use on the dome, such as an HDR (high dynamic range) texture intended\\n    for IBL (image based lighting). Defaults to None.\\n\\n    If None, the dome will emit a uniform color.\\n    \"\"\"\\n\\n    texture_format: Literal[\"automatic\", \"latlong\", \"mirroredBall\", \"angular\", \"cubeMapVerticalCross\"] = \"automatic\"\\n    \"\"\"The parametrization format of the color map file. Defaults to \"automatic\".\\n\\n    Valid values are:\\n\\n    * ``\"automatic\"``: Tries to determine the layout from the file itself. For example, Renderman texture files embed an explicit parameterization.\\n    * ``\"latlong\"``: Latitude as X, longitude as Y.\\n    * ``\"mirroredBall\"``: An image of the environment reflected in a sphere, using an implicitly orthogonal projection.\\n    * ``\"angular\"``: Similar to mirroredBall but the radial dimension is mapped linearly to the angle, providing better sampling at the edges.\\n    * ``\"cubeMapVerticalCross\"``: A cube map with faces laid out as a vertical cross.\\n    \"\"\"\\n\\n    visible_in_primary_ray: bool = True\\n    \"\"\"Whether the dome light is visible in the primary ray. Defaults to True.\\n\\n    If true, the texture in the sky is visible, otherwise the sky is black.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class CylinderLightCfg(LightCfg):\\n    \"\"\"Configuration parameters for creating a cylinder light in the scene.\\n\\n    A cylinder light is a light source that emits light from a cylinder. It is useful for simulating\\n    fluorescent lights. For more information, please refer to the documentation on\\n    `USDLux CylinderLight <https://openusd.org/dev/api/class_usd_lux_cylinder_light.html>`_.\\n\\n    .. note::\\n        The default values for the attributes are those specified in the their official documentation.\\n    \"\"\"\\n\\n    prim_type = \"CylinderLight\"\\n\\n    length: float = 1.0\\n    \"\"\"Length of the cylinder (in m). Defaults to 1.0m.\"\"\"\\n\\n    radius: float = 0.5\\n    \"\"\"Radius of the cylinder (in m). Defaults to 0.5m.\"\"\"\\n\\n    treat_as_line: bool = False\\n    \"\"\"Treats the cylinder as a line source, i.e. a zero-radius cylinder. Defaults to false.\"\"\"'),\n",
       " Document(metadata={}, page_content='class SphereLightCfg(LightCfg):\\n    \"\"\"Configuration parameters for creating a sphere light in the scene.\\n\\n    A sphere light is a light source that emits light outward from a sphere. For more information,\\n    please refer to the documentation on\\n    `USDLux SphereLight <https://openusd.org/dev/api/class_usd_lux_sphere_light.html>`_.\\n\\n    .. note::\\n        The default values for the attributes are those specified in the their official documentation.\\n    \"\"\"\\n\\n    prim_type = \"SphereLight\"\\n\\n    radius: float = 0.5\\n    \"\"\"Radius of the sphere. Defaults to 0.5m.\"\"\"\\n\\n    treat_as_point: bool = False\\n    \"\"\"Treats the sphere as a point source, i.e. a zero-radius sphere. Defaults to false.\"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_rigid_body_material(prim_path: str, cfg: physics_materials_cfg.RigidBodyMaterialCfg) -> Usd.Prim:\\n    \"\"\"Create material with rigid-body physics properties.\\n\\n    Rigid body materials are used to define the physical properties to meshes of a rigid body. These\\n    include the friction, restitution, and their respective combination modes. For more information on\\n    rigid body material, please refer to the `documentation on PxMaterial <https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/_api_build/classPxBaseMaterial.html>`_.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration for the physics material.\\n\\n    Returns:\\n        The spawned rigid body material prim.\\n\\n    Raises:\\n        ValueError:  When a prim already exists at the specified prim path and is not a material.\\n    \"\"\"\\n    # create material prim if no prim exists\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        _ = UsdShade.Material.Define(stage_utils.get_current_stage(), prim_path)\\n\\n    # obtain prim\\n    prim = prim_utils.get_prim_at_path(prim_path)\\n    # check if prim is a material\\n    if not prim.IsA(UsdShade.Material):\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\' but is not a material.\")\\n    # retrieve the USD rigid-body api\\n    usd_physics_material_api = UsdPhysics.MaterialAPI(prim)\\n    if not usd_physics_material_api:\\n        usd_physics_material_api = UsdPhysics.MaterialAPI.Apply(prim)\\n    # retrieve the collision api\\n    physx_material_api = PhysxSchema.PhysxMaterialAPI(prim)\\n    if not physx_material_api:\\n        physx_material_api = PhysxSchema.PhysxMaterialAPI.Apply(prim)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    del cfg[\"func\"]\\n    # set into USD API\\n    for attr_name in [\"static_friction\", \"dynamic_friction\", \"restitution\"]:\\n        value = cfg.pop(attr_name, None)\\n        safe_set_attribute_on_usd_schema(usd_physics_material_api, attr_name, value, camel_case=True)\\n    # set into PhysX API\\n    for attr_name, value in cfg.items():\\n        safe_set_attribute_on_usd_schema(physx_material_api, attr_name, value, camel_case=True)\\n    # return the prim\\n    return prim'),\n",
       " Document(metadata={}, page_content='def spawn_deformable_body_material(prim_path: str, cfg: physics_materials_cfg.DeformableBodyMaterialCfg) -> Usd.Prim:\\n    \"\"\"Create material with deformable-body physics properties.\\n\\n    Deformable body materials are used to define the physical properties to meshes of a deformable body. These\\n    include the friction and deformable body properties. For more information on deformable body material,\\n    please refer to the documentation on `PxFEMSoftBodyMaterial`_.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration for the physics material.\\n\\n    Returns:\\n        The spawned deformable body material prim.\\n\\n    Raises:\\n        ValueError:  When a prim already exists at the specified prim path and is not a material.\\n\\n    .. _PxFEMSoftBodyMaterial: https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/_api_build/structPxFEMSoftBodyMaterialModel.html\\n    \"\"\"\\n    # create material prim if no prim exists\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        _ = UsdShade.Material.Define(stage_utils.get_current_stage(), prim_path)\\n\\n    # obtain prim\\n    prim = prim_utils.get_prim_at_path(prim_path)\\n    # check if prim is a material\\n    if not prim.IsA(UsdShade.Material):\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\' but is not a material.\")\\n    # retrieve the deformable-body api\\n    physx_deformable_body_material_api = PhysxSchema.PhysxDeformableBodyMaterialAPI(prim)\\n    if not physx_deformable_body_material_api:\\n        physx_deformable_body_material_api = PhysxSchema.PhysxDeformableBodyMaterialAPI.Apply(prim)\\n\\n    # convert to dict\\n    cfg = cfg.to_dict()\\n    del cfg[\"func\"]\\n    # set into PhysX API\\n    for attr_name, value in cfg.items():\\n        safe_set_attribute_on_usd_schema(physx_deformable_body_material_api, attr_name, value, camel_case=True)\\n    # return the prim\\n    return prim'),\n",
       " Document(metadata={}, page_content='class PhysicsMaterialCfg:\\n    \"\"\"Configuration parameters for creating a physics material.\\n\\n    Physics material are PhysX schemas that can be applied to a USD material prim to define the\\n    physical properties related to the material. For example, the friction coefficient, restitution\\n    coefficient, etc. For more information on physics material, please refer to the\\n    `PhysX documentation <https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/_api_build/classPxBaseMaterial.html>`__.\\n    \"\"\"\\n\\n    func: Callable = MISSING\\n    \"\"\"Function to use for creating the material.\"\"\"'),\n",
       " Document(metadata={}, page_content='class RigidBodyMaterialCfg(PhysicsMaterialCfg):\\n    \"\"\"Physics material parameters for rigid bodies.\\n\\n    See :meth:`spawn_rigid_body_material` for more information.\\n\\n    Note:\\n        The default values are the `default values used by PhysX 5\\n        <https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/rigid-bodies.html#rigid-body-materials>`__.\\n    \"\"\"\\n\\n    func: Callable = physics_materials.spawn_rigid_body_material\\n\\n    static_friction: float = 0.5\\n    \"\"\"The static friction coefficient. Defaults to 0.5.\"\"\"\\n\\n    dynamic_friction: float = 0.5\\n    \"\"\"The dynamic friction coefficient. Defaults to 0.5.\"\"\"\\n\\n    restitution: float = 0.0\\n    \"\"\"The restitution coefficient. Defaults to 0.0.\"\"\"\\n\\n    improve_patch_friction: bool = True\\n    \"\"\"Whether to enable patch friction. Defaults to True.\"\"\"\\n\\n    friction_combine_mode: Literal[\"average\", \"min\", \"multiply\", \"max\"] = \"average\"\\n    \"\"\"Determines the way friction will be combined during collisions. Defaults to `\"average\"`.\\n\\n    .. attention::\\n\\n        When two physics materials with different combine modes collide, the combine mode with the higher\\n        priority will be used. The priority order is provided `here\\n        <https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/_api_build/structPxCombineMode.html>`__.\\n    \"\"\"\\n\\n    restitution_combine_mode: Literal[\"average\", \"min\", \"multiply\", \"max\"] = \"average\"\\n    \"\"\"Determines the way restitution coefficient will be combined during collisions. Defaults to `\"average\"`.\\n\\n    .. attention::\\n\\n        When two physics materials with different combine modes collide, the combine mode with the higher\\n        priority will be used. The priority order is provided `here\\n        <https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/_api_build/structPxCombineMode.html>`__.\\n    \"\"\"\\n\\n    compliant_contact_stiffness: float = 0.0\\n    \"\"\"Spring stiffness for a compliant contact model using implicit springs. Defaults to 0.0.\\n\\n    A higher stiffness results in behavior closer to a rigid contact. The compliant contact model is only enabled\\n    if the stiffness is larger than 0.\\n    \"\"\"\\n\\n    compliant_contact_damping: float = 0.0\\n    \"\"\"Damping coefficient for a compliant contact model using implicit springs. Defaults to 0.0.\\n\\n    Irrelevant if compliant contacts are disabled when :obj:`compliant_contact_stiffness` is set to zero and\\n    rigid contacts are active.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class DeformableBodyMaterialCfg(PhysicsMaterialCfg):\\n    \"\"\"Physics material parameters for deformable bodies.\\n\\n    See :meth:`spawn_deformable_body_material` for more information.\\n\\n    Note:\\n        The default values are the `default values used by PhysX 5\\n        <https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/deformable-bodies.html#deformable-body-material>`__.\\n    \"\"\"\\n\\n    func: Callable = physics_materials.spawn_deformable_body_material\\n\\n    density: float | None = None\\n    \"\"\"The material density. Defaults to None, in which case the simulation decides the default density.\"\"\"\\n\\n    dynamic_friction: float = 0.25\\n    \"\"\"The dynamic friction. Defaults to 0.25.\"\"\"\\n\\n    youngs_modulus: float = 50000000.0\\n    \"\"\"The Young\\'s modulus, which defines the body\\'s stiffness. Defaults to 50000000.0.\\n\\n    The Young\\'s modulus is a measure of the material\\'s ability to deform under stress. It is measured in Pascals (Pa).\\n    \"\"\"\\n\\n    poissons_ratio: float = 0.45\\n    \"\"\"The Poisson\\'s ratio which defines the body\\'s volume preservation. Defaults to 0.45.\\n\\n    The Poisson\\'s ratio is a measure of the material\\'s ability to expand in the lateral direction when compressed\\n    in the axial direction. It is a dimensionless number between 0 and 0.5. Using a value of 0.5 will make the\\n    material incompressible.\\n    \"\"\"\\n\\n    elasticity_damping: float = 0.005\\n    \"\"\"The elasticity damping for the deformable material. Defaults to 0.005.\"\"\"\\n\\n    damping_scale: float = 1.0\\n    \"\"\"The damping scale for the deformable material. Defaults to 1.0.\\n\\n    A scale of 1 corresponds to default damping. A value of 0 will only apply damping to certain motions leading\\n    to special effects that look similar to water filled soft bodies.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_preview_surface(prim_path: str, cfg: visual_materials_cfg.PreviewSurfaceCfg) -> Usd.Prim:\\n    \"\"\"Create a preview surface prim and override the settings with the given config.\\n\\n    A preview surface is a physically-based surface that handles simple shaders while supporting\\n    both *specular* and *metallic* workflows. All color inputs are in linear color space (RGB).\\n    For more information, see the `documentation <https://openusd.org/release/spec_usdpreviewsurface.html>`__.\\n\\n    The function calls the USD command `CreatePreviewSurfaceMaterialPrim`_ to create the prim.\\n\\n    .. _CreatePreviewSurfaceMaterialPrim: https://docs.omniverse.nvidia.com/kit/docs/omni.usd/latest/omni.usd.commands/omni.usd.commands.CreatePreviewSurfaceMaterialPrimCommand.html\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn material if it doesn\\'t exist.\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        omni.kit.commands.execute(\"CreatePreviewSurfaceMaterialPrim\", mtl_path=prim_path, select_new_prim=False)\\n    else:\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\'.\")\\n    # obtain prim\\n    prim = prim_utils.get_prim_at_path(f\"{prim_path}/Shader\")\\n    # apply properties\\n    cfg = cfg.to_dict()\\n    del cfg[\"func\"]\\n    for attr_name, attr_value in cfg.items():\\n        safe_set_attribute_on_usd_prim(prim, f\"inputs:{attr_name}\", attr_value, camel_case=True)\\n    # return prim\\n    return prim'),\n",
       " Document(metadata={}, page_content='def spawn_from_mdl_file(prim_path: str, cfg: visual_materials_cfg.MdlMaterialCfg) -> Usd.Prim:\\n    \"\"\"Load a material from its MDL file and override the settings with the given config.\\n\\n    NVIDIA\\'s `Material Definition Language (MDL) <https://www.nvidia.com/en-us/design-visualization/technologies/material-definition-language/>`__\\n    is a language for defining physically-based materials. The MDL file format is a binary format\\n    that can be loaded by Omniverse and other applications such as Adobe Substance Designer.\\n    To learn more about MDL, see the `documentation <https://docs.omniverse.nvidia.com/materials-and-rendering/latest/materials.html>`_.\\n\\n    The function calls the USD command `CreateMdlMaterialPrim`_ to create the prim.\\n\\n    .. _CreateMdlMaterialPrim: https://docs.omniverse.nvidia.com/kit/docs/omni.usd/latest/omni.usd.commands/omni.usd.commands.CreateMdlMaterialPrimCommand.html\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn material if it doesn\\'t exist.\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        # extract material name from path\\n        material_name = cfg.mdl_path.split(\"/\")[-1].split(\".\")[0]\\n        omni.kit.commands.execute(\\n            \"CreateMdlMaterialPrim\",\\n            mtl_url=cfg.mdl_path.format(NVIDIA_NUCLEUS_DIR=NVIDIA_NUCLEUS_DIR),\\n            mtl_name=material_name,\\n            mtl_path=prim_path,\\n            select_new_prim=False,\\n        )\\n    else:\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\'.\")\\n    # obtain prim\\n    prim = prim_utils.get_prim_at_path(f\"{prim_path}/Shader\")\\n    # apply properties\\n    cfg = cfg.to_dict()\\n    del cfg[\"func\"]\\n    del cfg[\"mdl_path\"]\\n    for attr_name, attr_value in cfg.items():\\n        safe_set_attribute_on_usd_prim(prim, f\"inputs:{attr_name}\", attr_value, camel_case=False)\\n    # return prim\\n    return prim'),\n",
       " Document(metadata={}, page_content='class VisualMaterialCfg:\\n    \"\"\"Configuration parameters for creating a visual material.\"\"\"\\n\\n    func: Callable = MISSING\\n    \"\"\"The function to use for creating the material.\"\"\"'),\n",
       " Document(metadata={}, page_content='class PreviewSurfaceCfg(VisualMaterialCfg):\\n    \"\"\"Configuration parameters for creating a preview surface.\\n\\n    See :meth:`spawn_preview_surface` for more information.\\n    \"\"\"\\n\\n    func: Callable = visual_materials.spawn_preview_surface\\n\\n    diffuse_color: tuple[float, float, float] = (0.18, 0.18, 0.18)\\n    \"\"\"The RGB diffusion color. This is the base color of the surface. Defaults to a dark gray.\"\"\"\\n    emissive_color: tuple[float, float, float] = (0.0, 0.0, 0.0)\\n    \"\"\"The RGB emission component of the surface. Defaults to black.\"\"\"\\n    roughness: float = 0.5\\n    \"\"\"The roughness for specular lobe. Ranges from 0 (smooth) to 1 (rough). Defaults to 0.5.\"\"\"\\n    metallic: float = 0.0\\n    \"\"\"The metallic component. Ranges from 0 (dielectric) to 1 (metal). Defaults to 0.\"\"\"\\n    opacity: float = 1.0\\n    \"\"\"The opacity of the surface. Ranges from 0 (transparent) to 1 (opaque). Defaults to 1.\\n\\n    Note:\\n        Opacity only affects the surface\\'s appearance during interactive rendering.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class MdlFileCfg(VisualMaterialCfg):\\n    \"\"\"Configuration parameters for loading an MDL material from a file.\\n\\n    See :meth:`spawn_from_mdl_file` for more information.\\n    \"\"\"\\n\\n    func: Callable = visual_materials.spawn_from_mdl_file\\n\\n    mdl_path: str = MISSING\\n    \"\"\"The path to the MDL material.\\n\\n    NVIDIA Omniverse provides various MDL materials in the NVIDIA Nucleus.\\n    To use these materials, you can set the path of the material in the nucleus directory\\n    using the ``{NVIDIA_NUCLEUS_DIR}`` variable. This is internally resolved to the path of the\\n    NVIDIA Nucleus directory on the host machine through the attribute\\n    :attr:`isaaclab.utils.assets.NVIDIA_NUCLEUS_DIR`.\\n\\n    For example, to use the \"Aluminum_Anodized\" material, you can set the path to:\\n    ``{NVIDIA_NUCLEUS_DIR}/Materials/Base/Metals/Aluminum_Anodized.mdl``.\\n    \"\"\"\\n    project_uvw: bool | None = None\\n    \"\"\"Whether to project the UVW coordinates of the material. Defaults to None.\\n\\n    If None, then the default setting in the MDL material will be used.\\n    \"\"\"\\n    albedo_brightness: float | None = None\\n    \"\"\"Multiplier for the diffuse color of the material. Defaults to None.\\n\\n    If None, then the default setting in the MDL material will be used.\\n    \"\"\"\\n    texture_scale: tuple[float, float] | None = None\\n    \"\"\"The scale of the texture. Defaults to None.\\n\\n    If None, then the default setting in the MDL material will be used.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class GlassMdlCfg(VisualMaterialCfg):\\n    \"\"\"Configuration parameters for loading a glass MDL material.\\n\\n    This is a convenience class for loading a glass MDL material. For more information on\\n    glass materials, see the `documentation <https://docs.omniverse.nvidia.com/materials-and-rendering/latest/materials.html#omniglass>`__.\\n\\n    .. note::\\n        The default values are taken from the glass material in the NVIDIA Nucleus.\\n    \"\"\"\\n\\n    func: Callable = visual_materials.spawn_from_mdl_file\\n\\n    mdl_path: str = \"OmniGlass.mdl\"\\n    \"\"\"The path to the MDL material. Defaults to the glass material in the NVIDIA Nucleus.\"\"\"\\n    glass_color: tuple[float, float, float] = (1.0, 1.0, 1.0)\\n    \"\"\"The RGB color or tint of the glass. Defaults to white.\"\"\"\\n    frosting_roughness: float = 0.0\\n    \"\"\"The amount of reflectivity of the surface. Ranges from 0 (perfectly clear) to 1 (frosted).\\n    Defaults to 0.\"\"\"\\n    thin_walled: bool = False\\n    \"\"\"Whether to perform thin-walled refraction. Defaults to False.\"\"\"\\n    glass_ior: float = 1.491\\n    \"\"\"The incidence of refraction to control how much light is bent when passing through the glass.\\n    Defaults to 1.491, which is the IOR of glass.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_mesh_sphere(\\n    prim_path: str,\\n    cfg: meshes_cfg.MeshSphereCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USD-Mesh sphere prim with the given attributes.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # create a trimesh sphere\\n    sphere = trimesh.creation.uv_sphere(radius=cfg.radius)\\n    # spawn the sphere as a mesh\\n    _spawn_mesh_geom_from_mesh(prim_path, cfg, sphere, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_mesh_cuboid(\\n    prim_path: str,\\n    cfg: meshes_cfg.MeshCuboidCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USD-Mesh cuboid prim with the given attributes.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"  # create a trimesh box\\n    box = trimesh.creation.box(cfg.size)\\n    # spawn the cuboid as a mesh\\n    _spawn_mesh_geom_from_mesh(prim_path, cfg, box, translation, orientation, None)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_mesh_cylinder(\\n    prim_path: str,\\n    cfg: meshes_cfg.MeshCylinderCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USD-Mesh cylinder prim with the given attributes.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # align axis from \"Z\" to input by rotating the cylinder\\n    axis = cfg.axis.upper()\\n    if axis == \"X\":\\n        transform = trimesh.transformations.rotation_matrix(np.pi / 2, [0, 1, 0])\\n    elif axis == \"Y\":\\n        transform = trimesh.transformations.rotation_matrix(-np.pi / 2, [1, 0, 0])\\n    else:\\n        transform = None\\n    # create a trimesh cylinder\\n    cylinder = trimesh.creation.cylinder(radius=cfg.radius, height=cfg.height, transform=transform)\\n    # spawn the cylinder as a mesh\\n    _spawn_mesh_geom_from_mesh(prim_path, cfg, cylinder, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_mesh_capsule(\\n    prim_path: str,\\n    cfg: meshes_cfg.MeshCapsuleCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USD-Mesh capsule prim with the given attributes.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # align axis from \"Z\" to input by rotating the cylinder\\n    axis = cfg.axis.upper()\\n    if axis == \"X\":\\n        transform = trimesh.transformations.rotation_matrix(np.pi / 2, [0, 1, 0])\\n    elif axis == \"Y\":\\n        transform = trimesh.transformations.rotation_matrix(-np.pi / 2, [1, 0, 0])\\n    else:\\n        transform = None\\n    # create a trimesh capsule\\n    capsule = trimesh.creation.capsule(radius=cfg.radius, height=cfg.height, transform=transform)\\n    # spawn capsule if it doesn\\'t exist.\\n    _spawn_mesh_geom_from_mesh(prim_path, cfg, capsule, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_mesh_cone(\\n    prim_path: str,\\n    cfg: meshes_cfg.MeshConeCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USD-Mesh cone prim with the given attributes.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # align axis from \"Z\" to input by rotating the cylinder\\n    axis = cfg.axis.upper()\\n    if axis == \"X\":\\n        transform = trimesh.transformations.rotation_matrix(np.pi / 2, [0, 1, 0])\\n    elif axis == \"Y\":\\n        transform = trimesh.transformations.rotation_matrix(-np.pi / 2, [1, 0, 0])\\n    else:\\n        transform = None\\n    # create a trimesh cone\\n    cone = trimesh.creation.cone(radius=cfg.radius, height=cfg.height, transform=transform)\\n    # spawn cone if it doesn\\'t exist.\\n    _spawn_mesh_geom_from_mesh(prim_path, cfg, cone, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def _spawn_mesh_geom_from_mesh(\\n    prim_path: str,\\n    cfg: meshes_cfg.MeshCfg,\\n    mesh: trimesh.Trimesh,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n    scale: tuple[float, float, float] | None = None,\\n):\\n    \"\"\"Create a `USDGeomMesh`_ prim from the given mesh.\\n\\n    This function is similar to :func:`shapes._spawn_geom_from_prim_type` but spawns the prim from a given mesh.\\n    In case of the mesh, it is spawned as a USDGeomMesh prim with the given vertices and faces.\\n\\n    There is a difference in how the properties are applied to the prim based on the type of object:\\n\\n    - Deformable body properties: The properties are applied to the mesh prim: ``{prim_path}/geometry/mesh``.\\n    - Collision properties: The properties are applied to the mesh prim: ``{prim_path}/geometry/mesh``.\\n    - Rigid body properties: The properties are applied to the parent prim: ``{prim_path}``.\\n\\n    Args:\\n        prim_path: The prim path to spawn the asset at.\\n        cfg: The config containing the properties to apply.\\n        mesh: The mesh to spawn the prim from.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n        scale: The scale to apply to the prim. Defaults to None, in which case this is set to identity.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n        ValueError: If both deformable and rigid properties are used.\\n        ValueError: If both deformable and collision properties are used.\\n        ValueError: If the physics material is not of the correct type. Deformable properties require a deformable\\n            physics material, and rigid properties require a rigid physics material.\\n\\n    .. _USDGeomMesh: https://openusd.org/dev/api/class_usd_geom_mesh.html\\n    \"\"\"\\n    # spawn geometry if it doesn\\'t exist.\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        prim_utils.create_prim(prim_path, prim_type=\"Xform\", translation=translation, orientation=orientation)\\n    else:\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\'.\")\\n\\n    # check that invalid schema types are not used\\n    if cfg.deformable_props is not None and cfg.rigid_props is not None:\\n        raise ValueError(\"Cannot use both deformable and rigid properties at the same time.\")\\n    if cfg.deformable_props is not None and cfg.collision_props is not None:\\n        raise ValueError(\"Cannot use both deformable and collision properties at the same time.\")\\n    # check material types are correct\\n    if cfg.deformable_props is not None and cfg.physics_material is not None:\\n        if not isinstance(cfg.physics_material, DeformableBodyMaterialCfg):\\n            raise ValueError(\"Deformable properties require a deformable physics material.\")\\n    if cfg.rigid_props is not None and cfg.physics_material is not None:\\n        if not isinstance(cfg.physics_material, RigidBodyMaterialCfg):\\n            raise ValueError(\"Rigid properties require a rigid physics material.\")\\n\\n    # create all the paths we need for clarity\\n    geom_prim_path = prim_path + \"/geometry\"\\n    mesh_prim_path = geom_prim_path + \"/mesh\"\\n\\n    # create the mesh prim\\n    mesh_prim = prim_utils.create_prim(\\n        mesh_prim_path,\\n        prim_type=\"Mesh\",\\n        scale=scale,\\n        attributes={\\n            \"points\": mesh.vertices,\\n            \"faceVertexIndices\": mesh.faces.flatten(),\\n            \"faceVertexCounts\": np.asarray([3] * len(mesh.faces)),\\n            \"subdivisionScheme\": \"bilinear\",\\n        },\\n    )\\n\\n    # note: in case of deformable objects, we need to apply the deformable properties to the mesh prim.\\n    #   this is different from rigid objects where we apply the properties to the parent prim.\\n    if cfg.deformable_props is not None:\\n        # apply mass properties\\n        if cfg.mass_props is not None:\\n            schemas.define_mass_properties(mesh_prim_path, cfg.mass_props)\\n        # apply deformable body properties\\n        schemas.define_deformable_body_properties(mesh_prim_path, cfg.deformable_props)\\n    elif cfg.collision_props is not None:\\n        # decide on type of collision approximation based on the mesh\\n        if cfg.__class__.__name__ == \"MeshSphereCfg\":\\n            collision_approximation = \"boundingSphere\"\\n        elif cfg.__class__.__name__ == \"MeshCuboidCfg\":\\n            collision_approximation = \"boundingCube\"\\n        else:\\n            # for: MeshCylinderCfg, MeshCapsuleCfg, MeshConeCfg\\n            collision_approximation = \"convexHull\"\\n        # apply collision approximation to mesh\\n        # note: for primitives, we use the convex hull approximation -- this should be sufficient for most cases.\\n        mesh_collision_api = UsdPhysics.MeshCollisionAPI.Apply(mesh_prim)\\n        mesh_collision_api.GetApproximationAttr().Set(collision_approximation)\\n        # apply collision properties\\n        schemas.define_collision_properties(mesh_prim_path, cfg.collision_props)\\n\\n    # apply visual material\\n    if cfg.visual_material is not None:\\n        if not cfg.visual_material_path.startswith(\"/\"):\\n            material_path = f\"{geom_prim_path}/{cfg.visual_material_path}\"\\n        else:\\n            material_path = cfg.visual_material_path\\n        # create material\\n        cfg.visual_material.func(material_path, cfg.visual_material)\\n        # apply material\\n        bind_visual_material(mesh_prim_path, material_path)\\n\\n    # apply physics material\\n    if cfg.physics_material is not None:\\n        if not cfg.physics_material_path.startswith(\"/\"):\\n            material_path = f\"{geom_prim_path}/{cfg.physics_material_path}\"\\n        else:\\n            material_path = cfg.physics_material_path\\n        # create material\\n        cfg.physics_material.func(material_path, cfg.physics_material)\\n        # apply material\\n        bind_physics_material(mesh_prim_path, material_path)\\n\\n    # note: we apply the rigid properties to the parent prim in case of rigid objects.\\n    if cfg.rigid_props is not None:\\n        # apply mass properties\\n        if cfg.mass_props is not None:\\n            schemas.define_mass_properties(prim_path, cfg.mass_props)\\n        # apply rigid properties\\n        schemas.define_rigid_body_properties(prim_path, cfg.rigid_props)'),\n",
       " Document(metadata={}, page_content='class MeshCfg(RigidObjectSpawnerCfg, DeformableObjectSpawnerCfg):\\n    \"\"\"Configuration parameters for a USD Geometry or Geom prim.\\n\\n    This class is similar to :class:`ShapeCfg` but is specifically for meshes.\\n\\n    Meshes support both rigid and deformable properties. However, their schemas are applied at\\n    different levels in the USD hierarchy based on the type of the object. These are described below:\\n\\n    - Deformable body properties: Applied to the mesh prim: ``{prim_path}/geometry/mesh``.\\n    - Collision properties: Applied to the mesh prim: ``{prim_path}/geometry/mesh``.\\n    - Rigid body properties: Applied to the parent prim: ``{prim_path}``.\\n\\n    where ``{prim_path}`` is the path to the prim in the USD stage and ``{prim_path}/geometry/mesh``\\n    is the path to the mesh prim.\\n\\n    .. note::\\n        There are mututally exclusive parameters for rigid and deformable properties. If both are set,\\n        then an error will be raised. This also holds if collision and deformable properties are set together.\\n\\n    \"\"\"\\n\\n    visual_material_path: str = \"material\"\\n    \"\"\"Path to the visual material to use for the prim. Defaults to \"material\".\\n\\n    If the path is relative, then it will be relative to the prim\\'s path.\\n    This parameter is ignored if `visual_material` is not None.\\n    \"\"\"\\n\\n    visual_material: materials.VisualMaterialCfg | None = None\\n    \"\"\"Visual material properties.\\n\\n    Note:\\n        If None, then no visual material will be added.\\n    \"\"\"\\n\\n    physics_material_path: str = \"material\"\\n    \"\"\"Path to the physics material to use for the prim. Defaults to \"material\".\\n\\n    If the path is relative, then it will be relative to the prim\\'s path.\\n    This parameter is ignored if `physics_material` is not None.\\n    \"\"\"\\n\\n    physics_material: materials.PhysicsMaterialCfg | None = None\\n    \"\"\"Physics material properties.\\n\\n    Note:\\n        If None, then no physics material will be added.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshSphereCfg(MeshCfg):\\n    \"\"\"Configuration parameters for a sphere mesh prim with deformable properties.\\n\\n    See :meth:`spawn_mesh_sphere` for more information.\\n    \"\"\"\\n\\n    func: Callable = meshes.spawn_mesh_sphere\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the sphere (in m).\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshCuboidCfg(MeshCfg):\\n    \"\"\"Configuration parameters for a cuboid mesh prim with deformable properties.\\n\\n    See :meth:`spawn_mesh_cuboid` for more information.\\n    \"\"\"\\n\\n    func: Callable = meshes.spawn_mesh_cuboid\\n\\n    size: tuple[float, float, float] = MISSING\\n    \"\"\"Size of the cuboid (in m).\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshCylinderCfg(MeshCfg):\\n    \"\"\"Configuration parameters for a cylinder mesh prim with deformable properties.\\n\\n    See :meth:`spawn_cylinder` for more information.\\n    \"\"\"\\n\\n    func: Callable = meshes.spawn_mesh_cylinder\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the cylinder (in m).\"\"\"\\n    height: float = MISSING\\n    \"\"\"Height of the cylinder (in m).\"\"\"\\n    axis: Literal[\"X\", \"Y\", \"Z\"] = \"Z\"\\n    \"\"\"Axis of the cylinder. Defaults to \"Z\".\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshCapsuleCfg(MeshCfg):\\n    \"\"\"Configuration parameters for a capsule mesh prim.\\n\\n    See :meth:`spawn_capsule` for more information.\\n    \"\"\"\\n\\n    func: Callable = meshes.spawn_mesh_capsule\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the capsule (in m).\"\"\"\\n    height: float = MISSING\\n    \"\"\"Height of the capsule (in m).\"\"\"\\n    axis: Literal[\"X\", \"Y\", \"Z\"] = \"Z\"\\n    \"\"\"Axis of the capsule. Defaults to \"Z\".\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshConeCfg(MeshCfg):\\n    \"\"\"Configuration parameters for a cone mesh prim.\\n\\n    See :meth:`spawn_cone` for more information.\\n    \"\"\"\\n\\n    func: Callable = meshes.spawn_mesh_cone\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the cone (in m).\"\"\"\\n    height: float = MISSING\\n    \"\"\"Height of the v (in m).\"\"\"\\n    axis: Literal[\"X\", \"Y\", \"Z\"] = \"Z\"\\n    \"\"\"Axis of the cone. Defaults to \"Z\".\"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_camera(\\n    prim_path: str,\\n    cfg: sensors_cfg.PinholeCameraCfg | sensors_cfg.FisheyeCameraCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USD camera prim with given projection type.\\n\\n    The function creates various attributes on the camera prim that specify the camera\\'s properties.\\n    These are later used by ``omni.replicator.core`` to render the scene with the given camera.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn camera if it doesn\\'t exist.\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        prim_utils.create_prim(prim_path, \"Camera\", translation=translation, orientation=orientation)\\n    else:\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\'.\")\\n\\n    # lock camera from viewport (this disables viewport movement for camera)\\n    if cfg.lock_camera:\\n        omni.kit.commands.execute(\\n            \"ChangePropertyCommand\",\\n            prop_path=Sdf.Path(f\"{prim_path}.omni:kit:cameraLock\"),\\n            value=True,\\n            prev=None,\\n            type_to_create_if_not_exist=Sdf.ValueTypeNames.Bool,\\n        )\\n    # decide the custom attributes to add\\n    if cfg.projection_type == \"pinhole\":\\n        attribute_types = CUSTOM_PINHOLE_CAMERA_ATTRIBUTES\\n    else:\\n        attribute_types = CUSTOM_FISHEYE_CAMERA_ATTRIBUTES\\n\\n    # TODO: Adjust to handle aperture offsets once supported by omniverse\\n    #   Internal ticket from rendering team: OM-42611\\n    if cfg.horizontal_aperture_offset > 1e-4 or cfg.vertical_aperture_offset > 1e-4:\\n        omni.log.warn(\"Camera aperture offsets are not supported by Omniverse. These parameters will be ignored.\")\\n\\n    # custom attributes in the config that are not USD Camera parameters\\n    non_usd_cfg_param_names = [\\n        \"func\",\\n        \"copy_from_source\",\\n        \"lock_camera\",\\n        \"visible\",\\n        \"semantic_tags\",\\n        \"from_intrinsic_matrix\",\\n    ]\\n    # get camera prim\\n    prim = prim_utils.get_prim_at_path(prim_path)\\n    # create attributes for the fisheye camera model\\n    # note: for pinhole those are already part of the USD camera prim\\n    for attr_name, attr_type in attribute_types.values():\\n        # check if attribute does not exist\\n        if prim.GetAttribute(attr_name).Get() is None:\\n            # create attribute based on type\\n            prim.CreateAttribute(attr_name, attr_type)\\n    # set attribute values\\n    for param_name, param_value in cfg.__dict__.items():\\n        # check if value is valid\\n        if param_value is None or param_name in non_usd_cfg_param_names:\\n            continue\\n        # obtain prim property name\\n        if param_name in attribute_types:\\n            # check custom attributes\\n            prim_prop_name = attribute_types[param_name][0]\\n        else:\\n            # convert attribute name in prim to cfg name\\n            prim_prop_name = to_camel_case(param_name, to=\"cC\")\\n        # get attribute from the class\\n        prim.GetAttribute(prim_prop_name).Set(param_value)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='class PinholeCameraCfg(SpawnerCfg):\\n    \"\"\"Configuration parameters for a USD camera prim with pinhole camera settings.\\n\\n    For more information on the parameters, please refer to the `camera documentation <https://docs.omniverse.nvidia.com/materials-and-rendering/latest/cameras.html>`__.\\n\\n    ..note ::\\n        Focal length as well as the aperture sizes and offsets are set as a tenth of the world unit. In our case, the\\n        world unit is Meter s.t. all of these values are set in cm.\\n\\n    .. note::\\n        The default values are taken from the `Replicator camera <https://docs.omniverse.nvidia.com/py/replicator/1.9.8/source/omni.replicator.core/docs/API.html#omni.replicator.core.create.camera>`__\\n        function.\\n    \"\"\"\\n\\n    func: Callable = sensors.spawn_camera\\n\\n    projection_type: str = \"pinhole\"\\n    \"\"\"Type of projection to use for the camera. Defaults to \"pinhole\".\\n\\n    Note:\\n        Currently only \"pinhole\" is supported.\\n    \"\"\"\\n\\n    clipping_range: tuple[float, float] = (0.01, 1e6)\\n    \"\"\"Near and far clipping distances (in m). Defaults to (0.01, 1e6).\\n\\n    The minimum clipping range will shift the camera forward by the specified distance. Don\\'t set it too high to\\n    avoid issues for distance related data types (e.g., ``distance_to_image_plane``).\\n    \"\"\"\\n\\n    focal_length: float = 24.0\\n    \"\"\"Perspective focal length (in cm). Defaults to 24.0cm.\\n\\n    Longer lens lengths narrower FOV, shorter lens lengths wider FOV.\\n    \"\"\"\\n\\n    focus_distance: float = 400.0\\n    \"\"\"Distance from the camera to the focus plane (in m). Defaults to 400.0.\\n\\n    The distance at which perfect sharpness is achieved.\\n    \"\"\"\\n\\n    f_stop: float = 0.0\\n    \"\"\"Lens aperture. Defaults to 0.0, which turns off focusing.\\n\\n    Controls Distance Blurring. Lower Numbers decrease focus range, larger numbers increase it.\\n    \"\"\"\\n\\n    horizontal_aperture: float = 20.955\\n    \"\"\"Horizontal aperture (in cm). Defaults to 20.955 cm.\\n\\n    Emulates sensor/film width on a camera.\\n\\n    Note:\\n        The default value is the horizontal aperture of a 35 mm spherical projector.\\n    \"\"\"\\n\\n    vertical_aperture: float | None = None\\n    r\"\"\"Vertical aperture (in mm). Defaults to None.\\n\\n    Emulates sensor/film height on a camera. If None, then the vertical aperture is calculated based on the\\n    horizontal aperture and the aspect ratio of the image to maintain squared pixels. This is calculated as:\\n\\n    .. math::\\n        \\\\text{vertical aperture} = \\\\text{horizontal aperture} \\\\times \\\\frac{\\\\text{height}}{\\\\text{width}}\\n    \"\"\"\\n\\n    horizontal_aperture_offset: float = 0.0\\n    \"\"\"Offsets Resolution/Film gate horizontally. Defaults to 0.0.\"\"\"\\n\\n    vertical_aperture_offset: float = 0.0\\n    \"\"\"Offsets Resolution/Film gate vertically. Defaults to 0.0.\"\"\"\\n\\n    lock_camera: bool = True\\n    \"\"\"Locks the camera in the Omniverse viewport. Defaults to True.\\n\\n    If True, then the camera remains fixed at its configured transform. This is useful when wanting to view\\n    the camera output on the GUI and not accidentally moving the camera through the GUI interactions.\\n    \"\"\"\\n\\n    @classmethod\\n    def from_intrinsic_matrix(\\n        cls,\\n        intrinsic_matrix: list[float],\\n        width: int,\\n        height: int,\\n        clipping_range: tuple[float, float] = (0.01, 1e6),\\n        focal_length: float | None = None,\\n        focus_distance: float = 400.0,\\n        f_stop: float = 0.0,\\n        projection_type: str = \"pinhole\",\\n        lock_camera: bool = True,\\n    ) -> PinholeCameraCfg:\\n        r\"\"\"Create a :class:`PinholeCameraCfg` class instance from an intrinsic matrix.\\n\\n        The intrinsic matrix is a 3x3 matrix that defines the mapping between the 3D world coordinates and\\n        the 2D image. The matrix is defined as:\\n\\n        .. math::\\n            I_{cam} = \\\\begin{bmatrix}\\n            f_x & 0 & c_x \\\\\\\\\\n            0 & f_y & c_y \\\\\\\\\\n            0 & 0 & 1\\n            \\\\\\\\end{bmatrix},\\n\\n        where :math:`f_x` and :math:`f_y` are the focal length along x and y direction, while :math:`c_x` and :math:`c_y` are the\\n        principle point offsets along x and y direction respectively.\\n\\n        Args:\\n            intrinsic_matrix: Intrinsic matrix of the camera in row-major format.\\n                The matrix is defined as [f_x, 0, c_x, 0, f_y, c_y, 0, 0, 1]. Shape is (9,).\\n            width: Width of the image (in pixels).\\n            height: Height of the image (in pixels).\\n            clipping_range: Near and far clipping distances (in m). Defaults to (0.01, 1e6).\\n            focal_length: Perspective focal length (in cm) used to calculate pixel size. Defaults to None. If None\\n                focal_length will be calculated 1 / width.\\n            focus_distance: Distance from the camera to the focus plane (in m). Defaults to 400.0 m.\\n            f_stop: Lens aperture. Defaults to 0.0, which turns off focusing.\\n            projection_type: Type of projection to use for the camera. Defaults to \"pinhole\".\\n            lock_camera: Locks the camera in the Omniverse viewport. Defaults to True.\\n\\n        Returns:\\n            An instance of the :class:`PinholeCameraCfg` class.\\n        \"\"\"\\n        # raise not implemented error is projection type is not pinhole\\n        if projection_type != \"pinhole\":\\n            raise NotImplementedError(\"Only pinhole projection type is supported.\")\\n\\n        usd_camera_params = sensor_utils.convert_camera_intrinsics_to_usd(\\n            intrinsic_matrix=intrinsic_matrix, height=height, width=width, focal_length=focal_length\\n        )\\n\\n        return cls(\\n            projection_type=projection_type,\\n            clipping_range=clipping_range,\\n            focal_length=usd_camera_params[\"focal_length\"],\\n            focus_distance=focus_distance,\\n            f_stop=f_stop,\\n            horizontal_aperture=usd_camera_params[\"horizontal_aperture\"],\\n            vertical_aperture=usd_camera_params[\"vertical_aperture\"],\\n            horizontal_aperture_offset=usd_camera_params[\"horizontal_aperture_offset\"],\\n            vertical_aperture_offset=usd_camera_params[\"vertical_aperture_offset\"],\\n            lock_camera=lock_camera,\\n        )'),\n",
       " Document(metadata={}, page_content='class FisheyeCameraCfg(PinholeCameraCfg):\\n    \"\"\"Configuration parameters for a USD camera prim with `fish-eye camera`_ settings.\\n\\n    For more information on the parameters, please refer to the\\n    `camera documentation <https://docs.omniverse.nvidia.com/materials-and-rendering/latest/cameras.html#fisheye-properties>`__.\\n\\n    .. note::\\n        The default values are taken from the `Replicator camera <https://docs.omniverse.nvidia.com/py/replicator/1.9.8/source/omni.replicator.core/docs/API.html#omni.replicator.core.create.camera>`__\\n        function.\\n\\n    .. _fish-eye camera: https://en.wikipedia.org/wiki/Fisheye_lens\\n    \"\"\"\\n\\n    func: Callable = sensors.spawn_camera\\n\\n    projection_type: Literal[\\n        \"fisheyePolynomial\",\\n        \"fisheyeSpherical\",\\n        \"fisheyeKannalaBrandtK3\",\\n        \"fisheyeRadTanThinPrism\",\\n        \"omniDirectionalStereo\",\\n    ] = \"fisheyePolynomial\"\\n    r\"\"\"Type of projection to use for the camera. Defaults to \"fisheyePolynomial\".\\n\\n    Available options:\\n\\n    - ``\"fisheyePolynomial\"``: Fisheye camera model with :math:`360^{\\\\circ}` spherical projection.\\n    - ``\"fisheyeSpherical\"``: Fisheye camera model with :math:`360^{\\\\circ}` full-frame projection.\\n    - ``\"fisheyeKannalaBrandtK3\"``: Fisheye camera model using the Kannala-Brandt K3 distortion model.\\n    - ``\"fisheyeRadTanThinPrism\"``: Fisheye camera model that combines radial and tangential distortions.\\n    - ``\"omniDirectionalStereo\"``: Fisheye camera model supporting :math:`360^{\\\\circ}` stereoscopic imaging.\\n    \"\"\"\\n\\n    fisheye_nominal_width: float = 1936.0\\n    \"\"\"Nominal width of fisheye lens model (in pixels). Defaults to 1936.0.\"\"\"\\n\\n    fisheye_nominal_height: float = 1216.0\\n    \"\"\"Nominal height of fisheye lens model (in pixels). Defaults to 1216.0.\"\"\"\\n\\n    fisheye_optical_centre_x: float = 970.94244\\n    \"\"\"Horizontal optical centre position of fisheye lens model (in pixels). Defaults to 970.94244.\"\"\"\\n\\n    fisheye_optical_centre_y: float = 600.37482\\n    \"\"\"Vertical optical centre position of fisheye lens model (in pixels). Defaults to 600.37482.\"\"\"\\n\\n    fisheye_max_fov: float = 200.0\\n    \"\"\"Maximum field of view of fisheye lens model (in degrees). Defaults to 200.0 degrees.\"\"\"\\n\\n    fisheye_polynomial_a: float = 0.0\\n    \"\"\"First component of fisheye polynomial. Defaults to 0.0.\"\"\"\\n\\n    fisheye_polynomial_b: float = 0.00245\\n    \"\"\"Second component of fisheye polynomial. Defaults to 0.00245.\"\"\"\\n\\n    fisheye_polynomial_c: float = 0.0\\n    \"\"\"Third component of fisheye polynomial. Defaults to 0.0.\"\"\"\\n\\n    fisheye_polynomial_d: float = 0.0\\n    \"\"\"Fourth component of fisheye polynomial. Defaults to 0.0.\"\"\"\\n\\n    fisheye_polynomial_e: float = 0.0\\n    \"\"\"Fifth component of fisheye polynomial. Defaults to 0.0.\"\"\"\\n\\n    fisheye_polynomial_f: float = 0.0\\n    \"\"\"Sixth component of fisheye polynomial. Defaults to 0.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_sphere(\\n    prim_path: str,\\n    cfg: shapes_cfg.SphereCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USDGeom-based sphere prim with the given attributes.\\n\\n    For more information, see `USDGeomSphere <https://openusd.org/dev/api/class_usd_geom_sphere.html>`_.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn sphere if it doesn\\'t exist.\\n    attributes = {\"radius\": cfg.radius}\\n    _spawn_geom_from_prim_type(prim_path, cfg, \"Sphere\", attributes, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_cuboid(\\n    prim_path: str,\\n    cfg: shapes_cfg.CuboidCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USDGeom-based cuboid prim with the given attributes.\\n\\n    For more information, see `USDGeomCube <https://openusd.org/dev/api/class_usd_geom_cube.html>`_.\\n\\n    Note:\\n        Since USD only supports cubes, we set the size of the cube to the minimum of the given size and\\n        scale the cube accordingly.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        If a prim already exists at the given path.\\n    \"\"\"\\n    # resolve the scale\\n    size = min(cfg.size)\\n    scale = [dim / size for dim in cfg.size]\\n    # spawn cuboid if it doesn\\'t exist.\\n    attributes = {\"size\": size}\\n    _spawn_geom_from_prim_type(prim_path, cfg, \"Cube\", attributes, translation, orientation, scale)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_cylinder(\\n    prim_path: str,\\n    cfg: shapes_cfg.CylinderCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USDGeom-based cylinder prim with the given attributes.\\n\\n    For more information, see `USDGeomCylinder <https://openusd.org/dev/api/class_usd_geom_cylinder.html>`_.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn cylinder if it doesn\\'t exist.\\n    attributes = {\"radius\": cfg.radius, \"height\": cfg.height, \"axis\": cfg.axis.upper()}\\n    _spawn_geom_from_prim_type(prim_path, cfg, \"Cylinder\", attributes, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_capsule(\\n    prim_path: str,\\n    cfg: shapes_cfg.CapsuleCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USDGeom-based capsule prim with the given attributes.\\n\\n    For more information, see `USDGeomCapsule <https://openusd.org/dev/api/class_usd_geom_capsule.html>`_.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn capsule if it doesn\\'t exist.\\n    attributes = {\"radius\": cfg.radius, \"height\": cfg.height, \"axis\": cfg.axis.upper()}\\n    _spawn_geom_from_prim_type(prim_path, cfg, \"Capsule\", attributes, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def spawn_cone(\\n    prim_path: str,\\n    cfg: shapes_cfg.ConeCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Create a USDGeom-based cone prim with the given attributes.\\n\\n    For more information, see `USDGeomCone <https://openusd.org/dev/api/class_usd_geom_cone.html>`_.\\n\\n    .. note::\\n        This function is decorated with :func:`clone` that resolves prim path into list of paths\\n        if the input prim path is a regex pattern. This is done to support spawning multiple assets\\n        from a single and cloning the USD prim at the given path expression.\\n\\n    Args:\\n        prim_path: The prim path or pattern to spawn the asset at. If the prim path is a regex pattern,\\n            then the asset is spawned at all the matching prim paths.\\n        cfg: The configuration instance.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n\\n    Returns:\\n        The created prim.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn cone if it doesn\\'t exist.\\n    attributes = {\"radius\": cfg.radius, \"height\": cfg.height, \"axis\": cfg.axis.upper()}\\n    _spawn_geom_from_prim_type(prim_path, cfg, \"Cone\", attributes, translation, orientation)\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_path)'),\n",
       " Document(metadata={}, page_content='def _spawn_geom_from_prim_type(\\n    prim_path: str,\\n    cfg: shapes_cfg.ShapeCfg,\\n    prim_type: str,\\n    attributes: dict,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n    scale: tuple[float, float, float] | None = None,\\n):\\n    \"\"\"Create a USDGeom-based prim with the given attributes.\\n\\n    To make the asset instanceable, we must follow a certain structure dictated by how USD scene-graph\\n    instancing and physics work. The rigid body component must be added to each instance and not the\\n    referenced asset (i.e. the prototype prim itself). This is because the rigid body component defines\\n    properties that are specific to each instance and cannot be shared under the referenced asset. For\\n    more information, please check the `documentation <https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/rigid-bodies.html#instancing-rigid-bodies>`_.\\n\\n    Due to the above, we follow the following structure:\\n\\n    * ``{prim_path}`` - The root prim that is an Xform with the rigid body and mass APIs if configured.\\n    * ``{prim_path}/geometry`` - The prim that contains the mesh and optionally the materials if configured.\\n      If instancing is enabled, this prim will be an instanceable reference to the prototype prim.\\n\\n    Args:\\n        prim_path: The prim path to spawn the asset at.\\n        cfg: The config containing the properties to apply.\\n        prim_type: The type of prim to create.\\n        attributes: The attributes to apply to the prim.\\n        translation: The translation to apply to the prim w.r.t. its parent prim. Defaults to None, in which case\\n            this is set to the origin.\\n        orientation: The orientation in (w, x, y, z) to apply to the prim w.r.t. its parent prim. Defaults to None,\\n            in which case this is set to identity.\\n        scale: The scale to apply to the prim. Defaults to None, in which case this is set to identity.\\n\\n    Raises:\\n        ValueError: If a prim already exists at the given path.\\n    \"\"\"\\n    # spawn geometry if it doesn\\'t exist.\\n    if not prim_utils.is_prim_path_valid(prim_path):\\n        prim_utils.create_prim(prim_path, prim_type=\"Xform\", translation=translation, orientation=orientation)\\n    else:\\n        raise ValueError(f\"A prim already exists at path: \\'{prim_path}\\'.\")\\n\\n    # create all the paths we need for clarity\\n    geom_prim_path = prim_path + \"/geometry\"\\n    mesh_prim_path = geom_prim_path + \"/mesh\"\\n\\n    # create the geometry prim\\n    prim_utils.create_prim(mesh_prim_path, prim_type, scale=scale, attributes=attributes)\\n    # apply collision properties\\n    if cfg.collision_props is not None:\\n        schemas.define_collision_properties(mesh_prim_path, cfg.collision_props)\\n    # apply visual material\\n    if cfg.visual_material is not None:\\n        if not cfg.visual_material_path.startswith(\"/\"):\\n            material_path = f\"{geom_prim_path}/{cfg.visual_material_path}\"\\n        else:\\n            material_path = cfg.visual_material_path\\n        # create material\\n        cfg.visual_material.func(material_path, cfg.visual_material)\\n        # apply material\\n        bind_visual_material(mesh_prim_path, material_path)\\n    # apply physics material\\n    if cfg.physics_material is not None:\\n        if not cfg.physics_material_path.startswith(\"/\"):\\n            material_path = f\"{geom_prim_path}/{cfg.physics_material_path}\"\\n        else:\\n            material_path = cfg.physics_material_path\\n        # create material\\n        cfg.physics_material.func(material_path, cfg.physics_material)\\n        # apply material\\n        bind_physics_material(mesh_prim_path, material_path)\\n\\n    # note: we apply rigid properties in the end to later make the instanceable prim\\n    # apply mass properties\\n    if cfg.mass_props is not None:\\n        schemas.define_mass_properties(prim_path, cfg.mass_props)\\n    # apply rigid body properties\\n    if cfg.rigid_props is not None:\\n        schemas.define_rigid_body_properties(prim_path, cfg.rigid_props)'),\n",
       " Document(metadata={}, page_content='class ShapeCfg(RigidObjectSpawnerCfg):\\n    \"\"\"Configuration parameters for a USD Geometry or Geom prim.\"\"\"\\n\\n    visual_material_path: str = \"material\"\\n    \"\"\"Path to the visual material to use for the prim. Defaults to \"material\".\\n\\n    If the path is relative, then it will be relative to the prim\\'s path.\\n    This parameter is ignored if `visual_material` is not None.\\n    \"\"\"\\n    visual_material: materials.VisualMaterialCfg | None = None\\n    \"\"\"Visual material properties.\\n\\n    Note:\\n        If None, then no visual material will be added.\\n    \"\"\"\\n\\n    physics_material_path: str = \"material\"\\n    \"\"\"Path to the physics material to use for the prim. Defaults to \"material\".\\n\\n    If the path is relative, then it will be relative to the prim\\'s path.\\n    This parameter is ignored if `physics_material` is not None.\\n    \"\"\"\\n    physics_material: materials.PhysicsMaterialCfg | None = None\\n    \"\"\"Physics material properties.\\n\\n    Note:\\n        If None, then no physics material will be added.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class SphereCfg(ShapeCfg):\\n    \"\"\"Configuration parameters for a sphere prim.\\n\\n    See :meth:`spawn_sphere` for more information.\\n    \"\"\"\\n\\n    func: Callable = shapes.spawn_sphere\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the sphere (in m).\"\"\"'),\n",
       " Document(metadata={}, page_content='class CuboidCfg(ShapeCfg):\\n    \"\"\"Configuration parameters for a cuboid prim.\\n\\n    See :meth:`spawn_cuboid` for more information.\\n    \"\"\"\\n\\n    func: Callable = shapes.spawn_cuboid\\n\\n    size: tuple[float, float, float] = MISSING\\n    \"\"\"Size of the cuboid.\"\"\"'),\n",
       " Document(metadata={}, page_content='class CylinderCfg(ShapeCfg):\\n    \"\"\"Configuration parameters for a cylinder prim.\\n\\n    See :meth:`spawn_cylinder` for more information.\\n    \"\"\"\\n\\n    func: Callable = shapes.spawn_cylinder\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the cylinder (in m).\"\"\"\\n    height: float = MISSING\\n    \"\"\"Height of the cylinder (in m).\"\"\"\\n    axis: Literal[\"X\", \"Y\", \"Z\"] = \"Z\"\\n    \"\"\"Axis of the cylinder. Defaults to \"Z\".\"\"\"'),\n",
       " Document(metadata={}, page_content='class CapsuleCfg(ShapeCfg):\\n    \"\"\"Configuration parameters for a capsule prim.\\n\\n    See :meth:`spawn_capsule` for more information.\\n    \"\"\"\\n\\n    func: Callable = shapes.spawn_capsule\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the capsule (in m).\"\"\"\\n    height: float = MISSING\\n    \"\"\"Height of the capsule (in m).\"\"\"\\n    axis: Literal[\"X\", \"Y\", \"Z\"] = \"Z\"\\n    \"\"\"Axis of the capsule. Defaults to \"Z\".\"\"\"'),\n",
       " Document(metadata={}, page_content='class ConeCfg(ShapeCfg):\\n    \"\"\"Configuration parameters for a cone prim.\\n\\n    See :meth:`spawn_cone` for more information.\\n    \"\"\"\\n\\n    func: Callable = shapes.spawn_cone\\n\\n    radius: float = MISSING\\n    \"\"\"Radius of the cone (in m).\"\"\"\\n    height: float = MISSING\\n    \"\"\"Height of the v (in m).\"\"\"\\n    axis: Literal[\"X\", \"Y\", \"Z\"] = \"Z\"\\n    \"\"\"Axis of the cone. Defaults to \"Z\".\"\"\"'),\n",
       " Document(metadata={}, page_content='def spawn_multi_asset(\\n    prim_path: str,\\n    cfg: wrappers_cfg.MultiAssetSpawnerCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Spawn multiple assets based on the provided configurations.\\n\\n    This function spawns multiple assets based on the provided configurations. The assets are spawned\\n    in the order they are provided in the list. If the :attr:`~MultiAssetSpawnerCfg.random_choice` parameter is\\n    set to True, a random asset configuration is selected for each spawn.\\n\\n    Args:\\n        prim_path: The prim path to spawn the assets.\\n        cfg: The configuration for spawning the assets.\\n        translation: The translation of the spawned assets. Default is None.\\n        orientation: The orientation of the spawned assets in (w, x, y, z) order. Default is None.\\n\\n    Returns:\\n        The created prim at the first prim path.\\n    \"\"\"\\n    # resolve: {SPAWN_NS}/AssetName\\n    # note: this assumes that the spawn namespace already exists in the stage\\n    root_path, asset_path = prim_path.rsplit(\"/\", 1)\\n    # check if input is a regex expression\\n    # note: a valid prim path can only contain alphanumeric characters, underscores, and forward slashes\\n    is_regex_expression = re.match(r\"^[a-zA-Z0-9/_]+$\", root_path) is None\\n\\n    # resolve matching prims for source prim path expression\\n    if is_regex_expression and root_path != \"\":\\n        source_prim_paths = sim_utils.find_matching_prim_paths(root_path)\\n        # if no matching prims are found, raise an error\\n        if len(source_prim_paths) == 0:\\n            raise RuntimeError(\\n                f\"Unable to find source prim path: \\'{root_path}\\'. Please create the prim before spawning.\"\\n            )\\n    else:\\n        source_prim_paths = [root_path]\\n\\n    # find a free prim path to hold all the template prims\\n    template_prim_path = stage_utils.get_next_free_path(\"/World/Template\")\\n    prim_utils.create_prim(template_prim_path, \"Scope\")\\n\\n    # spawn everything first in a \"Dataset\" prim\\n    proto_prim_paths = list()\\n    for index, asset_cfg in enumerate(cfg.assets_cfg):\\n        # append semantic tags if specified\\n        if cfg.semantic_tags is not None:\\n            if asset_cfg.semantic_tags is None:\\n                asset_cfg.semantic_tags = cfg.semantic_tags\\n            else:\\n                asset_cfg.semantic_tags += cfg.semantic_tags\\n        # override settings for properties\\n        attr_names = [\"mass_props\", \"rigid_props\", \"collision_props\", \"activate_contact_sensors\", \"deformable_props\"]\\n        for attr_name in attr_names:\\n            attr_value = getattr(cfg, attr_name)\\n            if hasattr(asset_cfg, attr_name) and attr_value is not None:\\n                setattr(asset_cfg, attr_name, attr_value)\\n        # spawn single instance\\n        proto_prim_path = f\"{template_prim_path}/Asset_{index:04d}\"\\n        asset_cfg.func(proto_prim_path, asset_cfg, translation=translation, orientation=orientation)\\n        # append to proto prim paths\\n        proto_prim_paths.append(proto_prim_path)\\n\\n    # resolve prim paths for spawning and cloning\\n    prim_paths = [f\"{source_prim_path}/{asset_path}\" for source_prim_path in source_prim_paths]\\n\\n    # acquire stage\\n    stage = stage_utils.get_current_stage()\\n\\n    # manually clone prims if the source prim path is a regex expression\\n    # note: unlike in the cloner API from Isaac Sim, we do not \"reset\" xforms on the copied prims.\\n    #   This is because the \"spawn\" calls during the creation of the proto prims already handles this operation.\\n    with Sdf.ChangeBlock():\\n        for index, prim_path in enumerate(prim_paths):\\n            # spawn single instance\\n            env_spec = Sdf.CreatePrimInLayer(stage.GetRootLayer(), prim_path)\\n            # randomly select an asset configuration\\n            if cfg.random_choice:\\n                proto_path = random.choice(proto_prim_paths)\\n            else:\\n                proto_path = proto_prim_paths[index % len(proto_prim_paths)]\\n            # copy the proto prim\\n            Sdf.CopySpec(env_spec.layer, Sdf.Path(proto_path), env_spec.layer, Sdf.Path(prim_path))\\n\\n    # delete the dataset prim after spawning\\n    prim_utils.delete_prim(template_prim_path)\\n\\n    # set carb setting to indicate Isaac Lab\\'s environments that different prims have been spawned\\n    # at varying prim paths. In this case, PhysX parser shouldn\\'t optimize the stage parsing.\\n    # the flag is mainly used to inform the user that they should disable `InteractiveScene.replicate_physics`\\n    carb_settings_iface = carb.settings.get_settings()\\n    carb_settings_iface.set_bool(\"/isaaclab/spawn/multi_assets\", True)\\n\\n    # return the prim\\n    return prim_utils.get_prim_at_path(prim_paths[0])'),\n",
       " Document(metadata={}, page_content='def spawn_multi_usd_file(\\n    prim_path: str,\\n    cfg: wrappers_cfg.MultiUsdFileCfg,\\n    translation: tuple[float, float, float] | None = None,\\n    orientation: tuple[float, float, float, float] | None = None,\\n) -> Usd.Prim:\\n    \"\"\"Spawn multiple USD files based on the provided configurations.\\n\\n    This function creates configuration instances corresponding the individual USD files and\\n    calls the :meth:`spawn_multi_asset` method to spawn them into the scene.\\n\\n    Args:\\n        prim_path: The prim path to spawn the assets.\\n        cfg: The configuration for spawning the assets.\\n        translation: The translation of the spawned assets. Default is None.\\n        orientation: The orientation of the spawned assets in (w, x, y, z) order. Default is None.\\n\\n    Returns:\\n        The created prim at the first prim path.\\n    \"\"\"\\n    # needed here to avoid circular imports\\n    from .wrappers_cfg import MultiAssetSpawnerCfg\\n\\n    # parse all the usd files\\n    if isinstance(cfg.usd_path, str):\\n        usd_paths = [cfg.usd_path]\\n    else:\\n        usd_paths = cfg.usd_path\\n\\n    # make a template usd config\\n    usd_template_cfg = UsdFileCfg()\\n    for attr_name, attr_value in cfg.__dict__.items():\\n        # skip names we know are not present\\n        if attr_name in [\"func\", \"usd_path\", \"random_choice\"]:\\n            continue\\n        # set the attribute into the template\\n        setattr(usd_template_cfg, attr_name, attr_value)\\n\\n    # create multi asset configuration of USD files\\n    multi_asset_cfg = MultiAssetSpawnerCfg(assets_cfg=[])\\n    for usd_path in usd_paths:\\n        usd_cfg = usd_template_cfg.replace(usd_path=usd_path)\\n        multi_asset_cfg.assets_cfg.append(usd_cfg)\\n    # set random choice\\n    multi_asset_cfg.random_choice = cfg.random_choice\\n\\n    # propagate the contact sensor settings\\n    # note: the default value for activate_contact_sensors in MultiAssetSpawnerCfg is False.\\n    #  This ends up overwriting the usd-template-cfg\\'s value when the `spawn_multi_asset`\\n    #  function is called. We hard-code the value to the usd-template-cfg\\'s value to ensure\\n    #  that the contact sensor settings are propagated correctly.\\n    if hasattr(cfg, \"activate_contact_sensors\"):\\n        multi_asset_cfg.activate_contact_sensors = cfg.activate_contact_sensors\\n\\n    # call the original function\\n    return spawn_multi_asset(prim_path, multi_asset_cfg, translation, orientation)'),\n",
       " Document(metadata={}, page_content='class MultiAssetSpawnerCfg(RigidObjectSpawnerCfg, DeformableObjectSpawnerCfg):\\n    \"\"\"Configuration parameters for loading multiple assets from their individual configurations.\\n\\n    Specifying values for any properties at the configuration level will override the settings of\\n    individual assets\\' configuration. For instance if the attribute\\n    :attr:`MultiAssetSpawnerCfg.mass_props` is specified, its value will overwrite the values of the\\n    mass properties in each configuration inside :attr:`assets_cfg` (wherever applicable).\\n    This is done to simplify configuring similar properties globally. By default, all properties are set to None.\\n\\n    The following is an exception to the above:\\n\\n    * :attr:`visible`: This parameter is ignored. Its value for the individual assets is used.\\n    * :attr:`semantic_tags`: If specified, it will be appended to each individual asset\\'s semantic tags.\\n\\n    \"\"\"\\n\\n    func = wrappers.spawn_multi_asset\\n\\n    assets_cfg: list[SpawnerCfg] = MISSING\\n    \"\"\"List of asset configurations to spawn.\"\"\"\\n\\n    random_choice: bool = True\\n    \"\"\"Whether to randomly select an asset configuration. Default is True.\\n\\n    If False, the asset configurations are spawned in the order they are provided in the list.\\n    If True, a random asset configuration is selected for each spawn.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class MultiUsdFileCfg(UsdFileCfg):\\n    \"\"\"Configuration parameters for loading multiple USD files.\\n\\n    Specifying values for any properties at the configuration level is applied to all the assets\\n    imported from their USD files.\\n\\n    .. tip::\\n        It is recommended that all the USD based assets follow a similar prim-hierarchy.\\n\\n    \"\"\"\\n\\n    func = wrappers.spawn_multi_usd_file\\n\\n    usd_path: str | list[str] = MISSING\\n    \"\"\"Path or a list of paths to the USD files to spawn asset from.\"\"\"\\n\\n    random_choice: bool = True\\n    \"\"\"Whether to randomly select an asset configuration. Default is True.\\n\\n    If False, the asset configurations are spawned in the order they are provided in the list.\\n    If True, a random asset configuration is selected for each spawn.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class TerrainGenerator:\\n    r\"\"\"Terrain generator to handle different terrain generation functions.\\n\\n    The terrains are represented as meshes. These are obtained either from height fields or by using the\\n    `trimesh <https://trimsh.org/trimesh.html>`__ library. The height field representation is more\\n    flexible, but it is less computationally and memory efficient than the trimesh representation.\\n\\n    All terrain generation functions take in the argument :obj:`difficulty` which determines the complexity\\n    of the terrain. The difficulty is a number between 0 and 1, where 0 is the easiest and 1 is the hardest.\\n    In most cases, the difficulty is used for linear interpolation between different terrain parameters.\\n    For example, in a pyramid stairs terrain the step height is interpolated between the specified minimum\\n    and maximum step height.\\n\\n    Each sub-terrain has a corresponding configuration class that can be used to specify the parameters\\n    of the terrain. The configuration classes are inherited from the :class:`SubTerrainBaseCfg` class\\n    which contains the common parameters for all terrains.\\n\\n    If a curriculum is used, the terrains are generated based on their difficulty parameter.\\n    The difficulty is varied linearly over the number of rows (i.e. along x) with a small random value\\n    added to the difficulty to ensure that the columns with the same sub-terrain type are not exactly\\n    the same. The difficulty parameter for a sub-terrain at a given row is calculated as:\\n\\n    .. math::\\n\\n        \\\\text{difficulty} = \\\\frac{\\\\text{row_id} + \\\\eta}{\\\\text{num_rows}} \\\\times (\\\\text{upper} - \\\\text{lower}) + \\\\text{lower}\\n\\n    where :math:`\\\\eta\\\\sim\\\\mathcal{U}(0, 1)` is a random perturbation to the difficulty, and\\n    :math:`(\\\\text{lower}, \\\\text{upper})` is the range of the difficulty parameter, specified using the\\n    :attr:`~TerrainGeneratorCfg.difficulty_range` parameter.\\n\\n    If a curriculum is not used, the terrains are generated randomly. In this case, the difficulty parameter\\n    is randomly sampled from the specified range, given by the :attr:`~TerrainGeneratorCfg.difficulty_range` parameter:\\n\\n    .. math::\\n\\n        \\\\text{difficulty} \\\\sim \\\\mathcal{U}(\\\\text{lower}, \\\\text{upper})\\n\\n    If the :attr:`~TerrainGeneratorCfg.flat_patch_sampling` is specified for a sub-terrain, flat patches are sampled\\n    on the terrain. These can be used for spawning robots, targets, etc. The sampled patches are stored\\n    in the :obj:`flat_patches` dictionary. The key specifies the intention of the flat patches and the\\n    value is a tensor containing the flat patches for each sub-terrain.\\n\\n    If the flag :attr:`~TerrainGeneratorCfg.use_cache` is set to True, the terrains are cached based on their\\n    sub-terrain configurations. This means that if the same sub-terrain configuration is used\\n    multiple times, the terrain is only generated once and then reused. This is useful when\\n    generating complex sub-terrains that take a long time to generate.\\n\\n    .. attention::\\n\\n        The terrain generation has its own seed parameter. This is set using the :attr:`TerrainGeneratorCfg.seed`\\n        parameter. If the seed is not set and the caching is disabled, the terrain generation may not be\\n        completely reproducible.\\n\\n    \"\"\"\\n\\n    terrain_mesh: trimesh.Trimesh\\n    \"\"\"A single trimesh.Trimesh object for all the generated sub-terrains.\"\"\"\\n    terrain_meshes: list[trimesh.Trimesh]\\n    \"\"\"List of trimesh.Trimesh objects for all the generated sub-terrains.\"\"\"\\n    terrain_origins: np.ndarray\\n    \"\"\"The origin of each sub-terrain. Shape is (num_rows, num_cols, 3).\"\"\"\\n    flat_patches: dict[str, torch.Tensor]\\n    \"\"\"A dictionary of sampled valid (flat) patches for each sub-terrain.\\n\\n    The dictionary keys are the names of the flat patch sampling configurations. This maps to a\\n    tensor containing the flat patches for each sub-terrain. The shape of the tensor is\\n    (num_rows, num_cols, num_patches, 3).\\n\\n    For instance, the key \"root_spawn\" maps to a tensor containing the flat patches for spawning an asset.\\n    Similarly, the key \"target_spawn\" maps to a tensor containing the flat patches for setting targets.\\n    \"\"\"\\n\\n    def __init__(self, cfg: TerrainGeneratorCfg, device: str = \"cpu\"):\\n        \"\"\"Initialize the terrain generator.\\n\\n        Args:\\n            cfg: Configuration for the terrain generator.\\n            device: The device to use for the flat patches tensor.\\n        \"\"\"\\n        # check inputs\\n        if len(cfg.sub_terrains) == 0:\\n            raise ValueError(\"No sub-terrains specified! Please add at least one sub-terrain.\")\\n        # store inputs\\n        self.cfg = cfg\\n        self.device = device\\n\\n        # set common values to all sub-terrains config\\n        for sub_cfg in self.cfg.sub_terrains.values():\\n            # size of all terrains\\n            sub_cfg.size = self.cfg.size\\n            # params for height field terrains\\n            if isinstance(sub_cfg, HfTerrainBaseCfg):\\n                sub_cfg.horizontal_scale = self.cfg.horizontal_scale\\n                sub_cfg.vertical_scale = self.cfg.vertical_scale\\n                sub_cfg.slope_threshold = self.cfg.slope_threshold\\n\\n        # throw a warning if the cache is enabled but the seed is not set\\n        if self.cfg.use_cache and self.cfg.seed is None:\\n            omni.log.warn(\\n                \"Cache is enabled but the seed is not set. The terrain generation will not be reproducible.\"\\n                \" Please set the seed in the terrain generator configuration to make the generation reproducible.\"\\n            )\\n\\n        # if the seed is not set, we assume there is a global seed set and use that.\\n        # this ensures that the terrain is reproducible if the seed is set at the beginning of the program.\\n        if self.cfg.seed is not None:\\n            seed = self.cfg.seed\\n        else:\\n            seed = np.random.get_state()[1][0]\\n        # set the seed for reproducibility\\n        # note: we create a new random number generator to avoid affecting the global state\\n        #  in the other places where random numbers are used.\\n        self.np_rng = np.random.default_rng(seed)\\n\\n        # buffer for storing valid patches\\n        self.flat_patches = {}\\n        # create a list of all sub-terrains\\n        self.terrain_meshes = list()\\n        self.terrain_origins = np.zeros((self.cfg.num_rows, self.cfg.num_cols, 3))\\n\\n        # parse configuration and add sub-terrains\\n        # create terrains based on curriculum or randomly\\n        if self.cfg.curriculum:\\n            with Timer(\"[INFO] Generating terrains based on curriculum took\"):\\n                self._generate_curriculum_terrains()\\n        else:\\n            with Timer(\"[INFO] Generating terrains randomly took\"):\\n                self._generate_random_terrains()\\n        # add a border around the terrains\\n        self._add_terrain_border()\\n        # combine all the sub-terrains into a single mesh\\n        self.terrain_mesh = trimesh.util.concatenate(self.terrain_meshes)\\n\\n        # color the terrain mesh\\n        if self.cfg.color_scheme == \"height\":\\n            self.terrain_mesh = color_meshes_by_height(self.terrain_mesh)\\n        elif self.cfg.color_scheme == \"random\":\\n            self.terrain_mesh.visual.vertex_colors = self.np_rng.choice(\\n                range(256), size=(len(self.terrain_mesh.vertices), 4)\\n            )\\n        elif self.cfg.color_scheme == \"none\":\\n            pass\\n        else:\\n            raise ValueError(f\"Invalid color scheme: {self.cfg.color_scheme}.\")\\n\\n        # offset the entire terrain and origins so that it is centered\\n        # -- terrain mesh\\n        transform = np.eye(4)\\n        transform[:2, -1] = -self.cfg.size[0] * self.cfg.num_rows * 0.5, -self.cfg.size[1] * self.cfg.num_cols * 0.5\\n        self.terrain_mesh.apply_transform(transform)\\n        # -- terrain origins\\n        self.terrain_origins += transform[:3, -1]\\n        # -- valid patches\\n        terrain_origins_torch = torch.tensor(self.terrain_origins, dtype=torch.float, device=self.device).unsqueeze(2)\\n        for name, value in self.flat_patches.items():\\n            self.flat_patches[name] = value + terrain_origins_torch\\n\\n    def __str__(self):\\n        \"\"\"Return a string representation of the terrain generator.\"\"\"\\n        msg = \"Terrain Generator:\"\\n        msg += f\"\\\\n\\\\tSeed: {self.cfg.seed}\"\\n        msg += f\"\\\\n\\\\tNumber of rows: {self.cfg.num_rows}\"\\n        msg += f\"\\\\n\\\\tNumber of columns: {self.cfg.num_cols}\"\\n        msg += f\"\\\\n\\\\tSub-terrain size: {self.cfg.size}\"\\n        msg += f\"\\\\n\\\\tSub-terrain types: {list(self.cfg.sub_terrains.keys())}\"\\n        msg += f\"\\\\n\\\\tCurriculum: {self.cfg.curriculum}\"\\n        msg += f\"\\\\n\\\\tDifficulty range: {self.cfg.difficulty_range}\"\\n        msg += f\"\\\\n\\\\tColor scheme: {self.cfg.color_scheme}\"\\n        msg += f\"\\\\n\\\\tUse cache: {self.cfg.use_cache}\"\\n        if self.cfg.use_cache:\\n            msg += f\"\\\\n\\\\tCache directory: {self.cfg.cache_dir}\"\\n\\n        return msg\\n\\n    \"\"\"\\n    Terrain generator functions.\\n    \"\"\"\\n\\n    def _generate_random_terrains(self):\\n        \"\"\"Add terrains based on randomly sampled difficulty parameter.\"\"\"\\n        # normalize the proportions of the sub-terrains\\n        proportions = np.array([sub_cfg.proportion for sub_cfg in self.cfg.sub_terrains.values()])\\n        proportions /= np.sum(proportions)\\n        # create a list of all terrain configs\\n        sub_terrains_cfgs = list(self.cfg.sub_terrains.values())\\n\\n        # randomly sample sub-terrains\\n        for index in range(self.cfg.num_rows * self.cfg.num_cols):\\n            # coordinate index of the sub-terrain\\n            (sub_row, sub_col) = np.unravel_index(index, (self.cfg.num_rows, self.cfg.num_cols))\\n            # randomly sample terrain index\\n            sub_index = self.np_rng.choice(len(proportions), p=proportions)\\n            # randomly sample difficulty parameter\\n            difficulty = self.np_rng.uniform(*self.cfg.difficulty_range)\\n            # generate terrain\\n            mesh, origin = self._get_terrain_mesh(difficulty, sub_terrains_cfgs[sub_index])\\n            # add to sub-terrains\\n            self._add_sub_terrain(mesh, origin, sub_row, sub_col, sub_terrains_cfgs[sub_index])\\n\\n    def _generate_curriculum_terrains(self):\\n        \"\"\"Add terrains based on the difficulty parameter.\"\"\"\\n        # normalize the proportions of the sub-terrains\\n        proportions = np.array([sub_cfg.proportion for sub_cfg in self.cfg.sub_terrains.values()])\\n        proportions /= np.sum(proportions)\\n\\n        # find the sub-terrain index for each column\\n        # we generate the terrains based on their proportion (not randomly sampled)\\n        sub_indices = []\\n        for index in range(self.cfg.num_cols):\\n            sub_index = np.min(np.where(index / self.cfg.num_cols + 0.001 < np.cumsum(proportions))[0])\\n            sub_indices.append(sub_index)\\n        sub_indices = np.array(sub_indices, dtype=np.int32)\\n        # create a list of all terrain configs\\n        sub_terrains_cfgs = list(self.cfg.sub_terrains.values())\\n\\n        # curriculum-based sub-terrains\\n        for sub_col in range(self.cfg.num_cols):\\n            for sub_row in range(self.cfg.num_rows):\\n                # vary the difficulty parameter linearly over the number of rows\\n                # note: based on the proportion, multiple columns can have the same sub-terrain type.\\n                #  Thus to increase the diversity along the rows, we add a small random value to the difficulty.\\n                #  This ensures that the terrains are not exactly the same. For example, if the\\n                #  the row index is 2 and the number of rows is 10, the nominal difficulty is 0.2.\\n                #  We add a small random value to the difficulty to make it between 0.2 and 0.3.\\n                lower, upper = self.cfg.difficulty_range\\n                difficulty = (sub_row + self.np_rng.uniform()) / self.cfg.num_rows\\n                difficulty = lower + (upper - lower) * difficulty\\n                # generate terrain\\n                mesh, origin = self._get_terrain_mesh(difficulty, sub_terrains_cfgs[sub_indices[sub_col]])\\n                # add to sub-terrains\\n                self._add_sub_terrain(mesh, origin, sub_row, sub_col, sub_terrains_cfgs[sub_indices[sub_col]])\\n\\n    \"\"\"\\n    Internal helper functions.\\n    \"\"\"\\n\\n    def _add_terrain_border(self):\\n        \"\"\"Add a surrounding border over all the sub-terrains into the terrain meshes.\"\"\"\\n        # border parameters\\n        border_size = (\\n            self.cfg.num_rows * self.cfg.size[0] + 2 * self.cfg.border_width,\\n            self.cfg.num_cols * self.cfg.size[1] + 2 * self.cfg.border_width,\\n        )\\n        inner_size = (self.cfg.num_rows * self.cfg.size[0], self.cfg.num_cols * self.cfg.size[1])\\n        border_center = (\\n            self.cfg.num_rows * self.cfg.size[0] / 2,\\n            self.cfg.num_cols * self.cfg.size[1] / 2,\\n            -self.cfg.border_height / 2,\\n        )\\n        # border mesh\\n        border_meshes = make_border(border_size, inner_size, height=abs(self.cfg.border_height), position=border_center)\\n        border = trimesh.util.concatenate(border_meshes)\\n        # update the faces to have minimal triangles\\n        selector = ~(np.asarray(border.triangles)[:, :, 2] < -0.1).any(1)\\n        border.update_faces(selector)\\n        # add the border to the list of meshes\\n        self.terrain_meshes.append(border)\\n\\n    def _add_sub_terrain(\\n        self, mesh: trimesh.Trimesh, origin: np.ndarray, row: int, col: int, sub_terrain_cfg: SubTerrainBaseCfg\\n    ):\\n        \"\"\"Add input sub-terrain to the list of sub-terrains.\\n\\n        This function adds the input sub-terrain mesh to the list of sub-terrains and updates the origin\\n        of the sub-terrain in the list of origins. It also samples flat patches if specified.\\n\\n        Args:\\n            mesh: The mesh of the sub-terrain.\\n            origin: The origin of the sub-terrain.\\n            row: The row index of the sub-terrain.\\n            col: The column index of the sub-terrain.\\n        \"\"\"\\n        # sample flat patches if specified\\n        if sub_terrain_cfg.flat_patch_sampling is not None:\\n            omni.log.info(f\"Sampling flat patches for sub-terrain at (row, col):  ({row}, {col})\")\\n            # convert the mesh to warp mesh\\n            wp_mesh = convert_to_warp_mesh(mesh.vertices, mesh.faces, device=self.device)\\n            # sample flat patches based on each patch configuration for that sub-terrain\\n            for name, patch_cfg in sub_terrain_cfg.flat_patch_sampling.items():\\n                patch_cfg: FlatPatchSamplingCfg\\n                # create the flat patches tensor (if not already created)\\n                if name not in self.flat_patches:\\n                    self.flat_patches[name] = torch.zeros(\\n                        (self.cfg.num_rows, self.cfg.num_cols, patch_cfg.num_patches, 3), device=self.device\\n                    )\\n                # add the flat patches to the tensor\\n                self.flat_patches[name][row, col] = find_flat_patches(\\n                    wp_mesh=wp_mesh,\\n                    origin=origin,\\n                    num_patches=patch_cfg.num_patches,\\n                    patch_radius=patch_cfg.patch_radius,\\n                    x_range=patch_cfg.x_range,\\n                    y_range=patch_cfg.y_range,\\n                    z_range=patch_cfg.z_range,\\n                    max_height_diff=patch_cfg.max_height_diff,\\n                )\\n\\n        # transform the mesh to the correct position\\n        transform = np.eye(4)\\n        transform[0:2, -1] = (row + 0.5) * self.cfg.size[0], (col + 0.5) * self.cfg.size[1]\\n        mesh.apply_transform(transform)\\n        # add mesh to the list\\n        self.terrain_meshes.append(mesh)\\n        # add origin to the list\\n        self.terrain_origins[row, col] = origin + transform[:3, -1]\\n\\n    def _get_terrain_mesh(self, difficulty: float, cfg: SubTerrainBaseCfg) -> tuple[trimesh.Trimesh, np.ndarray]:\\n        \"\"\"Generate a sub-terrain mesh based on the input difficulty parameter.\\n\\n        If caching is enabled, the sub-terrain is cached and loaded from the cache if it exists.\\n        The cache is stored in the cache directory specified in the configuration.\\n\\n        .. Note:\\n            This function centers the 2D center of the mesh and its specified origin such that the\\n            2D center becomes :math:`(0, 0)` instead of :math:`(size[0] / 2, size[1] / 2).\\n\\n        Args:\\n            difficulty: The difficulty parameter.\\n            cfg: The configuration of the sub-terrain.\\n\\n        Returns:\\n            The sub-terrain mesh and origin.\\n        \"\"\"\\n        # copy the configuration\\n        cfg = cfg.copy()\\n        # add other parameters to the sub-terrain configuration\\n        cfg.difficulty = float(difficulty)\\n        cfg.seed = self.cfg.seed\\n        # generate hash for the sub-terrain\\n        sub_terrain_hash = dict_to_md5_hash(cfg.to_dict())\\n        # generate the file name\\n        sub_terrain_cache_dir = os.path.join(self.cfg.cache_dir, sub_terrain_hash)\\n        sub_terrain_obj_filename = os.path.join(sub_terrain_cache_dir, \"mesh.obj\")\\n        sub_terrain_csv_filename = os.path.join(sub_terrain_cache_dir, \"origin.csv\")\\n        sub_terrain_meta_filename = os.path.join(sub_terrain_cache_dir, \"cfg.yaml\")\\n\\n        # check if hash exists - if true, load the mesh and origin and return\\n        if self.cfg.use_cache and os.path.exists(sub_terrain_obj_filename):\\n            # load existing mesh\\n            mesh = trimesh.load_mesh(sub_terrain_obj_filename, process=False)\\n            origin = np.loadtxt(sub_terrain_csv_filename, delimiter=\",\")\\n            # return the generated mesh\\n            return mesh, origin\\n\\n        # generate the terrain\\n        meshes, origin = cfg.function(difficulty, cfg)\\n        mesh = trimesh.util.concatenate(meshes)\\n        # offset mesh such that they are in their center\\n        transform = np.eye(4)\\n        transform[0:2, -1] = -cfg.size[0] * 0.5, -cfg.size[1] * 0.5\\n        mesh.apply_transform(transform)\\n        # change origin to be in the center of the sub-terrain\\n        origin += transform[0:3, -1]\\n\\n        # if caching is enabled, save the mesh and origin\\n        if self.cfg.use_cache:\\n            # create the cache directory\\n            os.makedirs(sub_terrain_cache_dir, exist_ok=True)\\n            # save the data\\n            mesh.export(sub_terrain_obj_filename)\\n            np.savetxt(sub_terrain_csv_filename, origin, delimiter=\",\", header=\"x,y,z\")\\n            dump_yaml(sub_terrain_meta_filename, cfg)\\n        # return the generated mesh\\n        return mesh, origin'),\n",
       " Document(metadata={}, page_content='class FlatPatchSamplingCfg:\\n    \"\"\"Configuration for sampling flat patches on the sub-terrain.\\n\\n    For a given sub-terrain, this configuration specifies how to sample flat patches on the terrain.\\n    The sampled flat patches can be used for spawning robots, targets, etc.\\n\\n    Please check the function :meth:`~isaaclab.terrains.utils.find_flat_patches` for more details.\\n    \"\"\"\\n\\n    num_patches: int = MISSING\\n    \"\"\"Number of patches to sample.\"\"\"\\n\\n    patch_radius: float | list[float] = MISSING\\n    \"\"\"Radius of the patches.\\n\\n    A list of radii can be provided to check for patches of different sizes. This is useful to deal with\\n    cases where the terrain may have holes or obstacles in some areas.\\n    \"\"\"\\n\\n    x_range: tuple[float, float] = (-1e6, 1e6)\\n    \"\"\"The range of x-coordinates to sample from. Defaults to (-1e6, 1e6).\\n\\n    This range is internally clamped to the size of the terrain mesh.\\n    \"\"\"\\n\\n    y_range: tuple[float, float] = (-1e6, 1e6)\\n    \"\"\"The range of y-coordinates to sample from. Defaults to (-1e6, 1e6).\\n\\n    This range is internally clamped to the size of the terrain mesh.\\n    \"\"\"\\n\\n    z_range: tuple[float, float] = (-1e6, 1e6)\\n    \"\"\"Allowed range of z-coordinates for the sampled patch. Defaults to (-1e6, 1e6).\"\"\"\\n\\n    max_height_diff: float = MISSING\\n    \"\"\"Maximum allowed height difference between the highest and lowest points on the patch.\"\"\"'),\n",
       " Document(metadata={}, page_content='class SubTerrainBaseCfg:\\n    \"\"\"Base class for terrain configurations.\\n\\n    All the sub-terrain configurations must inherit from this class.\\n\\n    The :attr:`size` attribute is the size of the generated sub-terrain. Based on this, the terrain must\\n    extend from :math:`(0, 0)` to :math:`(size[0], size[1])`.\\n    \"\"\"\\n\\n    function: Callable[[float, SubTerrainBaseCfg], tuple[list[trimesh.Trimesh], np.ndarray]] = MISSING\\n    \"\"\"Function to generate the terrain.\\n\\n    This function must take as input the terrain difficulty and the configuration parameters and\\n    return a tuple with a list of ``trimesh`` mesh objects and the terrain origin.\\n    \"\"\"\\n\\n    proportion: float = 1.0\\n    \"\"\"Proportion of the terrain to generate. Defaults to 1.0.\\n\\n    This is used to generate a mix of terrains. The proportion corresponds to the probability of sampling\\n    the particular terrain. For example, if there are two terrains, A and B, with proportions 0.3 and 0.7,\\n    respectively, then the probability of sampling terrain A is 0.3 and the probability of sampling terrain B\\n    is 0.7.\\n    \"\"\"\\n\\n    size: tuple[float, float] = (10.0, 10.0)\\n    \"\"\"The width (along x) and length (along y) of the terrain (in m). Defaults to (10.0, 10.0).\\n\\n    In case the :class:`~isaaclab.terrains.TerrainImporterCfg` is used, this parameter gets overridden by\\n    :attr:`isaaclab.scene.TerrainImporterCfg.size` attribute.\\n    \"\"\"\\n\\n    flat_patch_sampling: dict[str, FlatPatchSamplingCfg] | None = None\\n    \"\"\"Dictionary of configurations for sampling flat patches on the sub-terrain. Defaults to None,\\n    in which case no flat patch sampling is performed.\\n\\n    The keys correspond to the name of the flat patch sampling configuration and the values are the\\n    corresponding configurations.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class TerrainGeneratorCfg:\\n    \"\"\"Configuration for the terrain generator.\"\"\"\\n\\n    seed: int | None = None\\n    \"\"\"The seed for the random number generator. Defaults to None, in which case the seed from the\\n    current NumPy\\'s random state is used.\\n\\n    When the seed is set, the random number generator is initialized with the given seed. This ensures\\n    that the generated terrains are deterministic across different runs. If the seed is not set, the\\n    seed from the current NumPy\\'s random state is used. This assumes that the seed is set elsewhere in\\n    the code.\\n    \"\"\"\\n\\n    curriculum: bool = False\\n    \"\"\"Whether to use the curriculum mode. Defaults to False.\\n\\n    If True, the terrains are generated based on their difficulty parameter. Otherwise,\\n    they are randomly generated.\\n    \"\"\"\\n\\n    size: tuple[float, float] = MISSING\\n    \"\"\"The width (along x) and length (along y) of each sub-terrain (in m).\\n\\n    Note:\\n      This value is passed on to all the sub-terrain configurations.\\n    \"\"\"\\n\\n    border_width: float = 0.0\\n    \"\"\"The width of the border around the terrain (in m). Defaults to 0.0.\"\"\"\\n\\n    border_height: float = 1.0\\n    \"\"\"The height of the border around the terrain (in m). Defaults to 1.0.\\n\\n    .. note::\\n      The default border extends below the ground. If you want to make the border above the ground, choose a negative value.\\n    \"\"\"\\n\\n    num_rows: int = 1\\n    \"\"\"Number of rows of sub-terrains to generate. Defaults to 1.\"\"\"\\n\\n    num_cols: int = 1\\n    \"\"\"Number of columns of sub-terrains to generate. Defaults to 1.\"\"\"\\n\\n    color_scheme: Literal[\"height\", \"random\", \"none\"] = \"none\"\\n    \"\"\"Color scheme to use for the terrain. Defaults to \"none\".\\n\\n    The available color schemes are:\\n\\n    - \"height\": Color based on the height of the terrain.\\n    - \"random\": Random color scheme.\\n    - \"none\": No color scheme.\\n    \"\"\"\\n\\n    horizontal_scale: float = 0.1\\n    \"\"\"The discretization of the terrain along the x and y axes (in m). Defaults to 0.1.\\n\\n    This value is passed on to all the height field sub-terrain configurations.\\n    \"\"\"\\n\\n    vertical_scale: float = 0.005\\n    \"\"\"The discretization of the terrain along the z axis (in m). Defaults to 0.005.\\n\\n    This value is passed on to all the height field sub-terrain configurations.\\n    \"\"\"\\n\\n    slope_threshold: float | None = 0.75\\n    \"\"\"The slope threshold above which surfaces are made vertical. Defaults to 0.75.\\n\\n    If None no correction is applied.\\n\\n    This value is passed on to all the height field sub-terrain configurations.\\n    \"\"\"\\n\\n    sub_terrains: dict[str, SubTerrainBaseCfg] = MISSING\\n    \"\"\"Dictionary of sub-terrain configurations.\\n\\n    The keys correspond to the name of the sub-terrain configuration and the values are the corresponding\\n    configurations.\\n    \"\"\"\\n\\n    difficulty_range: tuple[float, float] = (0.0, 1.0)\\n    \"\"\"The range of difficulty values for the sub-terrains. Defaults to (0.0, 1.0).\\n\\n    If curriculum is enabled, the terrains will be generated based on this range in ascending order\\n    of difficulty. Otherwise, the terrains will be generated based on this range in a random order.\\n    \"\"\"\\n\\n    use_cache: bool = False\\n    \"\"\"Whether to load the sub-terrain from cache if it exists. Defaults to False.\\n\\n    If enabled, the generated terrains are stored in the cache directory. When generating terrains, the cache\\n    is checked to see if the terrain already exists. If it does, the terrain is loaded from the cache. Otherwise,\\n    the terrain is generated and stored in the cache. Caching can be used to speed up terrain generation.\\n    \"\"\"\\n\\n    cache_dir: str = \"/tmp/isaaclab/terrains\"\\n    \"\"\"The directory where the terrain cache is stored. Defaults to \"/tmp/isaaclab/terrains\".\"\"\"'),\n",
       " Document(metadata={}, page_content='class TerrainImporter:\\n    r\"\"\"A class to handle terrain meshes and import them into the simulator.\\n\\n    We assume that a terrain mesh comprises of sub-terrains that are arranged in a grid with\\n    rows ``num_rows`` and columns ``num_cols``. The terrain origins are the positions of the sub-terrains\\n    where the robot should be spawned.\\n\\n    Based on the configuration, the terrain importer handles computing the environment origins from the sub-terrain\\n    origins. In a typical setup, the number of sub-terrains (:math:`num\\\\_rows \\\\times num\\\\_cols`) is smaller than\\n    the number of environments (:math:`num\\\\_envs`). In this case, the environment origins are computed by\\n    sampling the sub-terrain origins.\\n\\n    If a curriculum is used, it is possible to update the environment origins to terrain origins that correspond\\n    to a harder difficulty. This is done by calling :func:`update_terrain_levels`. The idea comes from game-based\\n    curriculum. For example, in a game, the player starts with easy levels and progresses to harder levels.\\n    \"\"\"\\n\\n    terrain_prim_paths: list[str]\\n    \"\"\"A list containing the USD prim paths to the imported terrains.\"\"\"\\n\\n    terrain_origins: torch.Tensor | None\\n    \"\"\"The origins of the sub-terrains in the added terrain mesh. Shape is (num_rows, num_cols, 3).\\n\\n    If terrain origins is not None, the environment origins are computed based on the terrain origins.\\n    Otherwise, the environment origins are computed based on the grid spacing.\\n    \"\"\"\\n\\n    env_origins: torch.Tensor\\n    \"\"\"The origins of the environments. Shape is (num_envs, 3).\"\"\"\\n\\n    def __init__(self, cfg: TerrainImporterCfg):\\n        \"\"\"Initialize the terrain importer.\\n\\n        Args:\\n            cfg: The configuration for the terrain importer.\\n\\n        Raises:\\n            ValueError: If input terrain type is not supported.\\n            ValueError: If terrain type is \\'generator\\' and no configuration provided for ``terrain_generator``.\\n            ValueError: If terrain type is \\'usd\\' and no configuration provided for ``usd_path``.\\n            ValueError: If terrain type is \\'usd\\' or \\'plane\\' and no configuration provided for ``env_spacing``.\\n        \"\"\"\\n        # check that the config is valid\\n        cfg.validate()\\n        # store inputs\\n        self.cfg = cfg\\n        self.device = sim_utils.SimulationContext.instance().device  # type: ignore\\n\\n        # create buffers for the terrains\\n        self.terrain_prim_paths = list()\\n        self.terrain_origins = None\\n        self.env_origins = None  # assigned later when `configure_env_origins` is called\\n        # private variables\\n        self._terrain_flat_patches = dict()\\n\\n        # auto-import the terrain based on the config\\n        if self.cfg.terrain_type == \"generator\":\\n            # check config is provided\\n            if self.cfg.terrain_generator is None:\\n                raise ValueError(\"Input terrain type is \\'generator\\' but no value provided for \\'terrain_generator\\'.\")\\n            # generate the terrain\\n            terrain_generator = TerrainGenerator(cfg=self.cfg.terrain_generator, device=self.device)\\n            self.import_mesh(\"terrain\", terrain_generator.terrain_mesh)\\n            # configure the terrain origins based on the terrain generator\\n            self.configure_env_origins(terrain_generator.terrain_origins)\\n            # refer to the flat patches\\n            self._terrain_flat_patches = terrain_generator.flat_patches\\n        elif self.cfg.terrain_type == \"usd\":\\n            # check if config is provided\\n            if self.cfg.usd_path is None:\\n                raise ValueError(\"Input terrain type is \\'usd\\' but no value provided for \\'usd_path\\'.\")\\n            # import the terrain\\n            self.import_usd(\"terrain\", self.cfg.usd_path)\\n            # configure the origins in a grid\\n            self.configure_env_origins()\\n        elif self.cfg.terrain_type == \"plane\":\\n            # load the plane\\n            self.import_ground_plane(\"terrain\")\\n            # configure the origins in a grid\\n            self.configure_env_origins()\\n        else:\\n            raise ValueError(f\"Terrain type \\'{self.cfg.terrain_type}\\' not available.\")\\n\\n        # set initial state of debug visualization\\n        self.set_debug_vis(self.cfg.debug_vis)\\n\\n    \"\"\"\\n    Properties.\\n    \"\"\"\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the terrain importer has a debug visualization implemented.\\n\\n        This always returns True.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def flat_patches(self) -> dict[str, torch.Tensor]:\\n        \"\"\"A dictionary containing the sampled valid (flat) patches for the terrain.\\n\\n        This is only available if the terrain type is \\'generator\\'. For other terrain types, this feature\\n        is not available and the function returns an empty dictionary.\\n\\n        Please refer to the :attr:`TerrainGenerator.flat_patches` for more information.\\n        \"\"\"\\n        return self._terrain_flat_patches\\n\\n    @property\\n    def terrain_names(self) -> list[str]:\\n        \"\"\"A list of names of the imported terrains.\"\"\"\\n        return [f\"\\'{path.split(\\'/\\')[-1]}\\'\" for path in self.terrain_prim_paths]\\n\\n    \"\"\"\\n    Operations - Visibility.\\n    \"\"\"\\n\\n    def set_debug_vis(self, debug_vis: bool) -> bool:\\n        \"\"\"Set the debug visualization of the terrain importer.\\n\\n        Args:\\n            debug_vis: Whether to visualize the terrain origins.\\n\\n        Returns:\\n            Whether the debug visualization was successfully set. False if the terrain\\n            importer does not support debug visualization.\\n\\n        Raises:\\n            RuntimeError: If terrain origins are not configured.\\n        \"\"\"\\n        # create a marker if necessary\\n        if debug_vis:\\n            if not hasattr(self, \"origin_visualizer\"):\\n                self.origin_visualizer = VisualizationMarkers(\\n                    cfg=FRAME_MARKER_CFG.replace(prim_path=\"/Visuals/TerrainOrigin\")\\n                )\\n                if self.terrain_origins is not None:\\n                    self.origin_visualizer.visualize(self.terrain_origins.reshape(-1, 3))\\n                elif self.env_origins is not None:\\n                    self.origin_visualizer.visualize(self.env_origins.reshape(-1, 3))\\n                else:\\n                    raise RuntimeError(\"Terrain origins are not configured.\")\\n            # set visibility\\n            self.origin_visualizer.set_visibility(True)\\n        else:\\n            if hasattr(self, \"origin_visualizer\"):\\n                self.origin_visualizer.set_visibility(False)\\n        # report success\\n        return True\\n\\n    \"\"\"\\n    Operations - Import.\\n    \"\"\"\\n\\n    def import_ground_plane(self, name: str, size: tuple[float, float] = (2.0e6, 2.0e6)):\\n        \"\"\"Add a plane to the terrain importer.\\n\\n        Args:\\n            name: The name of the imported terrain. This name is used to create the USD prim\\n                corresponding to the terrain.\\n            size: The size of the plane. Defaults to (2.0e6, 2.0e6).\\n\\n        Raises:\\n            ValueError: If a terrain with the same name already exists.\\n        \"\"\"\\n        # create prim path for the terrain\\n        prim_path = self.cfg.prim_path + f\"/{name}\"\\n        # check if key exists\\n        if prim_path in self.terrain_prim_paths:\\n            raise ValueError(\\n                f\"A terrain with the name \\'{name}\\' already exists. Existing terrains: {\\', \\'.join(self.terrain_names)}.\"\\n            )\\n        # store the mesh name\\n        self.terrain_prim_paths.append(prim_path)\\n\\n        # obtain ground plane color from the configured visual material\\n        color = (0.0, 0.0, 0.0)\\n        if self.cfg.visual_material is not None:\\n            material = self.cfg.visual_material.to_dict()\\n            # defaults to the `GroundPlaneCfg` color if diffuse color attribute is not found\\n            if \"diffuse_color\" in material:\\n                color = material[\"diffuse_color\"]\\n            else:\\n                omni.log.warn(\\n                    \"Visual material specified for ground plane but no diffuse color found.\"\\n                    \" Using default color: (0.0, 0.0, 0.0)\"\\n                )\\n\\n        # get the mesh\\n        ground_plane_cfg = sim_utils.GroundPlaneCfg(physics_material=self.cfg.physics_material, size=size, color=color)\\n        ground_plane_cfg.func(prim_path, ground_plane_cfg)\\n\\n    def import_mesh(self, name: str, mesh: trimesh.Trimesh):\\n        \"\"\"Import a mesh into the simulator.\\n\\n        The mesh is imported into the simulator under the prim path ``cfg.prim_path/{key}``. The created path\\n        contains the mesh as a :class:`pxr.UsdGeom` instance along with visual or physics material prims.\\n\\n        Args:\\n            name: The name of the imported terrain. This name is used to create the USD prim\\n                corresponding to the terrain.\\n            mesh: The mesh to import.\\n\\n        Raises:\\n            ValueError: If a terrain with the same name already exists.\\n        \"\"\"\\n        # create prim path for the terrain\\n        prim_path = self.cfg.prim_path + f\"/{name}\"\\n        # check if key exists\\n        if prim_path in self.terrain_prim_paths:\\n            raise ValueError(\\n                f\"A terrain with the name \\'{name}\\' already exists. Existing terrains: {\\', \\'.join(self.terrain_names)}.\"\\n            )\\n        # store the mesh name\\n        self.terrain_prim_paths.append(prim_path)\\n\\n        # import the mesh\\n        create_prim_from_mesh(\\n            prim_path, mesh, visual_material=self.cfg.visual_material, physics_material=self.cfg.physics_material\\n        )\\n\\n    def import_usd(self, name: str, usd_path: str):\\n        \"\"\"Import a mesh from a USD file.\\n\\n        This function imports a USD file into the simulator as a terrain. It parses the USD file and\\n        stores the mesh under the prim path ``cfg.prim_path/{key}``. If multiple meshes are present in\\n        the USD file, only the first mesh is imported.\\n\\n        The function doe not apply any material properties to the mesh. The material properties should\\n        be defined in the USD file.\\n\\n        Args:\\n            name: The name of the imported terrain. This name is used to create the USD prim\\n                corresponding to the terrain.\\n            usd_path: The path to the USD file.\\n\\n        Raises:\\n            ValueError: If a terrain with the same name already exists.\\n        \"\"\"\\n        # create prim path for the terrain\\n        prim_path = self.cfg.prim_path + f\"/{name}\"\\n        # check if key exists\\n        if prim_path in self.terrain_prim_paths:\\n            raise ValueError(\\n                f\"A terrain with the name \\'{name}\\' already exists. Existing terrains: {\\', \\'.join(self.terrain_names)}.\"\\n            )\\n        # store the mesh name\\n        self.terrain_prim_paths.append(prim_path)\\n\\n        # add the prim path\\n        cfg = sim_utils.UsdFileCfg(usd_path=usd_path)\\n        cfg.func(prim_path, cfg)\\n\\n    \"\"\"\\n    Operations - Origins.\\n    \"\"\"\\n\\n    def configure_env_origins(self, origins: np.ndarray | torch.Tensor | None = None):\\n        \"\"\"Configure the origins of the environments based on the added terrain.\\n\\n        Args:\\n            origins: The origins of the sub-terrains. Shape is (num_rows, num_cols, 3).\\n        \"\"\"\\n        # decide whether to compute origins in a grid or based on curriculum\\n        if origins is not None:\\n            # convert to numpy\\n            if isinstance(origins, np.ndarray):\\n                origins = torch.from_numpy(origins)\\n            # store the origins\\n            self.terrain_origins = origins.to(self.device, dtype=torch.float)\\n            # compute environment origins\\n            self.env_origins = self._compute_env_origins_curriculum(self.cfg.num_envs, self.terrain_origins)\\n        else:\\n            self.terrain_origins = None\\n            # check if env spacing is valid\\n            if self.cfg.env_spacing is None:\\n                raise ValueError(\"Environment spacing must be specified for configuring grid-like origins.\")\\n            # compute environment origins\\n            self.env_origins = self._compute_env_origins_grid(self.cfg.num_envs, self.cfg.env_spacing)\\n\\n    def update_env_origins(self, env_ids: torch.Tensor, move_up: torch.Tensor, move_down: torch.Tensor):\\n        \"\"\"Update the environment origins based on the terrain levels.\"\"\"\\n        # check if grid-like spawning\\n        if self.terrain_origins is None:\\n            return\\n        # update terrain level for the envs\\n        self.terrain_levels[env_ids] += 1 * move_up - 1 * move_down\\n        # robots that solve the last level are sent to a random one\\n        # the minimum level is zero\\n        self.terrain_levels[env_ids] = torch.where(\\n            self.terrain_levels[env_ids] >= self.max_terrain_level,\\n            torch.randint_like(self.terrain_levels[env_ids], self.max_terrain_level),\\n            torch.clip(self.terrain_levels[env_ids], 0),\\n        )\\n        # update the env origins\\n        self.env_origins[env_ids] = self.terrain_origins[self.terrain_levels[env_ids], self.terrain_types[env_ids]]\\n\\n    \"\"\"\\n    Internal helpers.\\n    \"\"\"\\n\\n    def _compute_env_origins_curriculum(self, num_envs: int, origins: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Compute the origins of the environments defined by the sub-terrains origins.\"\"\"\\n        # extract number of rows and cols\\n        num_rows, num_cols = origins.shape[:2]\\n        # maximum initial level possible for the terrains\\n        if self.cfg.max_init_terrain_level is None:\\n            max_init_level = num_rows - 1\\n        else:\\n            max_init_level = min(self.cfg.max_init_terrain_level, num_rows - 1)\\n        # store maximum terrain level possible\\n        self.max_terrain_level = num_rows\\n        # define all terrain levels and types available\\n        self.terrain_levels = torch.randint(0, max_init_level + 1, (num_envs,), device=self.device)\\n        self.terrain_types = torch.div(\\n            torch.arange(num_envs, device=self.device), (num_envs / num_cols), rounding_mode=\"floor\"\\n        ).to(torch.long)\\n        # create tensor based on number of environments\\n        env_origins = torch.zeros(num_envs, 3, device=self.device)\\n        env_origins[:] = origins[self.terrain_levels, self.terrain_types]\\n        return env_origins\\n\\n    def _compute_env_origins_grid(self, num_envs: int, env_spacing: float) -> torch.Tensor:\\n        \"\"\"Compute the origins of the environments in a grid based on configured spacing.\"\"\"\\n        # create tensor based on number of environments\\n        env_origins = torch.zeros(num_envs, 3, device=self.device)\\n        # create a grid of origins\\n        num_rows = np.ceil(num_envs / int(np.sqrt(num_envs)))\\n        num_cols = np.ceil(num_envs / num_rows)\\n        ii, jj = torch.meshgrid(\\n            torch.arange(num_rows, device=self.device), torch.arange(num_cols, device=self.device), indexing=\"ij\"\\n        )\\n        env_origins[:, 0] = -(ii.flatten()[:num_envs] - (num_rows - 1) / 2) * env_spacing\\n        env_origins[:, 1] = (jj.flatten()[:num_envs] - (num_cols - 1) / 2) * env_spacing\\n        env_origins[:, 2] = 0.0\\n        return env_origins\\n\\n    \"\"\"\\n    Deprecated.\\n    \"\"\"\\n\\n    @property\\n    def warp_meshes(self):\\n        \"\"\"A dictionary containing the terrain\\'s names and their warp meshes.\\n\\n        .. deprecated:: v2.1.0\\n            The `warp_meshes` attribute is deprecated. It is no longer stored inside the class.\\n        \"\"\"\\n        omni.log.warn(\\n            \"The `warp_meshes` attribute is deprecated. It is no longer stored inside the `TerrainImporter` class.\"\\n            \" Returning an empty dictionary.\"\\n        )\\n        return {}\\n\\n    @property\\n    def meshes(self) -> dict[str, trimesh.Trimesh]:\\n        \"\"\"A dictionary containing the terrain\\'s names and their tri-meshes.\\n\\n        .. deprecated:: v2.1.0\\n            The `meshes` attribute is deprecated. It is no longer stored inside the class.\\n        \"\"\"\\n        omni.log.warn(\\n            \"The `meshes` attribute is deprecated. It is no longer stored inside the `TerrainImporter` class.\"\\n            \" Returning an empty dictionary.\"\\n        )\\n        return {}'),\n",
       " Document(metadata={}, page_content='class TerrainImporterCfg:\\n    \"\"\"Configuration for the terrain manager.\"\"\"\\n\\n    class_type: type = TerrainImporter\\n    \"\"\"The class to use for the terrain importer.\\n\\n    Defaults to :class:`isaaclab.terrains.terrain_importer.TerrainImporter`.\\n    \"\"\"\\n\\n    collision_group: int = -1\\n    \"\"\"The collision group of the terrain. Defaults to -1.\"\"\"\\n\\n    prim_path: str = MISSING\\n    \"\"\"The absolute path of the USD terrain prim.\\n\\n    All sub-terrains are imported relative to this prim path.\\n    \"\"\"\\n\\n    num_envs: int = 1\\n    \"\"\"The number of environment origins to consider. Defaults to 1.\\n\\n    In case, the :class:`~isaaclab.scene.InteractiveSceneCfg` is used, this parameter gets overridden by\\n    :attr:`isaaclab.scene.InteractiveSceneCfg.num_envs` attribute.\\n    \"\"\"\\n\\n    terrain_type: Literal[\"generator\", \"plane\", \"usd\"] = \"generator\"\\n    \"\"\"The type of terrain to generate. Defaults to \"generator\".\\n\\n    Available options are \"plane\", \"usd\", and \"generator\".\\n    \"\"\"\\n\\n    terrain_generator: TerrainGeneratorCfg | None = None\\n    \"\"\"The terrain generator configuration.\\n\\n    Only used if ``terrain_type`` is set to \"generator\".\\n    \"\"\"\\n\\n    usd_path: str | None = None\\n    \"\"\"The path to the USD file containing the terrain.\\n\\n    Only used if ``terrain_type`` is set to \"usd\".\\n    \"\"\"\\n\\n    env_spacing: float | None = None\\n    \"\"\"The spacing between environment origins when defined in a grid. Defaults to None.\\n\\n    Note:\\n      This parameter is used only when the ``terrain_type`` is \"plane\" or \"usd\".\\n    \"\"\"\\n\\n    visual_material: sim_utils.VisualMaterialCfg | None = sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 0.0, 0.0))\\n    \"\"\"The visual material of the terrain. Defaults to a dark gray color material.\\n\\n    This parameter is used for both the \"generator\" and \"plane\" terrains.\\n\\n    - If the ``terrain_type`` is \"generator\", then the material is created at the path\\n      ``{prim_path}/visualMaterial`` and applied to all the sub-terrains.\\n    - If the ``terrain_type`` is \"plane\", then the diffuse color of the material is set to\\n      to the grid color of the imported ground plane.\\n    \"\"\"\\n\\n    physics_material: sim_utils.RigidBodyMaterialCfg = sim_utils.RigidBodyMaterialCfg()\\n    \"\"\"The physics material of the terrain. Defaults to a default physics material.\\n\\n    The material is created at the path: ``{prim_path}/physicsMaterial``.\\n\\n    .. note::\\n        This parameter is used only when the ``terrain_type`` is \"generator\" or \"plane\".\\n    \"\"\"\\n\\n    max_init_terrain_level: int | None = None\\n    \"\"\"The maximum initial terrain level for defining environment origins. Defaults to None.\\n\\n    The terrain levels are specified by the number of rows in the grid arrangement of\\n    sub-terrains. If None, then the initial terrain level is set to the maximum\\n    terrain level available (``num_rows - 1``).\\n\\n    Note:\\n      This parameter is used only when sub-terrain origins are defined.\\n    \"\"\"\\n\\n    debug_vis: bool = False\\n    \"\"\"Whether to enable visualization of terrain origins for the terrain. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='def color_meshes_by_height(meshes: list[trimesh.Trimesh], **kwargs) -> trimesh.Trimesh:\\n    \"\"\"\\n    Color the vertices of a trimesh object based on the z-coordinate (height) of each vertex,\\n    using the Turbo colormap. If the z-coordinates are all the same, the vertices will be colored\\n    with a single color.\\n\\n    Args:\\n        meshes: A list of trimesh objects.\\n\\n    Keyword Args:\\n        color: A list of 3 integers in the range [0,255] representing the RGB\\n            color of the mesh. Used when the z-coordinates of all vertices are the same.\\n            Defaults to [172, 216, 230].\\n        color_map: The name of the color map to be used. Defaults to \"turbo\".\\n\\n    Returns:\\n        A trimesh object with the vertices colored based on the z-coordinate (height) of each vertex.\\n    \"\"\"\\n    # Combine all meshes into a single mesh\\n    mesh = trimesh.util.concatenate(meshes)\\n    # Get the z-coordinates of each vertex\\n    heights = mesh.vertices[:, 2]\\n    # Check if the z-coordinates are all the same\\n    if np.max(heights) == np.min(heights):\\n        # Obtain a single color: light blue\\n        color = kwargs.pop(\"color\", (172, 216, 230))\\n        color = np.asarray(color, dtype=np.uint8)\\n        # Set the color for all vertices\\n        mesh.visual.vertex_colors = color\\n    else:\\n        # Normalize the heights to [0,1]\\n        heights_normalized = (heights - np.min(heights)) / (np.max(heights) - np.min(heights))\\n        # clip lower and upper bounds to have better color mapping\\n        heights_normalized = np.clip(heights_normalized, 0.1, 0.9)\\n        # Get the color for each vertex based on the height\\n        color_map = kwargs.pop(\"color_map\", \"turbo\")\\n        colors = trimesh.visual.color.interpolate(heights_normalized, color_map=color_map)\\n        # Set the vertex colors\\n        mesh.visual.vertex_colors = colors\\n    # Return the mesh\\n    return mesh'),\n",
       " Document(metadata={}, page_content='def create_prim_from_mesh(prim_path: str, mesh: trimesh.Trimesh, **kwargs):\\n    \"\"\"Create a USD prim with mesh defined from vertices and triangles.\\n\\n    The function creates a USD prim with a mesh defined from vertices and triangles. It performs the\\n    following steps:\\n\\n    - Create a USD Xform prim at the path :obj:`prim_path`.\\n    - Create a USD prim with a mesh defined from the input vertices and triangles at the path :obj:`{prim_path}/mesh`.\\n    - Assign a physics material to the mesh at the path :obj:`{prim_path}/physicsMaterial`.\\n    - Assign a visual material to the mesh at the path :obj:`{prim_path}/visualMaterial`.\\n\\n    Args:\\n        prim_path: The path to the primitive to be created.\\n        mesh: The mesh to be used for the primitive.\\n\\n    Keyword Args:\\n        translation: The translation of the terrain. Defaults to None.\\n        orientation: The orientation of the terrain. Defaults to None.\\n        visual_material: The visual material to apply. Defaults to None.\\n        physics_material: The physics material to apply. Defaults to None.\\n    \"\"\"\\n    # need to import these here to prevent isaacsim launching when importing this module\\n    import isaacsim.core.utils.prims as prim_utils\\n    from pxr import UsdGeom\\n\\n    import isaaclab.sim as sim_utils\\n\\n    # create parent prim\\n    prim_utils.create_prim(prim_path, \"Xform\")\\n    # create mesh prim\\n    prim = prim_utils.create_prim(\\n        f\"{prim_path}/mesh\",\\n        \"Mesh\",\\n        translation=kwargs.get(\"translation\"),\\n        orientation=kwargs.get(\"orientation\"),\\n        attributes={\\n            \"points\": mesh.vertices,\\n            \"faceVertexIndices\": mesh.faces.flatten(),\\n            \"faceVertexCounts\": np.asarray([3] * len(mesh.faces)),\\n            \"subdivisionScheme\": \"bilinear\",\\n        },\\n    )\\n    # apply collider properties\\n    collider_cfg = sim_utils.CollisionPropertiesCfg(collision_enabled=True)\\n    sim_utils.define_collision_properties(prim.GetPrimPath(), collider_cfg)\\n    # add rgba color to the mesh primvars\\n    if mesh.visual.vertex_colors is not None:\\n        # obtain color from the mesh\\n        rgba_colors = np.asarray(mesh.visual.vertex_colors).astype(np.float32) / 255.0\\n        # displayColor is a primvar attribute that is used to color the mesh\\n        color_prim_attr = prim.GetAttribute(\"primvars:displayColor\")\\n        color_prim_var = UsdGeom.Primvar(color_prim_attr)\\n        color_prim_var.SetInterpolation(UsdGeom.Tokens.vertex)\\n        color_prim_attr.Set(rgba_colors[:, :3])\\n        # displayOpacity is a primvar attribute that is used to set the opacity of the mesh\\n        display_prim_attr = prim.GetAttribute(\"primvars:displayOpacity\")\\n        display_prim_var = UsdGeom.Primvar(display_prim_attr)\\n        display_prim_var.SetInterpolation(UsdGeom.Tokens.vertex)\\n        display_prim_var.Set(rgba_colors[:, 3])\\n\\n    # create visual material\\n    if kwargs.get(\"visual_material\") is not None:\\n        visual_material_cfg: sim_utils.VisualMaterialCfg = kwargs.get(\"visual_material\")\\n        # spawn the material\\n        visual_material_cfg.func(f\"{prim_path}/visualMaterial\", visual_material_cfg)\\n        sim_utils.bind_visual_material(prim.GetPrimPath(), f\"{prim_path}/visualMaterial\")\\n    # create physics material\\n    if kwargs.get(\"physics_material\") is not None:\\n        physics_material_cfg: sim_utils.RigidBodyMaterialCfg = kwargs.get(\"physics_material\")\\n        # spawn the material\\n        physics_material_cfg.func(f\"{prim_path}/physicsMaterial\", physics_material_cfg)\\n        sim_utils.bind_physics_material(prim.GetPrimPath(), f\"{prim_path}/physicsMaterial\")'),\n",
       " Document(metadata={}, page_content='def find_flat_patches(\\n    wp_mesh: wp.Mesh,\\n    num_patches: int,\\n    patch_radius: float | list[float],\\n    origin: np.ndarray | torch.Tensor | tuple[float, float, float],\\n    x_range: tuple[float, float],\\n    y_range: tuple[float, float],\\n    z_range: tuple[float, float],\\n    max_height_diff: float,\\n) -> torch.Tensor:\\n    \"\"\"Finds flat patches of given radius in the input mesh.\\n\\n    The function finds flat patches of given radius based on the search space defined by the input ranges.\\n    The search space is characterized by origin in the mesh frame, and the x, y, and z ranges. The x and y\\n    ranges are used to sample points in the 2D region around the origin, and the z range is used to filter\\n    patches based on the height of the points.\\n\\n    The function performs rejection sampling to find the patches based on the following steps:\\n\\n    1. Sample patch locations in the 2D region around the origin.\\n    2. Define a ring of points around each patch location to query the height of the points using ray-casting.\\n    3. Reject patches that are outside the z range or have a height difference that is too large.\\n    4. Keep sampling until all patches are valid.\\n\\n    Args:\\n        wp_mesh: The warp mesh to find patches in.\\n        num_patches: The desired number of patches to find.\\n        patch_radius: The radii used to form patches. If a list is provided, multiple patch sizes are checked.\\n            This is useful to deal with holes or other artifacts in the mesh.\\n        origin: The origin defining the center of the search space. This is specified in the mesh frame.\\n        x_range: The range of X coordinates to sample from.\\n        y_range: The range of Y coordinates to sample from.\\n        z_range: The range of valid Z coordinates used for filtering patches.\\n        max_height_diff: The maximum allowable distance between the lowest and highest points\\n            on a patch to consider it as valid. If the difference is greater than this value,\\n            the patch is rejected.\\n\\n    Returns:\\n        A tensor of shape (num_patches, 3) containing the flat patches. The patches are defined in the mesh frame.\\n\\n    Raises:\\n        RuntimeError: If the function fails to find valid patches. This can happen if the input parameters\\n            are not suitable for finding valid patches and maximum number of iterations is reached.\\n    \"\"\"\\n    # set device to warp mesh device\\n    device = wp.device_to_torch(wp_mesh.device)\\n\\n    # resolve inputs to consistent type\\n    # -- patch radii\\n    if isinstance(patch_radius, float):\\n        patch_radius = [patch_radius]\\n    # -- origin\\n    if isinstance(origin, np.ndarray):\\n        origin = torch.from_numpy(origin).to(torch.float).to(device)\\n    elif isinstance(origin, torch.Tensor):\\n        origin = origin.to(device)\\n    else:\\n        origin = torch.tensor(origin, dtype=torch.float, device=device)\\n\\n    # create ranges for the x and y coordinates around the origin.\\n    # The provided ranges are bounded by the mesh\\'s bounding box.\\n    x_range = (\\n        max(x_range[0] + origin[0].item(), wp_mesh.points.numpy()[:, 0].min()),\\n        min(x_range[1] + origin[0].item(), wp_mesh.points.numpy()[:, 0].max()),\\n    )\\n    y_range = (\\n        max(y_range[0] + origin[1].item(), wp_mesh.points.numpy()[:, 1].min()),\\n        min(y_range[1] + origin[1].item(), wp_mesh.points.numpy()[:, 1].max()),\\n    )\\n    z_range = (\\n        z_range[0] + origin[2].item(),\\n        z_range[1] + origin[2].item(),\\n    )\\n\\n    # create a circle of points around (0, 0) to query validity of the patches\\n    # the ring of points is uniformly distributed around the circle\\n    angle = torch.linspace(0, 2 * np.pi, 10, device=device)\\n    query_x = []\\n    query_y = []\\n    for radius in patch_radius:\\n        query_x.append(radius * torch.cos(angle))\\n        query_y.append(radius * torch.sin(angle))\\n    query_x = torch.cat(query_x).unsqueeze(1)  # dim: (num_radii * 10, 1)\\n    query_y = torch.cat(query_y).unsqueeze(1)  # dim: (num_radii * 10, 1)\\n    # dim: (num_radii * 10, 3)\\n    query_points = torch.cat([query_x, query_y, torch.zeros_like(query_x)], dim=-1)\\n\\n    # create buffers\\n    # -- a buffer to store indices of points that are not valid\\n    points_ids = torch.arange(num_patches, device=device)\\n    # -- a buffer to store the flat patches locations\\n    flat_patches = torch.zeros(num_patches, 3, device=device)\\n\\n    # sample points and raycast to find the height.\\n    # 1. Reject points that are outside the z_range or have a height difference that is too large.\\n    # 2. Keep sampling until all points are valid.\\n    iter_count = 0\\n    while len(points_ids) > 0 and iter_count < 10000:\\n        # sample points in the 2D region around the origin\\n        pos_x = torch.empty(len(points_ids), device=device).uniform_(*x_range)\\n        pos_y = torch.empty(len(points_ids), device=device).uniform_(*y_range)\\n        flat_patches[points_ids, :2] = torch.stack([pos_x, pos_y], dim=-1)\\n\\n        # define the query points to check validity of the patch\\n        # dim: (num_patches, num_radii * 10, 3)\\n        points = flat_patches[points_ids].unsqueeze(1) + query_points\\n        points[..., 2] = 100.0\\n        # ray-cast direction is downwards\\n        dirs = torch.zeros_like(points)\\n        dirs[..., 2] = -1.0\\n\\n        # ray-cast to find the height of the patches\\n        ray_hits = raycast_mesh(points.view(-1, 3), dirs.view(-1, 3), wp_mesh)[0]\\n        heights = ray_hits.view(points.shape)[..., 2]\\n        # set the height of the patches\\n        # note: for invalid patches, they would be overwritten in the next iteration\\n        #   so it\\'s safe to set the height to the last value\\n        flat_patches[points_ids, 2] = heights[..., -1]\\n\\n        # check validity\\n        # -- height is within the z range\\n        not_valid = torch.any(torch.logical_or(heights < z_range[0], heights > z_range[1]), dim=1)\\n        # -- height difference is within the max height difference\\n        not_valid = torch.logical_or(not_valid, (heights.max(dim=1)[0] - heights.min(dim=1)[0]) > max_height_diff)\\n\\n        # remove invalid patches indices\\n        points_ids = points_ids[not_valid]\\n        # increment count\\n        iter_count += 1\\n\\n    # check all patches are valid\\n    if len(points_ids) > 0:\\n        raise RuntimeError(\\n            \"Failed to find valid patches! Please check the input parameters.\"\\n            f\"\\\\n\\\\tMaximum number of iterations reached: {iter_count}\"\\n            f\"\\\\n\\\\tNumber of invalid patches: {len(points_ids)}\"\\n            f\"\\\\n\\\\tMaximum height difference: {max_height_diff}\"\\n        )\\n\\n    # return the flat patches (in the mesh frame)\\n    return flat_patches - origin'),\n",
       " Document(metadata={}, page_content='def random_uniform_terrain(difficulty: float, cfg: hf_terrains_cfg.HfRandomUniformTerrainCfg) -> np.ndarray:\\n    \"\"\"Generate a terrain with height sampled uniformly from a specified range.\\n\\n    .. image:: ../../_static/terrains/height_field/random_uniform_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Note:\\n        The :obj:`difficulty` parameter is ignored for this terrain.\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        The height field of the terrain as a 2D numpy array with discretized heights.\\n        The shape of the array is (width, length), where width and length are the number of points\\n        along the x and y axis, respectively.\\n\\n    Raises:\\n        ValueError: When the downsampled scale is smaller than the horizontal scale.\\n    \"\"\"\\n    # check parameters\\n    # -- horizontal scale\\n    if cfg.downsampled_scale is None:\\n        cfg.downsampled_scale = cfg.horizontal_scale\\n    elif cfg.downsampled_scale < cfg.horizontal_scale:\\n        raise ValueError(\\n            \"Downsampled scale must be larger than or equal to the horizontal scale:\"\\n            f\" {cfg.downsampled_scale} < {cfg.horizontal_scale}.\"\\n        )\\n\\n    # switch parameters to discrete units\\n    # -- horizontal scale\\n    width_pixels = int(cfg.size[0] / cfg.horizontal_scale)\\n    length_pixels = int(cfg.size[1] / cfg.horizontal_scale)\\n    # -- downsampled scale\\n    width_downsampled = int(cfg.size[0] / cfg.downsampled_scale)\\n    length_downsampled = int(cfg.size[1] / cfg.downsampled_scale)\\n    # -- height\\n    height_min = int(cfg.noise_range[0] / cfg.vertical_scale)\\n    height_max = int(cfg.noise_range[1] / cfg.vertical_scale)\\n    height_step = int(cfg.noise_step / cfg.vertical_scale)\\n\\n    # create range of heights possible\\n    height_range = np.arange(height_min, height_max + height_step, height_step)\\n    # sample heights randomly from the range along a grid\\n    height_field_downsampled = np.random.choice(height_range, size=(width_downsampled, length_downsampled))\\n    # create interpolation function for the sampled heights\\n    x = np.linspace(0, cfg.size[0] * cfg.horizontal_scale, width_downsampled)\\n    y = np.linspace(0, cfg.size[1] * cfg.horizontal_scale, length_downsampled)\\n    func = interpolate.RectBivariateSpline(x, y, height_field_downsampled)\\n\\n    # interpolate the sampled heights to obtain the height field\\n    x_upsampled = np.linspace(0, cfg.size[0] * cfg.horizontal_scale, width_pixels)\\n    y_upsampled = np.linspace(0, cfg.size[1] * cfg.horizontal_scale, length_pixels)\\n    z_upsampled = func(x_upsampled, y_upsampled)\\n    # round off the interpolated heights to the nearest vertical step\\n    return np.rint(z_upsampled).astype(np.int16)'),\n",
       " Document(metadata={}, page_content='def pyramid_sloped_terrain(difficulty: float, cfg: hf_terrains_cfg.HfPyramidSlopedTerrainCfg) -> np.ndarray:\\n    \"\"\"Generate a terrain with a truncated pyramid structure.\\n\\n    The terrain is a pyramid-shaped sloped surface with a slope of :obj:`slope` that trims into a flat platform\\n    at the center. The slope is defined as the ratio of the height change along the x axis to the width along the\\n    x axis. For example, a slope of 1.0 means that the height changes by 1 unit for every 1 unit of width.\\n\\n    If the :obj:`cfg.inverted` flag is set to :obj:`True`, the terrain is inverted such that\\n    the platform is at the bottom.\\n\\n    .. image:: ../../_static/terrains/height_field/pyramid_sloped_terrain.jpg\\n       :width: 40%\\n\\n    .. image:: ../../_static/terrains/height_field/inverted_pyramid_sloped_terrain.jpg\\n       :width: 40%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        The height field of the terrain as a 2D numpy array with discretized heights.\\n        The shape of the array is (width, length), where width and length are the number of points\\n        along the x and y axis, respectively.\\n    \"\"\"\\n    # resolve terrain configuration\\n    if cfg.inverted:\\n        slope = -cfg.slope_range[0] - difficulty * (cfg.slope_range[1] - cfg.slope_range[0])\\n    else:\\n        slope = cfg.slope_range[0] + difficulty * (cfg.slope_range[1] - cfg.slope_range[0])\\n\\n    # switch parameters to discrete units\\n    # -- horizontal scale\\n    width_pixels = int(cfg.size[0] / cfg.horizontal_scale)\\n    length_pixels = int(cfg.size[1] / cfg.horizontal_scale)\\n    # -- height\\n    # we want the height to be 1/2 of the width since the terrain is a pyramid\\n    height_max = int(slope * cfg.size[0] / 2 / cfg.vertical_scale)\\n    # -- center of the terrain\\n    center_x = int(width_pixels / 2)\\n    center_y = int(length_pixels / 2)\\n\\n    # create a meshgrid of the terrain\\n    x = np.arange(0, width_pixels)\\n    y = np.arange(0, length_pixels)\\n    xx, yy = np.meshgrid(x, y, sparse=True)\\n    # offset the meshgrid to the center of the terrain\\n    xx = (center_x - np.abs(center_x - xx)) / center_x\\n    yy = (center_y - np.abs(center_y - yy)) / center_y\\n    # reshape the meshgrid to be 2D\\n    xx = xx.reshape(width_pixels, 1)\\n    yy = yy.reshape(1, length_pixels)\\n    # create a sloped surface\\n    hf_raw = np.zeros((width_pixels, length_pixels))\\n    hf_raw = height_max * xx * yy\\n\\n    # create a flat platform at the center of the terrain\\n    platform_width = int(cfg.platform_width / cfg.horizontal_scale / 2)\\n    # get the height of the platform at the corner of the platform\\n    x_pf = width_pixels // 2 - platform_width\\n    y_pf = length_pixels // 2 - platform_width\\n    z_pf = hf_raw[x_pf, y_pf]\\n    hf_raw = np.clip(hf_raw, min(0, z_pf), max(0, z_pf))\\n\\n    # round off the heights to the nearest vertical step\\n    return np.rint(hf_raw).astype(np.int16)'),\n",
       " Document(metadata={}, page_content='def pyramid_stairs_terrain(difficulty: float, cfg: hf_terrains_cfg.HfPyramidStairsTerrainCfg) -> np.ndarray:\\n    \"\"\"Generate a terrain with a pyramid stair pattern.\\n\\n    The terrain is a pyramid stair pattern which trims to a flat platform at the center of the terrain.\\n\\n    If the :obj:`cfg.inverted` flag is set to :obj:`True`, the terrain is inverted such that\\n    the platform is at the bottom.\\n\\n    .. image:: ../../_static/terrains/height_field/pyramid_stairs_terrain.jpg\\n       :width: 40%\\n\\n    .. image:: ../../_static/terrains/height_field/inverted_pyramid_stairs_terrain.jpg\\n       :width: 40%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        The height field of the terrain as a 2D numpy array with discretized heights.\\n        The shape of the array is (width, length), where width and length are the number of points\\n        along the x and y axis, respectively.\\n    \"\"\"\\n    # resolve terrain configuration\\n    step_height = cfg.step_height_range[0] + difficulty * (cfg.step_height_range[1] - cfg.step_height_range[0])\\n    if cfg.inverted:\\n        step_height *= -1\\n    # switch parameters to discrete units\\n    # -- terrain\\n    width_pixels = int(cfg.size[0] / cfg.horizontal_scale)\\n    length_pixels = int(cfg.size[1] / cfg.horizontal_scale)\\n    # -- stairs\\n    step_width = int(cfg.step_width / cfg.horizontal_scale)\\n    step_height = int(step_height / cfg.vertical_scale)\\n    # -- platform\\n    platform_width = int(cfg.platform_width / cfg.horizontal_scale)\\n\\n    # create a terrain with a flat platform at the center\\n    hf_raw = np.zeros((width_pixels, length_pixels))\\n    # add the steps\\n    current_step_height = 0\\n    start_x, start_y = 0, 0\\n    stop_x, stop_y = width_pixels, length_pixels\\n    while (stop_x - start_x) > platform_width and (stop_y - start_y) > platform_width:\\n        # increment position\\n        # -- x\\n        start_x += step_width\\n        stop_x -= step_width\\n        # -- y\\n        start_y += step_width\\n        stop_y -= step_width\\n        # increment height\\n        current_step_height += step_height\\n        # add the step\\n        hf_raw[start_x:stop_x, start_y:stop_y] = current_step_height\\n\\n    # round off the heights to the nearest vertical step\\n    return np.rint(hf_raw).astype(np.int16)'),\n",
       " Document(metadata={}, page_content='def discrete_obstacles_terrain(difficulty: float, cfg: hf_terrains_cfg.HfDiscreteObstaclesTerrainCfg) -> np.ndarray:\\n    \"\"\"Generate a terrain with randomly generated obstacles as pillars with positive and negative heights.\\n\\n    The terrain is a flat platform at the center of the terrain with randomly generated obstacles as pillars\\n    with positive and negative height. The obstacles are randomly generated cuboids with a random width and\\n    height. They are placed randomly on the terrain with a minimum distance of :obj:`cfg.platform_width`\\n    from the center of the terrain.\\n\\n    .. image:: ../../_static/terrains/height_field/discrete_obstacles_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        The height field of the terrain as a 2D numpy array with discretized heights.\\n        The shape of the array is (width, length), where width and length are the number of points\\n        along the x and y axis, respectively.\\n    \"\"\"\\n    # resolve terrain configuration\\n    obs_height = cfg.obstacle_height_range[0] + difficulty * (\\n        cfg.obstacle_height_range[1] - cfg.obstacle_height_range[0]\\n    )\\n\\n    # switch parameters to discrete units\\n    # -- terrain\\n    width_pixels = int(cfg.size[0] / cfg.horizontal_scale)\\n    length_pixels = int(cfg.size[1] / cfg.horizontal_scale)\\n    # -- obstacles\\n    obs_height = int(obs_height / cfg.vertical_scale)\\n    obs_width_min = int(cfg.obstacle_width_range[0] / cfg.horizontal_scale)\\n    obs_width_max = int(cfg.obstacle_width_range[1] / cfg.horizontal_scale)\\n    # -- center of the terrain\\n    platform_width = int(cfg.platform_width / cfg.horizontal_scale)\\n\\n    # create discrete ranges for the obstacles\\n    # -- shape\\n    obs_width_range = np.arange(obs_width_min, obs_width_max, 4)\\n    obs_length_range = np.arange(obs_width_min, obs_width_max, 4)\\n    # -- position\\n    obs_x_range = np.arange(0, width_pixels, 4)\\n    obs_y_range = np.arange(0, length_pixels, 4)\\n\\n    # create a terrain with a flat platform at the center\\n    hf_raw = np.zeros((width_pixels, length_pixels))\\n    # generate the obstacles\\n    for _ in range(cfg.num_obstacles):\\n        # sample size\\n        if cfg.obstacle_height_mode == \"choice\":\\n            height = np.random.choice([-obs_height, -obs_height // 2, obs_height // 2, obs_height])\\n        elif cfg.obstacle_height_mode == \"fixed\":\\n            height = obs_height\\n        else:\\n            raise ValueError(f\"Unknown obstacle height mode \\'{cfg.obstacle_height_mode}\\'. Must be \\'choice\\' or \\'fixed\\'.\")\\n        width = int(np.random.choice(obs_width_range))\\n        length = int(np.random.choice(obs_length_range))\\n        # sample position\\n        x_start = int(np.random.choice(obs_x_range))\\n        y_start = int(np.random.choice(obs_y_range))\\n        # clip start position to the terrain\\n        if x_start + width > width_pixels:\\n            x_start = width_pixels - width\\n        if y_start + length > length_pixels:\\n            y_start = length_pixels - length\\n        # add to terrain\\n        hf_raw[x_start : x_start + width, y_start : y_start + length] = height\\n    # clip the terrain to the platform\\n    x1 = (width_pixels - platform_width) // 2\\n    x2 = (width_pixels + platform_width) // 2\\n    y1 = (length_pixels - platform_width) // 2\\n    y2 = (length_pixels + platform_width) // 2\\n    hf_raw[x1:x2, y1:y2] = 0\\n    # round off the heights to the nearest vertical step\\n    return np.rint(hf_raw).astype(np.int16)'),\n",
       " Document(metadata={}, page_content='def wave_terrain(difficulty: float, cfg: hf_terrains_cfg.HfWaveTerrainCfg) -> np.ndarray:\\n    r\"\"\"Generate a terrain with a wave pattern.\\n\\n    The terrain is a flat platform at the center of the terrain with a wave pattern. The wave pattern\\n    is generated by adding sinusoidal waves based on the number of waves and the amplitude of the waves.\\n\\n    The height of the terrain at a point :math:`(x, y)` is given by:\\n\\n    .. math::\\n\\n        h(x, y) =  A \\\\left(\\\\sin\\\\left(\\\\frac{2 \\\\pi x}{\\\\lambda}\\\\right) + \\\\cos\\\\left(\\\\frac{2 \\\\pi y}{\\\\lambda}\\\\right) \\\\right)\\n\\n    where :math:`A` is the amplitude of the waves, :math:`\\\\lambda` is the wavelength of the waves.\\n\\n    .. image:: ../../_static/terrains/height_field/wave_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        The height field of the terrain as a 2D numpy array with discretized heights.\\n        The shape of the array is (width, length), where width and length are the number of points\\n        along the x and y axis, respectively.\\n\\n    Raises:\\n        ValueError: When the number of waves is non-positive.\\n    \"\"\"\\n    # check number of waves\\n    if cfg.num_waves < 0:\\n        raise ValueError(f\"Number of waves must be a positive integer. Got: {cfg.num_waves}.\")\\n\\n    # resolve terrain configuration\\n    amplitude = cfg.amplitude_range[0] + difficulty * (cfg.amplitude_range[1] - cfg.amplitude_range[0])\\n    # switch parameters to discrete units\\n    # -- terrain\\n    width_pixels = int(cfg.size[0] / cfg.horizontal_scale)\\n    length_pixels = int(cfg.size[1] / cfg.horizontal_scale)\\n    amplitude_pixels = int(0.5 * amplitude / cfg.vertical_scale)\\n\\n    # compute the wave number: nu = 2 * pi / lambda\\n    wave_length = length_pixels / cfg.num_waves\\n    wave_number = 2 * np.pi / wave_length\\n    # create meshgrid for the terrain\\n    x = np.arange(0, width_pixels)\\n    y = np.arange(0, length_pixels)\\n    xx, yy = np.meshgrid(x, y, sparse=True)\\n    xx = xx.reshape(width_pixels, 1)\\n    yy = yy.reshape(1, length_pixels)\\n\\n    # create a terrain with a flat platform at the center\\n    hf_raw = np.zeros((width_pixels, length_pixels))\\n    # add the waves\\n    hf_raw += amplitude_pixels * (np.cos(yy * wave_number) + np.sin(xx * wave_number))\\n    # round off the heights to the nearest vertical step\\n    return np.rint(hf_raw).astype(np.int16)'),\n",
       " Document(metadata={}, page_content='def stepping_stones_terrain(difficulty: float, cfg: hf_terrains_cfg.HfSteppingStonesTerrainCfg) -> np.ndarray:\\n    \"\"\"Generate a terrain with a stepping stones pattern.\\n\\n    The terrain is a stepping stones pattern which trims to a flat platform at the center of the terrain.\\n\\n    .. image:: ../../_static/terrains/height_field/stepping_stones_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        The height field of the terrain as a 2D numpy array with discretized heights.\\n        The shape of the array is (width, length), where width and length are the number of points\\n        along the x and y axis, respectively.\\n    \"\"\"\\n    # resolve terrain configuration\\n    stone_width = cfg.stone_width_range[1] - difficulty * (cfg.stone_width_range[1] - cfg.stone_width_range[0])\\n    stone_distance = cfg.stone_distance_range[0] + difficulty * (\\n        cfg.stone_distance_range[1] - cfg.stone_distance_range[0]\\n    )\\n\\n    # switch parameters to discrete units\\n    # -- terrain\\n    width_pixels = int(cfg.size[0] / cfg.horizontal_scale)\\n    length_pixels = int(cfg.size[1] / cfg.horizontal_scale)\\n    # -- stones\\n    stone_distance = int(stone_distance / cfg.horizontal_scale)\\n    stone_width = int(stone_width / cfg.horizontal_scale)\\n    stone_height_max = int(cfg.stone_height_max / cfg.vertical_scale)\\n    # -- holes\\n    holes_depth = int(cfg.holes_depth / cfg.vertical_scale)\\n    # -- platform\\n    platform_width = int(cfg.platform_width / cfg.horizontal_scale)\\n    # create range of heights\\n    stone_height_range = np.arange(-stone_height_max - 1, stone_height_max, step=1)\\n\\n    # create a terrain with a flat platform at the center\\n    hf_raw = np.full((width_pixels, length_pixels), holes_depth)\\n    # add the stones\\n    start_x, start_y = 0, 0\\n    # -- if the terrain is longer than it is wide then fill the terrain column by column\\n    if length_pixels >= width_pixels:\\n        while start_y < length_pixels:\\n            # ensure that stone stops along y-axis\\n            stop_y = min(length_pixels, start_y + stone_width)\\n            # randomly sample x-position\\n            start_x = np.random.randint(0, stone_width)\\n            stop_x = max(0, start_x - stone_distance)\\n            # fill first stone\\n            hf_raw[0:stop_x, start_y:stop_y] = np.random.choice(stone_height_range)\\n            # fill row with stones\\n            while start_x < width_pixels:\\n                stop_x = min(width_pixels, start_x + stone_width)\\n                hf_raw[start_x:stop_x, start_y:stop_y] = np.random.choice(stone_height_range)\\n                start_x += stone_width + stone_distance\\n            # update y-position\\n            start_y += stone_width + stone_distance\\n    elif width_pixels > length_pixels:\\n        while start_x < width_pixels:\\n            # ensure that stone stops along x-axis\\n            stop_x = min(width_pixels, start_x + stone_width)\\n            # randomly sample y-position\\n            start_y = np.random.randint(0, stone_width)\\n            stop_y = max(0, start_y - stone_distance)\\n            # fill first stone\\n            hf_raw[start_x:stop_x, 0:stop_y] = np.random.choice(stone_height_range)\\n            # fill column with stones\\n            while start_y < length_pixels:\\n                stop_y = min(length_pixels, start_y + stone_width)\\n                hf_raw[start_x:stop_x, start_y:stop_y] = np.random.choice(stone_height_range)\\n                start_y += stone_width + stone_distance\\n            # update x-position\\n            start_x += stone_width + stone_distance\\n    # add the platform in the center\\n    x1 = (width_pixels - platform_width) // 2\\n    x2 = (width_pixels + platform_width) // 2\\n    y1 = (length_pixels - platform_width) // 2\\n    y2 = (length_pixels + platform_width) // 2\\n    hf_raw[x1:x2, y1:y2] = 0\\n    # round off the heights to the nearest vertical step\\n    return np.rint(hf_raw).astype(np.int16)'),\n",
       " Document(metadata={}, page_content='class HfTerrainBaseCfg(SubTerrainBaseCfg):\\n    \"\"\"The base configuration for height field terrains.\"\"\"\\n\\n    border_width: float = 0.0\\n    \"\"\"The width of the border/padding around the terrain (in m). Defaults to 0.0.\\n\\n    The border width is subtracted from the :obj:`size` of the terrain. If non-zero, it must be\\n    greater than or equal to the :obj:`horizontal scale`.\\n    \"\"\"\\n    horizontal_scale: float = 0.1\\n    \"\"\"The discretization of the terrain along the x and y axes (in m). Defaults to 0.1.\"\"\"\\n    vertical_scale: float = 0.005\\n    \"\"\"The discretization of the terrain along the z axis (in m). Defaults to 0.005.\"\"\"\\n    slope_threshold: float | None = None\\n    \"\"\"The slope threshold above which surfaces are made vertical. Defaults to None,\\n    in which case no correction is applied.\"\"\"'),\n",
       " Document(metadata={}, page_content='class HfRandomUniformTerrainCfg(HfTerrainBaseCfg):\\n    \"\"\"Configuration for a random uniform height field terrain.\"\"\"\\n\\n    function = hf_terrains.random_uniform_terrain\\n\\n    noise_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height noise (i.e. along z) of the terrain (in m).\"\"\"\\n    noise_step: float = MISSING\\n    \"\"\"The minimum height (in m) change between two points.\"\"\"\\n    downsampled_scale: float | None = None\\n    \"\"\"The distance between two randomly sampled points on the terrain. Defaults to None,\\n    in which case the :obj:`horizontal scale` is used.\\n\\n    The heights are sampled at this resolution and interpolation is performed for intermediate points.\\n    This must be larger than or equal to the :obj:`horizontal scale`.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class HfPyramidSlopedTerrainCfg(HfTerrainBaseCfg):\\n    \"\"\"Configuration for a pyramid sloped height field terrain.\"\"\"\\n\\n    function = hf_terrains.pyramid_sloped_terrain\\n\\n    slope_range: tuple[float, float] = MISSING\\n    \"\"\"The slope of the terrain (in radians).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"\\n    inverted: bool = False\\n    \"\"\"Whether the pyramid is inverted. Defaults to False.\\n\\n    If True, the terrain is inverted such that the platform is at the bottom and the slopes are upwards.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class HfInvertedPyramidSlopedTerrainCfg(HfPyramidSlopedTerrainCfg):\\n    \"\"\"Configuration for an inverted pyramid sloped height field terrain.\\n\\n    Note:\\n        This is a subclass of :class:`HfPyramidSlopedTerrainCfg` with :obj:`inverted` set to True.\\n        We make it as a separate class to make it easier to distinguish between the two and match\\n        the naming convention of the other terrains.\\n    \"\"\"\\n\\n    inverted: bool = True'),\n",
       " Document(metadata={}, page_content='class HfPyramidStairsTerrainCfg(HfTerrainBaseCfg):\\n    \"\"\"Configuration for a pyramid stairs height field terrain.\"\"\"\\n\\n    function = hf_terrains.pyramid_stairs_terrain\\n\\n    step_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the steps (in m).\"\"\"\\n    step_width: float = MISSING\\n    \"\"\"The width of the steps (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"\\n    inverted: bool = False\\n    \"\"\"Whether the pyramid stairs is inverted. Defaults to False.\\n\\n    If True, the terrain is inverted such that the platform is at the bottom and the stairs are upwards.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class HfInvertedPyramidStairsTerrainCfg(HfPyramidStairsTerrainCfg):\\n    \"\"\"Configuration for an inverted pyramid stairs height field terrain.\\n\\n    Note:\\n        This is a subclass of :class:`HfPyramidStairsTerrainCfg` with :obj:`inverted` set to True.\\n        We make it as a separate class to make it easier to distinguish between the two and match\\n        the naming convention of the other terrains.\\n    \"\"\"\\n\\n    inverted: bool = True'),\n",
       " Document(metadata={}, page_content='class HfDiscreteObstaclesTerrainCfg(HfTerrainBaseCfg):\\n    \"\"\"Configuration for a discrete obstacles height field terrain.\"\"\"\\n\\n    function = hf_terrains.discrete_obstacles_terrain\\n\\n    obstacle_height_mode: str = \"choice\"\\n    \"\"\"The mode to use for the obstacle height. Defaults to \"choice\".\\n\\n    The following modes are supported: \"choice\", \"fixed\".\\n    \"\"\"\\n    obstacle_width_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum width of the obstacles (in m).\"\"\"\\n    obstacle_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the obstacles (in m).\"\"\"\\n    num_obstacles: int = MISSING\\n    \"\"\"The number of obstacles to generate.\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class HfWaveTerrainCfg(HfTerrainBaseCfg):\\n    \"\"\"Configuration for a wave height field terrain.\"\"\"\\n\\n    function = hf_terrains.wave_terrain\\n\\n    amplitude_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum amplitude of the wave (in m).\"\"\"\\n    num_waves: int = 1.0\\n    \"\"\"The number of waves to generate. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class HfSteppingStonesTerrainCfg(HfTerrainBaseCfg):\\n    \"\"\"Configuration for a stepping stones height field terrain.\"\"\"\\n\\n    function = hf_terrains.stepping_stones_terrain\\n\\n    stone_height_max: float = MISSING\\n    \"\"\"The maximum height of the stones (in m).\"\"\"\\n    stone_width_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum width of the stones (in m).\"\"\"\\n    stone_distance_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum distance between stones (in m).\"\"\"\\n    holes_depth: float = -10.0\\n    \"\"\"The depth of the holes (negative obstacles). Defaults to -10.0.\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='def height_field_to_mesh(func: Callable) -> Callable:\\n    \"\"\"Decorator to convert a height field function to a mesh function.\\n\\n    This decorator converts a height field function to a mesh function by sampling the heights\\n    at a specified resolution and performing interpolation to obtain the intermediate heights.\\n    Additionally, it adds a border around the terrain to avoid artifacts at the edges.\\n\\n    Args:\\n        func: The height field function to convert. The function should return a 2D numpy array\\n            with the heights of the terrain.\\n\\n    Returns:\\n        The mesh function. The mesh function returns a tuple containing a list of ``trimesh``\\n        mesh objects and the origin of the terrain.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapper(difficulty: float, cfg: HfTerrainBaseCfg):\\n        # check valid border width\\n        if cfg.border_width > 0 and cfg.border_width < cfg.horizontal_scale:\\n            raise ValueError(\\n                f\"The border width ({cfg.border_width}) must be greater than or equal to the\"\\n                f\" horizontal scale ({cfg.horizontal_scale}).\"\\n            )\\n        # allocate buffer for height field (with border)\\n        width_pixels = int(cfg.size[0] / cfg.horizontal_scale) + 1\\n        length_pixels = int(cfg.size[1] / cfg.horizontal_scale) + 1\\n        border_pixels = int(cfg.border_width / cfg.horizontal_scale) + 1\\n        heights = np.zeros((width_pixels, length_pixels), dtype=np.int16)\\n        # override size of the terrain to account for the border\\n        sub_terrain_size = [width_pixels - 2 * border_pixels, length_pixels - 2 * border_pixels]\\n        sub_terrain_size = [dim * cfg.horizontal_scale for dim in sub_terrain_size]\\n        # update the config\\n        terrain_size = copy.deepcopy(cfg.size)\\n        cfg.size = tuple(sub_terrain_size)\\n        # generate the height field\\n        z_gen = func(difficulty, cfg)\\n        # handle the border for the terrain\\n        heights[border_pixels:-border_pixels, border_pixels:-border_pixels] = z_gen\\n        # set terrain size back to config\\n        cfg.size = terrain_size\\n\\n        # convert to trimesh\\n        vertices, triangles = convert_height_field_to_mesh(\\n            heights, cfg.horizontal_scale, cfg.vertical_scale, cfg.slope_threshold\\n        )\\n        mesh = trimesh.Trimesh(vertices=vertices, faces=triangles)\\n        # compute origin\\n        x1 = int((cfg.size[0] * 0.5 - 1) / cfg.horizontal_scale)\\n        x2 = int((cfg.size[0] * 0.5 + 1) / cfg.horizontal_scale)\\n        y1 = int((cfg.size[1] * 0.5 - 1) / cfg.horizontal_scale)\\n        y2 = int((cfg.size[1] * 0.5 + 1) / cfg.horizontal_scale)\\n        origin_z = np.max(heights[x1:x2, y1:y2]) * cfg.vertical_scale\\n        origin = np.array([0.5 * cfg.size[0], 0.5 * cfg.size[1], origin_z])\\n        # return mesh and origin\\n        return [mesh], origin\\n\\n    return wrapper'),\n",
       " Document(metadata={}, page_content='def convert_height_field_to_mesh(\\n    height_field: np.ndarray, horizontal_scale: float, vertical_scale: float, slope_threshold: float | None = None\\n) -> tuple[np.ndarray, np.ndarray]:\\n    \"\"\"Convert a height-field array to a triangle mesh represented by vertices and triangles.\\n\\n    This function converts a height-field array to a triangle mesh represented by vertices and triangles.\\n    The height-field array is assumed to be a 2D array of floats, where each element represents the height\\n    of the terrain at that location. The height-field array is assumed to be in the form of a matrix, where\\n    the first dimension represents the x-axis and the second dimension represents the y-axis.\\n\\n    The function can also correct vertical surfaces above the provide slope threshold. This is helpful to\\n    avoid having long vertical surfaces in the mesh. The correction is done by moving the vertices of the\\n    vertical surfaces to minimum of the two neighboring vertices.\\n\\n    The correction is done in the following way:\\n    If :math:`\\\\\\\\frac{y_2 - y_1}{x_2 - x_1} > threshold`, then move A to A\\' (i.e., set :math:`x_1\\' = x_2`).\\n    This is repeated along all directions.\\n\\n    .. code-block:: none\\n\\n                B(x_2,y_2)\\n                    /|\\n                   / |\\n                  /  |\\n        (x_1,y_1)A---A\\'(x_1\\',y_1)\\n\\n    Args:\\n        height_field: The input height-field array.\\n        horizontal_scale: The discretization of the terrain along the x and y axis.\\n        vertical_scale: The discretization of the terrain along the z axis.\\n        slope_threshold: The slope threshold above which surfaces are made vertical.\\n            Defaults to None, in which case no correction is applied.\\n\\n    Returns:\\n        The vertices and triangles of the mesh:\\n        - **vertices** (np.ndarray(float)): Array of shape (num_vertices, 3).\\n          Each row represents the location of each vertex (in m).\\n        - **triangles** (np.ndarray(int)): Array of shape (num_triangles, 3).\\n          Each row represents the indices of the 3 vertices connected by this triangle.\\n    \"\"\"\\n    # read height field\\n    num_rows, num_cols = height_field.shape\\n    # create a mesh grid of the height field\\n    y = np.linspace(0, (num_cols - 1) * horizontal_scale, num_cols)\\n    x = np.linspace(0, (num_rows - 1) * horizontal_scale, num_rows)\\n    yy, xx = np.meshgrid(y, x)\\n    # copy height field to avoid modifying the original array\\n    hf = height_field.copy()\\n\\n    # correct vertical surfaces above the slope threshold\\n    if slope_threshold is not None:\\n        # scale slope threshold based on the horizontal and vertical scale\\n        slope_threshold *= horizontal_scale / vertical_scale\\n        # allocate arrays to store the movement of the vertices\\n        move_x = np.zeros((num_rows, num_cols))\\n        move_y = np.zeros((num_rows, num_cols))\\n        move_corners = np.zeros((num_rows, num_cols))\\n        # move vertices along the x-axis\\n        move_x[: num_rows - 1, :] += hf[1:num_rows, :] - hf[: num_rows - 1, :] > slope_threshold\\n        move_x[1:num_rows, :] -= hf[: num_rows - 1, :] - hf[1:num_rows, :] > slope_threshold\\n        # move vertices along the y-axis\\n        move_y[:, : num_cols - 1] += hf[:, 1:num_cols] - hf[:, : num_cols - 1] > slope_threshold\\n        move_y[:, 1:num_cols] -= hf[:, : num_cols - 1] - hf[:, 1:num_cols] > slope_threshold\\n        # move vertices along the corners\\n        move_corners[: num_rows - 1, : num_cols - 1] += (\\n            hf[1:num_rows, 1:num_cols] - hf[: num_rows - 1, : num_cols - 1] > slope_threshold\\n        )\\n        move_corners[1:num_rows, 1:num_cols] -= (\\n            hf[: num_rows - 1, : num_cols - 1] - hf[1:num_rows, 1:num_cols] > slope_threshold\\n        )\\n        xx += (move_x + move_corners * (move_x == 0)) * horizontal_scale\\n        yy += (move_y + move_corners * (move_y == 0)) * horizontal_scale\\n\\n    # create vertices for the mesh\\n    vertices = np.zeros((num_rows * num_cols, 3), dtype=np.float32)\\n    vertices[:, 0] = xx.flatten()\\n    vertices[:, 1] = yy.flatten()\\n    vertices[:, 2] = hf.flatten() * vertical_scale\\n    # create triangles for the mesh\\n    triangles = -np.ones((2 * (num_rows - 1) * (num_cols - 1), 3), dtype=np.uint32)\\n    for i in range(num_rows - 1):\\n        ind0 = np.arange(0, num_cols - 1) + i * num_cols\\n        ind1 = ind0 + 1\\n        ind2 = ind0 + num_cols\\n        ind3 = ind2 + 1\\n        start = 2 * i * (num_cols - 1)\\n        stop = start + 2 * (num_cols - 1)\\n        triangles[start:stop:2, 0] = ind0\\n        triangles[start:stop:2, 1] = ind3\\n        triangles[start:stop:2, 2] = ind1\\n        triangles[start + 1 : stop : 2, 0] = ind0\\n        triangles[start + 1 : stop : 2, 1] = ind2\\n        triangles[start + 1 : stop : 2, 2] = ind3\\n\\n    return vertices, triangles'),\n",
       " Document(metadata={}, page_content='def flat_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshPlaneTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a flat terrain as a plane.\\n\\n    .. image:: ../../_static/terrains/trimesh/flat_terrain.jpg\\n       :width: 45%\\n       :align: center\\n\\n    Note:\\n        The :obj:`difficulty` parameter is ignored for this terrain.\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # compute the position of the terrain\\n    origin = (cfg.size[0] / 2.0, cfg.size[1] / 2.0, 0.0)\\n    # compute the vertices of the terrain\\n    plane_mesh = make_plane(cfg.size, 0.0, center_zero=False)\\n    # return the tri-mesh and the position\\n    return [plane_mesh], np.array(origin)'),\n",
       " Document(metadata={}, page_content='def pyramid_stairs_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshPyramidStairsTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with a pyramid stair pattern.\\n\\n    The terrain is a pyramid stair pattern which trims to a flat platform at the center of the terrain.\\n\\n    If :obj:`cfg.holes` is True, the terrain will have pyramid stairs of length or width\\n    :obj:`cfg.platform_width` (depending on the direction) with no steps in the remaining area. Additionally,\\n    no border will be added.\\n\\n    .. image:: ../../_static/terrains/trimesh/pyramid_stairs_terrain.jpg\\n       :width: 45%\\n\\n    .. image:: ../../_static/terrains/trimesh/pyramid_stairs_terrain_with_holes.jpg\\n       :width: 45%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # resolve the terrain configuration\\n    step_height = cfg.step_height_range[0] + difficulty * (cfg.step_height_range[1] - cfg.step_height_range[0])\\n\\n    # compute number of steps in x and y direction\\n    num_steps_x = (cfg.size[0] - 2 * cfg.border_width - cfg.platform_width) // (2 * cfg.step_width) + 1\\n    num_steps_y = (cfg.size[1] - 2 * cfg.border_width - cfg.platform_width) // (2 * cfg.step_width) + 1\\n    # we take the minimum number of steps in x and y direction\\n    num_steps = int(min(num_steps_x, num_steps_y))\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n\\n    # generate the border if needed\\n    if cfg.border_width > 0.0 and not cfg.holes:\\n        # obtain a list of meshes for the border\\n        border_center = [0.5 * cfg.size[0], 0.5 * cfg.size[1], -step_height / 2]\\n        border_inner_size = (cfg.size[0] - 2 * cfg.border_width, cfg.size[1] - 2 * cfg.border_width)\\n        make_borders = make_border(cfg.size, border_inner_size, step_height, border_center)\\n        # add the border meshes to the list of meshes\\n        meshes_list += make_borders\\n\\n    # generate the terrain\\n    # -- compute the position of the center of the terrain\\n    terrain_center = [0.5 * cfg.size[0], 0.5 * cfg.size[1], 0.0]\\n    terrain_size = (cfg.size[0] - 2 * cfg.border_width, cfg.size[1] - 2 * cfg.border_width)\\n    # -- generate the stair pattern\\n    for k in range(num_steps):\\n        # check if we need to add holes around the steps\\n        if cfg.holes:\\n            box_size = (cfg.platform_width, cfg.platform_width)\\n        else:\\n            box_size = (terrain_size[0] - 2 * k * cfg.step_width, terrain_size[1] - 2 * k * cfg.step_width)\\n        # compute the quantities of the box\\n        # -- location\\n        box_z = terrain_center[2] + k * step_height / 2.0\\n        box_offset = (k + 0.5) * cfg.step_width\\n        # -- dimensions\\n        box_height = (k + 2) * step_height\\n        # generate the boxes\\n        # top/bottom\\n        box_dims = (box_size[0], cfg.step_width, box_height)\\n        # -- top\\n        box_pos = (terrain_center[0], terrain_center[1] + terrain_size[1] / 2.0 - box_offset, box_z)\\n        box_top = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # -- bottom\\n        box_pos = (terrain_center[0], terrain_center[1] - terrain_size[1] / 2.0 + box_offset, box_z)\\n        box_bottom = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # right/left\\n        if cfg.holes:\\n            box_dims = (cfg.step_width, box_size[1], box_height)\\n        else:\\n            box_dims = (cfg.step_width, box_size[1] - 2 * cfg.step_width, box_height)\\n        # -- right\\n        box_pos = (terrain_center[0] + terrain_size[0] / 2.0 - box_offset, terrain_center[1], box_z)\\n        box_right = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # -- left\\n        box_pos = (terrain_center[0] - terrain_size[0] / 2.0 + box_offset, terrain_center[1], box_z)\\n        box_left = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # add the boxes to the list of meshes\\n        meshes_list += [box_top, box_bottom, box_right, box_left]\\n\\n    # generate final box for the middle of the terrain\\n    box_dims = (\\n        terrain_size[0] - 2 * num_steps * cfg.step_width,\\n        terrain_size[1] - 2 * num_steps * cfg.step_width,\\n        (num_steps + 2) * step_height,\\n    )\\n    box_pos = (terrain_center[0], terrain_center[1], terrain_center[2] + num_steps * step_height / 2)\\n    box_middle = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n    meshes_list.append(box_middle)\\n    # origin of the terrain\\n    origin = np.array([terrain_center[0], terrain_center[1], (num_steps + 1) * step_height])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def inverted_pyramid_stairs_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshInvertedPyramidStairsTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with a inverted pyramid stair pattern.\\n\\n    The terrain is an inverted pyramid stair pattern which trims to a flat platform at the center of the terrain.\\n\\n    If :obj:`cfg.holes` is True, the terrain will have pyramid stairs of length or width\\n    :obj:`cfg.platform_width` (depending on the direction) with no steps in the remaining area. Additionally,\\n    no border will be added.\\n\\n    .. image:: ../../_static/terrains/trimesh/inverted_pyramid_stairs_terrain.jpg\\n       :width: 45%\\n\\n    .. image:: ../../_static/terrains/trimesh/inverted_pyramid_stairs_terrain_with_holes.jpg\\n       :width: 45%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # resolve the terrain configuration\\n    step_height = cfg.step_height_range[0] + difficulty * (cfg.step_height_range[1] - cfg.step_height_range[0])\\n\\n    # compute number of steps in x and y direction\\n    num_steps_x = (cfg.size[0] - 2 * cfg.border_width - cfg.platform_width) // (2 * cfg.step_width) + 1\\n    num_steps_y = (cfg.size[1] - 2 * cfg.border_width - cfg.platform_width) // (2 * cfg.step_width) + 1\\n    # we take the minimum number of steps in x and y direction\\n    num_steps = int(min(num_steps_x, num_steps_y))\\n    # total height of the terrain\\n    total_height = (num_steps + 1) * step_height\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n\\n    # generate the border if needed\\n    if cfg.border_width > 0.0 and not cfg.holes:\\n        # obtain a list of meshes for the border\\n        border_center = [0.5 * cfg.size[0], 0.5 * cfg.size[1], -0.5 * step_height]\\n        border_inner_size = (cfg.size[0] - 2 * cfg.border_width, cfg.size[1] - 2 * cfg.border_width)\\n        make_borders = make_border(cfg.size, border_inner_size, step_height, border_center)\\n        # add the border meshes to the list of meshes\\n        meshes_list += make_borders\\n    # generate the terrain\\n    # -- compute the position of the center of the terrain\\n    terrain_center = [0.5 * cfg.size[0], 0.5 * cfg.size[1], 0.0]\\n    terrain_size = (cfg.size[0] - 2 * cfg.border_width, cfg.size[1] - 2 * cfg.border_width)\\n    # -- generate the stair pattern\\n    for k in range(num_steps):\\n        # check if we need to add holes around the steps\\n        if cfg.holes:\\n            box_size = (cfg.platform_width, cfg.platform_width)\\n        else:\\n            box_size = (terrain_size[0] - 2 * k * cfg.step_width, terrain_size[1] - 2 * k * cfg.step_width)\\n        # compute the quantities of the box\\n        # -- location\\n        box_z = terrain_center[2] - total_height / 2 - (k + 1) * step_height / 2.0\\n        box_offset = (k + 0.5) * cfg.step_width\\n        # -- dimensions\\n        box_height = total_height - (k + 1) * step_height\\n        # generate the boxes\\n        # top/bottom\\n        box_dims = (box_size[0], cfg.step_width, box_height)\\n        # -- top\\n        box_pos = (terrain_center[0], terrain_center[1] + terrain_size[1] / 2.0 - box_offset, box_z)\\n        box_top = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # -- bottom\\n        box_pos = (terrain_center[0], terrain_center[1] - terrain_size[1] / 2.0 + box_offset, box_z)\\n        box_bottom = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # right/left\\n        if cfg.holes:\\n            box_dims = (cfg.step_width, box_size[1], box_height)\\n        else:\\n            box_dims = (cfg.step_width, box_size[1] - 2 * cfg.step_width, box_height)\\n        # -- right\\n        box_pos = (terrain_center[0] + terrain_size[0] / 2.0 - box_offset, terrain_center[1], box_z)\\n        box_right = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # -- left\\n        box_pos = (terrain_center[0] - terrain_size[0] / 2.0 + box_offset, terrain_center[1], box_z)\\n        box_left = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n        # add the boxes to the list of meshes\\n        meshes_list += [box_top, box_bottom, box_right, box_left]\\n    # generate final box for the middle of the terrain\\n    box_dims = (\\n        terrain_size[0] - 2 * num_steps * cfg.step_width,\\n        terrain_size[1] - 2 * num_steps * cfg.step_width,\\n        step_height,\\n    )\\n    box_pos = (terrain_center[0], terrain_center[1], terrain_center[2] - total_height - step_height / 2)\\n    box_middle = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n    meshes_list.append(box_middle)\\n    # origin of the terrain\\n    origin = np.array([terrain_center[0], terrain_center[1], -(num_steps + 1) * step_height])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def random_grid_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshRandomGridTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with cells of random heights and fixed width.\\n\\n    The terrain is generated in the x-y plane and has a height of 1.0. It is then divided into a grid of the\\n    specified size :obj:`cfg.grid_width`. Each grid cell is then randomly shifted in the z-direction by a value uniformly\\n    sampled between :obj:`cfg.grid_height_range`. At the center of the terrain, a platform of the specified width\\n    :obj:`cfg.platform_width` is generated.\\n\\n    If :obj:`cfg.holes` is True, the terrain will have randomized grid cells only along the plane extending\\n    from the platform (like a plus sign). The remaining area remains empty and no border will be added.\\n\\n    .. image:: ../../_static/terrains/trimesh/random_grid_terrain.jpg\\n       :width: 45%\\n\\n    .. image:: ../../_static/terrains/trimesh/random_grid_terrain_with_holes.jpg\\n       :width: 45%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n\\n    Raises:\\n        ValueError: If the terrain is not square. This method only supports square terrains.\\n        RuntimeError: If the grid width is large such that the border width is negative.\\n    \"\"\"\\n    # check to ensure square terrain\\n    if cfg.size[0] != cfg.size[1]:\\n        raise ValueError(f\"The terrain must be square. Received size: {cfg.size}.\")\\n    # resolve the terrain configuration\\n    grid_height = cfg.grid_height_range[0] + difficulty * (cfg.grid_height_range[1] - cfg.grid_height_range[0])\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # compute the number of boxes in each direction\\n    num_boxes_x = int(cfg.size[0] / cfg.grid_width)\\n    num_boxes_y = int(cfg.size[1] / cfg.grid_width)\\n    # constant parameters\\n    terrain_height = 1.0\\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n\\n    # generate the border\\n    border_width = cfg.size[0] - min(num_boxes_x, num_boxes_y) * cfg.grid_width\\n    if border_width > 0:\\n        # compute parameters for the border\\n        border_center = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -terrain_height / 2)\\n        border_inner_size = (cfg.size[0] - border_width, cfg.size[1] - border_width)\\n        # create border meshes\\n        make_borders = make_border(cfg.size, border_inner_size, terrain_height, border_center)\\n        meshes_list += make_borders\\n    else:\\n        raise RuntimeError(\"Border width must be greater than 0! Adjust the parameter \\'cfg.grid_width\\'.\")\\n\\n    # create a template grid of terrain height\\n    grid_dim = [cfg.grid_width, cfg.grid_width, terrain_height]\\n    grid_position = [0.5 * cfg.grid_width, 0.5 * cfg.grid_width, -terrain_height / 2]\\n    template_box = trimesh.creation.box(grid_dim, trimesh.transformations.translation_matrix(grid_position))\\n    # extract vertices and faces of the box to create a template\\n    template_vertices = template_box.vertices  # (8, 3)\\n    template_faces = template_box.faces\\n\\n    # repeat the template box vertices to span the terrain (num_boxes_x * num_boxes_y, 8, 3)\\n    vertices = torch.tensor(template_vertices, device=device).repeat(num_boxes_x * num_boxes_y, 1, 1)\\n    # create a meshgrid to offset the vertices\\n    x = torch.arange(0, num_boxes_x, device=device)\\n    y = torch.arange(0, num_boxes_y, device=device)\\n    xx, yy = torch.meshgrid(x, y, indexing=\"ij\")\\n    xx = xx.flatten().view(-1, 1)\\n    yy = yy.flatten().view(-1, 1)\\n    xx_yy = torch.cat((xx, yy), dim=1)\\n    # offset the vertices\\n    offsets = cfg.grid_width * xx_yy + border_width / 2\\n    vertices[:, :, :2] += offsets.unsqueeze(1)\\n    # mask the vertices to create holes, s.t. only grids along the x and y axis are present\\n    if cfg.holes:\\n        # -- x-axis\\n        mask_x = torch.logical_and(\\n            (vertices[:, :, 0] > (cfg.size[0] - border_width - cfg.platform_width) / 2).all(dim=1),\\n            (vertices[:, :, 0] < (cfg.size[0] + border_width + cfg.platform_width) / 2).all(dim=1),\\n        )\\n        vertices_x = vertices[mask_x]\\n        # -- y-axis\\n        mask_y = torch.logical_and(\\n            (vertices[:, :, 1] > (cfg.size[1] - border_width - cfg.platform_width) / 2).all(dim=1),\\n            (vertices[:, :, 1] < (cfg.size[1] + border_width + cfg.platform_width) / 2).all(dim=1),\\n        )\\n        vertices_y = vertices[mask_y]\\n        # -- combine these vertices\\n        vertices = torch.cat((vertices_x, vertices_y))\\n    # add noise to the vertices to have a random height over each grid cell\\n    num_boxes = len(vertices)\\n    # create noise for the z-axis\\n    h_noise = torch.zeros((num_boxes, 3), device=device)\\n    h_noise[:, 2].uniform_(-grid_height, grid_height)\\n    # reshape noise to match the vertices (num_boxes, 4, 3)\\n    # only the top vertices of the box are affected\\n    vertices_noise = torch.zeros((num_boxes, 4, 3), device=device)\\n    vertices_noise += h_noise.unsqueeze(1)\\n    # add height only to the top vertices of the box\\n    vertices[vertices[:, :, 2] == 0] += vertices_noise.view(-1, 3)\\n    # move to numpy\\n    vertices = vertices.reshape(-1, 3).cpu().numpy()\\n\\n    # create faces for boxes (num_boxes, 12, 3). Each box has 6 faces, each face has 2 triangles.\\n    faces = torch.tensor(template_faces, device=device).repeat(num_boxes, 1, 1)\\n    face_offsets = torch.arange(0, num_boxes, device=device).unsqueeze(1).repeat(1, 12) * 8\\n    faces += face_offsets.unsqueeze(2)\\n    # move to numpy\\n    faces = faces.view(-1, 3).cpu().numpy()\\n    # convert to trimesh\\n    grid_mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\\n    meshes_list.append(grid_mesh)\\n\\n    # add a platform in the center of the terrain that is accessible from all sides\\n    dim = (cfg.platform_width, cfg.platform_width, terrain_height + grid_height)\\n    pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -terrain_height / 2 + grid_height / 2)\\n    box_platform = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n    meshes_list.append(box_platform)\\n\\n    # specify the origin of the terrain\\n    origin = np.array([0.5 * cfg.size[0], 0.5 * cfg.size[1], grid_height])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def rails_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshRailsTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with box rails as extrusions.\\n\\n    The terrain contains two sets of box rails created as extrusions. The first set  (inner rails) is extruded from\\n    the platform at the center of the terrain, and the second set is extruded between the first set of rails\\n    and the terrain border. Each set of rails is extruded to the same height.\\n\\n    .. image:: ../../_static/terrains/trimesh/rails_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. this is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # resolve the terrain configuration\\n    rail_height = cfg.rail_height_range[1] - difficulty * (cfg.rail_height_range[1] - cfg.rail_height_range[0])\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # extract quantities\\n    rail_1_thickness, rail_2_thickness = cfg.rail_thickness_range\\n    rail_center = (0.5 * cfg.size[0], 0.5 * cfg.size[1], rail_height * 0.5)\\n    # constants for terrain generation\\n    terrain_height = 1.0\\n    rail_2_ratio = 0.6\\n\\n    # generate first set of rails\\n    rail_1_inner_size = (cfg.platform_width, cfg.platform_width)\\n    rail_1_outer_size = (cfg.platform_width + 2.0 * rail_1_thickness, cfg.platform_width + 2.0 * rail_1_thickness)\\n    meshes_list += make_border(rail_1_outer_size, rail_1_inner_size, rail_height, rail_center)\\n    # generate second set of rails\\n    rail_2_inner_x = cfg.platform_width + (cfg.size[0] - cfg.platform_width) * rail_2_ratio\\n    rail_2_inner_y = cfg.platform_width + (cfg.size[1] - cfg.platform_width) * rail_2_ratio\\n    rail_2_inner_size = (rail_2_inner_x, rail_2_inner_y)\\n    rail_2_outer_size = (rail_2_inner_x + 2.0 * rail_2_thickness, rail_2_inner_y + 2.0 * rail_2_thickness)\\n    meshes_list += make_border(rail_2_outer_size, rail_2_inner_size, rail_height, rail_center)\\n    # generate the ground\\n    dim = (cfg.size[0], cfg.size[1], terrain_height)\\n    pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -terrain_height / 2)\\n    ground_meshes = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n    meshes_list.append(ground_meshes)\\n\\n    # specify the origin of the terrain\\n    origin = np.array([pos[0], pos[1], 0.0])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def pit_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshPitTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with a pit with levels (stairs) leading out of the pit.\\n\\n    The terrain contains a platform at the center and a staircase leading out of the pit.\\n    The staircase is a series of steps that are aligned along the x- and y- axis. The steps are\\n    created by extruding a ring along the x- and y- axis. If :obj:`is_double_pit` is True, the pit\\n    contains two levels.\\n\\n    .. image:: ../../_static/terrains/trimesh/pit_terrain.jpg\\n       :width: 40%\\n\\n    .. image:: ../../_static/terrains/trimesh/pit_terrain_with_two_levels.jpg\\n       :width: 40%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # resolve the terrain configuration\\n    pit_depth = cfg.pit_depth_range[0] + difficulty * (cfg.pit_depth_range[1] - cfg.pit_depth_range[0])\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # extract quantities\\n    inner_pit_size = (cfg.platform_width, cfg.platform_width)\\n    total_depth = pit_depth\\n    # constants for terrain generation\\n    terrain_height = 1.0\\n    ring_2_ratio = 0.6\\n\\n    # if the pit is double, the inner ring is smaller to fit the second level\\n    if cfg.double_pit:\\n        # increase the total height of the pit\\n        total_depth *= 2.0\\n        # reduce the size of the inner ring\\n        inner_pit_x = cfg.platform_width + (cfg.size[0] - cfg.platform_width) * ring_2_ratio\\n        inner_pit_y = cfg.platform_width + (cfg.size[1] - cfg.platform_width) * ring_2_ratio\\n        inner_pit_size = (inner_pit_x, inner_pit_y)\\n\\n    # generate the pit (outer ring)\\n    pit_center = [0.5 * cfg.size[0], 0.5 * cfg.size[1], -total_depth * 0.5]\\n    meshes_list += make_border(cfg.size, inner_pit_size, total_depth, pit_center)\\n    # generate the second level of the pit (inner ring)\\n    if cfg.double_pit:\\n        pit_center[2] = -total_depth\\n        meshes_list += make_border(inner_pit_size, (cfg.platform_width, cfg.platform_width), total_depth, pit_center)\\n    # generate the ground\\n    dim = (cfg.size[0], cfg.size[1], terrain_height)\\n    pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -total_depth - terrain_height / 2)\\n    ground_meshes = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n    meshes_list.append(ground_meshes)\\n\\n    # specify the origin of the terrain\\n    origin = np.array([pos[0], pos[1], -total_depth])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def box_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshBoxTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with boxes (similar to a pyramid).\\n\\n    The terrain has a ground with boxes on top of it that are stacked on top of each other.\\n    The boxes are created by extruding a rectangle along the z-axis. If :obj:`double_box` is True,\\n    then two boxes of height :obj:`box_height` are stacked on top of each other.\\n\\n    .. image:: ../../_static/terrains/trimesh/box_terrain.jpg\\n       :width: 40%\\n\\n    .. image:: ../../_static/terrains/trimesh/box_terrain_with_two_boxes.jpg\\n       :width: 40%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # resolve the terrain configuration\\n    box_height = cfg.box_height_range[0] + difficulty * (cfg.box_height_range[1] - cfg.box_height_range[0])\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # extract quantities\\n    total_height = box_height\\n    if cfg.double_box:\\n        total_height *= 2.0\\n    # constants for terrain generation\\n    terrain_height = 1.0\\n    box_2_ratio = 0.6\\n\\n    # Generate the top box\\n    dim = (cfg.platform_width, cfg.platform_width, terrain_height + total_height)\\n    pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], (total_height - terrain_height) / 2)\\n    box_mesh = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n    meshes_list.append(box_mesh)\\n    # Generate the lower box\\n    if cfg.double_box:\\n        # calculate the size of the lower box\\n        outer_box_x = cfg.platform_width + (cfg.size[0] - cfg.platform_width) * box_2_ratio\\n        outer_box_y = cfg.platform_width + (cfg.size[1] - cfg.platform_width) * box_2_ratio\\n        # create the lower box\\n        dim = (outer_box_x, outer_box_y, terrain_height + total_height / 2)\\n        pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], (total_height - terrain_height) / 2 - total_height / 4)\\n        box_mesh = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n        meshes_list.append(box_mesh)\\n    # Generate the ground\\n    pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -terrain_height / 2)\\n    dim = (cfg.size[0], cfg.size[1], terrain_height)\\n    ground_mesh = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n    meshes_list.append(ground_mesh)\\n\\n    # specify the origin of the terrain\\n    origin = np.array([pos[0], pos[1], total_height])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def gap_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshGapTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with a gap around the platform.\\n\\n    The terrain has a ground with a platform in the middle. The platform is surrounded by a gap\\n    of width :obj:`gap_width` on all sides.\\n\\n    .. image:: ../../_static/terrains/trimesh/gap_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # resolve the terrain configuration\\n    gap_width = cfg.gap_width_range[0] + difficulty * (cfg.gap_width_range[1] - cfg.gap_width_range[0])\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # constants for terrain generation\\n    terrain_height = 1.0\\n    terrain_center = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -terrain_height / 2)\\n\\n    # Generate the outer ring\\n    inner_size = (cfg.platform_width + 2 * gap_width, cfg.platform_width + 2 * gap_width)\\n    meshes_list += make_border(cfg.size, inner_size, terrain_height, terrain_center)\\n    # Generate the inner box\\n    box_dim = (cfg.platform_width, cfg.platform_width, terrain_height)\\n    box = trimesh.creation.box(box_dim, trimesh.transformations.translation_matrix(terrain_center))\\n    meshes_list.append(box)\\n\\n    # specify the origin of the terrain\\n    origin = np.array([terrain_center[0], terrain_center[1], 0.0])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def floating_ring_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshFloatingRingTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with a floating square ring.\\n\\n    The terrain has a ground with a floating ring in the middle. The ring extends from the center from\\n    :obj:`platform_width` to :obj:`platform_width` + :obj:`ring_width` in the x and y directions.\\n    The thickness of the ring is :obj:`ring_thickness` and the height of the ring from the terrain\\n    is :obj:`ring_height`.\\n\\n    .. image:: ../../_static/terrains/trimesh/floating_ring_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n    \"\"\"\\n    # resolve the terrain configuration\\n    ring_height = cfg.ring_height_range[1] - difficulty * (cfg.ring_height_range[1] - cfg.ring_height_range[0])\\n    ring_width = cfg.ring_width_range[0] + difficulty * (cfg.ring_width_range[1] - cfg.ring_width_range[0])\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # constants for terrain generation\\n    terrain_height = 1.0\\n\\n    # Generate the floating ring\\n    ring_center = (0.5 * cfg.size[0], 0.5 * cfg.size[1], ring_height + 0.5 * cfg.ring_thickness)\\n    ring_outer_size = (cfg.platform_width + 2 * ring_width, cfg.platform_width + 2 * ring_width)\\n    ring_inner_size = (cfg.platform_width, cfg.platform_width)\\n    meshes_list += make_border(ring_outer_size, ring_inner_size, cfg.ring_thickness, ring_center)\\n    # Generate the ground\\n    dim = (cfg.size[0], cfg.size[1], terrain_height)\\n    pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -terrain_height / 2)\\n    ground = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n    meshes_list.append(ground)\\n\\n    # specify the origin of the terrain\\n    origin = np.asarray([pos[0], pos[1], 0.0])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def star_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshStarTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with a star.\\n\\n    The terrain has a ground with a cylinder in the middle. The star is made of :obj:`num_bars` bars\\n    with a width of :obj:`bar_width` and a height of :obj:`bar_height`. The bars are evenly\\n    spaced around the cylinder and connect to the peripheral of the terrain.\\n\\n    .. image:: ../../_static/terrains/trimesh/star_terrain.jpg\\n       :width: 40%\\n       :align: center\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n\\n    Raises:\\n        ValueError: If :obj:`num_bars` is less than 2.\\n    \"\"\"\\n    # check the number of bars\\n    if cfg.num_bars < 2:\\n        raise ValueError(f\"The number of bars in the star must be greater than 2. Received: {cfg.num_bars}\")\\n\\n    # resolve the terrain configuration\\n    bar_height = cfg.bar_height_range[0] + difficulty * (cfg.bar_height_range[1] - cfg.bar_height_range[0])\\n    bar_width = cfg.bar_width_range[1] - difficulty * (cfg.bar_width_range[1] - cfg.bar_width_range[0])\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # Generate a platform in the middle\\n    platform_center = (0.5 * cfg.size[0], 0.5 * cfg.size[1], -bar_height / 2)\\n    platform_transform = trimesh.transformations.translation_matrix(platform_center)\\n    platform = trimesh.creation.cylinder(\\n        cfg.platform_width * 0.5, bar_height, sections=2 * cfg.num_bars, transform=platform_transform\\n    )\\n    meshes_list.append(platform)\\n    # Generate bars to connect the platform to the terrain\\n    transform = np.eye(4)\\n    transform[:3, -1] = np.asarray(platform_center)\\n    yaw = 0.0\\n    for _ in range(cfg.num_bars):\\n        # compute the length of the bar based on the yaw\\n        # length changes since the bar is connected to a square border\\n        bar_length = cfg.size[0]\\n        if yaw < 0.25 * np.pi:\\n            bar_length /= np.math.cos(yaw)\\n        elif yaw < 0.75 * np.pi:\\n            bar_length /= np.math.sin(yaw)\\n        else:\\n            bar_length /= np.math.cos(np.pi - yaw)\\n        # compute the transform of the bar\\n        transform[0:3, 0:3] = tf.Rotation.from_euler(\"z\", yaw).as_matrix()\\n        # add the bar to the mesh\\n        dim = [bar_length - bar_width, bar_width, bar_height]\\n        bar = trimesh.creation.box(dim, transform)\\n        meshes_list.append(bar)\\n        # increment the yaw\\n        yaw += np.pi / cfg.num_bars\\n    # Generate the exterior border\\n    inner_size = (cfg.size[0] - 2 * bar_width, cfg.size[1] - 2 * bar_width)\\n    meshes_list += make_border(cfg.size, inner_size, bar_height, platform_center)\\n    # Generate the ground\\n    ground = make_plane(cfg.size, -bar_height, center_zero=False)\\n    meshes_list.append(ground)\\n    # specify the origin of the terrain\\n    origin = np.asarray([0.5 * cfg.size[0], 0.5 * cfg.size[1], 0.0])\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='def repeated_objects_terrain(\\n    difficulty: float, cfg: mesh_terrains_cfg.MeshRepeatedObjectsTerrainCfg\\n) -> tuple[list[trimesh.Trimesh], np.ndarray]:\\n    \"\"\"Generate a terrain with a set of repeated objects.\\n\\n    The terrain has a ground with a platform in the middle. The objects are randomly placed on the\\n    terrain s.t. they do not overlap with the platform.\\n\\n    Depending on the object type, the objects are generated with different parameters. The objects\\n    The types of objects that can be generated are: ``\"cylinder\"``, ``\"box\"``, ``\"cone\"``.\\n\\n    The object parameters are specified in the configuration as curriculum parameters. The difficulty\\n    is used to linearly interpolate between the minimum and maximum values of the parameters.\\n\\n    .. image:: ../../_static/terrains/trimesh/repeated_objects_cylinder_terrain.jpg\\n       :width: 30%\\n\\n    .. image:: ../../_static/terrains/trimesh/repeated_objects_box_terrain.jpg\\n       :width: 30%\\n\\n    .. image:: ../../_static/terrains/trimesh/repeated_objects_pyramid_terrain.jpg\\n       :width: 30%\\n\\n    Args:\\n        difficulty: The difficulty of the terrain. This is a value between 0 and 1.\\n        cfg: The configuration for the terrain.\\n\\n    Returns:\\n        A tuple containing the tri-mesh of the terrain and the origin of the terrain (in m).\\n\\n    Raises:\\n        ValueError: If the object type is not supported. It must be either a string or a callable.\\n    \"\"\"\\n    # import the object functions -- this is done here to avoid circular imports\\n    from .mesh_terrains_cfg import (\\n        MeshRepeatedBoxesTerrainCfg,\\n        MeshRepeatedCylindersTerrainCfg,\\n        MeshRepeatedPyramidsTerrainCfg,\\n    )\\n\\n    # if object type is a string, get the function: make_{object_type}\\n    if isinstance(cfg.object_type, str):\\n        object_func = globals().get(f\"make_{cfg.object_type}\")\\n    else:\\n        object_func = cfg.object_type\\n    if not callable(object_func):\\n        raise ValueError(f\"The attribute \\'object_type\\' must be a string or a callable. Received: {object_func}\")\\n\\n    # Resolve the terrain configuration\\n    # -- pass parameters to make calling simpler\\n    cp_0 = cfg.object_params_start\\n    cp_1 = cfg.object_params_end\\n    # -- common parameters\\n    num_objects = cp_0.num_objects + int(difficulty * (cp_1.num_objects - cp_0.num_objects))\\n    height = cp_0.height + difficulty * (cp_1.height - cp_0.height)\\n    # -- object specific parameters\\n    # note: SIM114 requires duplicated logical blocks under a single body.\\n    if isinstance(cfg, MeshRepeatedBoxesTerrainCfg):\\n        cp_0: MeshRepeatedBoxesTerrainCfg.ObjectCfg\\n        cp_1: MeshRepeatedBoxesTerrainCfg.ObjectCfg\\n        object_kwargs = {\\n            \"length\": cp_0.size[0] + difficulty * (cp_1.size[0] - cp_0.size[0]),\\n            \"width\": cp_0.size[1] + difficulty * (cp_1.size[1] - cp_0.size[1]),\\n            \"max_yx_angle\": cp_0.max_yx_angle + difficulty * (cp_1.max_yx_angle - cp_0.max_yx_angle),\\n            \"degrees\": cp_0.degrees,\\n        }\\n    elif isinstance(cfg, MeshRepeatedPyramidsTerrainCfg):  # noqa: SIM114\\n        cp_0: MeshRepeatedPyramidsTerrainCfg.ObjectCfg\\n        cp_1: MeshRepeatedPyramidsTerrainCfg.ObjectCfg\\n        object_kwargs = {\\n            \"radius\": cp_0.radius + difficulty * (cp_1.radius - cp_0.radius),\\n            \"max_yx_angle\": cp_0.max_yx_angle + difficulty * (cp_1.max_yx_angle - cp_0.max_yx_angle),\\n            \"degrees\": cp_0.degrees,\\n        }\\n    elif isinstance(cfg, MeshRepeatedCylindersTerrainCfg):  # noqa: SIM114\\n        cp_0: MeshRepeatedCylindersTerrainCfg.ObjectCfg\\n        cp_1: MeshRepeatedCylindersTerrainCfg.ObjectCfg\\n        object_kwargs = {\\n            \"radius\": cp_0.radius + difficulty * (cp_1.radius - cp_0.radius),\\n            \"max_yx_angle\": cp_0.max_yx_angle + difficulty * (cp_1.max_yx_angle - cp_0.max_yx_angle),\\n            \"degrees\": cp_0.degrees,\\n        }\\n    else:\\n        raise ValueError(f\"Unknown terrain configuration: {cfg}\")\\n    # constants for the terrain\\n    platform_clearance = 0.1\\n\\n    # initialize list of meshes\\n    meshes_list = list()\\n    # compute quantities\\n    origin = np.asarray((0.5 * cfg.size[0], 0.5 * cfg.size[1], 0.5 * height))\\n    platform_corners = np.asarray([\\n        [origin[0] - cfg.platform_width / 2, origin[1] - cfg.platform_width / 2],\\n        [origin[0] + cfg.platform_width / 2, origin[1] + cfg.platform_width / 2],\\n    ])\\n    platform_corners[0, :] *= 1 - platform_clearance\\n    platform_corners[1, :] *= 1 + platform_clearance\\n    # sample valid center for objects\\n    object_centers = np.zeros((num_objects, 3))\\n    # use a mask to track invalid objects that still require sampling\\n    mask_objects_left = np.ones((num_objects,), dtype=bool)\\n    # loop until no objects are left to sample\\n    while np.any(mask_objects_left):\\n        # only sample the centers of the remaining invalid objects\\n        num_objects_left = mask_objects_left.sum()\\n        object_centers[mask_objects_left, 0] = np.random.uniform(0, cfg.size[0], num_objects_left)\\n        object_centers[mask_objects_left, 1] = np.random.uniform(0, cfg.size[1], num_objects_left)\\n        # filter out the centers that are on the platform\\n        is_within_platform_x = np.logical_and(\\n            object_centers[mask_objects_left, 0] >= platform_corners[0, 0],\\n            object_centers[mask_objects_left, 0] <= platform_corners[1, 0],\\n        )\\n        is_within_platform_y = np.logical_and(\\n            object_centers[mask_objects_left, 1] >= platform_corners[0, 1],\\n            object_centers[mask_objects_left, 1] <= platform_corners[1, 1],\\n        )\\n        # update the mask to track the validity of the objects sampled in this iteration\\n        mask_objects_left[mask_objects_left] = np.logical_and(is_within_platform_x, is_within_platform_y)\\n\\n    # generate obstacles (but keep platform clean)\\n    for index in range(len(object_centers)):\\n        # randomize the height of the object\\n        ob_height = height + np.random.uniform(-cfg.max_height_noise, cfg.max_height_noise)\\n        if ob_height > 0.0:\\n            object_mesh = object_func(center=object_centers[index], height=ob_height, **object_kwargs)\\n            meshes_list.append(object_mesh)\\n\\n    # generate a ground plane for the terrain\\n    ground_plane = make_plane(cfg.size, height=0.0, center_zero=False)\\n    meshes_list.append(ground_plane)\\n    # generate a platform in the middle\\n    dim = (cfg.platform_width, cfg.platform_width, 0.5 * height)\\n    pos = (0.5 * cfg.size[0], 0.5 * cfg.size[1], 0.25 * height)\\n    platform = trimesh.creation.box(dim, trimesh.transformations.translation_matrix(pos))\\n    meshes_list.append(platform)\\n\\n    return meshes_list, origin'),\n",
       " Document(metadata={}, page_content='class MeshPlaneTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a plane mesh terrain.\"\"\"\\n\\n    function = mesh_terrains.flat_terrain'),\n",
       " Document(metadata={}, page_content='class MeshPyramidStairsTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a pyramid stair mesh terrain.\"\"\"\\n\\n    function = mesh_terrains.pyramid_stairs_terrain\\n\\n    border_width: float = 0.0\\n    \"\"\"The width of the border around the terrain (in m). Defaults to 0.0.\\n\\n    The border is a flat terrain with the same height as the terrain.\\n    \"\"\"\\n    step_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the steps (in m).\"\"\"\\n    step_width: float = MISSING\\n    \"\"\"The width of the steps (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"\\n    holes: bool = False\\n    \"\"\"If True, the terrain will have holes in the steps. Defaults to False.\\n\\n    If :obj:`holes` is True, the terrain will have pyramid stairs of length or width\\n    :obj:`platform_width` (depending on the direction) with no steps in the remaining area. Additionally,\\n    no border will be added.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshInvertedPyramidStairsTerrainCfg(MeshPyramidStairsTerrainCfg):\\n    \"\"\"Configuration for an inverted pyramid stair mesh terrain.\\n\\n    Note:\\n        This is the same as :class:`MeshPyramidStairsTerrainCfg` except that the steps are inverted.\\n    \"\"\"\\n\\n    function = mesh_terrains.inverted_pyramid_stairs_terrain'),\n",
       " Document(metadata={}, page_content='class MeshRandomGridTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a random grid mesh terrain.\"\"\"\\n\\n    function = mesh_terrains.random_grid_terrain\\n\\n    grid_width: float = MISSING\\n    \"\"\"The width of the grid cells (in m).\"\"\"\\n    grid_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the grid cells (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"\\n    holes: bool = False\\n    \"\"\"If True, the terrain will have holes in the steps. Defaults to False.\\n\\n    If :obj:`holes` is True, the terrain will have randomized grid cells only along the plane extending\\n    from the platform (like a plus sign). The remaining area remains empty and no border will be added.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshRailsTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a terrain with box rails as extrusions.\"\"\"\\n\\n    function = mesh_terrains.rails_terrain\\n\\n    rail_thickness_range: tuple[float, float] = MISSING\\n    \"\"\"The thickness of the inner and outer rails (in m).\"\"\"\\n    rail_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the rails (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshPitTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a terrain with a pit that leads out of the pit.\"\"\"\\n\\n    function = mesh_terrains.pit_terrain\\n\\n    pit_depth_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the pit (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"\\n    double_pit: bool = False\\n    \"\"\"If True, the pit contains two levels of stairs. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshBoxTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a terrain with boxes (similar to a pyramid).\"\"\"\\n\\n    function = mesh_terrains.box_terrain\\n\\n    box_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the box (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"\\n    double_box: bool = False\\n    \"\"\"If True, the pit contains two levels of stairs/boxes. Defaults to False.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshGapTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a terrain with a gap around the platform.\"\"\"\\n\\n    function = mesh_terrains.gap_terrain\\n\\n    gap_width_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum width of the gap (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshFloatingRingTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a terrain with a floating ring around the center.\"\"\"\\n\\n    function = mesh_terrains.floating_ring_terrain\\n\\n    ring_width_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum width of the ring (in m).\"\"\"\\n    ring_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the ring (in m).\"\"\"\\n    ring_thickness: float = MISSING\\n    \"\"\"The thickness (along z) of the ring (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the square platform at the center of the terrain. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshStarTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Configuration for a terrain with a star pattern.\"\"\"\\n\\n    function = mesh_terrains.star_terrain\\n\\n    num_bars: int = MISSING\\n    \"\"\"The number of bars per-side the star. Must be greater than 2.\"\"\"\\n    bar_width_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum width of the bars in the star (in m).\"\"\"\\n    bar_height_range: tuple[float, float] = MISSING\\n    \"\"\"The minimum and maximum height of the bars in the star (in m).\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the cylindrical platform at the center of the terrain. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshRepeatedObjectsTerrainCfg(SubTerrainBaseCfg):\\n    \"\"\"Base configuration for a terrain with repeated objects.\"\"\"\\n\\n    @configclass\\n    class ObjectCfg:\\n        \"\"\"Configuration of repeated objects.\"\"\"\\n\\n        num_objects: int = MISSING\\n        \"\"\"The number of objects to add to the terrain.\"\"\"\\n        height: float = MISSING\\n        \"\"\"The height (along z) of the object (in m).\"\"\"\\n\\n    function = mesh_terrains.repeated_objects_terrain\\n\\n    object_type: Literal[\"cylinder\", \"box\", \"cone\"] | callable = MISSING\\n    \"\"\"The type of object to generate.\\n\\n    The type can be a string or a callable. If it is a string, the function will look for a function called\\n    ``make_{object_type}`` in the current module scope. If it is a callable, the function will\\n    use the callable to generate the object.\\n    \"\"\"\\n    object_params_start: ObjectCfg = MISSING\\n    \"\"\"The object curriculum parameters at the start of the curriculum.\"\"\"\\n    object_params_end: ObjectCfg = MISSING\\n    \"\"\"The object curriculum parameters at the end of the curriculum.\"\"\"\\n\\n    max_height_noise: float = 0.0\\n    \"\"\"The maximum amount of noise to add to the height of the objects (in m). Defaults to 0.0.\"\"\"\\n    platform_width: float = 1.0\\n    \"\"\"The width of the cylindrical platform at the center of the terrain. Defaults to 1.0.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshRepeatedPyramidsTerrainCfg(MeshRepeatedObjectsTerrainCfg):\\n    \"\"\"Configuration for a terrain with repeated pyramids.\"\"\"\\n\\n    @configclass\\n    class ObjectCfg(MeshRepeatedObjectsTerrainCfg.ObjectCfg):\\n        \"\"\"Configuration for a curriculum of repeated pyramids.\"\"\"\\n\\n        radius: float = MISSING\\n        \"\"\"The radius of the pyramids (in m).\"\"\"\\n        max_yx_angle: float = 0.0\\n        \"\"\"The maximum angle along the y and x axis. Defaults to 0.0.\"\"\"\\n        degrees: bool = True\\n        \"\"\"Whether the angle is in degrees. Defaults to True.\"\"\"\\n\\n    object_type = mesh_utils_terrains.make_cone\\n\\n    object_params_start: ObjectCfg = MISSING\\n    \"\"\"The object curriculum parameters at the start of the curriculum.\"\"\"\\n    object_params_end: ObjectCfg = MISSING\\n    \"\"\"The object curriculum parameters at the end of the curriculum.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshRepeatedBoxesTerrainCfg(MeshRepeatedObjectsTerrainCfg):\\n    \"\"\"Configuration for a terrain with repeated boxes.\"\"\"\\n\\n    @configclass\\n    class ObjectCfg(MeshRepeatedObjectsTerrainCfg.ObjectCfg):\\n        \"\"\"Configuration for repeated boxes.\"\"\"\\n\\n        size: tuple[float, float] = MISSING\\n        \"\"\"The width (along x) and length (along y) of the box (in m).\"\"\"\\n        max_yx_angle: float = 0.0\\n        \"\"\"The maximum angle along the y and x axis. Defaults to 0.0.\"\"\"\\n        degrees: bool = True\\n        \"\"\"Whether the angle is in degrees. Defaults to True.\"\"\"\\n\\n    object_type = mesh_utils_terrains.make_box\\n\\n    object_params_start: ObjectCfg = MISSING\\n    \"\"\"The box curriculum parameters at the start of the curriculum.\"\"\"\\n    object_params_end: ObjectCfg = MISSING\\n    \"\"\"The box curriculum parameters at the end of the curriculum.\"\"\"'),\n",
       " Document(metadata={}, page_content='class MeshRepeatedCylindersTerrainCfg(MeshRepeatedObjectsTerrainCfg):\\n    \"\"\"Configuration for a terrain with repeated cylinders.\"\"\"\\n\\n    @configclass\\n    class ObjectCfg(MeshRepeatedObjectsTerrainCfg.ObjectCfg):\\n        \"\"\"Configuration for repeated cylinder.\"\"\"\\n\\n        radius: float = MISSING\\n        \"\"\"The radius of the pyramids (in m).\"\"\"\\n        max_yx_angle: float = 0.0\\n        \"\"\"The maximum angle along the y and x axis. Defaults to 0.0.\"\"\"\\n        degrees: bool = True\\n        \"\"\"Whether the angle is in degrees. Defaults to True.\"\"\"\\n\\n    object_type = mesh_utils_terrains.make_cylinder\\n\\n    object_params_start: ObjectCfg = MISSING\\n    \"\"\"The box curriculum parameters at the start of the curriculum.\"\"\"\\n    object_params_end: ObjectCfg = MISSING\\n    \"\"\"The box curriculum parameters at the end of the curriculum.\"\"\"'),\n",
       " Document(metadata={}, page_content='def make_plane(size: tuple[float, float], height: float, center_zero: bool = True) -> trimesh.Trimesh:\\n    \"\"\"Generate a plane mesh.\\n\\n    If :obj:`center_zero` is True, the origin is at center of the plane mesh i.e. the mesh extends from\\n    :math:`(-size[0] / 2, -size[1] / 2, 0)` to :math:`(size[0] / 2, size[1] / 2, height)`.\\n    Otherwise, the origin is :math:`(size[0] / 2, size[1] / 2)` and the mesh extends from\\n    :math:`(0, 0, 0)` to :math:`(size[0], size[1], height)`.\\n\\n    Args:\\n        size: The length (along x) and width (along y) of the terrain (in m).\\n        height: The height of the plane (in m).\\n        center_zero: Whether the 2D origin of the plane is set to the center of mesh.\\n            Defaults to True.\\n\\n    Returns:\\n        A trimesh.Trimesh objects for the plane.\\n    \"\"\"\\n    # compute the vertices of the terrain\\n    x0 = [size[0], size[1], height]\\n    x1 = [size[0], 0.0, height]\\n    x2 = [0.0, size[1], height]\\n    x3 = [0.0, 0.0, height]\\n    # generate the tri-mesh with two triangles\\n    vertices = np.array([x0, x1, x2, x3])\\n    faces = np.array([[1, 0, 2], [2, 3, 1]])\\n    plane_mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\\n    # center the plane at the origin\\n    if center_zero:\\n        plane_mesh.apply_translation(-np.array([size[0] / 2.0, size[1] / 2.0, 0.0]))\\n    # return the tri-mesh and the position\\n    return plane_mesh'),\n",
       " Document(metadata={}, page_content='def make_border(\\n    size: tuple[float, float], inner_size: tuple[float, float], height: float, position: tuple[float, float, float]\\n) -> list[trimesh.Trimesh]:\\n    \"\"\"Generate meshes for a rectangular border with a hole in the middle.\\n\\n    .. code:: text\\n\\n        +---------------------+\\n        |#####################|\\n        |##+---------------+##|\\n        |##|               |##|\\n        |##|               |##| length\\n        |##|               |##| (y-axis)\\n        |##|               |##|\\n        |##+---------------+##|\\n        |#####################|\\n        +---------------------+\\n              width (x-axis)\\n\\n    Args:\\n        size: The length (along x) and width (along y) of the terrain (in m).\\n        inner_size: The inner length (along x) and width (along y) of the hole (in m).\\n        height: The height of the border (in m).\\n        position: The center of the border (in m).\\n\\n    Returns:\\n        A list of trimesh.Trimesh objects that represent the border.\\n    \"\"\"\\n    # compute thickness of the border\\n    thickness_x = (size[0] - inner_size[0]) / 2.0\\n    thickness_y = (size[1] - inner_size[1]) / 2.0\\n    # generate tri-meshes for the border\\n    # top/bottom border\\n    box_dims = (size[0], thickness_y, height)\\n    # -- top\\n    box_pos = (position[0], position[1] + inner_size[1] / 2.0 + thickness_y / 2.0, position[2])\\n    box_mesh_top = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n    # -- bottom\\n    box_pos = (position[0], position[1] - inner_size[1] / 2.0 - thickness_y / 2.0, position[2])\\n    box_mesh_bottom = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n    # left/right border\\n    box_dims = (thickness_x, inner_size[1], height)\\n    # -- left\\n    box_pos = (position[0] - inner_size[0] / 2.0 - thickness_x / 2.0, position[1], position[2])\\n    box_mesh_left = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n    # -- right\\n    box_pos = (position[0] + inner_size[0] / 2.0 + thickness_x / 2.0, position[1], position[2])\\n    box_mesh_right = trimesh.creation.box(box_dims, trimesh.transformations.translation_matrix(box_pos))\\n    # return the tri-meshes\\n    return [box_mesh_left, box_mesh_right, box_mesh_top, box_mesh_bottom]'),\n",
       " Document(metadata={}, page_content='def make_box(\\n    length: float,\\n    width: float,\\n    height: float,\\n    center: tuple[float, float, float],\\n    max_yx_angle: float = 0,\\n    degrees: bool = True,\\n) -> trimesh.Trimesh:\\n    \"\"\"Generate a box mesh with a random orientation.\\n\\n    Args:\\n        length: The length (along x) of the box (in m).\\n        width: The width (along y) of the box (in m).\\n        height: The height of the cylinder (in m).\\n        center: The center of the cylinder (in m).\\n        max_yx_angle: The maximum angle along the y and x axis. Defaults to 0.\\n        degrees: Whether the angle is in degrees. Defaults to True.\\n\\n    Returns:\\n        A trimesh.Trimesh object for the cylinder.\\n    \"\"\"\\n    # create a pose for the cylinder\\n    transform = np.eye(4)\\n    transform[0:3, -1] = np.asarray(center)\\n    # -- create a random rotation\\n    euler_zyx = tf.Rotation.random().as_euler(\"zyx\")  # returns rotation of shape (3,)\\n    # -- cap the rotation along the y and x axis\\n    if degrees:\\n        max_yx_angle = max_yx_angle / 180.0\\n    euler_zyx[1:] *= max_yx_angle\\n    # -- apply the rotation\\n    transform[0:3, 0:3] = tf.Rotation.from_euler(\"zyx\", euler_zyx).as_matrix()\\n    # create the box\\n    dims = (length, width, height)\\n    return trimesh.creation.box(dims, transform=transform)'),\n",
       " Document(metadata={}, page_content='def make_cylinder(\\n    radius: float, height: float, center: tuple[float, float, float], max_yx_angle: float = 0, degrees: bool = True\\n) -> trimesh.Trimesh:\\n    \"\"\"Generate a cylinder mesh with a random orientation.\\n\\n    Args:\\n        radius: The radius of the cylinder (in m).\\n        height: The height of the cylinder (in m).\\n        center: The center of the cylinder (in m).\\n        max_yx_angle: The maximum angle along the y and x axis. Defaults to 0.\\n        degrees: Whether the angle is in degrees. Defaults to True.\\n\\n    Returns:\\n        A trimesh.Trimesh object for the cylinder.\\n    \"\"\"\\n    # create a pose for the cylinder\\n    transform = np.eye(4)\\n    transform[0:3, -1] = np.asarray(center)\\n    # -- create a random rotation\\n    euler_zyx = tf.Rotation.random().as_euler(\"zyx\")  # returns rotation of shape (3,)\\n    # -- cap the rotation along the y and x axis\\n    if degrees:\\n        max_yx_angle = max_yx_angle / 180.0\\n    euler_zyx[1:] *= max_yx_angle\\n    # -- apply the rotation\\n    transform[0:3, 0:3] = tf.Rotation.from_euler(\"zyx\", euler_zyx).as_matrix()\\n    # create the cylinder\\n    return trimesh.creation.cylinder(radius, height, sections=np.random.randint(4, 6), transform=transform)'),\n",
       " Document(metadata={}, page_content='def make_cone(\\n    radius: float, height: float, center: tuple[float, float, float], max_yx_angle: float = 0, degrees: bool = True\\n) -> trimesh.Trimesh:\\n    \"\"\"Generate a cone mesh with a random orientation.\\n\\n    Args:\\n        radius: The radius of the cone (in m).\\n        height: The height of the cone (in m).\\n        center: The center of the cone (in m).\\n        max_yx_angle: The maximum angle along the y and x axis. Defaults to 0.\\n        degrees: Whether the angle is in degrees. Defaults to True.\\n\\n    Returns:\\n        A trimesh.Trimesh object for the cone.\\n    \"\"\"\\n    # create a pose for the cylinder\\n    transform = np.eye(4)\\n    transform[0:3, -1] = np.asarray(center)\\n    # -- create a random rotation\\n    euler_zyx = tf.Rotation.random().as_euler(\"zyx\")  # returns rotation of shape (3,)\\n    # -- cap the rotation along the y and x axis\\n    if degrees:\\n        max_yx_angle = max_yx_angle / 180.0\\n    euler_zyx[1:] *= max_yx_angle\\n    # -- apply the rotation\\n    transform[0:3, 0:3] = tf.Rotation.from_euler(\"zyx\", euler_zyx).as_matrix()\\n    # create the cone\\n    return trimesh.creation.cone(radius, height, sections=np.random.randint(4, 6), transform=transform)'),\n",
       " Document(metadata={}, page_content='class ImagePlot(UIWidgetWrapper):\\n    \"\"\"An image plot widget to display live data.\\n\\n    It has the following Layout where the mode frame is only useful for depth images:\\n    +-------------------------------------------------------+\\n    |                  containing_frame                     |\\n    |+-----------------------------------------------------+|\\n    |                   main_plot_frame                     |\\n    ||+---------------------------------------------------+||\\n    |||                    plot_frames                    |||\\n    |||                                                   |||\\n    |||                                                   |||\\n    |||               (Image Plot Data)                   |||\\n    |||                                                   |||\\n    |||                                                   |||\\n    |||+-------------------------------------------------+|||\\n    |||                   mode_frame                      |||\\n    |||                                                   |||\\n    |||    [x][Absolute] [x][Grayscaled] [ ][Colorized]   |||\\n    |+-----------------------------------------------------+|\\n    +-------------------------------------------------------+\\n\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        image: Optional[np.ndarray] = None,\\n        label: str = \"\",\\n        widget_height: int = 200,\\n        show_min_max: bool = True,\\n        unit: tuple[float, str] = (1, \"\"),\\n    ):\\n        \"\"\"Create an XY plot UI Widget with axis scaling, legends, and support for multiple plots.\\n\\n        Overlapping data is most accurately plotted when centered in the frame with reasonable axis scaling.\\n        Pressing down the mouse gives the x and y values of each function at an x coordinate.\\n\\n        Args:\\n            image: Image to display\\n            label: Short descriptive text to the left of the plot\\n            widget_height: Height of the plot in pixels\\n            show_min_max: Whether to show the min and max values of the image\\n            unit: Tuple of (scale, name) for the unit of the image\\n        \"\"\"\\n        self._show_min_max = show_min_max\\n        self._unit_scale = unit[0]\\n        self._unit_name = unit[1]\\n\\n        self._curr_mode = \"None\"\\n\\n        self._has_built = False\\n\\n        self._enabled = True\\n\\n        self._byte_provider = omni.ui.ByteImageProvider()\\n        if image is None:\\n            carb.log_warn(\"image is NONE\")\\n            image = np.ones((480, 640, 3), dtype=np.uint8) * 255\\n            image[:, :, 0] = 0\\n            image[:, :240, 1] = 0\\n\\n        # if image is channel first, convert to channel last\\n        if image.ndim == 3 and image.shape[0] in [1, 3, 4]:\\n            image = np.moveaxis(image, 0, -1)\\n\\n        self._aspect_ratio = image.shape[1] / image.shape[0]\\n        self._widget_height = widget_height\\n        self._label = label\\n        self.update_image(image)\\n\\n        plot_frame = self._create_ui_widget()\\n\\n        super().__init__(plot_frame)\\n\\n    def setEnabled(self, enabled: bool):\\n        self._enabled = enabled\\n\\n    def update_image(self, image: np.ndarray):\\n        if not self._enabled:\\n            return\\n\\n        # if image is channel first, convert to channel last\\n        if image.ndim == 3 and image.shape[0] in [1, 3, 4]:\\n            image = np.moveaxis(image, 0, -1)\\n\\n        height, width = image.shape[:2]\\n\\n        if self._curr_mode == \"Normalization\":\\n            image = (image - image.min()) / (image.max() - image.min())\\n            image = (image * 255).astype(np.uint8)\\n        elif self._curr_mode == \"Colorization\":\\n            if image.ndim == 3 and image.shape[2] == 3:\\n                omni.log.warn(\"Colorization mode is only available for single channel images\")\\n            else:\\n                image = (image - image.min()) / (image.max() - image.min())\\n                colormap = cm.get_cmap(\"jet\")\\n                if image.ndim == 3 and image.shape[2] == 1:\\n                    image = (colormap(image).squeeze(2) * 255).astype(np.uint8)\\n                else:\\n                    image = (colormap(image) * 255).astype(np.uint8)\\n\\n        # convert image to 4-channel RGBA\\n        if image.ndim == 2 or (image.ndim == 3 and image.shape[2] == 1):\\n            image = np.dstack((image, image, image, np.full((height, width, 1), 255, dtype=np.uint8)))\\n\\n        elif image.ndim == 3 and image.shape[2] == 3:\\n            image = np.dstack((image, np.full((height, width, 1), 255, dtype=np.uint8)))\\n\\n        self._byte_provider.set_bytes_data(image.flatten().data, [width, height])\\n\\n    def update_min_max(self, image: np.ndarray):\\n        if self._show_min_max and hasattr(self, \"_min_max_label\"):\\n            non_inf = image[np.isfinite(image)].flatten()\\n            if len(non_inf) > 0:\\n                self._min_max_label.text = self._get_unit_description(\\n                    np.min(non_inf), np.max(non_inf), np.median(non_inf)\\n                )\\n            else:\\n                self._min_max_label.text = self._get_unit_description(0, 0)\\n\\n    def _create_ui_widget(self):\\n        containing_frame = omni.ui.Frame(build_fn=self._build_widget)\\n        return containing_frame\\n\\n    def _get_unit_description(self, min_value: float, max_value: float, median_value: float = None):\\n        return (\\n            f\"Min: {min_value * self._unit_scale:.2f} {self._unit_name} Max:\"\\n            f\" {max_value * self._unit_scale:.2f} {self._unit_name}\"\\n            + (f\" Median: {median_value * self._unit_scale:.2f} {self._unit_name}\" if median_value is not None else \"\")\\n        )\\n\\n    def _build_widget(self):\\n\\n        with omni.ui.VStack(spacing=3):\\n            with omni.ui.HStack():\\n                # Write the leftmost label for what this plot is\\n                omni.ui.Label(\\n                    self._label,\\n                    width=isaacsim.gui.components.ui_utils.LABEL_WIDTH,\\n                    alignment=omni.ui.Alignment.LEFT_TOP,\\n                )\\n                with omni.ui.Frame(width=self._aspect_ratio * self._widget_height, height=self._widget_height):\\n                    self._base_plot = omni.ui.ImageWithProvider(self._byte_provider)\\n\\n            if self._show_min_max:\\n                self._min_max_label = omni.ui.Label(self._get_unit_description(0, 0))\\n\\n            omni.ui.Spacer(height=8)\\n            self._mode_frame = omni.ui.Frame(build_fn=self._build_mode_frame)\\n\\n            omni.ui.Spacer(width=5)\\n        self._has_built = True\\n\\n    def _build_mode_frame(self):\\n        \"\"\"Build the frame containing the mode selection for the plots.\\n\\n        This is an internal function to build the frame containing the mode selection for the plots. This function\\n        should only be called from within the build function of a frame.\\n\\n        The built widget has the following layout:\\n        +-------------------------------------------------------+\\n        |                   legends_frame                       |\\n        ||+---------------------------------------------------+||\\n        |||                                                   |||\\n        |||    [x][Series 1] [x][Series 2] [ ][Series 3]      |||\\n        |||                                                   |||\\n        |||+-------------------------------------------------+|||\\n        |+-----------------------------------------------------+|\\n        +-------------------------------------------------------+\\n        \"\"\"\\n        with omni.ui.HStack():\\n            with omni.ui.HStack():\\n\\n                def _change_mode(value):\\n                    self._curr_mode = value\\n\\n                isaacsim.gui.components.ui_utils.dropdown_builder(\\n                    label=\"Mode\",\\n                    type=\"dropdown\",\\n                    items=[\"Original\", \"Normalization\", \"Colorization\"],\\n                    tooltip=\"Select a mode\",\\n                    on_clicked_fn=_change_mode,\\n                )'),\n",
       " Document(metadata={}, page_content='class LiveLinePlot(UIWidgetWrapper):\\n    \"\"\"A 2D line plot widget to display live data.\\n\\n\\n    This widget is used to display live data in a 2D line plot. It can be used to display multiple series\\n    in the same plot.\\n\\n    It has the following Layout:\\n    +-------------------------------------------------------+\\n    |                  containing_frame                     |\\n    |+-----------------------------------------------------+|\\n    |                   main_plot_frame                     |\\n    ||+---------------------------------------------------+||\\n    |||         plot_frames + grid lines (Z_stacked)      |||\\n    |||                                                   |||\\n    |||                                                   |||\\n    |||               (Live Plot Data)                    |||\\n    |||                                                   |||\\n    |||                                                   |||\\n    |||+-------------------------------------------------+|||\\n    |||                   legends_frame                   |||\\n    |||                                                   |||\\n    |||    [x][Series 1] [x][Series 2] [ ][Series 3]      |||\\n    |||+-------------------------------------------------+|||\\n    |||                   limits_frame                    |||\\n    |||                                                   |||\\n    |||        [Y-Limits] [min] [max] [Autoscale]         |||\\n    |||+-------------------------------------------------+|||\\n    |||                   filter_frame                    |||\\n    |||                                                   |||\\n    |||                                                   |||\\n    |+-----------------------------------------------------+|\\n    +-------------------------------------------------------+\\n\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        y_data: list[list[float]],\\n        y_min: float = -10,\\n        y_max: float = 10,\\n        plot_height: int = 150,\\n        show_legend: bool = True,\\n        legends: list[str] | None = None,\\n        max_datapoints: int = 200,\\n    ):\\n        \"\"\"Create a new LiveLinePlot widget.\\n\\n        Args:\\n            y_data: A list of lists of floats containing the data to plot. Each list of floats represents a series in the plot.\\n            y_min: The minimum y value to display. Defaults to -10.\\n            y_max: The maximum y value to display. Defaults to 10.\\n            plot_height: The height of the plot in pixels. Defaults to 150.\\n            show_legend: Whether to display the legend. Defaults to True.\\n            legends: A list of strings containing the legend labels for each series. If None, the default labels are \"Series_0\", \"Series_1\", etc. Defaults to None.\\n            max_datapoints: The maximum number of data points to display. If the number of data points exceeds this value, the oldest data points are removed. Defaults to 200.\\n        \"\"\"\\n        super().__init__(self._create_ui_widget())\\n        self.plot_height = plot_height\\n        self.show_legend = show_legend\\n        self._legends = legends if legends is not None else [\"Series_\" + str(i) for i in range(len(y_data))]\\n        self._y_data = y_data\\n        self._colors = self._get_distinct_hex_colors(len(y_data))\\n        self._y_min = y_min if y_min is not None else -10\\n        self._y_max = y_max if y_max is not None else 10\\n        self._max_data_points = max_datapoints\\n        self._show_legend = show_legend\\n        self._series_visible = [True for _ in range(len(y_data))]\\n        self._plot_frames = []\\n        self._plots = []\\n        self._plot_selected_values = []\\n        self._is_built = False\\n        self._filter_frame = None\\n        self._filter_mode = None\\n        self._last_values = None\\n        self._is_paused = False\\n\\n        # Gets populated when widget is built\\n        self._main_plot_frame = None\\n\\n        self._autoscale_model = omni.ui.SimpleBoolModel(True)\\n\\n    \"\"\"Properties\"\"\"\\n\\n    @property\\n    def autoscale_mode(self) -> bool:\\n        return self._autoscale_model.as_bool\\n\\n    @property\\n    def y_data(self) -> list[list[float]]:\\n        \"\"\"The current data in the plot.\"\"\"\\n        return self._y_data\\n\\n    @property\\n    def y_min(self) -> float:\\n        \"\"\"The current minimum y value.\"\"\"\\n        return self._y_min\\n\\n    @property\\n    def y_max(self) -> float:\\n        \"\"\"The current maximum y value.\"\"\"\\n        return self._y_max\\n\\n    @property\\n    def legends(self) -> list[str]:\\n        \"\"\"The current legend labels.\"\"\"\\n        return self._legends\\n\\n    \"\"\" General Functions \"\"\"\\n\\n    def clear(self):\\n        \"\"\"Clears the plot.\"\"\"\\n        self._y_data = [[] for _ in range(len(self._y_data))]\\n        self._last_values = None\\n\\n        for plt in self._plots:\\n            plt.set_data()\\n\\n        # self._container_frame.rebuild()\\n\\n    def add_datapoint(self, y_coords: list[float]):\\n        \"\"\"Add a data point to the plot.\\n\\n        The data point is added to the end of the plot. If the number of data points exceeds the maximum number\\n        of data points, the oldest data point is removed.\\n\\n        ``y_coords`` is assumed to be a list of floats with the same length as the number of series in the plot.\\n\\n        Args:\\n            y_coords: A list of floats containing the y coordinates of the new data points.\\n        \"\"\"\\n\\n        for idx, y_coord in enumerate(y_coords):\\n\\n            if len(self._y_data[idx]) > self._max_data_points:\\n                self._y_data[idx] = self._y_data[idx][1:]\\n\\n            if self._filter_mode == \"Lowpass\":\\n                if self._last_values is not None:\\n                    alpha = 0.8\\n                    y_coord = self._y_data[idx][-1] * alpha + y_coord * (1 - alpha)\\n            elif self._filter_mode == \"Integrate\":\\n                if self._last_values is not None:\\n                    y_coord = self._y_data[idx][-1] + y_coord\\n            elif self._filter_mode == \"Derivative\":\\n                if self._last_values is not None:\\n                    y_coord = (y_coord - self._last_values[idx]) / SimulationContext.instance().get_rendering_dt()\\n\\n            self._y_data[idx].append(float(y_coord))\\n\\n        if self._main_plot_frame is None:\\n            # Widget not built, not visible\\n            return\\n\\n        # Check if the widget has been built, i.e. the plot references have been created.\\n        if not self._is_built or self._is_paused:\\n            return\\n\\n        if len(self._y_data) != len(self._plots):\\n            # Plots gotten out of sync, rebuild the widget\\n            self._main_plot_frame.rebuild()\\n            return\\n\\n        if self.autoscale_mode:\\n            self._rescale_btn_pressed()\\n\\n        for idx, plt in enumerate(self._plots):\\n            plt.set_data(*self._y_data[idx])\\n\\n        self._last_values = y_coords\\n        # Autoscale the y-axis to the current data\\n\\n    \"\"\"\\n        Internal functions for building the UI.\\n    \"\"\"\\n\\n    def _build_stacked_plots(self, grid: bool = True):\\n        \"\"\"Builds multiple plots stacked on top of each other to display multiple series.\\n\\n        This is an internal function to build the plots. It should not be called from outside the class and only\\n        from within the build function of a frame.\\n\\n        The built widget has the following layout:\\n        +-------------------------------------------------------+\\n        |                   main_plot_frame                     |\\n        ||+---------------------------------------------------+||\\n        |||                                                   |||\\n        ||| y_max|*******-------------------*******|          |||\\n        |||      |-------*****-----------**--------|          |||\\n        |||     0|------------**-----***-----------|          |||\\n        |||      |--------------***----------------|          |||\\n        ||| y_min|---------------------------------|          |||\\n        |||                                                   |||\\n        |||+-------------------------------------------------+|||\\n\\n\\n        Args:\\n            grid: Whether to display grid lines. Defaults to True.\\n        \"\"\"\\n\\n        # Reset lists which are populated in the build function\\n        self._plot_frames = []\\n\\n        # Define internal builder function\\n        def _build_single_plot(y_data: list[float], color: int, plot_idx: int):\\n            \"\"\"Build a single plot.\\n\\n            This is an internal function to build a single plot with the given data and color. This function\\n            should only be called from within the build function of a frame.\\n\\n            Args:\\n                y_data: The data to plot.\\n                color: The color of the plot.\\n            \"\"\"\\n            plot = omni.ui.Plot(\\n                omni.ui.Type.LINE,\\n                self._y_min,\\n                self._y_max,\\n                *y_data,\\n                height=self.plot_height,\\n                style={\"color\": color, \"background_color\": 0x0},\\n            )\\n\\n            if len(self._plots) <= plot_idx:\\n                self._plots.append(plot)\\n                self._plot_selected_values.append(omni.ui.SimpleStringModel(\"\"))\\n            else:\\n                self._plots[plot_idx] = plot\\n\\n        # Begin building the widget\\n        with omni.ui.HStack():\\n            # Space to the left to add y-axis labels\\n            omni.ui.Spacer(width=20)\\n\\n            # Built plots for each time series stacked on top of each other\\n            with omni.ui.ZStack():\\n                # Background rectangle\\n                omni.ui.Rectangle(\\n                    height=self.plot_height,\\n                    style={\\n                        \"background_color\": 0x0,\\n                        \"border_color\": omni.ui.color.white,\\n                        \"border_width\": 0.4,\\n                        \"margin\": 0.0,\\n                    },\\n                )\\n\\n                # Draw grid lines and labels\\n                if grid:\\n                    # Calculate the number of grid lines to display\\n                    # Absolute range of the plot\\n                    plot_range = self._y_max - self._y_min\\n                    grid_resolution = 10 ** np.floor(np.log10(0.5 * plot_range))\\n\\n                    plot_range /= grid_resolution\\n\\n                    # Fraction of the plot range occupied by the first and last grid line\\n                    first_space = (self._y_max / grid_resolution) - np.floor(self._y_max / grid_resolution)\\n                    last_space = np.ceil(self._y_min / grid_resolution) - self._y_min / grid_resolution\\n\\n                    # Number of grid lines to display\\n                    n_lines = int(plot_range - first_space - last_space)\\n\\n                    plot_resolution = self.plot_height / plot_range\\n\\n                    with omni.ui.VStack():\\n                        omni.ui.Spacer(height=plot_resolution * first_space)\\n\\n                        # Draw grid lines\\n                        with omni.ui.VGrid(row_height=plot_resolution):\\n                            for grid_line_idx in range(n_lines):\\n                                # Create grid line\\n                                with omni.ui.ZStack():\\n                                    omni.ui.Line(\\n                                        style={\\n                                            \"color\": 0xAA8A8777,\\n                                            \"background_color\": 0x0,\\n                                            \"border_width\": 0.4,\\n                                        },\\n                                        alignment=omni.ui.Alignment.CENTER_TOP,\\n                                        height=0,\\n                                    )\\n                                    with omni.ui.Placer(offset_x=-20):\\n                                        omni.ui.Label(\\n                                            f\"{(self._y_max - first_space * grid_resolution - grid_line_idx * grid_resolution):.3f}\",\\n                                            width=8,\\n                                            height=8,\\n                                            alignment=omni.ui.Alignment.RIGHT_TOP,\\n                                            style={\\n                                                \"color\": 0xFFFFFFFF,\\n                                                \"font_size\": 8,\\n                                            },\\n                                        )\\n\\n                # Create plots for each series\\n                for idx, (data, color) in enumerate(zip(self._y_data, self._colors)):\\n                    plot_frame = omni.ui.Frame(\\n                        build_fn=lambda y_data=data, plot_idx=idx, color=color: _build_single_plot(\\n                            y_data, color, plot_idx\\n                        ),\\n                    )\\n                    plot_frame.visible = self._series_visible[idx]\\n                    self._plot_frames.append(plot_frame)\\n\\n                # Create an invisible frame on top that will give a helpful tooltip\\n                self._tooltip_frame = omni.ui.Plot(\\n                    height=self.plot_height,\\n                    style={\"color\": 0xFFFFFFFF, \"background_color\": 0x0},\\n                )\\n\\n                self._tooltip_frame.set_mouse_pressed_fn(self._mouse_moved_on_plot)\\n\\n                # Create top label for the y-axis\\n                with omni.ui.Placer(offset_x=-20, offset_y=-8):\\n                    omni.ui.Label(\\n                        f\"{self._y_max:.3f}\",\\n                        width=8,\\n                        height=2,\\n                        alignment=omni.ui.Alignment.LEFT_TOP,\\n                        style={\"color\": 0xFFFFFFFF, \"font_size\": 8},\\n                    )\\n\\n                # Create bottom label for the y-axis\\n                with omni.ui.Placer(offset_x=-20, offset_y=self.plot_height):\\n                    omni.ui.Label(\\n                        f\"{self._y_min:.3f}\",\\n                        width=8,\\n                        height=2,\\n                        alignment=omni.ui.Alignment.LEFT_BOTTOM,\\n                        style={\"color\": 0xFFFFFFFF, \"font_size\": 8},\\n                    )\\n\\n    def _mouse_moved_on_plot(self, x, y, *args):\\n        # Show a tooltip with x,y and function values\\n        if len(self._y_data) == 0 or len(self._y_data[0]) == 0:\\n            # There is no data in the plots, so do nothing\\n            return\\n\\n        for idx, plot in enumerate(self._plots):\\n            x_pos = plot.screen_position_x\\n            width = plot.computed_width\\n\\n            location_x = (x - x_pos) / width\\n\\n            data = self._y_data[idx]\\n            n_samples = len(data)\\n            selected_sample = int(location_x * n_samples)\\n            value = data[selected_sample]\\n            # save the value in scientific notation\\n            self._plot_selected_values[idx].set_value(f\"{value:.3f}\")\\n\\n    def _build_legends_frame(self):\\n        \"\"\"Build the frame containing the legend for the plots.\\n\\n        This is an internal function to build the frame containing the legend for the plots. This function\\n        should only be called from within the build function of a frame.\\n\\n        The built widget has the following layout:\\n        +-------------------------------------------------------+\\n        |                   legends_frame                       |\\n        ||+---------------------------------------------------+||\\n        |||                                                   |||\\n        |||    [x][Series 1] [x][Series 2] [ ][Series 3]      |||\\n        |||                                                   |||\\n        |||+-------------------------------------------------+|||\\n        |+-----------------------------------------------------+|\\n        +-------------------------------------------------------+\\n        \"\"\"\\n        if not self._show_legend:\\n            return\\n\\n        with omni.ui.HStack():\\n            omni.ui.Spacer(width=32)\\n\\n            # Find the longest legend to determine the width of the frame\\n            max_legend = max([len(legend) for legend in self._legends])\\n            CHAR_WIDTH = 8\\n            with omni.ui.VGrid(\\n                row_height=isaacsim.gui.components.ui_utils.LABEL_HEIGHT,\\n                column_width=max_legend * CHAR_WIDTH + 6,\\n            ):\\n                for idx in range(len(self._y_data)):\\n                    with omni.ui.HStack():\\n                        model = omni.ui.SimpleBoolModel()\\n                        model.set_value(self._series_visible[idx])\\n                        omni.ui.CheckBox(model=model, tooltip=\"\", width=4)\\n                        model.add_value_changed_fn(lambda val, idx=idx: self._change_plot_visibility(idx, val.as_bool))\\n                        omni.ui.Spacer(width=2)\\n                        with omni.ui.VStack():\\n                            omni.ui.Label(\\n                                self._legends[idx],\\n                                width=max_legend * CHAR_WIDTH,\\n                                alignment=omni.ui.Alignment.LEFT,\\n                                style={\"color\": self._colors[idx], \"font_size\": 12},\\n                            )\\n                            omni.ui.StringField(\\n                                model=self._plot_selected_values[idx],\\n                                width=max_legend * CHAR_WIDTH,\\n                                alignment=omni.ui.Alignment.LEFT,\\n                                style={\"color\": self._colors[idx], \"font_size\": 10},\\n                                read_only=True,\\n                            )\\n\\n    def _build_limits_frame(self):\\n        \"\"\"Build the frame containing the controls for the y-axis limits.\\n\\n        This is an internal function to build the frame containing the controls for the y-axis limits. This function\\n        should only be called from within the build function of a frame.\\n\\n        The built widget has the following layout:\\n        +-------------------------------------------------------+\\n        |                   limits_frame                        |\\n        ||+---------------------------------------------------+||\\n        |||                                                   |||\\n        |||         Limits    [min] [max] [Re-Sacle]          |||\\n        |||         Autoscale[x]                              |||\\n        |||    -------------------------------------------    |||\\n        |||+-------------------------------------------------+|||\\n        \"\"\"\\n        with omni.ui.VStack():\\n            with omni.ui.HStack():\\n                omni.ui.Label(\\n                    \"Limits\",\\n                    width=isaacsim.gui.components.ui_utils.LABEL_WIDTH,\\n                    alignment=omni.ui.Alignment.LEFT_CENTER,\\n                )\\n\\n                self.lower_limit_drag = omni.ui.FloatDrag(name=\"min\", enabled=True, alignment=omni.ui.Alignment.CENTER)\\n                y_min_model = self.lower_limit_drag.model\\n                y_min_model.set_value(self._y_min)\\n                y_min_model.add_value_changed_fn(lambda x: self._set_y_min(x.as_float))\\n                omni.ui.Spacer(width=2)\\n\\n                self.upper_limit_drag = omni.ui.FloatDrag(name=\"max\", enabled=True, alignment=omni.ui.Alignment.CENTER)\\n                y_max_model = self.upper_limit_drag.model\\n                y_max_model.set_value(self._y_max)\\n                y_max_model.add_value_changed_fn(lambda x: self._set_y_max(x.as_float))\\n                omni.ui.Spacer(width=2)\\n\\n                omni.ui.Button(\\n                    \"Re-Scale\",\\n                    width=isaacsim.gui.components.ui_utils.BUTTON_WIDTH,\\n                    clicked_fn=self._rescale_btn_pressed,\\n                    alignment=omni.ui.Alignment.LEFT_CENTER,\\n                    style=isaacsim.gui.components.ui_utils.get_style(),\\n                )\\n\\n                omni.ui.CheckBox(model=self._autoscale_model, tooltip=\"\", width=4)\\n\\n            omni.ui.Line(\\n                style={\"color\": 0x338A8777},\\n                width=omni.ui.Fraction(1),\\n                alignment=omni.ui.Alignment.CENTER,\\n            )\\n\\n    def _build_filter_frame(self):\\n        \"\"\"Build the frame containing the filter controls.\\n\\n        This is an internal function to build the frame containing the filter controls. This function\\n        should only be called from within the build function of a frame.\\n\\n        The built widget has the following layout:\\n        +-------------------------------------------------------+\\n        |                   filter_frame                        |\\n        ||+---------------------------------------------------+||\\n        |||                                                   |||\\n        |||                                                   |||\\n        |||                                                   |||\\n        |||+-------------------------------------------------+|||\\n        |+-----------------------------------------------------+|\\n        +-------------------------------------------------------+\\n        \"\"\"\\n        with omni.ui.VStack():\\n            with omni.ui.HStack():\\n\\n                def _filter_changed(value):\\n                    self.clear()\\n                    self._filter_mode = value\\n\\n                isaacsim.gui.components.ui_utils.dropdown_builder(\\n                    label=\"Filter\",\\n                    type=\"dropdown\",\\n                    items=[\"None\", \"Lowpass\", \"Integrate\", \"Derivative\"],\\n                    tooltip=\"Select a filter\",\\n                    on_clicked_fn=_filter_changed,\\n                )\\n\\n                def _toggle_paused():\\n                    self._is_paused = not self._is_paused\\n\\n                # Button\\n                omni.ui.Button(\\n                    \"Play/Pause\",\\n                    width=isaacsim.gui.components.ui_utils.BUTTON_WIDTH,\\n                    clicked_fn=_toggle_paused,\\n                    alignment=omni.ui.Alignment.LEFT_CENTER,\\n                    style=isaacsim.gui.components.ui_utils.get_style(),\\n                )\\n\\n    def _create_ui_widget(self):\\n        \"\"\"Create the full UI widget.\"\"\"\\n\\n        def _build_widget():\\n            self._is_built = False\\n            with omni.ui.VStack():\\n                self._main_plot_frame = omni.ui.Frame(build_fn=self._build_stacked_plots)\\n                omni.ui.Spacer(height=8)\\n                self._legends_frame = omni.ui.Frame(build_fn=self._build_legends_frame)\\n                omni.ui.Spacer(height=8)\\n                self._limits_frame = omni.ui.Frame(build_fn=self._build_limits_frame)\\n                omni.ui.Spacer(height=8)\\n                self._filter_frame = omni.ui.Frame(build_fn=self._build_filter_frame)\\n            self._is_built = True\\n\\n        containing_frame = omni.ui.Frame(build_fn=_build_widget)\\n\\n        return containing_frame\\n\\n    \"\"\" UI Actions Listener Functions \"\"\"\\n\\n    def _change_plot_visibility(self, idx: int, visible: bool):\\n        \"\"\"Change the visibility of a plot at position idx.\"\"\"\\n        self._series_visible[idx] = visible\\n        self._plot_frames[idx].visible = visible\\n        # self._main_plot_frame.rebuild()\\n\\n    def _set_y_min(self, val: float):\\n        \"\"\"Update the y-axis minimum.\"\"\"\\n        self._y_min = val\\n        self.lower_limit_drag.model.set_value(val)\\n        self._main_plot_frame.rebuild()\\n\\n    def _set_y_max(self, val: float):\\n        \"\"\"Update the y-axis maximum.\"\"\"\\n        self._y_max = val\\n        self.upper_limit_drag.model.set_value(val)\\n        self._main_plot_frame.rebuild()\\n\\n    def _rescale_btn_pressed(self):\\n        \"\"\"Autoscale the y-axis to the current data.\"\"\"\\n        if any(self._series_visible):\\n            y_min = np.round(\\n                min([min(y) for idx, y in enumerate(self._y_data) if self._series_visible[idx]]),\\n                4,\\n            )\\n            y_max = np.round(\\n                max([max(y) for idx, y in enumerate(self._y_data) if self._series_visible[idx]]),\\n                4,\\n            )\\n            if y_min == y_max:\\n                y_max += 1e-4  # Make sure axes don\\'t collapse\\n\\n            self._y_max = y_max\\n            self._y_min = y_min\\n\\n        if hasattr(self, \"lower_limit_drag\") and hasattr(self, \"upper_limit_drag\"):\\n            self.lower_limit_drag.model.set_value(self._y_min)\\n            self.upper_limit_drag.model.set_value(self._y_max)\\n\\n        self._main_plot_frame.rebuild()\\n\\n    \"\"\" Helper Functions \"\"\"\\n\\n    def _get_distinct_hex_colors(self, num_colors) -> list[int]:\\n        \"\"\"\\n        This function returns a list of distinct colors for plotting.\\n\\n        Args:\\n            num_colors (int): the number of colors to generate\\n\\n        Returns:\\n            List[int]: a list of distinct colors in hexadecimal format 0xFFBBGGRR\\n        \"\"\"\\n        # Generate equally spaced colors in HSV space\\n        rgb_colors = [\\n            colorsys.hsv_to_rgb(hue / num_colors, 0.75, 1) for hue in np.linspace(0, num_colors - 1, num_colors)\\n        ]\\n        # Convert to 0-255 RGB values\\n        rgb_colors = [[int(c * 255) for c in rgb] for rgb in rgb_colors]\\n        # Convert to 0xFFBBGGRR format\\n        hex_colors = [0xFF * 16**6 + c[2] * 16**4 + c[1] * 16**2 + c[0] for c in rgb_colors]\\n        return hex_colors'),\n",
       " Document(metadata={}, page_content='class ManagerLiveVisualizerCfg:\\n    \"\"\"Configuration for the :class:`ManagerLiveVisualizer` class.\"\"\"\\n\\n    debug_vis: bool = False\\n    \"\"\"Flag used to set status of the live visualizers on startup. Defaults to False, which means closed.\"\"\"\\n\\n    manager_name: str = MISSING\\n    \"\"\"Manager name that corresponds to the manager of interest in the ManagerBasedEnv and ManagerBasedRLEnv\"\"\"\\n\\n    term_names: list[str] | dict[str, list[str]] | None = None\\n    \"\"\"Specific term names specified in a Manager config that are chosen to be plotted. Defaults to None.\\n\\n    If None all terms will be plotted. For managers that utilize Groups (i.e. ObservationGroup) use a dictionary of\\n    {group_names: [term_names]}.\\n    \"\"\"'),\n",
       " Document(metadata={}, page_content='class ManagerLiveVisualizer(UiVisualizerBase):\\n    \"\"\"A interface object used to transfer data from a manager to a UI widget.\\n\\n    This class handles the creation of UI Widgets for selected terms given a :class:`ManagerLiveVisualizerCfg`.\\n    It iterates through the terms of the manager and creates a visualizer for each term. If the term is a single\\n    variable or a multi-variable signal, it creates a :class:`LiveLinePlot`. If the term is an image (2D or RGB),\\n    it creates an :class:`ImagePlot`. The visualizer can be toggled on and off using the\\n    :attr:`ManagerLiveVisualizerCfg.debug_vis` flag in the configuration.\\n    \"\"\"\\n\\n    def __init__(self, manager: ManagerBase, cfg: ManagerLiveVisualizerCfg = ManagerLiveVisualizerCfg()):\\n        \"\"\"Initialize ManagerLiveVisualizer.\\n\\n        Args:\\n            manager: The manager with terms to be plotted. The manager must have a :meth:`get_active_iterable_terms` method.\\n            cfg: The configuration file used to select desired manager terms to be plotted.\\n        \"\"\"\\n\\n        self._manager = manager\\n        self.debug_vis = cfg.debug_vis\\n        self._env_idx: int = 0\\n        self.cfg = cfg\\n        self._viewer_env_idx = 0\\n        self._vis_frame: omni.ui.Frame\\n        self._vis_window: omni.ui.Window\\n\\n        # evaluate chosen terms if no terms provided use all available.\\n        self.term_names = []\\n\\n        if self.cfg.term_names is not None:\\n            # extract chosen terms\\n            if isinstance(self.cfg.term_names, list):\\n                for term_name in self.cfg.term_names:\\n                    if term_name in self._manager.active_terms:\\n                        self.term_names.append(term_name)\\n                    else:\\n                        omni.log.error(\\n                            f\"ManagerVisualizer Failure: ManagerTerm ({term_name}) does not exist in\"\\n                            f\" Manager({self.cfg.manager_name})\"\\n                        )\\n\\n            # extract chosen group-terms\\n            elif isinstance(self.cfg.term_names, dict):\\n                # if manager is using groups and terms are saved as a dictionary\\n                if isinstance(self._manager.active_terms, dict):\\n                    for group, terms in self.cfg.term_names:\\n                        if group in self._manager.active_terms.keys():\\n                            for term_name in terms:\\n                                if term_name in self._manager.active_terms[group]:\\n                                    self.term_names.append(f\"{group}-{term_name}\")\\n                                else:\\n                                    omni.log.error(\\n                                        f\"ManagerVisualizer Failure: ManagerTerm ({term_name}) does not exist in\"\\n                                        f\" Group({group})\"\\n                                    )\\n                        else:\\n                            omni.log.error(\\n                                f\"ManagerVisualizer Failure: Group ({group}) does not exist in\"\\n                                f\" Manager({self.cfg.manager_name})\"\\n                            )\\n                else:\\n                    omni.log.error(\\n                        f\"ManagerVisualizer Failure: Manager({self.cfg.manager_name}) does not utilize grouping of\"\\n                        \" terms.\"\\n                    )\\n\\n    #\\n    # Implementation checks\\n    #\\n\\n    @property\\n    def get_vis_frame(self) -> omni.ui.Frame:\\n        \"\"\"Returns the UI Frame object tied to this visualizer.\"\"\"\\n        return self._vis_frame\\n\\n    @property\\n    def get_vis_window(self) -> omni.ui.Window:\\n        \"\"\"Returns the UI Window object tied to this visualizer.\"\"\"\\n        return self._vis_window\\n\\n    #\\n    # Setters\\n    #\\n\\n    def set_debug_vis(self, debug_vis: bool):\\n        \"\"\"Set the debug visualization external facing function.\\n\\n        Args:\\n            debug_vis: Whether to enable or disable the debug visualization.\\n        \"\"\"\\n        self._set_debug_vis_impl(debug_vis)\\n\\n    #\\n    # Implementations\\n    #\\n\\n    def _set_env_selection_impl(self, env_idx: int):\\n        \"\"\"Update the index of the selected environment to display.\\n\\n        Args:\\n            env_idx: The index of the selected environment.\\n        \"\"\"\\n        if env_idx > 0 and env_idx < self._manager.num_envs:\\n            self._env_idx = env_idx\\n        else:\\n            omni.log.warn(f\"Environment index is out of range (0, {self._manager.num_envs - 1})\")\\n\\n    def _set_vis_frame_impl(self, frame: omni.ui.Frame):\\n        \"\"\"Updates the assigned frame that can be used for visualizations.\\n\\n        Args:\\n            frame: The debug visualization frame.\\n        \"\"\"\\n        self._vis_frame = frame\\n\\n    def _debug_vis_callback(self, event):\\n        \"\"\"Callback for the debug visualization event.\"\"\"\\n\\n        if not SimulationContext.instance().is_playing():\\n            # Visualizers have not been created yet.\\n            return\\n\\n        # get updated data and update visualization\\n        for (_, term), vis in zip(\\n            self._manager.get_active_iterable_terms(env_idx=self._env_idx), self._term_visualizers\\n        ):\\n            if isinstance(vis, LiveLinePlot):\\n                vis.add_datapoint(term)\\n            elif isinstance(vis, ImagePlot):\\n                vis.update_image(numpy.array(term))\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set the debug visualization implementation.\\n\\n        Args:\\n            debug_vis: Whether to enable or disable debug visualization.\\n        \"\"\"\\n\\n        if not hasattr(self, \"_vis_frame\"):\\n            raise RuntimeError(\"No frame set for debug visualization.\")\\n\\n        # Clear internal visualizers\\n        self._term_visualizers = []\\n        self._vis_frame.clear()\\n\\n        if debug_vis:\\n            # if enabled create a subscriber for the post update event if it doesn\\'t exist\\n            if not hasattr(self, \"_debug_vis_handle\") or self._debug_vis_handle is None:\\n                app_interface = omni.kit.app.get_app_interface()\\n                self._debug_vis_handle = app_interface.get_post_update_event_stream().create_subscription_to_pop(\\n                    lambda event, obj=weakref.proxy(self): obj._debug_vis_callback(event)\\n                )\\n        else:\\n            # if disabled remove the subscriber if it exists\\n            if self._debug_vis_handle is not None:\\n                self._debug_vis_handle.unsubscribe()\\n                self._debug_vis_handle = None\\n\\n            self._vis_frame.visible = False\\n            return\\n\\n        self._vis_frame.visible = True\\n\\n        with self._vis_frame:\\n            with omni.ui.VStack():\\n                # Add a plot in a collapsible frame for each term available\\n                for name, term in self._manager.get_active_iterable_terms(env_idx=self._env_idx):\\n                    if name in self.term_names or len(self.term_names) == 0:\\n                        frame = omni.ui.CollapsableFrame(\\n                            name,\\n                            collapsed=False,\\n                            style={\"border_color\": 0xFF8A8777, \"padding\": 4},\\n                        )\\n                        with frame:\\n                            # create line plot for single or multi-variable signals\\n                            len_term_shape = len(numpy.array(term).shape)\\n                            if len_term_shape <= 2:\\n                                plot = LiveLinePlot(y_data=[[elem] for elem in term], plot_height=150, show_legend=True)\\n                                self._term_visualizers.append(plot)\\n                            # create an image plot for 2d and greater data (i.e. mono and rgb images)\\n                            elif len_term_shape == 3:\\n                                image = ImagePlot(image=numpy.array(term), label=name)\\n                                self._term_visualizers.append(image)\\n                            else:\\n                                omni.log.warn(\\n                                    f\"ManagerLiveVisualizer: Term ({name}) is not a supported data type for\"\\n                                    \" visualization.\"\\n                                )\\n                        frame.collapsed = True\\n\\n        self._debug_vis = debug_vis'),\n",
       " Document(metadata={}, page_content='class DefaultManagerBasedEnvLiveVisCfg:\\n    \"\"\"Default configuration to use for the ManagerBasedEnv. Each chosen manager assumes all terms will be plotted.\"\"\"\\n\\n    action_live_vis = ManagerLiveVisualizerCfg(manager_name=\"action_manager\")\\n    observation_live_vis = ManagerLiveVisualizerCfg(manager_name=\"observation_manager\")'),\n",
       " Document(metadata={}, page_content='class DefaultManagerBasedRLEnvLiveVisCfg(DefaultManagerBasedEnvLiveVisCfg):\\n    \"\"\"Default configuration to use for the ManagerBasedRLEnv. Each chosen manager assumes all terms will be plotted.\"\"\"\\n\\n    curriculum_live_vis = ManagerLiveVisualizerCfg(manager_name=\"curriculum_manager\")\\n    command_live_vis = ManagerLiveVisualizerCfg(manager_name=\"command_manager\")\\n    reward_live_vis = ManagerLiveVisualizerCfg(manager_name=\"reward_manager\")\\n    termination_live_vis = ManagerLiveVisualizerCfg(manager_name=\"termination_manager\")'),\n",
       " Document(metadata={}, page_content='class EnvLiveVisualizer:\\n    \"\"\"A class to handle all ManagerLiveVisualizers used in an Environment.\"\"\"\\n\\n    def __init__(self, cfg: object, managers: dict[str, ManagerBase]):\\n        \"\"\"Initialize the EnvLiveVisualizer.\\n\\n        Args:\\n            cfg: The configuration file containing terms of ManagerLiveVisualizers.\\n            managers: A dictionary of labeled managers. i.e. {\"manager_name\",manager}.\\n        \"\"\"\\n        self.cfg = cfg\\n        self.managers = managers\\n        self._prepare_terms()\\n\\n    def _prepare_terms(self):\\n        self._manager_visualizers: dict[str, ManagerLiveVisualizer] = dict()\\n\\n        # check if config is dict already\\n        if isinstance(self.cfg, dict):\\n            cfg_items = self.cfg.items()\\n        else:\\n            cfg_items = self.cfg.__dict__.items()\\n\\n        for term_name, term_cfg in cfg_items:\\n            # check if term config is None\\n            if term_cfg is None:\\n                continue\\n            # check if term config is viable\\n            if isinstance(term_cfg, ManagerLiveVisualizerCfg):\\n                # find appropriate manager name\\n                manager = self.managers[term_cfg.manager_name]\\n                self._manager_visualizers[term_cfg.manager_name] = ManagerLiveVisualizer(manager=manager, cfg=term_cfg)\\n            else:\\n                raise TypeError(\\n                    f\"Provided EnvLiveVisualizer term: \\'{term_name}\\' is not of type ManagerLiveVisualizerCfg\"\\n                )\\n\\n    @property\\n    def manager_visualizers(self) -> dict[str, ManagerLiveVisualizer]:\\n        \"\"\"A dictionary of labeled ManagerLiveVisualizers associated manager name as key.\"\"\"\\n        return self._manager_visualizers'),\n",
       " Document(metadata={}, page_content='class UiVisualizerBase:\\n    \"\"\"Base Class for components that support debug visualizations that requires access to some UI elements.\\n\\n    This class provides a set of functions that can be used to assign ui interfaces.\\n\\n    The following functions are provided:\\n\\n    * :func:`set_debug_vis`: Assigns a debug visualization interface. This function is called by the main UI\\n        when the checkbox for debug visualization is toggled.\\n    * :func:`set_vis_frame`: Assigns a small frame within the isaac lab tab that can be used to visualize debug\\n        information. Such as e.g. plots or images. It is called by the main UI on startup to create the frame.\\n    * :func:`set_window`: Assigngs the main window that is used by the main UI. This allows the user\\n        to have full controller over all UI elements. But be warned, with great power comes great responsibility.\\n    \"\"\"\\n\\n    \"\"\"\\n    Exposed Properties\\n    \"\"\"\\n\\n    @property\\n    def has_debug_vis_implementation(self) -> bool:\\n        \"\"\"Whether the component has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_debug_vis_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    @property\\n    def has_vis_frame_implementation(self) -> bool:\\n        \"\"\"Whether the component has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_vis_frame_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    @property\\n    def has_window_implementation(self) -> bool:\\n        \"\"\"Whether the component has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_window_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    @property\\n    def has_env_selection_implementation(self) -> bool:\\n        \"\"\"Whether the component has a debug visualization implemented.\"\"\"\\n        # check if function raises NotImplementedError\\n        source_code = inspect.getsource(self._set_env_selection_impl)\\n        return \"NotImplementedError\" not in source_code\\n\\n    \"\"\"\\n    Exposed Setters\\n    \"\"\"\\n\\n    def set_env_selection(self, env_selection: int) -> bool:\\n        \"\"\"Sets the selected environment id.\\n\\n        This function is called by the main UI when the user selects a different environment.\\n\\n        Args:\\n            env_selection: The currently selected environment id.\\n\\n        Returns:\\n            Whether the environment selection was successfully set. False if the component\\n            does not support environment selection.\\n        \"\"\"\\n        # check if environment selection is supported\\n        if not self.has_env_selection_implementation:\\n            return False\\n        # set environment selection\\n        self._set_env_selection_impl(env_selection)\\n        return True\\n\\n    def set_window(self, window: omni.ui.Window) -> bool:\\n        \"\"\"Sets the current main ui window.\\n\\n        This function is called by the main UI when the window is created. It allows the component\\n        to add custom UI elements to the window or to control the window and its elements.\\n\\n        Args:\\n            window: The ui window.\\n\\n        Returns:\\n            Whether the window was successfully set. False if the component\\n            does not support this functionality.\\n        \"\"\"\\n        # check if window is supported\\n        if not self.has_window_implementation:\\n            return False\\n        # set window\\n        self._set_window_impl(window)\\n        return True\\n\\n    def set_vis_frame(self, vis_frame: omni.ui.Frame) -> bool:\\n        \"\"\"Sets the debug visualization frame.\\n\\n        This function is called by the main UI when the window is created. It allows the component\\n        to modify a small frame within the orbit tab that can be used to visualize debug information.\\n\\n        Args:\\n            vis_frame: The debug visualization frame.\\n\\n        Returns:\\n            Whether the debug visualization frame was successfully set. False if the component\\n            does not support debug visualization.\\n        \"\"\"\\n        # check if debug visualization is supported\\n        if not self.has_vis_frame_implementation:\\n            return False\\n        # set debug visualization frame\\n        self._set_vis_frame_impl(vis_frame)\\n        return True\\n\\n    \"\"\"\\n    Internal Implementation\\n    \"\"\"\\n\\n    def _set_env_selection_impl(self, env_idx: int):\\n        \"\"\"Set the environment selection.\"\"\"\\n        raise NotImplementedError(f\"Environment selection is not implemented for {self.__class__.__name__}.\")\\n\\n    def _set_window_impl(self, window: omni.ui.Window):\\n        \"\"\"Set the window.\"\"\"\\n        raise NotImplementedError(f\"Window is not implemented for {self.__class__.__name__}.\")\\n\\n    def _set_debug_vis_impl(self, debug_vis: bool):\\n        \"\"\"Set debug visualization state.\"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")\\n\\n    def _set_vis_frame_impl(self, vis_frame: omni.ui.Frame):\\n        \"\"\"Set debug visualization into visualization objects.\\n\\n        This function is responsible for creating the visualization objects if they don\\'t exist\\n        and input ``debug_vis`` is True. If the visualization objects exist, the function should\\n        set their visibility into the stage.\\n        \"\"\"\\n        raise NotImplementedError(f\"Debug visualization is not implemented for {self.__class__.__name__}.\")'),\n",
       " Document(metadata={}, page_content='class UIWidgetWrapper:\\n    \"\"\"\\n    Base class for creating wrappers around any subclass of omni.ui.Widget in order to provide an easy interface\\n    for creating and managing specific types of widgets such as state buttons or file pickers.\\n    \"\"\"\\n\\n    def __init__(self, container_frame: omni.ui.Frame):\\n        self._container_frame = container_frame\\n\\n    @property\\n    def container_frame(self) -> omni.ui.Frame:\\n        return self._container_frame\\n\\n    @property\\n    def enabled(self) -> bool:\\n        return self.container_frame.enabled\\n\\n    @enabled.setter\\n    def enabled(self, value: bool):\\n        self.container_frame.enabled = value\\n\\n    @property\\n    def visible(self) -> bool:\\n        return self.container_frame.visible\\n\\n    @visible.setter\\n    def visible(self, value: bool):\\n        self.container_frame.visible = value\\n\\n    def cleanup(self):\\n        \"\"\"\\n        Perform any necessary cleanup\\n        \"\"\"\\n        pass'),\n",
       " Document(metadata={}, page_content='class SimpleTextWidget(ui.Widget):\\n    def __init__(self, text: str | None = \"Simple Text\", style: dict[str, Any] | None = None, **kwargs):\\n        super().__init__(**kwargs)\\n        if style is None:\\n            style = {\"font_size\": 1, \"color\": 0xFFFFFFFF}\\n        self._text = text\\n        self._style = style\\n        self._ui_label = None\\n        self._build_ui()\\n\\n    def set_label_text(self, text: str):\\n        \"\"\"Update the text displayed by the label.\"\"\"\\n        self._text = text\\n        if self._ui_label:\\n            self._ui_label.text = self._text\\n\\n    def _build_ui(self):\\n        \"\"\"Build the UI with a window-like rectangle and centered label.\"\"\"\\n        with ui.ZStack():\\n            ui.Rectangle(style={\"Rectangle\": {\"background_color\": 0xFF454545, \"border_radius\": 0.1}})\\n            with ui.VStack(alignment=ui.Alignment.CENTER):\\n                self._ui_label = ui.Label(self._text, style=self._style, alignment=ui.Alignment.CENTER)'),\n",
       " Document(metadata={}, page_content='def compute_widget_dimensions(\\n    text: str, font_size: float, max_width: float, min_width: float\\n) -> tuple[float, float, list[str]]:\\n    \"\"\"\\n    Estimate widget dimensions based on text content.\\n\\n    Returns:\\n        actual_width (float): The width, clamped between min_width and max_width.\\n        actual_height (float): The computed height based on wrapped text lines.\\n        lines (List[str]): The list of wrapped text lines.\\n    \"\"\"\\n    # Estimate average character width.\\n    char_width = 0.6 * font_size\\n    max_chars_per_line = int(max_width / char_width)\\n    lines = textwrap.wrap(text, width=max_chars_per_line)\\n    if not lines:\\n        lines = [text]\\n    computed_width = max(len(line) for line in lines) * char_width\\n    actual_width = max(min(computed_width, max_width), min_width)\\n    line_height = 1.2 * font_size\\n    actual_height = len(lines) * line_height\\n    return actual_width, actual_height, lines'),\n",
       " Document(metadata={}, page_content='def show_instruction(\\n    text: str,\\n    prim_path_source: str | None = None,\\n    translation: Gf.Vec3d = Gf.Vec3d(0, 0, 0),\\n    display_duration: float | None = 5.0,\\n    max_width: float = 2.5,\\n    min_width: float = 1.0,  # Prevent widget from being too narrow.\\n    font_size: float = 0.1,\\n    target_prim_path: str = \"/newPrim\",\\n) -> UiContainer | None:\\n    \"\"\"\\n    Create and display the instruction widget based on the given text.\\n\\n    The widget\\'s width and height are computed dynamically based on the input text.\\n    It automatically wraps text that is too long and adjusts the widget\\'s height\\n    accordingly. If a display duration is provided (non-zero), the widget is automatically\\n    hidden after that many seconds.\\n\\n    Args:\\n        text (str): The instruction text to display.\\n        prim_path_source (Optional[str]): The prim path to be used as a spatial sourcey\\n            for the widget.\\n        translation (Gf.Vec3d): A translation vector specifying the widget\\'s position.\\n        display_duration (Optional[float]): The time in seconds to display the widget before\\n            automatically hiding it. If None or 0, the widget remains visible until manually\\n            hidden.\\n        target_prim_path (str): The target path where the copied prim will be created.\\n            Defaults to \"/newPrim\".\\n\\n    Returns:\\n        UiContainer: The container instance holding the instruction widget.\\n    \"\"\"\\n    global camera_facing_widget_container, camera_facing_widget_timers\\n\\n    # Check if widget exists and has different text\\n    if target_prim_path in camera_facing_widget_container:\\n        container, current_text = camera_facing_widget_container[target_prim_path]\\n        if current_text == text:\\n            return container\\n\\n        # Cancel existing timer if there is one\\n        if target_prim_path in camera_facing_widget_timers:\\n            camera_facing_widget_timers[target_prim_path].cancel()\\n            del camera_facing_widget_timers[target_prim_path]\\n\\n        container.root.clear()\\n        del camera_facing_widget_container[target_prim_path]\\n\\n    # Clean up existing widget\\n    if get_prim_at_path(target_prim_path):\\n        delete_prim(target_prim_path)\\n\\n    # Compute dimensions and wrap text.\\n    width, height, lines = compute_widget_dimensions(text, font_size, max_width, min_width)\\n    wrapped_text = \"\\\\n\".join(lines)\\n\\n    # Create the widget component.\\n    widget_component = WidgetComponent(\\n        SimpleTextWidget,\\n        width=width,\\n        height=height,\\n        resolution_scale=300,\\n        widget_args=[wrapped_text, {\"font_size\": font_size}],\\n    )\\n\\n    copied_prim = omni.kit.commands.execute(\\n        \"CopyPrim\",\\n        path_from=prim_path_source,\\n        path_to=target_prim_path,\\n        exclusive_select=False,\\n        copy_to_introducing_layer=False,\\n    )\\n\\n    space_stack = []\\n    if copied_prim is not None:\\n        space_stack.append(SpatialSource.new_prim_path_source(target_prim_path))\\n\\n    space_stack.extend([\\n        SpatialSource.new_translation_source(translation),\\n        SpatialSource.new_look_at_camera_source(),\\n    ])\\n\\n    # Create the UI container with the widget.\\n    container = UiContainer(\\n        widget_component,\\n        space_stack=space_stack,\\n    )\\n    camera_facing_widget_container[target_prim_path] = (container, text)\\n\\n    # Schedule auto-hide after the specified display_duration if provided.\\n    if display_duration:\\n        timer = asyncio.get_event_loop().call_later(display_duration, functools.partial(hide, target_prim_path))\\n        camera_facing_widget_timers[target_prim_path] = timer\\n\\n    return container'),\n",
       " Document(metadata={}, page_content='def hide(target_prim_path: str = \"/newPrim\") -> None:\\n    \"\"\"\\n    Hide and clean up a specific instruction widget.\\n    Also cleans up associated timer.\\n    \"\"\"\\n    global camera_facing_widget_container, camera_facing_widget_timers\\n\\n    if target_prim_path in camera_facing_widget_container:\\n        container, _ = camera_facing_widget_container[target_prim_path]\\n        container.root.clear()\\n        del camera_facing_widget_container[target_prim_path]\\n\\n    if target_prim_path in camera_facing_widget_timers:\\n        del camera_facing_widget_timers[target_prim_path]'),\n",
       " Document(metadata={}, page_content='def convert_to_torch(\\n    array: TensorData,\\n    dtype: torch.dtype = None,\\n    device: torch.device | str | None = None,\\n) -> torch.Tensor:\\n    \"\"\"Converts a given array into a torch tensor.\\n\\n    The function tries to convert the array to a torch tensor. If the array is a numpy/warp arrays, or python\\n    list/tuples, it is converted to a torch tensor. If the array is already a torch tensor, it is returned\\n    directly.\\n\\n    If ``device`` is None, then the function deduces the current device of the data. For numpy arrays,\\n    this defaults to \"cpu\", for torch tensors it is \"cpu\" or \"cuda\", and for warp arrays it is \"cuda\".\\n\\n    Note:\\n        Since PyTorch does not support unsigned integer types, unsigned integer arrays are converted to\\n        signed integer arrays. This is done by casting the array to the corresponding signed integer type.\\n\\n    Args:\\n        array: The input array. It can be a numpy array, warp array, python list/tuple, or torch tensor.\\n        dtype: Target data-type for the tensor.\\n        device: The target device for the tensor. Defaults to None.\\n\\n    Returns:\\n        The converted array as torch tensor.\\n    \"\"\"\\n    # Convert array to tensor\\n    # if the datatype is not currently supported by torch we need to improvise\\n    # supported types are: https://pytorch.org/docs/stable/tensors.html\\n    if isinstance(array, torch.Tensor):\\n        tensor = array\\n    elif isinstance(array, np.ndarray):\\n        if array.dtype == np.uint32:\\n            array = array.astype(np.int32)\\n        # need to deal with object arrays (np.void) separately\\n        tensor = torch.from_numpy(array)\\n    elif isinstance(array, wp.array):\\n        if array.dtype == wp.uint32:\\n            array = array.view(wp.int32)\\n        tensor = wp.to_torch(array)\\n    else:\\n        tensor = torch.Tensor(array)\\n    # Convert tensor to the right device\\n    if device is not None and str(tensor.device) != str(device):\\n        tensor = tensor.to(device)\\n    # Convert dtype of tensor if requested\\n    if dtype is not None and tensor.dtype != dtype:\\n        tensor = tensor.type(dtype)\\n\\n    return tensor'),\n",
       " Document(metadata={}, page_content='def check_file_path(path: str) -> Literal[0, 1, 2]:\\n    \"\"\"Checks if a file exists on the Nucleus Server or locally.\\n\\n    Args:\\n        path: The path to the file.\\n\\n    Returns:\\n        The status of the file. Possible values are listed below.\\n\\n        * :obj:`0` if the file does not exist\\n        * :obj:`1` if the file exists locally\\n        * :obj:`2` if the file exists on the Nucleus Server\\n    \"\"\"\\n    if os.path.isfile(path):\\n        return 1\\n    # we need to convert backslash to forward slash on Windows for omni.client API\\n    elif omni.client.stat(path.replace(os.sep, \"/\"))[0] == omni.client.Result.OK:\\n        return 2\\n    else:\\n        return 0'),\n",
       " Document(metadata={}, page_content='def retrieve_file_path(path: str, download_dir: str | None = None, force_download: bool = True) -> str:\\n    \"\"\"Retrieves the path to a file on the Nucleus Server or locally.\\n\\n    If the file exists locally, then the absolute path to the file is returned.\\n    If the file exists on the Nucleus Server, then the file is downloaded to the local machine\\n    and the absolute path to the file is returned.\\n\\n    Args:\\n        path: The path to the file.\\n        download_dir: The directory where the file should be downloaded. Defaults to None, in which\\n            case the file is downloaded to the system\\'s temporary directory.\\n        force_download: Whether to force download the file from the Nucleus Server. This will overwrite\\n            the local file if it exists. Defaults to True.\\n\\n    Returns:\\n        The path to the file on the local machine.\\n\\n    Raises:\\n        FileNotFoundError: When the file not found locally or on Nucleus Server.\\n        RuntimeError: When the file cannot be copied from the Nucleus Server to the local machine. This\\n            can happen when the file already exists locally and :attr:`force_download` is set to False.\\n    \"\"\"\\n    # check file status\\n    file_status = check_file_path(path)\\n    if file_status == 1:\\n        return os.path.abspath(path)\\n    elif file_status == 2:\\n        # resolve download directory\\n        if download_dir is None:\\n            download_dir = tempfile.gettempdir()\\n        else:\\n            download_dir = os.path.abspath(download_dir)\\n        # create download directory if it does not exist\\n        if not os.path.exists(download_dir):\\n            os.makedirs(download_dir)\\n        # download file in temp directory using os\\n        file_name = os.path.basename(omni.client.break_url(path.replace(os.sep, \"/\")).path)\\n        target_path = os.path.join(download_dir, file_name)\\n        # check if file already exists locally\\n        if not os.path.isfile(target_path) or force_download:\\n            # copy file to local machine\\n            result = omni.client.copy(path.replace(os.sep, \"/\"), target_path, omni.client.CopyBehavior.OVERWRITE)\\n            if result != omni.client.Result.OK and force_download:\\n                raise RuntimeError(f\"Unable to copy file: \\'{path}\\'. Is the Nucleus Server running?\")\\n        return os.path.abspath(target_path)\\n    else:\\n        raise FileNotFoundError(f\"Unable to find the file: {path}\")'),\n",
       " Document(metadata={}, page_content='def read_file(path: str) -> io.BytesIO:\\n    \"\"\"Reads a file from the Nucleus Server or locally.\\n\\n    Args:\\n        path: The path to the file.\\n\\n    Raises:\\n        FileNotFoundError: When the file not found locally or on Nucleus Server.\\n\\n    Returns:\\n        The content of the file.\\n    \"\"\"\\n    # check file status\\n    file_status = check_file_path(path)\\n    if file_status == 1:\\n        with open(path, \"rb\") as f:\\n            return io.BytesIO(f.read())\\n    elif file_status == 2:\\n        file_content = omni.client.read_file(path.replace(os.sep, \"/\"))[2]\\n        return io.BytesIO(memoryview(file_content).tobytes())\\n    else:\\n        raise FileNotFoundError(f\"Unable to find the file: {path}\")'),\n",
       " Document(metadata={}, page_content='def __dataclass_transform__():\\n    \"\"\"Add annotations decorator for PyLance.\"\"\"\\n    return lambda a: a'),\n",
       " Document(metadata={}, page_content='def configclass(cls, **kwargs):\\n    \"\"\"Wrapper around `dataclass` functionality to add extra checks and utilities.\\n\\n    As of Python 3.7, the standard dataclasses have two main issues which makes them non-generic for\\n    configuration use-cases. These include:\\n\\n    1. Requiring a type annotation for all its members.\\n    2. Requiring explicit usage of :meth:`field(default_factory=...)` to reinitialize mutable variables.\\n\\n    This function provides a decorator that wraps around Python\\'s `dataclass`_ utility to deal with\\n    the above two issues. It also provides additional helper functions for dictionary <-> class\\n    conversion and easily copying class instances.\\n\\n    Usage:\\n\\n    .. code-block:: python\\n\\n        from dataclasses import MISSING\\n\\n        from isaaclab.utils.configclass import configclass\\n\\n\\n        @configclass\\n        class ViewerCfg:\\n            eye: list = [7.5, 7.5, 7.5]  # field missing on purpose\\n            lookat: list = field(default_factory=[0.0, 0.0, 0.0])\\n\\n\\n        @configclass\\n        class EnvCfg:\\n            num_envs: int = MISSING\\n            episode_length: int = 2000\\n            viewer: ViewerCfg = ViewerCfg()\\n\\n        # create configuration instance\\n        env_cfg = EnvCfg(num_envs=24)\\n\\n        # print information as a dictionary\\n        print(env_cfg.to_dict())\\n\\n        # create a copy of the configuration\\n        env_cfg_copy = env_cfg.copy()\\n\\n        # replace arbitrary fields using keyword arguments\\n        env_cfg_copy = env_cfg_copy.replace(num_envs=32)\\n\\n    Args:\\n        cls: The class to wrap around.\\n        **kwargs: Additional arguments to pass to :func:`dataclass`.\\n\\n    Returns:\\n        The wrapped class.\\n\\n    .. _dataclass: https://docs.python.org/3/library/dataclasses.html\\n    \"\"\"\\n    # add type annotations\\n    _add_annotation_types(cls)\\n    # add field factory\\n    _process_mutable_types(cls)\\n    # copy mutable members\\n    # note: we check if user defined __post_init__ function exists and augment it with our own\\n    if hasattr(cls, \"__post_init__\"):\\n        setattr(cls, \"__post_init__\", _combined_function(cls.__post_init__, _custom_post_init))\\n    else:\\n        setattr(cls, \"__post_init__\", _custom_post_init)\\n    # add helper functions for dictionary conversion\\n    setattr(cls, \"to_dict\", _class_to_dict)\\n    setattr(cls, \"from_dict\", _update_class_from_dict)\\n    setattr(cls, \"replace\", _replace_class_with_kwargs)\\n    setattr(cls, \"copy\", _copy_class)\\n    setattr(cls, \"validate\", _validate)\\n    # wrap around dataclass\\n    cls = dataclass(cls, **kwargs)\\n    # return wrapped class\\n    return cls'),\n",
       " Document(metadata={}, page_content='def _class_to_dict(obj: object) -> dict[str, Any]:\\n    \"\"\"Convert an object into dictionary recursively.\\n\\n    Args:\\n        obj: The object to convert.\\n\\n    Returns:\\n        Converted dictionary mapping.\\n    \"\"\"\\n    return class_to_dict(obj)'),\n",
       " Document(metadata={}, page_content='def _update_class_from_dict(obj, data: dict[str, Any]) -> None:\\n    \"\"\"Reads a dictionary and sets object variables recursively.\\n\\n    This function performs in-place update of the class member attributes.\\n\\n    Args:\\n        obj: The object to update.\\n        data: Input (nested) dictionary to update from.\\n\\n    Raises:\\n        TypeError: When input is not a dictionary.\\n        ValueError: When dictionary has a value that does not match default config type.\\n        KeyError: When dictionary has a key that does not exist in the default config type.\\n    \"\"\"\\n    update_class_from_dict(obj, data, _ns=\"\")'),\n",
       " Document(metadata={}, page_content='def _replace_class_with_kwargs(obj: object, **kwargs) -> object:\\n    \"\"\"Return a new object replacing specified fields with new values.\\n\\n    This is especially useful for frozen classes.  Example usage:\\n\\n    .. code-block:: python\\n\\n        @configclass(frozen=True)\\n        class C:\\n            x: int\\n            y: int\\n\\n        c = C(1, 2)\\n        c1 = c.replace(x=3)\\n        assert c1.x == 3 and c1.y == 2\\n\\n    Args:\\n        obj: The object to replace.\\n        **kwargs: The fields to replace and their new values.\\n\\n    Returns:\\n        The new object.\\n    \"\"\"\\n    return replace(obj, **kwargs)'),\n",
       " Document(metadata={}, page_content='def _copy_class(obj: object) -> object:\\n    \"\"\"Return a new object with the same fields as the original.\"\"\"\\n    return replace(obj)'),\n",
       " Document(metadata={}, page_content='def _add_annotation_types(cls):\\n    \"\"\"Add annotations to all elements in the dataclass.\\n\\n    By definition in Python, a field is defined as a class variable that has a type annotation.\\n\\n    In case type annotations are not provided, dataclass ignores those members when :func:`__dict__()` is called.\\n    This function adds these annotations to the class variable to prevent any issues in case the user forgets to\\n    specify the type annotation.\\n\\n    This makes the following a feasible operation:\\n\\n    @dataclass\\n    class State:\\n        pos = (0.0, 0.0, 0.0)\\n           ^^\\n           If the function is NOT used, the following type-error is returned:\\n           TypeError: \\'pos\\' is a field but has no type annotation\\n    \"\"\"\\n    # get type hints\\n    hints = {}\\n    # iterate over class inheritance\\n    # we add annotations from base classes first\\n    for base in reversed(cls.__mro__):\\n        # check if base is object\\n        if base is object:\\n            continue\\n        # get base class annotations\\n        ann = base.__dict__.get(\"__annotations__\", {})\\n        # directly add all annotations from base class\\n        hints.update(ann)\\n        # iterate over base class members\\n        # Note: Do not change this to dir(base) since it orders the members alphabetically.\\n        #   This is not desirable since the order of the members is important in some cases.\\n        for key in base.__dict__:\\n            # get class member\\n            value = getattr(base, key)\\n            # skip members\\n            if _skippable_class_member(key, value, hints):\\n                continue\\n            # add type annotations for members that don\\'t have explicit type annotations\\n            # for these, we deduce the type from the default value\\n            if not isinstance(value, type):\\n                if key not in hints:\\n                    # check if var type is not MISSING\\n                    # we cannot deduce type from MISSING!\\n                    if value is MISSING:\\n                        raise TypeError(\\n                            f\"Missing type annotation for \\'{key}\\' in class \\'{cls.__name__}\\'.\"\\n                            \" Please add a type annotation or set a default value.\"\\n                        )\\n                    # add type annotation\\n                    hints[key] = type(value)\\n            elif key != value.__name__:\\n                # note: we don\\'t want to add type annotations for nested configclass. Thus, we check if\\n                #   the name of the type matches the name of the variable.\\n                # since Python 3.10, type hints are stored as strings\\n                hints[key] = f\"type[{value.__name__}]\"\\n\\n    # Note: Do not change this line. `cls.__dict__.get(\"__annotations__\", {})` is different from\\n    #   `cls.__annotations__` because of inheritance.\\n    cls.__annotations__ = cls.__dict__.get(\"__annotations__\", {})\\n    cls.__annotations__ = hints'),\n",
       " Document(metadata={}, page_content='def _validate(obj: object, prefix: str = \"\") -> list[str]:\\n    \"\"\"Check the validity of configclass object.\\n\\n    This function checks if the object is a valid configclass object. A valid configclass object contains no MISSING\\n    entries.\\n\\n    Args:\\n        obj: The object to check.\\n        prefix: The prefix to add to the missing fields. Defaults to \\'\\'.\\n\\n    Returns:\\n        A list of missing fields.\\n\\n    Raises:\\n        TypeError: When the object is not a valid configuration object.\\n    \"\"\"\\n    missing_fields = []\\n\\n    if type(obj) is type(MISSING):\\n        missing_fields.append(prefix)\\n        return missing_fields\\n    elif isinstance(obj, (list, tuple)):\\n        for index, item in enumerate(obj):\\n            current_path = f\"{prefix}[{index}]\"\\n            missing_fields.extend(_validate(item, prefix=current_path))\\n        return missing_fields\\n    elif isinstance(obj, dict):\\n        obj_dict = obj\\n    elif hasattr(obj, \"__dict__\"):\\n        obj_dict = obj.__dict__\\n    else:\\n        return missing_fields\\n\\n    for key, value in obj_dict.items():\\n        # disregard builtin attributes\\n        if key.startswith(\"__\"):\\n            continue\\n        current_path = f\"{prefix}.{key}\" if prefix else key\\n        missing_fields.extend(_validate(value, prefix=current_path))\\n\\n    # raise an error only once at the top-level call\\n    if prefix == \"\" and missing_fields:\\n        formatted_message = \"\\\\n\".join(f\"  - {field}\" for field in missing_fields)\\n        raise TypeError(\\n            f\"Missing values detected in object {obj.__class__.__name__} for the following\"\\n            f\" fields:\\\\n{formatted_message}\\\\n\"\\n        )\\n    return missing_fields'),\n",
       " Document(metadata={}, page_content='def _process_mutable_types(cls):\\n    \"\"\"Initialize all mutable elements through :obj:`dataclasses.Field` to avoid unnecessary complaints.\\n\\n    By default, dataclass requires usage of :obj:`field(default_factory=...)` to reinitialize mutable objects every time a new\\n    class instance is created. If a member has a mutable type and it is created without specifying the `field(default_factory=...)`,\\n    then Python throws an error requiring the usage of `default_factory`.\\n\\n    Additionally, Python only explicitly checks for field specification when the type is a list, set or dict. This misses the\\n    use-case where the type is class itself. Thus, the code silently carries a bug with it which can lead to undesirable effects.\\n\\n    This function deals with this issue\\n\\n    This makes the following a feasible operation:\\n\\n    @dataclass\\n    class State:\\n        pos: list = [0.0, 0.0, 0.0]\\n           ^^\\n           If the function is NOT used, the following value-error is returned:\\n           ValueError: mutable default <class \\'list\\'> for field pos is not allowed: use default_factory\\n    \"\"\"\\n    # note: Need to set this up in the same order as annotations. Otherwise, it\\n    #   complains about missing positional arguments.\\n    ann = cls.__dict__.get(\"__annotations__\", {})\\n\\n    # iterate over all class members and store them in a dictionary\\n    class_members = {}\\n    for base in reversed(cls.__mro__):\\n        # check if base is object\\n        if base is object:\\n            continue\\n        # iterate over base class members\\n        for key in base.__dict__:\\n            # get class member\\n            f = getattr(base, key)\\n            # skip members\\n            if _skippable_class_member(key, f):\\n                continue\\n            # store class member if it is not a type or if it is already present in annotations\\n            if not isinstance(f, type) or key in ann:\\n                class_members[key] = f\\n        # iterate over base class data fields\\n        # in previous call, things that became a dataclass field were removed from class members\\n        # so we need to add them back here as a dataclass field directly\\n        for key, f in base.__dict__.get(\"__dataclass_fields__\", {}).items():\\n            # store class member\\n            if not isinstance(f, type):\\n                class_members[key] = f\\n\\n    # check that all annotations are present in class members\\n    # note: mainly for debugging purposes\\n    if len(class_members) != len(ann):\\n        raise ValueError(\\n            f\"In class \\'{cls.__name__}\\', number of annotations ({len(ann)}) does not match number of class members\"\\n            f\" ({len(class_members)}). Please check that all class members have type annotations and/or a default\"\\n            \" value. If you don\\'t want to specify a default value, please use the literal `dataclasses.MISSING`.\"\\n        )\\n    # iterate over annotations and add field factory for mutable types\\n    for key in ann:\\n        # find matching field in class\\n        value = class_members.get(key, MISSING)\\n        # check if key belongs to ClassVar\\n        # in that case, we cannot use default_factory!\\n        origin = getattr(ann[key], \"__origin__\", None)\\n        if origin is ClassVar:\\n            continue\\n        # check if f is MISSING\\n        # note: commented out for now since it causes issue with inheritance\\n        #   of dataclasses when parent have some positional and some keyword arguments.\\n        # Ref: https://stackoverflow.com/questions/51575931/class-inheritance-in-python-3-7-dataclasses\\n        # TODO: check if this is fixed in Python 3.10\\n        # if f is MISSING:\\n        #     continue\\n        if isinstance(value, Field):\\n            setattr(cls, key, value)\\n        elif not isinstance(value, type):\\n            # create field factory for mutable types\\n            value = field(default_factory=_return_f(value))\\n            setattr(cls, key, value)'),\n",
       " Document(metadata={}, page_content='def _custom_post_init(obj):\\n    \"\"\"Deepcopy all elements to avoid shared memory issues for mutable objects in dataclasses initialization.\\n\\n    This function is called explicitly instead of as a part of :func:`_process_mutable_types()` to prevent mapping\\n    proxy type i.e. a read only proxy for mapping objects. The error is thrown when using hierarchical data-classes\\n    for configuration.\\n    \"\"\"\\n    for key in dir(obj):\\n        # skip dunder members\\n        if key.startswith(\"__\"):\\n            continue\\n        # get data member\\n        value = getattr(obj, key)\\n        # check annotation\\n        ann = obj.__class__.__dict__.get(key)\\n        # duplicate data members that are mutable\\n        if not callable(value) and not isinstance(ann, property):\\n            setattr(obj, key, deepcopy(value))'),\n",
       " Document(metadata={}, page_content='def _combined_function(f1: Callable, f2: Callable) -> Callable:\\n    \"\"\"Combine two functions into one.\\n\\n    Args:\\n        f1: The first function.\\n        f2: The second function.\\n\\n    Returns:\\n        The combined function.\\n    \"\"\"\\n\\n    def _combined(*args, **kwargs):\\n        # call both functions\\n        f1(*args, **kwargs)\\n        f2(*args, **kwargs)\\n\\n    return _combined'),\n",
       " Document(metadata={}, page_content='def _skippable_class_member(key: str, value: Any, hints: dict | None = None) -> bool:\\n    \"\"\"Check if the class member should be skipped in configclass processing.\\n\\n    The following members are skipped:\\n\\n    * Dunder members: ``__name__``, ``__module__``, ``__qualname__``, ``__annotations__``, ``__dict__``.\\n    * Manually-added special class functions: From :obj:`_CONFIGCLASS_METHODS`.\\n    * Members that are already present in the type annotations.\\n    * Functions bounded to class object or class.\\n    * Properties bounded to class object.\\n\\n    Args:\\n        key: The class member name.\\n        value: The class member value.\\n        hints: The type hints for the class. Defaults to None, in which case, the\\n            members existence in type hints are not checked.\\n\\n    Returns:\\n        True if the class member should be skipped, False otherwise.\\n    \"\"\"\\n    # skip dunder members\\n    if key.startswith(\"__\"):\\n        return True\\n    # skip manually-added special class functions\\n    if key in _CONFIGCLASS_METHODS:\\n        return True\\n    # check if key is already present\\n    if hints is not None and key in hints:\\n        return True\\n    # skip functions bounded to class\\n    if callable(value):\\n        # FIXME: This doesn\\'t yet work for static methods because they are essentially seen as function types.\\n        # check for class methods\\n        if isinstance(value, types.MethodType):\\n            return True\\n        # check for instance methods\\n        signature = inspect.signature(value)\\n        if \"self\" in signature.parameters or \"cls\" in signature.parameters:\\n            return True\\n    # skip property methods\\n    if isinstance(value, property):\\n        return True\\n    # Otherwise, don\\'t skip\\n    return False'),\n",
       " Document(metadata={}, page_content='def _return_f(f: Any) -> Callable[[], Any]:\\n    \"\"\"Returns default factory function for creating mutable/immutable variables.\\n\\n    This function should be used to create default factory functions for variables.\\n\\n    Example:\\n\\n        .. code-block:: python\\n\\n            value = field(default_factory=_return_f(value))\\n            setattr(cls, key, value)\\n    \"\"\"\\n\\n    def _wrap():\\n        if isinstance(f, Field):\\n            if f.default_factory is MISSING:\\n                return deepcopy(f.default)\\n            else:\\n                return f.default_factory\\n        else:\\n            return deepcopy(f)\\n\\n    return _wrap'),\n",
       " Document(metadata={}, page_content='def class_to_dict(obj: object) -> dict[str, Any]:\\n    \"\"\"Convert an object into dictionary recursively.\\n\\n    Note:\\n        Ignores all names starting with \"__\" (i.e. built-in methods).\\n\\n    Args:\\n        obj: An instance of a class to convert.\\n\\n    Raises:\\n        ValueError: When input argument is not an object.\\n\\n    Returns:\\n        Converted dictionary mapping.\\n    \"\"\"\\n    # check that input data is class instance\\n    if not hasattr(obj, \"__class__\"):\\n        raise ValueError(f\"Expected a class instance. Received: {type(obj)}.\")\\n    # convert object to dictionary\\n    if isinstance(obj, dict):\\n        obj_dict = obj\\n    elif isinstance(obj, torch.Tensor):\\n        # We have to treat torch tensors specially because `torch.tensor.__dict__` returns an empty\\n        # dict, which would mean that a torch.tensor would be stored as an empty dict. Instead we\\n        # want to store it directly as the tensor.\\n        return obj\\n    elif hasattr(obj, \"__dict__\"):\\n        obj_dict = obj.__dict__\\n    else:\\n        return obj\\n\\n    # convert to dictionary\\n    data = dict()\\n    for key, value in obj_dict.items():\\n        # disregard builtin attributes\\n        if key.startswith(\"__\"):\\n            continue\\n        # check if attribute is callable -- function\\n        if callable(value):\\n            data[key] = callable_to_string(value)\\n        # check if attribute is a dictionary\\n        elif hasattr(value, \"__dict__\") or isinstance(value, dict):\\n            data[key] = class_to_dict(value)\\n        # check if attribute is a list or tuple\\n        elif isinstance(value, (list, tuple)):\\n            data[key] = type(value)([class_to_dict(v) for v in value])\\n        else:\\n            data[key] = value\\n    return data'),\n",
       " Document(metadata={}, page_content='def update_class_from_dict(obj, data: dict[str, Any], _ns: str = \"\") -> None:\\n    \"\"\"Reads a dictionary and sets object variables recursively.\\n\\n    This function performs in-place update of the class member attributes.\\n\\n    Args:\\n        obj: An instance of a class to update.\\n        data: Input dictionary to update from.\\n        _ns: Namespace of the current object. This is useful for nested configuration\\n            classes or dictionaries. Defaults to \"\".\\n\\n    Raises:\\n        TypeError: When input is not a dictionary.\\n        ValueError: When dictionary has a value that does not match default config type.\\n        KeyError: When dictionary has a key that does not exist in the default config type.\\n    \"\"\"\\n    for key, value in data.items():\\n        # key_ns is the full namespace of the key\\n        key_ns = _ns + \"/\" + key\\n        # check if key is present in the object\\n        if hasattr(obj, key) or isinstance(obj, dict):\\n            obj_mem = obj[key] if isinstance(obj, dict) else getattr(obj, key)\\n            if isinstance(value, Mapping):\\n                # recursively call if it is a dictionary\\n                update_class_from_dict(obj_mem, value, _ns=key_ns)\\n                continue\\n            if isinstance(value, Iterable) and not isinstance(value, str):\\n                # check length of value to be safe\\n                if len(obj_mem) != len(value) and obj_mem is not None:\\n                    raise ValueError(\\n                        f\"[Config]: Incorrect length under namespace: {key_ns}.\"\\n                        f\" Expected: {len(obj_mem)}, Received: {len(value)}.\"\\n                    )\\n                if isinstance(obj_mem, tuple):\\n                    value = tuple(value)\\n                else:\\n                    set_obj = True\\n                    # recursively call if iterable contains dictionaries\\n                    for i in range(len(obj_mem)):\\n                        if isinstance(value[i], dict):\\n                            update_class_from_dict(obj_mem[i], value[i], _ns=key_ns)\\n                            set_obj = False\\n                    # do not set value to obj, otherwise it overwrites the cfg class with the dict\\n                    if not set_obj:\\n                        continue\\n            elif callable(obj_mem):\\n                # update function name\\n                value = string_to_callable(value)\\n            elif isinstance(value, type(obj_mem)) or value is None:\\n                pass\\n            else:\\n                raise ValueError(\\n                    f\"[Config]: Incorrect type under namespace: {key_ns}.\"\\n                    f\" Expected: {type(obj_mem)}, Received: {type(value)}.\"\\n                )\\n            # set value\\n            if isinstance(obj, dict):\\n                obj[key] = value\\n            else:\\n                setattr(obj, key, value)\\n        else:\\n            raise KeyError(f\"[Config]: Key not found under namespace: {key_ns}.\")'),\n",
       " Document(metadata={}, page_content='def dict_to_md5_hash(data: object) -> str:\\n    \"\"\"Convert a dictionary into a hashable key using MD5 hash.\\n\\n    Args:\\n        data: Input dictionary or configuration object to convert.\\n\\n    Returns:\\n        A string object of double length containing only hexadecimal digits.\\n    \"\"\"\\n    # convert to dictionary\\n    if isinstance(data, dict):\\n        encoded_buffer = json.dumps(data, sort_keys=True).encode()\\n    else:\\n        encoded_buffer = json.dumps(class_to_dict(data), sort_keys=True).encode()\\n    # compute hash using MD5\\n    data_hash = hashlib.md5()\\n    data_hash.update(encoded_buffer)\\n    # return the hash key\\n    return data_hash.hexdigest()'),\n",
       " Document(metadata={}, page_content='def convert_dict_to_backend(\\n    data: dict, backend: str = \"numpy\", array_types: Iterable[str] = (\"numpy\", \"torch\", \"warp\")\\n) -> dict:\\n    \"\"\"Convert all arrays or tensors in a dictionary to a given backend.\\n\\n    This function iterates over the dictionary, converts all arrays or tensors with the given types to\\n    the desired backend, and stores them in a new dictionary. It also works with nested dictionaries.\\n\\n    Currently supported backends are \"numpy\", \"torch\", and \"warp\".\\n\\n    Note:\\n        This function only converts arrays or tensors. Other types of data are left unchanged. Mutable types\\n        (e.g. lists) are referenced by the new dictionary, so they are not copied.\\n\\n    Args:\\n        data: An input dict containing array or tensor data as values.\\n        backend: The backend (\"numpy\", \"torch\", \"warp\") to which arrays in this dict should be converted.\\n            Defaults to \"numpy\".\\n        array_types: A list containing the types of arrays that should be converted to\\n            the desired backend. Defaults to (\"numpy\", \"torch\", \"warp\").\\n\\n    Raises:\\n        ValueError: If the specified ``backend`` or ``array_types`` are unknown, i.e. not in the list of supported\\n            backends (\"numpy\", \"torch\", \"warp\").\\n\\n    Returns:\\n        The updated dict with the data converted to the desired backend.\\n    \"\"\"\\n    # THINK: Should we also support converting to a specific device, e.g. \"cuda:0\"?\\n    # Check the backend is valid.\\n    if backend not in TENSOR_TYPE_CONVERSIONS:\\n        raise ValueError(f\"Unknown backend \\'{backend}\\'. Supported backends are \\'numpy\\', \\'torch\\', and \\'warp\\'.\")\\n    # Define the conversion functions for each backend.\\n    tensor_type_conversions = TENSOR_TYPE_CONVERSIONS[backend]\\n\\n    # Parse the array types and convert them to the corresponding types: \"numpy\" -> np.ndarray, etc.\\n    parsed_types = list()\\n    for t in array_types:\\n        # Check type is valid.\\n        if t not in TENSOR_TYPES:\\n            raise ValueError(f\"Unknown array type: \\'{t}\\'. Supported array types are \\'numpy\\', \\'torch\\', and \\'warp\\'.\")\\n        # Exclude types that match the backend, since we do not need to convert these.\\n        if t == backend:\\n            continue\\n        # Convert the string types to the corresponding types.\\n        parsed_types.append(TENSOR_TYPES[t])\\n\\n    # Convert the data to the desired backend.\\n    output_dict = dict()\\n    for key, value in data.items():\\n        # Obtain the data type of the current value.\\n        data_type = type(value)\\n        # -- arrays\\n        if data_type in parsed_types:\\n            # check if we have a known conversion.\\n            if data_type not in tensor_type_conversions:\\n                raise ValueError(f\"No registered conversion for data type: {data_type} to {backend}!\")\\n            # convert the data to the desired backend.\\n            output_dict[key] = tensor_type_conversions[data_type](value)\\n        # -- nested dictionaries\\n        elif isinstance(data[key], dict):\\n            output_dict[key] = convert_dict_to_backend(value)\\n        # -- everything else\\n        else:\\n            output_dict[key] = value\\n\\n    return output_dict'),\n",
       " Document(metadata={}, page_content='def update_dict(orig_dict: dict, new_dict: collections.abc.Mapping) -> dict:\\n    \"\"\"Updates existing dictionary with values from a new dictionary.\\n\\n    This function mimics the dict.update() function. However, it works for\\n    nested dictionaries as well.\\n\\n    Args:\\n        orig_dict: The original dictionary to insert items to.\\n        new_dict: The new dictionary to insert items from.\\n\\n    Returns:\\n        The updated dictionary.\\n    \"\"\"\\n    for keyname, value in new_dict.items():\\n        if isinstance(value, collections.abc.Mapping):\\n            orig_dict[keyname] = update_dict(orig_dict.get(keyname, {}), value)\\n        else:\\n            orig_dict[keyname] = value\\n    return orig_dict'),\n",
       " Document(metadata={}, page_content='def replace_slices_with_strings(data: dict) -> dict:\\n    \"\"\"Replace slice objects with their string representations in a dictionary.\\n\\n    Args:\\n        data: The dictionary to process.\\n\\n    Returns:\\n        The dictionary with slice objects replaced by their string representations.\\n    \"\"\"\\n    if isinstance(data, dict):\\n        return {k: replace_slices_with_strings(v) for k, v in data.items()}\\n    elif isinstance(data, slice):\\n        return f\"slice({data.start},{data.stop},{data.step})\"\\n    else:\\n        return data'),\n",
       " Document(metadata={}, page_content='def replace_strings_with_slices(data: dict) -> dict:\\n    \"\"\"Replace string representations of slices with slice objects in a dictionary.\\n\\n    Args:\\n        data: The dictionary to process.\\n\\n    Returns:\\n        The dictionary with string representations of slices replaced by slice objects.\\n    \"\"\"\\n    if isinstance(data, dict):\\n        return {k: replace_strings_with_slices(v) for k, v in data.items()}\\n    elif isinstance(data, str) and data.startswith(\"slice(\"):\\n        return string_to_slice(data)\\n    else:\\n        return data'),\n",
       " Document(metadata={}, page_content='def print_dict(val, nesting: int = -4, start: bool = True):\\n    \"\"\"Outputs a nested dictionary.\"\"\"\\n    if isinstance(val, dict):\\n        if not start:\\n            print(\"\")\\n        nesting += 4\\n        for k in val:\\n            print(nesting * \" \", end=\"\")\\n            print(k, end=\": \")\\n            print_dict(val[k], nesting, start=False)\\n    else:\\n        # deal with functions in print statements\\n        if callable(val):\\n            print(callable_to_string(val))\\n        else:\\n            print(val)'),\n",
       " Document(metadata={}, page_content='def scale_transform(x: torch.Tensor, lower: torch.Tensor, upper: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Normalizes a given input tensor to a range of [-1, 1].\\n\\n    .. note::\\n        It uses pytorch broadcasting functionality to deal with batched input.\\n\\n    Args:\\n        x: Input tensor of shape (N, dims).\\n        lower: The minimum value of the tensor. Shape is (N, dims) or (dims,).\\n        upper: The maximum value of the tensor. Shape is (N, dims) or (dims,).\\n\\n    Returns:\\n        Normalized transform of the tensor. Shape is (N, dims).\\n    \"\"\"\\n    # default value of center\\n    offset = (lower + upper) * 0.5\\n    # return normalized tensor\\n    return 2 * (x - offset) / (upper - lower)'),\n",
       " Document(metadata={}, page_content='def unscale_transform(x: torch.Tensor, lower: torch.Tensor, upper: torch.Tensor) -> torch.Tensor:\\n    \"\"\"De-normalizes a given input tensor from range of [-1, 1] to (lower, upper).\\n\\n    .. note::\\n        It uses pytorch broadcasting functionality to deal with batched input.\\n\\n    Args:\\n        x: Input tensor of shape (N, dims).\\n        lower: The minimum value of the tensor. Shape is (N, dims) or (dims,).\\n        upper: The maximum value of the tensor. Shape is (N, dims) or (dims,).\\n\\n    Returns:\\n        De-normalized transform of the tensor. Shape is (N, dims).\\n    \"\"\"\\n    # default value of center\\n    offset = (lower + upper) * 0.5\\n    # return normalized tensor\\n    return x * (upper - lower) * 0.5 + offset'),\n",
       " Document(metadata={}, page_content='def saturate(x: torch.Tensor, lower: torch.Tensor, upper: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Clamps a given input tensor to (lower, upper).\\n\\n    It uses pytorch broadcasting functionality to deal with batched input.\\n\\n    Args:\\n        x: Input tensor of shape (N, dims).\\n        lower: The minimum value of the tensor. Shape is (N, dims) or (dims,).\\n        upper: The maximum value of the tensor. Shape is (N, dims) or (dims,).\\n\\n    Returns:\\n        Clamped transform of the tensor. Shape is (N, dims).\\n    \"\"\"\\n    return torch.max(torch.min(x, upper), lower)'),\n",
       " Document(metadata={}, page_content='def normalize(x: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\\n    \"\"\"Normalizes a given input tensor to unit length.\\n\\n    Args:\\n        x: Input tensor of shape (N, dims).\\n        eps: A small value to avoid division by zero. Defaults to 1e-9.\\n\\n    Returns:\\n        Normalized tensor of shape (N, dims).\\n    \"\"\"\\n    return x / x.norm(p=2, dim=-1).clamp(min=eps, max=None).unsqueeze(-1)'),\n",
       " Document(metadata={}, page_content='def wrap_to_pi(angles: torch.Tensor) -> torch.Tensor:\\n    r\"\"\"Wraps input angles (in radians) to the range :math:`[-\\\\pi, \\\\pi]`.\\n\\n    This function wraps angles in radians to the range :math:`[-\\\\pi, \\\\pi]`, such that\\n    :math:`\\\\pi` maps to :math:`\\\\pi`, and :math:`-\\\\pi` maps to :math:`-\\\\pi`. In general,\\n    odd positive multiples of :math:`\\\\pi` are mapped to :math:`\\\\pi`, and odd negative\\n    multiples of :math:`\\\\pi` are mapped to :math:`-\\\\pi`.\\n\\n    The function behaves similar to MATLAB\\'s `wrapToPi <https://www.mathworks.com/help/map/ref/wraptopi.html>`_\\n    function.\\n\\n    Args:\\n        angles: Input angles of any shape.\\n\\n    Returns:\\n        Angles in the range :math:`[-\\\\pi, \\\\pi]`.\\n    \"\"\"\\n    # wrap to [0, 2*pi)\\n    wrapped_angle = (angles + torch.pi) % (2 * torch.pi)\\n    # map to [-pi, pi]\\n    # we check for zero in wrapped angle to make it go to pi when input angle is odd multiple of pi\\n    return torch.where((wrapped_angle == 0) & (angles > 0), torch.pi, wrapped_angle - torch.pi)'),\n",
       " Document(metadata={}, page_content='def copysign(mag: float, other: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Create a new floating-point tensor with the magnitude of input and the sign of other, element-wise.\\n\\n    Note:\\n        The implementation follows from `torch.copysign`. The function allows a scalar magnitude.\\n\\n    Args:\\n        mag: The magnitude scalar.\\n        other: The tensor containing values whose signbits are applied to magnitude.\\n\\n    Returns:\\n        The output tensor.\\n    \"\"\"\\n    mag_torch = torch.tensor(mag, device=other.device, dtype=torch.float).repeat(other.shape[0])\\n    return torch.abs(mag_torch) * torch.sign(other)'),\n",
       " Document(metadata={}, page_content='def quat_unique(q: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Convert a unit quaternion to a standard form where the real part is non-negative.\\n\\n    Quaternion representations have a singularity since ``q`` and ``-q`` represent the same\\n    rotation. This function ensures the real part of the quaternion is non-negative.\\n\\n    Args:\\n        q: The quaternion orientation in (w, x, y, z). Shape is (..., 4).\\n\\n    Returns:\\n        Standardized quaternions. Shape is (..., 4).\\n    \"\"\"\\n    return torch.where(q[..., 0:1] < 0, -q, q)'),\n",
       " Document(metadata={}, page_content='def matrix_from_quat(quaternions: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Convert rotations given as quaternions to rotation matrices.\\n\\n    Args:\\n        quaternions: The quaternion orientation in (w, x, y, z). Shape is (..., 4).\\n\\n    Returns:\\n        Rotation matrices. The shape is (..., 3, 3).\\n\\n    Reference:\\n        https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/transforms/rotation_conversions.py#L41-L70\\n    \"\"\"\\n    r, i, j, k = torch.unbind(quaternions, -1)\\n    # pyre-fixme[58]: `/` is not supported for operand types `float` and `Tensor`.\\n    two_s = 2.0 / (quaternions * quaternions).sum(-1)\\n\\n    o = torch.stack(\\n        (\\n            1 - two_s * (j * j + k * k),\\n            two_s * (i * j - k * r),\\n            two_s * (i * k + j * r),\\n            two_s * (i * j + k * r),\\n            1 - two_s * (i * i + k * k),\\n            two_s * (j * k - i * r),\\n            two_s * (i * k - j * r),\\n            two_s * (j * k + i * r),\\n            1 - two_s * (i * i + j * j),\\n        ),\\n        -1,\\n    )\\n    return o.reshape(quaternions.shape[:-1] + (3, 3))'),\n",
       " Document(metadata={}, page_content='def convert_quat(quat: torch.Tensor | np.ndarray, to: Literal[\"xyzw\", \"wxyz\"] = \"xyzw\") -> torch.Tensor | np.ndarray:\\n    \"\"\"Converts quaternion from one convention to another.\\n\\n    The convention to convert TO is specified as an optional argument. If to == \\'xyzw\\',\\n    then the input is in \\'wxyz\\' format, and vice-versa.\\n\\n    Args:\\n        quat: The quaternion of shape (..., 4).\\n        to: Convention to convert quaternion to.. Defaults to \"xyzw\".\\n\\n    Returns:\\n        The converted quaternion in specified convention.\\n\\n    Raises:\\n        ValueError: Invalid input argument `to`, i.e. not \"xyzw\" or \"wxyz\".\\n        ValueError: Invalid shape of input `quat`, i.e. not (..., 4,).\\n    \"\"\"\\n    # check input is correct\\n    if quat.shape[-1] != 4:\\n        msg = f\"Expected input quaternion shape mismatch: {quat.shape} != (..., 4).\"\\n        raise ValueError(msg)\\n    if to not in [\"xyzw\", \"wxyz\"]:\\n        msg = f\"Expected input argument `to` to be \\'xyzw\\' or \\'wxyz\\'. Received: {to}.\"\\n        raise ValueError(msg)\\n    # check if input is numpy array (we support this backend since some classes use numpy)\\n    if isinstance(quat, np.ndarray):\\n        # use numpy functions\\n        if to == \"xyzw\":\\n            # wxyz -> xyzw\\n            return np.roll(quat, -1, axis=-1)\\n        else:\\n            # xyzw -> wxyz\\n            return np.roll(quat, 1, axis=-1)\\n    else:\\n        # convert to torch (sanity check)\\n        if not isinstance(quat, torch.Tensor):\\n            quat = torch.tensor(quat, dtype=float)\\n        # convert to specified quaternion type\\n        if to == \"xyzw\":\\n            # wxyz -> xyzw\\n            return quat.roll(-1, dims=-1)\\n        else:\\n            # xyzw -> wxyz\\n            return quat.roll(1, dims=-1)'),\n",
       " Document(metadata={}, page_content='def quat_conjugate(q: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Computes the conjugate of a quaternion.\\n\\n    Args:\\n        q: The quaternion orientation in (w, x, y, z). Shape is (..., 4).\\n\\n    Returns:\\n        The conjugate quaternion in (w, x, y, z). Shape is (..., 4).\\n    \"\"\"\\n    shape = q.shape\\n    q = q.reshape(-1, 4)\\n    return torch.cat((q[:, 0:1], -q[:, 1:]), dim=-1).view(shape)'),\n",
       " Document(metadata={}, page_content='def quat_inv(q: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Compute the inverse of a quaternion.\\n\\n    Args:\\n        q: The quaternion orientation in (w, x, y, z). Shape is (N, 4).\\n\\n    Returns:\\n        The inverse quaternion in (w, x, y, z). Shape is (N, 4).\\n    \"\"\"\\n    return normalize(quat_conjugate(q))'),\n",
       " Document(metadata={}, page_content='def quat_from_euler_xyz(roll: torch.Tensor, pitch: torch.Tensor, yaw: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Convert rotations given as Euler angles in radians to Quaternions.\\n\\n    Note:\\n        The euler angles are assumed in XYZ convention.\\n\\n    Args:\\n        roll: Rotation around x-axis (in radians). Shape is (N,).\\n        pitch: Rotation around y-axis (in radians). Shape is (N,).\\n        yaw: Rotation around z-axis (in radians). Shape is (N,).\\n\\n    Returns:\\n        The quaternion in (w, x, y, z). Shape is (N, 4).\\n    \"\"\"\\n    cy = torch.cos(yaw * 0.5)\\n    sy = torch.sin(yaw * 0.5)\\n    cr = torch.cos(roll * 0.5)\\n    sr = torch.sin(roll * 0.5)\\n    cp = torch.cos(pitch * 0.5)\\n    sp = torch.sin(pitch * 0.5)\\n    # compute quaternion\\n    qw = cy * cr * cp + sy * sr * sp\\n    qx = cy * sr * cp - sy * cr * sp\\n    qy = cy * cr * sp + sy * sr * cp\\n    qz = sy * cr * cp - cy * sr * sp\\n\\n    return torch.stack([qw, qx, qy, qz], dim=-1)'),\n",
       " Document(metadata={}, page_content='def _sqrt_positive_part(x: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Returns torch.sqrt(torch.max(0, x)) but with a zero sub-gradient where x is 0.\\n\\n    Reference:\\n        https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/transforms/rotation_conversions.py#L91-L99\\n    \"\"\"\\n    ret = torch.zeros_like(x)\\n    positive_mask = x > 0\\n    ret[positive_mask] = torch.sqrt(x[positive_mask])\\n    return ret'),\n",
       " Document(metadata={}, page_content='def quat_from_matrix(matrix: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Convert rotations given as rotation matrices to quaternions.\\n\\n    Args:\\n        matrix: The rotation matrices. Shape is (..., 3, 3).\\n\\n    Returns:\\n        The quaternion in (w, x, y, z). Shape is (..., 4).\\n\\n    Reference:\\n        https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/transforms/rotation_conversions.py#L102-L161\\n    \"\"\"\\n    if matrix.size(-1) != 3 or matrix.size(-2) != 3:\\n        raise ValueError(f\"Invalid rotation matrix shape {matrix.shape}.\")\\n\\n    batch_dim = matrix.shape[:-2]\\n    m00, m01, m02, m10, m11, m12, m20, m21, m22 = torch.unbind(matrix.reshape(batch_dim + (9,)), dim=-1)\\n\\n    q_abs = _sqrt_positive_part(\\n        torch.stack(\\n            [\\n                1.0 + m00 + m11 + m22,\\n                1.0 + m00 - m11 - m22,\\n                1.0 - m00 + m11 - m22,\\n                1.0 - m00 - m11 + m22,\\n            ],\\n            dim=-1,\\n        )\\n    )\\n\\n    # we produce the desired quaternion multiplied by each of r, i, j, k\\n    quat_by_rijk = torch.stack(\\n        [\\n            # pyre-fixme[58]: `**` is not supported for operand types `Tensor` and `int`.\\n            torch.stack([q_abs[..., 0] ** 2, m21 - m12, m02 - m20, m10 - m01], dim=-1),\\n            # pyre-fixme[58]: `**` is not supported for operand types `Tensor` and `int`.\\n            torch.stack([m21 - m12, q_abs[..., 1] ** 2, m10 + m01, m02 + m20], dim=-1),\\n            # pyre-fixme[58]: `**` is not supported for operand types `Tensor` and `int`.\\n            torch.stack([m02 - m20, m10 + m01, q_abs[..., 2] ** 2, m12 + m21], dim=-1),\\n            # pyre-fixme[58]: `**` is not supported for operand types `Tensor` and `int`.\\n            torch.stack([m10 - m01, m20 + m02, m21 + m12, q_abs[..., 3] ** 2], dim=-1),\\n        ],\\n        dim=-2,\\n    )\\n\\n    # We floor here at 0.1 but the exact level is not important; if q_abs is small,\\n    # the candidate won\\'t be picked.\\n    flr = torch.tensor(0.1).to(dtype=q_abs.dtype, device=q_abs.device)\\n    quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].max(flr))\\n\\n    # if not for numerical problems, quat_candidates[i] should be same (up to a sign),\\n    # forall i; we pick the best-conditioned one (with the largest denominator)\\n    return quat_candidates[torch.nn.functional.one_hot(q_abs.argmax(dim=-1), num_classes=4) > 0.5, :].reshape(\\n        batch_dim + (4,)\\n    )'),\n",
       " Document(metadata={}, page_content='def _axis_angle_rotation(axis: Literal[\"X\", \"Y\", \"Z\"], angle: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Return the rotation matrices for one of the rotations about an axis of which Euler angles describe,\\n    for each value of the angle given.\\n\\n    Args:\\n        axis: Axis label \"X\" or \"Y or \"Z\".\\n        angle: Euler angles in radians of any shape.\\n\\n    Returns:\\n        Rotation matrices. Shape is (..., 3, 3).\\n\\n    Reference:\\n        https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/transforms/rotation_conversions.py#L164-L191\\n    \"\"\"\\n    cos = torch.cos(angle)\\n    sin = torch.sin(angle)\\n    one = torch.ones_like(angle)\\n    zero = torch.zeros_like(angle)\\n\\n    if axis == \"X\":\\n        R_flat = (one, zero, zero, zero, cos, -sin, zero, sin, cos)\\n    elif axis == \"Y\":\\n        R_flat = (cos, zero, sin, zero, one, zero, -sin, zero, cos)\\n    elif axis == \"Z\":\\n        R_flat = (cos, -sin, zero, sin, cos, zero, zero, zero, one)\\n    else:\\n        raise ValueError(\"letter must be either X, Y or Z.\")\\n\\n    return torch.stack(R_flat, -1).reshape(angle.shape + (3, 3))'),\n",
       " Document(metadata={}, page_content='def matrix_from_euler(euler_angles: torch.Tensor, convention: str) -> torch.Tensor:\\n    \"\"\"\\n    Convert rotations given as Euler angles in radians to rotation matrices.\\n\\n    Args:\\n        euler_angles: Euler angles in radians. Shape is (..., 3).\\n        convention: Convention string of three uppercase letters from {\"X\", \"Y\", and \"Z\"}.\\n            For example, \"XYZ\" means that the rotations should be applied first about x,\\n            then y, then z.\\n\\n    Returns:\\n        Rotation matrices. Shape is (..., 3, 3).\\n\\n    Reference:\\n        https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/transforms/rotation_conversions.py#L194-L220\\n    \"\"\"\\n    if euler_angles.dim() == 0 or euler_angles.shape[-1] != 3:\\n        raise ValueError(\"Invalid input euler angles.\")\\n    if len(convention) != 3:\\n        raise ValueError(\"Convention must have 3 letters.\")\\n    if convention[1] in (convention[0], convention[2]):\\n        raise ValueError(f\"Invalid convention {convention}.\")\\n    for letter in convention:\\n        if letter not in (\"X\", \"Y\", \"Z\"):\\n            raise ValueError(f\"Invalid letter {letter} in convention string.\")\\n    matrices = [_axis_angle_rotation(c, e) for c, e in zip(convention, torch.unbind(euler_angles, -1))]\\n    # return functools.reduce(torch.matmul, matrices)\\n    return torch.matmul(torch.matmul(matrices[0], matrices[1]), matrices[2])'),\n",
       " Document(metadata={}, page_content='def euler_xyz_from_quat(quat: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\\n    \"\"\"Convert rotations given as quaternions to Euler angles in radians.\\n\\n    Note:\\n        The euler angles are assumed in XYZ convention.\\n\\n    Args:\\n        quat: The quaternion orientation in (w, x, y, z). Shape is (N, 4).\\n\\n    Returns:\\n        A tuple containing roll-pitch-yaw. Each element is a tensor of shape (N,).\\n\\n    Reference:\\n        https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles\\n    \"\"\"\\n    q_w, q_x, q_y, q_z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]\\n    # roll (x-axis rotation)\\n    sin_roll = 2.0 * (q_w * q_x + q_y * q_z)\\n    cos_roll = 1 - 2 * (q_x * q_x + q_y * q_y)\\n    roll = torch.atan2(sin_roll, cos_roll)\\n\\n    # pitch (y-axis rotation)\\n    sin_pitch = 2.0 * (q_w * q_y - q_z * q_x)\\n    pitch = torch.where(torch.abs(sin_pitch) >= 1, copysign(torch.pi / 2.0, sin_pitch), torch.asin(sin_pitch))\\n\\n    # yaw (z-axis rotation)\\n    sin_yaw = 2.0 * (q_w * q_z + q_x * q_y)\\n    cos_yaw = 1 - 2 * (q_y * q_y + q_z * q_z)\\n    yaw = torch.atan2(sin_yaw, cos_yaw)\\n\\n    return roll % (2 * torch.pi), pitch % (2 * torch.pi), yaw % (2 * torch.pi)'),\n",
       " Document(metadata={}, page_content='def axis_angle_from_quat(quat: torch.Tensor, eps: float = 1.0e-6) -> torch.Tensor:\\n    \"\"\"Convert rotations given as quaternions to axis/angle.\\n\\n    Args:\\n        quat: The quaternion orientation in (w, x, y, z). Shape is (..., 4).\\n        eps: The tolerance for Taylor approximation. Defaults to 1.0e-6.\\n\\n    Returns:\\n        Rotations given as a vector in axis angle form. Shape is (..., 3).\\n        The vector\\'s magnitude is the angle turned anti-clockwise in radians around the vector\\'s direction.\\n\\n    Reference:\\n        https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/transforms/rotation_conversions.py#L526-L554\\n    \"\"\"\\n    # Modified to take in quat as [q_w, q_x, q_y, q_z]\\n    # Quaternion is [q_w, q_x, q_y, q_z] = [cos(theta/2), n_x * sin(theta/2), n_y * sin(theta/2), n_z * sin(theta/2)]\\n    # Axis-angle is [a_x, a_y, a_z] = [theta * n_x, theta * n_y, theta * n_z]\\n    # Thus, axis-angle is [q_x, q_y, q_z] / (sin(theta/2) / theta)\\n    # When theta = 0, (sin(theta/2) / theta) is undefined\\n    # However, as theta --> 0, we can use the Taylor approximation 1/2 - theta^2 / 48\\n    quat = quat * (1.0 - 2.0 * (quat[..., 0:1] < 0.0))\\n    mag = torch.linalg.norm(quat[..., 1:], dim=-1)\\n    half_angle = torch.atan2(mag, quat[..., 0])\\n    angle = 2.0 * half_angle\\n    # check whether to apply Taylor approximation\\n    sin_half_angles_over_angles = torch.where(\\n        angle.abs() > eps, torch.sin(half_angle) / angle, 0.5 - angle * angle / 48\\n    )\\n    return quat[..., 1:4] / sin_half_angles_over_angles.unsqueeze(-1)'),\n",
       " Document(metadata={}, page_content='def quat_from_angle_axis(angle: torch.Tensor, axis: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Convert rotations given as angle-axis to quaternions.\\n\\n    Args:\\n        angle: The angle turned anti-clockwise in radians around the vector\\'s direction. Shape is (N,).\\n        axis: The axis of rotation. Shape is (N, 3).\\n\\n    Returns:\\n        The quaternion in (w, x, y, z). Shape is (N, 4).\\n    \"\"\"\\n    theta = (angle / 2).unsqueeze(-1)\\n    xyz = normalize(axis) * theta.sin()\\n    w = theta.cos()\\n    return normalize(torch.cat([w, xyz], dim=-1))'),\n",
       " Document(metadata={}, page_content='def quat_mul(q1: torch.Tensor, q2: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Multiply two quaternions together.\\n\\n    Args:\\n        q1: The first quaternion in (w, x, y, z). Shape is (..., 4).\\n        q2: The second quaternion in (w, x, y, z). Shape is (..., 4).\\n\\n    Returns:\\n        The product of the two quaternions in (w, x, y, z). Shape is (..., 4).\\n\\n    Raises:\\n        ValueError: Input shapes of ``q1`` and ``q2`` are not matching.\\n    \"\"\"\\n    # check input is correct\\n    if q1.shape != q2.shape:\\n        msg = f\"Expected input quaternion shape mismatch: {q1.shape} != {q2.shape}.\"\\n        raise ValueError(msg)\\n    # reshape to (N, 4) for multiplication\\n    shape = q1.shape\\n    q1 = q1.reshape(-1, 4)\\n    q2 = q2.reshape(-1, 4)\\n    # extract components from quaternions\\n    w1, x1, y1, z1 = q1[:, 0], q1[:, 1], q1[:, 2], q1[:, 3]\\n    w2, x2, y2, z2 = q2[:, 0], q2[:, 1], q2[:, 2], q2[:, 3]\\n    # perform multiplication\\n    ww = (z1 + x1) * (x2 + y2)\\n    yy = (w1 - y1) * (w2 + z2)\\n    zz = (w1 + y1) * (w2 - z2)\\n    xx = ww + yy + zz\\n    qq = 0.5 * (xx + (z1 - x1) * (x2 - y2))\\n    w = qq - ww + (z1 - y1) * (y2 - z2)\\n    x = qq - xx + (x1 + w1) * (x2 + w2)\\n    y = qq - yy + (w1 - x1) * (y2 + z2)\\n    z = qq - zz + (z1 + y1) * (w2 - x2)\\n\\n    return torch.stack([w, x, y, z], dim=-1).view(shape)'),\n",
       " Document(metadata={}, page_content='def yaw_quat(quat: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Extract the yaw component of a quaternion.\\n\\n    Args:\\n        quat: The orientation in (w, x, y, z). Shape is (..., 4)\\n\\n    Returns:\\n        A quaternion with only yaw component.\\n    \"\"\"\\n    shape = quat.shape\\n    quat_yaw = quat.view(-1, 4)\\n    qw = quat_yaw[:, 0]\\n    qx = quat_yaw[:, 1]\\n    qy = quat_yaw[:, 2]\\n    qz = quat_yaw[:, 3]\\n    yaw = torch.atan2(2 * (qw * qz + qx * qy), 1 - 2 * (qy * qy + qz * qz))\\n    quat_yaw = torch.zeros_like(quat_yaw)\\n    quat_yaw[:, 3] = torch.sin(yaw / 2)\\n    quat_yaw[:, 0] = torch.cos(yaw / 2)\\n    quat_yaw = normalize(quat_yaw)\\n    return quat_yaw.view(shape)'),\n",
       " Document(metadata={}, page_content='def quat_box_minus(q1: torch.Tensor, q2: torch.Tensor) -> torch.Tensor:\\n    \"\"\"The box-minus operator (quaternion difference) between two quaternions.\\n\\n    Args:\\n        q1: The first quaternion in (w, x, y, z). Shape is (N, 4).\\n        q2: The second quaternion in (w, x, y, z). Shape is (N, 4).\\n\\n    Returns:\\n        The difference between the two quaternions. Shape is (N, 3).\\n\\n    Reference:\\n        https://github.com/ANYbotics/kindr/blob/master/doc/cheatsheet/cheatsheet_latest.pdf\\n    \"\"\"\\n    quat_diff = quat_mul(q1, quat_conjugate(q2))  # q1 * q2^-1\\n    return axis_angle_from_quat(quat_diff)'),\n",
       " Document(metadata={}, page_content='def quat_box_plus(q: torch.Tensor, delta: torch.Tensor, eps: float = 1.0e-6) -> torch.Tensor:\\n    \"\"\"The box-plus operator (quaternion update) to apply an increment to a quaternion.\\n\\n    Args:\\n        q: The initial quaternion in (w, x, y, z). Shape is (N, 4).\\n        delta: The axis-angle perturbation. Shape is (N, 3).\\n            eps: A small value to avoid division by zero. Defaults to 1e-6.\\n\\n    Returns:\\n        The updated quaternion after applying the perturbation. Shape is (N, 4).\\n\\n    Reference:\\n        https://github.com/ANYbotics/kindr/blob/master/doc/cheatsheet/cheatsheet_latest.pdf\\n    \"\"\"\\n    delta_norm = torch.clamp_min(torch.linalg.norm(delta, dim=-1, keepdim=True), min=eps)\\n    delta_quat = quat_from_angle_axis(delta_norm.squeeze(-1), delta / delta_norm)  # exp(dq)\\n    new_quat = quat_mul(delta_quat, q)  # Apply perturbation\\n    return quat_unique(new_quat)'),\n",
       " Document(metadata={}, page_content='def quat_apply(quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Apply a quaternion rotation to a vector.\\n\\n    Args:\\n        quat: The quaternion in (w, x, y, z). Shape is (..., 4).\\n        vec: The vector in (x, y, z). Shape is (..., 3).\\n\\n    Returns:\\n        The rotated vector in (x, y, z). Shape is (..., 3).\\n    \"\"\"\\n    # store shape\\n    shape = vec.shape\\n    # reshape to (N, 3) for multiplication\\n    quat = quat.reshape(-1, 4)\\n    vec = vec.reshape(-1, 3)\\n    # extract components from quaternions\\n    xyz = quat[:, 1:]\\n    t = xyz.cross(vec, dim=-1) * 2\\n    return (vec + quat[:, 0:1] * t + xyz.cross(t, dim=-1)).view(shape)'),\n",
       " Document(metadata={}, page_content='def quat_apply_inverse(quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Apply an inverse quaternion rotation to a vector.\\n\\n    Args:\\n        quat: The quaternion in (w, x, y, z). Shape is (..., 4).\\n        vec: The vector in (x, y, z). Shape is (..., 3).\\n\\n    Returns:\\n        The rotated vector in (x, y, z). Shape is (..., 3).\\n    \"\"\"\\n    # store shape\\n    shape = vec.shape\\n    # reshape to (N, 3) for multiplication\\n    quat = quat.reshape(-1, 4)\\n    vec = vec.reshape(-1, 3)\\n    # extract components from quaternions\\n    xyz = quat[:, 1:]\\n    t = xyz.cross(vec, dim=-1) * 2\\n    return (vec - quat[:, 0:1] * t + xyz.cross(t, dim=-1)).view(shape)'),\n",
       " Document(metadata={}, page_content='def quat_apply_yaw(quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Rotate a vector only around the yaw-direction.\\n\\n    Args:\\n        quat: The orientation in (w, x, y, z). Shape is (N, 4).\\n        vec: The vector in (x, y, z). Shape is (N, 3).\\n\\n    Returns:\\n        The rotated vector in (x, y, z). Shape is (N, 3).\\n    \"\"\"\\n    quat_yaw = yaw_quat(quat)\\n    return quat_apply(quat_yaw, vec)'),\n",
       " Document(metadata={}, page_content='def quat_rotate(q: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Rotate a vector by a quaternion along the last dimension of q and v.\\n    .. deprecated v2.1.0:\\n         This function will be removed in a future release in favor of the faster implementation :meth:`quat_apply`.\\n\\n    Args:\\n        q: The quaternion in (w, x, y, z). Shape is (..., 4).\\n        v: The vector in (x, y, z). Shape is (..., 3).\\n\\n    Returns:\\n        The rotated vector in (x, y, z). Shape is (..., 3).\\n    \"\"\"\\n    # deprecation\\n    omni.log.warn(\\n        \"The function \\'quat_rotate\\' will be deprecated in favor of the faster method \\'quat_apply\\'.\"\\n        \" Please use \\'quat_apply\\' instead....\"\\n    )\\n    return quat_apply(q, v)'),\n",
       " Document(metadata={}, page_content='def quat_rotate_inverse(q: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Rotate a vector by the inverse of a quaternion along the last dimension of q and v.\\n\\n    .. deprecated v2.1.0:\\n         This function will be removed in a future release in favor of the faster implementation :meth:`quat_apply_inverse`.\\n    Args:\\n        q: The quaternion in (w, x, y, z). Shape is (..., 4).\\n        v: The vector in (x, y, z). Shape is (..., 3).\\n\\n    Returns:\\n        The rotated vector in (x, y, z). Shape is (..., 3).\\n    \"\"\"\\n    omni.log.warn(\\n        \"The function \\'quat_rotate_inverse\\' will be deprecated in favor of the faster method \\'quat_apply_inverse\\'.\"\\n        \" Please use \\'quat_apply_inverse\\' instead....\"\\n    )\\n    return quat_apply_inverse(q, v)'),\n",
       " Document(metadata={}, page_content='def quat_error_magnitude(q1: torch.Tensor, q2: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Computes the rotation difference between two quaternions.\\n\\n    Args:\\n        q1: The first quaternion in (w, x, y, z). Shape is (..., 4).\\n        q2: The second quaternion in (w, x, y, z). Shape is (..., 4).\\n\\n    Returns:\\n        Angular error between input quaternions in radians.\\n    \"\"\"\\n    axis_angle_error = quat_box_minus(q1, q2)\\n    return torch.norm(axis_angle_error, dim=-1)'),\n",
       " Document(metadata={}, page_content='def skew_symmetric_matrix(vec: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Computes the skew-symmetric matrix of a vector.\\n\\n    Args:\\n        vec: The input vector. Shape is (3,) or (N, 3).\\n\\n    Returns:\\n        The skew-symmetric matrix. Shape is (1, 3, 3) or (N, 3, 3).\\n\\n    Raises:\\n        ValueError: If input tensor is not of shape (..., 3).\\n    \"\"\"\\n    # check input is correct\\n    if vec.shape[-1] != 3:\\n        raise ValueError(f\"Expected input vector shape mismatch: {vec.shape} != (..., 3).\")\\n    # unsqueeze the last dimension\\n    if vec.ndim == 1:\\n        vec = vec.unsqueeze(0)\\n    # create a skew-symmetric matrix\\n    skew_sym_mat = torch.zeros(vec.shape[0], 3, 3, device=vec.device, dtype=vec.dtype)\\n    skew_sym_mat[:, 0, 1] = -vec[:, 2]\\n    skew_sym_mat[:, 0, 2] = vec[:, 1]\\n    skew_sym_mat[:, 1, 2] = -vec[:, 0]\\n    skew_sym_mat[:, 1, 0] = vec[:, 2]\\n    skew_sym_mat[:, 2, 0] = -vec[:, 1]\\n    skew_sym_mat[:, 2, 1] = vec[:, 0]\\n\\n    return skew_sym_mat'),\n",
       " Document(metadata={}, page_content='def is_identity_pose(pos: torch.tensor, rot: torch.tensor) -> bool:\\n    \"\"\"Checks if input poses are identity transforms.\\n\\n    The function checks if the input position and orientation are close to zero and\\n    identity respectively using L2-norm. It does NOT check the error in the orientation.\\n\\n    Args:\\n        pos: The cartesian position. Shape is (N, 3).\\n        rot: The quaternion in (w, x, y, z). Shape is (N, 4).\\n\\n    Returns:\\n        True if all the input poses result in identity transform. Otherwise, False.\\n    \"\"\"\\n    # create identity transformations\\n    pos_identity = torch.zeros_like(pos)\\n    rot_identity = torch.zeros_like(rot)\\n    rot_identity[..., 0] = 1\\n    # compare input to identity\\n    return torch.allclose(pos, pos_identity) and torch.allclose(rot, rot_identity)'),\n",
       " Document(metadata={}, page_content='def combine_frame_transforms(\\n    t01: torch.Tensor, q01: torch.Tensor, t12: torch.Tensor | None = None, q12: torch.Tensor | None = None\\n) -> tuple[torch.Tensor, torch.Tensor]:\\n    r\"\"\"Combine transformations between two reference frames into a stationary frame.\\n\\n    It performs the following transformation operation: :math:`T_{02} = T_{01} \\\\times T_{12}`,\\n    where :math:`T_{AB}` is the homogeneous transformation matrix from frame A to B.\\n\\n    Args:\\n        t01: Position of frame 1 w.r.t. frame 0. Shape is (N, 3).\\n        q01: Quaternion orientation of frame 1 w.r.t. frame 0 in (w, x, y, z). Shape is (N, 4).\\n        t12: Position of frame 2 w.r.t. frame 1. Shape is (N, 3).\\n            Defaults to None, in which case the position is assumed to be zero.\\n        q12: Quaternion orientation of frame 2 w.r.t. frame 1 in (w, x, y, z). Shape is (N, 4).\\n            Defaults to None, in which case the orientation is assumed to be identity.\\n\\n    Returns:\\n        A tuple containing the position and orientation of frame 2 w.r.t. frame 0.\\n        Shape of the tensors are (N, 3) and (N, 4) respectively.\\n    \"\"\"\\n    # compute orientation\\n    if q12 is not None:\\n        q02 = quat_mul(q01, q12)\\n    else:\\n        q02 = q01\\n    # compute translation\\n    if t12 is not None:\\n        t02 = t01 + quat_apply(q01, t12)\\n    else:\\n        t02 = t01\\n\\n    return t02, q02'),\n",
       " Document(metadata={}, page_content='def rigid_body_twist_transform(\\n    v0: torch.Tensor, w0: torch.Tensor, t01: torch.Tensor, q01: torch.Tensor\\n) -> tuple[torch.Tensor, torch.Tensor]:\\n    r\"\"\"Transform the linear and angular velocity of a rigid body between reference frames.\\n\\n    Given the twist of 0 relative to frame 0, this function computes the twist of 1 relative to frame 1\\n    from the position and orientation of frame 1 relative to frame 0. The transformation follows the\\n    equations:\\n\\n    .. math::\\n\\n        w_11 = R_{10} w_00 = R_{01}^{-1} w_00\\n        v_11 = R_{10} v_00 + R_{10} (w_00 \\\\times t_01) = R_{01}^{-1} (v_00 + (w_00 \\\\times t_01))\\n\\n    where\\n\\n        - :math:`R_{01}` is the rotation matrix from frame 0 to frame 1 derived from quaternion :math:`q_{01}`.\\n        - :math:`t_{01}` is the position of frame 1 relative to frame 0 expressed in frame 0\\n        - :math:`w_0` is the angular velocity of 0 in frame 0\\n        - :math:`v_0` is the linear velocity of 0 in frame 0\\n\\n    Args:\\n        v0: Linear velocity of 0 in frame 0. Shape is (N, 3).\\n        w0: Angular velocity of 0 in frame 0. Shape is (N, 3).\\n        t01: Position of frame 1 w.r.t. frame 0. Shape is (N, 3).\\n        q01: Quaternion orientation of frame 1 w.r.t. frame 0 in (w, x, y, z). Shape is (N, 4).\\n\\n    Returns:\\n        A tuple containing:\\n        - The transformed linear velocity in frame 1. Shape is (N, 3).\\n        - The transformed angular velocity in frame 1. Shape is (N, 3).\\n    \"\"\"\\n    w1 = quat_rotate_inverse(q01, w0)\\n    v1 = quat_rotate_inverse(q01, v0 + torch.cross(w0, t01, dim=-1))\\n    return v1, w1'),\n",
       " Document(metadata={}, page_content='def subtract_frame_transforms(\\n    t01: torch.Tensor, q01: torch.Tensor, t02: torch.Tensor | None = None, q02: torch.Tensor | None = None\\n) -> tuple[torch.Tensor, torch.Tensor]:\\n    r\"\"\"Subtract transformations between two reference frames into a stationary frame.\\n\\n    It performs the following transformation operation: :math:`T_{12} = T_{01}^{-1} \\\\times T_{02}`,\\n    where :math:`T_{AB}` is the homogeneous transformation matrix from frame A to B.\\n\\n    Args:\\n        t01: Position of frame 1 w.r.t. frame 0. Shape is (N, 3).\\n        q01: Quaternion orientation of frame 1 w.r.t. frame 0 in (w, x, y, z). Shape is (N, 4).\\n        t02: Position of frame 2 w.r.t. frame 0. Shape is (N, 3).\\n            Defaults to None, in which case the position is assumed to be zero.\\n        q02: Quaternion orientation of frame 2 w.r.t. frame 0 in (w, x, y, z). Shape is (N, 4).\\n            Defaults to None, in which case the orientation is assumed to be identity.\\n\\n    Returns:\\n        A tuple containing the position and orientation of frame 2 w.r.t. frame 1.\\n        Shape of the tensors are (N, 3) and (N, 4) respectively.\\n    \"\"\"\\n    # compute orientation\\n    q10 = quat_inv(q01)\\n    if q02 is not None:\\n        q12 = quat_mul(q10, q02)\\n    else:\\n        q12 = q10\\n    # compute translation\\n    if t02 is not None:\\n        t12 = quat_apply(q10, t02 - t01)\\n    else:\\n        t12 = quat_apply(q10, -t01)\\n    return t12, q12'),\n",
       " Document(metadata={}, page_content='def compute_pose_error(\\n    t01: torch.Tensor,\\n    q01: torch.Tensor,\\n    t02: torch.Tensor,\\n    q02: torch.Tensor,\\n    rot_error_type: Literal[\"quat\", \"axis_angle\"] = \"axis_angle\",\\n) -> tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"Compute the position and orientation error between source and target frames.\\n\\n    Args:\\n        t01: Position of source frame. Shape is (N, 3).\\n        q01: Quaternion orientation of source frame in (w, x, y, z). Shape is (N, 4).\\n        t02: Position of target frame. Shape is (N, 3).\\n        q02: Quaternion orientation of target frame in (w, x, y, z). Shape is (N, 4).\\n        rot_error_type: The rotation error type to return: \"quat\", \"axis_angle\".\\n            Defaults to \"axis_angle\".\\n\\n    Returns:\\n        A tuple containing position and orientation error. Shape of position error is (N, 3).\\n        Shape of orientation error depends on the value of :attr:`rot_error_type`:\\n\\n        - If :attr:`rot_error_type` is \"quat\", the orientation error is returned\\n          as a quaternion. Shape is (N, 4).\\n        - If :attr:`rot_error_type` is \"axis_angle\", the orientation error is\\n          returned as an axis-angle vector. Shape is (N, 3).\\n\\n    Raises:\\n        ValueError: Invalid rotation error type.\\n    \"\"\"\\n    # Compute quaternion error (i.e., difference quaternion)\\n    # Reference: https://personal.utdallas.edu/~sxb027100/dock/quaternion.html\\n    # q_current_norm = q_current * q_current_conj\\n    source_quat_norm = quat_mul(q01, quat_conjugate(q01))[:, 0]\\n    # q_current_inv = q_current_conj / q_current_norm\\n    source_quat_inv = quat_conjugate(q01) / source_quat_norm.unsqueeze(-1)\\n    # q_error = q_target * q_current_inv\\n    quat_error = quat_mul(q02, source_quat_inv)\\n\\n    # Compute position error\\n    pos_error = t02 - t01\\n\\n    # return error based on specified type\\n    if rot_error_type == \"quat\":\\n        return pos_error, quat_error\\n    elif rot_error_type == \"axis_angle\":\\n        # Convert to axis-angle error\\n        axis_angle_error = axis_angle_from_quat(quat_error)\\n        return pos_error, axis_angle_error\\n    else:\\n        raise ValueError(f\"Unsupported orientation error type: {rot_error_type}. Valid: \\'quat\\', \\'axis_angle\\'.\")'),\n",
       " Document(metadata={}, page_content='def apply_delta_pose(\\n    source_pos: torch.Tensor, source_rot: torch.Tensor, delta_pose: torch.Tensor, eps: float = 1.0e-6\\n) -> tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"Applies delta pose transformation on source pose.\\n\\n    The first three elements of `delta_pose` are interpreted as cartesian position displacement.\\n    The remaining three elements of `delta_pose` are interpreted as orientation displacement\\n    in the angle-axis format.\\n\\n    Args:\\n        source_pos: Position of source frame. Shape is (N, 3).\\n        source_rot: Quaternion orientation of source frame in (w, x, y, z). Shape is (N, 4)..\\n        delta_pose: Position and orientation displacements. Shape is (N, 6).\\n        eps: The tolerance to consider orientation displacement as zero. Defaults to 1.0e-6.\\n\\n    Returns:\\n        A tuple containing the displaced position and orientation frames.\\n        Shape of the tensors are (N, 3) and (N, 4) respectively.\\n    \"\"\"\\n    # number of poses given\\n    num_poses = source_pos.shape[0]\\n    device = source_pos.device\\n\\n    # interpret delta_pose[:, 0:3] as target position displacements\\n    target_pos = source_pos + delta_pose[:, 0:3]\\n    # interpret delta_pose[:, 3:6] as target rotation displacements\\n    rot_actions = delta_pose[:, 3:6]\\n    angle = torch.linalg.vector_norm(rot_actions, dim=1)\\n    axis = rot_actions / angle.unsqueeze(-1)\\n    # change from axis-angle to quat convention\\n    identity_quat = torch.tensor([1.0, 0.0, 0.0, 0.0], device=device).repeat(num_poses, 1)\\n    rot_delta_quat = torch.where(\\n        angle.unsqueeze(-1).repeat(1, 4) > eps, quat_from_angle_axis(angle, axis), identity_quat\\n    )\\n    # TODO: Check if this is the correct order for this multiplication.\\n    target_rot = quat_mul(rot_delta_quat, source_rot)\\n\\n    return target_pos, target_rot'),\n",
       " Document(metadata={}, page_content='def transform_points(\\n    points: torch.Tensor, pos: torch.Tensor | None = None, quat: torch.Tensor | None = None\\n) -> torch.Tensor:\\n    r\"\"\"Transform input points in a given frame to a target frame.\\n\\n    This function transform points from a source frame to a target frame. The transformation is defined by the\\n    position :math:`t` and orientation :math:`R` of the target frame in the source frame.\\n\\n    .. math::\\n        p_{target} = R_{target} \\\\times p_{source} + t_{target}\\n\\n    If the input `points` is a batch of points, the inputs `pos` and `quat` must be either a batch of\\n    positions and quaternions or a single position and quaternion. If the inputs `pos` and `quat` are\\n    a single position and quaternion, the same transformation is applied to all points in the batch.\\n\\n    If either the inputs :attr:`pos` and :attr:`quat` are None, the corresponding transformation is not applied.\\n\\n    Args:\\n        points: Points to transform. Shape is (N, P, 3) or (P, 3).\\n        pos: Position of the target frame. Shape is (N, 3) or (3,).\\n            Defaults to None, in which case the position is assumed to be zero.\\n        quat: Quaternion orientation of the target frame in (w, x, y, z). Shape is (N, 4) or (4,).\\n            Defaults to None, in which case the orientation is assumed to be identity.\\n\\n    Returns:\\n        Transformed points in the target frame. Shape is (N, P, 3) or (P, 3).\\n\\n    Raises:\\n        ValueError: If the inputs `points` is not of shape (N, P, 3) or (P, 3).\\n        ValueError: If the inputs `pos` is not of shape (N, 3) or (3,).\\n        ValueError: If the inputs `quat` is not of shape (N, 4) or (4,).\\n    \"\"\"\\n    points_batch = points.clone()\\n    # check if inputs are batched\\n    is_batched = points_batch.dim() == 3\\n    # -- check inputs\\n    if points_batch.dim() == 2:\\n        points_batch = points_batch[None]  # (P, 3) -> (1, P, 3)\\n    if points_batch.dim() != 3:\\n        raise ValueError(f\"Expected points to have dim = 2 or dim = 3: got shape {points.shape}\")\\n    if not (pos is None or pos.dim() == 1 or pos.dim() == 2):\\n        raise ValueError(f\"Expected pos to have dim = 1 or dim = 2: got shape {pos.shape}\")\\n    if not (quat is None or quat.dim() == 1 or quat.dim() == 2):\\n        raise ValueError(f\"Expected quat to have dim = 1 or dim = 2: got shape {quat.shape}\")\\n    # -- rotation\\n    if quat is not None:\\n        # convert to batched rotation matrix\\n        rot_mat = matrix_from_quat(quat)\\n        if rot_mat.dim() == 2:\\n            rot_mat = rot_mat[None]  # (3, 3) -> (1, 3, 3)\\n        # convert points to matching batch size (N, P, 3) -> (N, 3, P)\\n        # and apply rotation\\n        points_batch = torch.matmul(rot_mat, points_batch.transpose_(1, 2))\\n        # (N, 3, P) -> (N, P, 3)\\n        points_batch = points_batch.transpose_(1, 2)\\n    # -- translation\\n    if pos is not None:\\n        # convert to batched translation vector\\n        if pos.dim() == 1:\\n            pos = pos[None, None, :]  # (3,) -> (1, 1, 3)\\n        else:\\n            pos = pos[:, None, :]  # (N, 3) -> (N, 1, 3)\\n        # apply translation\\n        points_batch += pos\\n    # -- return points in same shape as input\\n    if not is_batched:\\n        points_batch = points_batch.squeeze(0)  # (1, P, 3) -> (P, 3)\\n\\n    return points_batch'),\n",
       " Document(metadata={}, page_content='def orthogonalize_perspective_depth(depth: torch.Tensor, intrinsics: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Converts perspective depth image to orthogonal depth image.\\n\\n    Perspective depth images contain distances measured from the camera\\'s optical center.\\n    Meanwhile, orthogonal depth images provide the distance from the camera\\'s image plane.\\n    This method uses the camera geometry to convert perspective depth to orthogonal depth image.\\n\\n    The function assumes that the width and height are both greater than 1.\\n\\n    Args:\\n        depth: The perspective depth images. Shape is (H, W) or or (H, W, 1) or (N, H, W) or (N, H, W, 1).\\n        intrinsics: The camera\\'s calibration matrix. If a single matrix is provided, the same\\n            calibration matrix is used across all the depth images in the batch.\\n            Shape is (3, 3) or (N, 3, 3).\\n\\n    Returns:\\n        The orthogonal depth images. Shape matches the input shape of depth images.\\n\\n    Raises:\\n        ValueError: When depth is not of shape (H, W) or (H, W, 1) or (N, H, W) or (N, H, W, 1).\\n        ValueError: When intrinsics is not of shape (3, 3) or (N, 3, 3).\\n    \"\"\"\\n    # Clone inputs to avoid in-place modifications\\n    perspective_depth_batch = depth.clone()\\n    intrinsics_batch = intrinsics.clone()\\n\\n    # Check if inputs are batched\\n    is_batched = perspective_depth_batch.dim() == 4 or (\\n        perspective_depth_batch.dim() == 3 and perspective_depth_batch.shape[-1] != 1\\n    )\\n\\n    # Track whether the last dimension was singleton\\n    add_last_dim = False\\n    if perspective_depth_batch.dim() == 4 and perspective_depth_batch.shape[-1] == 1:\\n        add_last_dim = True\\n        perspective_depth_batch = perspective_depth_batch.squeeze(dim=3)  # (N, H, W, 1) -> (N, H, W)\\n    if perspective_depth_batch.dim() == 3 and perspective_depth_batch.shape[-1] == 1:\\n        add_last_dim = True\\n        perspective_depth_batch = perspective_depth_batch.squeeze(dim=2)  # (H, W, 1) -> (H, W)\\n\\n    if perspective_depth_batch.dim() == 2:\\n        perspective_depth_batch = perspective_depth_batch[None]  # (H, W) -> (1, H, W)\\n\\n    if intrinsics_batch.dim() == 2:\\n        intrinsics_batch = intrinsics_batch[None]  # (3, 3) -> (1, 3, 3)\\n\\n    if is_batched and intrinsics_batch.shape[0] == 1:\\n        intrinsics_batch = intrinsics_batch.expand(perspective_depth_batch.shape[0], -1, -1)  # (1, 3, 3) -> (N, 3, 3)\\n\\n    # Validate input shapes\\n    if perspective_depth_batch.dim() != 3:\\n        raise ValueError(f\"Expected depth images to have 2, 3, or 4 dimensions; got {depth.shape}.\")\\n    if intrinsics_batch.dim() != 3:\\n        raise ValueError(f\"Expected intrinsics to have shape (3, 3) or (N, 3, 3); got {intrinsics.shape}.\")\\n\\n    # Image dimensions\\n    im_height, im_width = perspective_depth_batch.shape[1:]\\n\\n    # Get the intrinsics parameters\\n    fx = intrinsics_batch[:, 0, 0].view(-1, 1, 1)\\n    fy = intrinsics_batch[:, 1, 1].view(-1, 1, 1)\\n    cx = intrinsics_batch[:, 0, 2].view(-1, 1, 1)\\n    cy = intrinsics_batch[:, 1, 2].view(-1, 1, 1)\\n\\n    # Create meshgrid of pixel coordinates\\n    u_grid = torch.arange(im_width, device=depth.device, dtype=depth.dtype)\\n    v_grid = torch.arange(im_height, device=depth.device, dtype=depth.dtype)\\n    u_grid, v_grid = torch.meshgrid(u_grid, v_grid, indexing=\"xy\")\\n\\n    # Expand the grids for batch processing\\n    u_grid = u_grid.unsqueeze(0).expand(perspective_depth_batch.shape[0], -1, -1)\\n    v_grid = v_grid.unsqueeze(0).expand(perspective_depth_batch.shape[0], -1, -1)\\n\\n    # Compute the squared terms for efficiency\\n    x_term = ((u_grid - cx) / fx) ** 2\\n    y_term = ((v_grid - cy) / fy) ** 2\\n\\n    # Calculate the orthogonal (normal) depth\\n    orthogonal_depth = perspective_depth_batch / torch.sqrt(1 + x_term + y_term)\\n\\n    # Restore the last dimension if it was present in the input\\n    if add_last_dim:\\n        orthogonal_depth = orthogonal_depth.unsqueeze(-1)\\n\\n    # Return to original shape if input was not batched\\n    if not is_batched:\\n        orthogonal_depth = orthogonal_depth.squeeze(0)\\n\\n    return orthogonal_depth'),\n",
       " Document(metadata={}, page_content='def unproject_depth(depth: torch.Tensor, intrinsics: torch.Tensor, is_ortho: bool = True) -> torch.Tensor:\\n    r\"\"\"Un-project depth image into a pointcloud.\\n\\n    This function converts orthogonal or perspective depth images into points given the calibration matrix\\n    of the camera. It uses the following transformation based on camera geometry:\\n\\n    .. math::\\n        p_{3D} = K^{-1} \\\\times [u, v, 1]^T \\\\times d\\n\\n    where :math:`p_{3D}` is the 3D point, :math:`d` is the depth value (measured from the image plane),\\n    :math:`u` and :math:`v` are the pixel coordinates and :math:`K` is the intrinsic matrix.\\n\\n    The function assumes that the width and height are both greater than 1. This makes the function\\n    deal with many possible shapes of depth images and intrinsics matrices.\\n\\n    .. note::\\n        If :attr:`is_ortho` is False, the input depth images are transformed to orthogonal depth images\\n        by using the :meth:`orthogonalize_perspective_depth` method.\\n\\n    Args:\\n        depth: The depth measurement. Shape is (H, W) or or (H, W, 1) or (N, H, W) or (N, H, W, 1).\\n        intrinsics: The camera\\'s calibration matrix. If a single matrix is provided, the same\\n            calibration matrix is used across all the depth images in the batch.\\n            Shape is (3, 3) or (N, 3, 3).\\n        is_ortho: Whether the input depth image is orthogonal or perspective depth image. If True, the input\\n            depth image is considered as the *orthogonal* type, where the measurements are from the camera\\'s\\n            image plane. If False, the depth image is considered as the *perspective* type, where the\\n            measurements are from the camera\\'s optical center. Defaults to True.\\n\\n    Returns:\\n        The 3D coordinates of points. Shape is (P, 3) or (N, P, 3).\\n\\n    Raises:\\n        ValueError: When depth is not of shape (H, W) or (H, W, 1) or (N, H, W) or (N, H, W, 1).\\n        ValueError: When intrinsics is not of shape (3, 3) or (N, 3, 3).\\n    \"\"\"\\n    # clone inputs to avoid in-place modifications\\n    intrinsics_batch = intrinsics.clone()\\n    # convert depth image to orthogonal if needed\\n    if not is_ortho:\\n        depth_batch = orthogonalize_perspective_depth(depth, intrinsics)\\n    else:\\n        depth_batch = depth.clone()\\n\\n    # check if inputs are batched\\n    is_batched = depth_batch.dim() == 4 or (depth_batch.dim() == 3 and depth_batch.shape[-1] != 1)\\n    # make sure inputs are batched\\n    if depth_batch.dim() == 3 and depth_batch.shape[-1] == 1:\\n        depth_batch = depth_batch.squeeze(dim=2)  # (H, W, 1) -> (H, W)\\n    if depth_batch.dim() == 2:\\n        depth_batch = depth_batch[None]  # (H, W) -> (1, H, W)\\n    if depth_batch.dim() == 4 and depth_batch.shape[-1] == 1:\\n        depth_batch = depth_batch.squeeze(dim=3)  # (N, H, W, 1) -> (N, H, W)\\n    if intrinsics_batch.dim() == 2:\\n        intrinsics_batch = intrinsics_batch[None]  # (3, 3) -> (1, 3, 3)\\n    # check shape of inputs\\n    if depth_batch.dim() != 3:\\n        raise ValueError(f\"Expected depth images to have dim = 2 or 3 or 4: got shape {depth.shape}\")\\n    if intrinsics_batch.dim() != 3:\\n        raise ValueError(f\"Expected intrinsics to have shape (3, 3) or (N, 3, 3): got shape {intrinsics.shape}\")\\n\\n    # get image height and width\\n    im_height, im_width = depth_batch.shape[1:]\\n    # create image points in homogeneous coordinates (3, H x W)\\n    indices_u = torch.arange(im_width, device=depth.device, dtype=depth.dtype)\\n    indices_v = torch.arange(im_height, device=depth.device, dtype=depth.dtype)\\n    img_indices = torch.stack(torch.meshgrid([indices_u, indices_v], indexing=\"ij\"), dim=0).reshape(2, -1)\\n    pixels = torch.nn.functional.pad(img_indices, (0, 0, 0, 1), mode=\"constant\", value=1.0)\\n    pixels = pixels.unsqueeze(0)  # (3, H x W) -> (1, 3, H x W)\\n\\n    # unproject points into 3D space\\n    points = torch.matmul(torch.inverse(intrinsics_batch), pixels)  # (N, 3, H x W)\\n    points = points / points[:, -1, :].unsqueeze(1)  # normalize by last coordinate\\n    # flatten depth image (N, H, W) -> (N, H x W)\\n    depth_batch = depth_batch.transpose_(1, 2).reshape(depth_batch.shape[0], -1).unsqueeze(2)\\n    depth_batch = depth_batch.expand(-1, -1, 3)\\n    # scale points by depth\\n    points_xyz = points.transpose_(1, 2) * depth_batch  # (N, H x W, 3)\\n\\n    # return points in same shape as input\\n    if not is_batched:\\n        points_xyz = points_xyz.squeeze(0)\\n\\n    return points_xyz'),\n",
       " Document(metadata={}, page_content='def project_points(points: torch.Tensor, intrinsics: torch.Tensor) -> torch.Tensor:\\n    r\"\"\"Projects 3D points into 2D image plane.\\n\\n    This project 3D points into a 2D image plane. The transformation is defined by the intrinsic\\n    matrix of the camera.\\n\\n    .. math::\\n\\n        \\\\begin{align}\\n            p &= K \\\\times p_{3D}  = \\\\\\\\\\n            p_{2D} &= \\\\begin{pmatrix} u \\\\\\\\ v \\\\\\\\  d \\\\end{pmatrix}\\n                    = \\\\begin{pmatrix} p[0] / p[2] \\\\\\\\  p[1] / p[2] \\\\\\\\ Z \\\\end{pmatrix}\\n        \\\\end{align}\\n\\n    where :math:`p_{2D} = (u, v, d)` is the projected 3D point, :math:`p_{3D} = (X, Y, Z)` is the\\n    3D point and :math:`K \\\\in \\\\mathbb{R}^{3 \\\\times 3}` is the intrinsic matrix.\\n\\n    If `points` is a batch of 3D points and `intrinsics` is a single intrinsic matrix, the same\\n    calibration matrix is applied to all points in the batch.\\n\\n    Args:\\n        points: The 3D coordinates of points. Shape is (P, 3) or (N, P, 3).\\n        intrinsics: Camera\\'s calibration matrix. Shape is (3, 3) or (N, 3, 3).\\n\\n    Returns:\\n        Projected 3D coordinates of points. Shape is (P, 3) or (N, P, 3).\\n    \"\"\"\\n    # clone the inputs to avoid in-place operations modifying the original data\\n    points_batch = points.clone()\\n    intrinsics_batch = intrinsics.clone()\\n\\n    # check if inputs are batched\\n    is_batched = points_batch.dim() == 2\\n    # make sure inputs are batched\\n    if points_batch.dim() == 2:\\n        points_batch = points_batch[None]  # (P, 3) -> (1, P, 3)\\n    if intrinsics_batch.dim() == 2:\\n        intrinsics_batch = intrinsics_batch[None]  # (3, 3) -> (1, 3, 3)\\n    # check shape of inputs\\n    if points_batch.dim() != 3:\\n        raise ValueError(f\"Expected points to have dim = 3: got shape {points.shape}.\")\\n    if intrinsics_batch.dim() != 3:\\n        raise ValueError(f\"Expected intrinsics to have shape (3, 3) or (N, 3, 3): got shape {intrinsics.shape}.\")\\n\\n    # project points into 2D image plane\\n    points_2d = torch.matmul(intrinsics_batch, points_batch.transpose(1, 2))\\n    points_2d = points_2d / points_2d[:, -1, :].unsqueeze(1)  # normalize by last coordinate\\n    points_2d = points_2d.transpose_(1, 2)  # (N, 3, P) -> (N, P, 3)\\n    # replace last coordinate with depth\\n    points_2d[:, :, -1] = points_batch[:, :, -1]\\n\\n    # return points in same shape as input\\n    if not is_batched:\\n        points_2d = points_2d.squeeze(0)  # (1, 3, P) -> (3, P)\\n\\n    return points_2d'),\n",
       " Document(metadata={}, page_content='def default_orientation(num: int, device: str) -> torch.Tensor:\\n    \"\"\"Returns identity rotation transform.\\n\\n    Args:\\n        num: The number of rotations to sample.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Identity quaternion in (w, x, y, z). Shape is (num, 4).\\n    \"\"\"\\n    quat = torch.zeros((num, 4), dtype=torch.float, device=device)\\n    quat[..., 0] = 1.0\\n\\n    return quat'),\n",
       " Document(metadata={}, page_content='def random_orientation(num: int, device: str) -> torch.Tensor:\\n    \"\"\"Returns sampled rotation in 3D as quaternion.\\n\\n    Args:\\n        num: The number of rotations to sample.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Sampled quaternion in (w, x, y, z). Shape is (num, 4).\\n\\n    Reference:\\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.random.html\\n    \"\"\"\\n    # sample random orientation from normal distribution\\n    quat = torch.randn((num, 4), dtype=torch.float, device=device)\\n    # normalize the quaternion\\n    return torch.nn.functional.normalize(quat, p=2.0, dim=-1, eps=1e-12)'),\n",
       " Document(metadata={}, page_content='def random_yaw_orientation(num: int, device: str) -> torch.Tensor:\\n    \"\"\"Returns sampled rotation around z-axis.\\n\\n    Args:\\n        num: The number of rotations to sample.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Sampled quaternion in (w, x, y, z). Shape is (num, 4).\\n    \"\"\"\\n    roll = torch.zeros(num, dtype=torch.float, device=device)\\n    pitch = torch.zeros(num, dtype=torch.float, device=device)\\n    yaw = 2 * torch.pi * torch.rand(num, dtype=torch.float, device=device)\\n\\n    return quat_from_euler_xyz(roll, pitch, yaw)'),\n",
       " Document(metadata={}, page_content='def sample_triangle(lower: float, upper: float, size: int | tuple[int, ...], device: str) -> torch.Tensor:\\n    \"\"\"Randomly samples tensor from a triangular distribution.\\n\\n    Args:\\n        lower: The lower range of the sampled tensor.\\n        upper: The upper range of the sampled tensor.\\n        size: The shape of the tensor.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Sampled tensor. Shape is based on :attr:`size`.\\n    \"\"\"\\n    # convert to tuple\\n    if isinstance(size, int):\\n        size = (size,)\\n    # create random tensor in the range [-1, 1]\\n    r = 2 * torch.rand(*size, device=device) - 1\\n    # convert to triangular distribution\\n    r = torch.where(r < 0.0, -torch.sqrt(-r), torch.sqrt(r))\\n    # rescale back to [0, 1]\\n    r = (r + 1.0) / 2.0\\n    # rescale to range [lower, upper]\\n    return (upper - lower) * r + lower'),\n",
       " Document(metadata={}, page_content='def sample_uniform(\\n    lower: torch.Tensor | float, upper: torch.Tensor | float, size: int | tuple[int, ...], device: str\\n) -> torch.Tensor:\\n    \"\"\"Sample uniformly within a range.\\n\\n    Args:\\n        lower: Lower bound of uniform range.\\n        upper: Upper bound of uniform range.\\n        size: The shape of the tensor.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Sampled tensor. Shape is based on :attr:`size`.\\n    \"\"\"\\n    # convert to tuple\\n    if isinstance(size, int):\\n        size = (size,)\\n    # return tensor\\n    return torch.rand(*size, device=device) * (upper - lower) + lower'),\n",
       " Document(metadata={}, page_content='def sample_log_uniform(\\n    lower: torch.Tensor | float, upper: torch.Tensor | float, size: int | tuple[int, ...], device: str\\n) -> torch.Tensor:\\n    r\"\"\"Sample using log-uniform distribution within a range.\\n\\n    The log-uniform distribution is defined as a uniform distribution in the log-space. It\\n    is useful for sampling values that span several orders of magnitude. The sampled values\\n    are uniformly distributed in the log-space and then exponentiated to get the final values.\\n\\n    .. math::\\n\\n        x = \\\\exp(\\\\text{uniform}(\\\\log(\\\\text{lower}), \\\\log(\\\\text{upper})))\\n\\n    Args:\\n        lower: Lower bound of uniform range.\\n        upper: Upper bound of uniform range.\\n        size: The shape of the tensor.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Sampled tensor. Shape is based on :attr:`size`.\\n    \"\"\"\\n    # cast to tensor if not already\\n    if not isinstance(lower, torch.Tensor):\\n        lower = torch.tensor(lower, dtype=torch.float, device=device)\\n    if not isinstance(upper, torch.Tensor):\\n        upper = torch.tensor(upper, dtype=torch.float, device=device)\\n    # sample in log-space and exponentiate\\n    return torch.exp(sample_uniform(torch.log(lower), torch.log(upper), size, device))'),\n",
       " Document(metadata={}, page_content='def sample_gaussian(\\n    mean: torch.Tensor | float, std: torch.Tensor | float, size: int | tuple[int, ...], device: str\\n) -> torch.Tensor:\\n    \"\"\"Sample using gaussian distribution.\\n\\n    Args:\\n        mean: Mean of the gaussian.\\n        std: Std of the gaussian.\\n        size: The shape of the tensor.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Sampled tensor.\\n    \"\"\"\\n    if isinstance(mean, float):\\n        if isinstance(size, int):\\n            size = (size,)\\n        return torch.normal(mean=mean, std=std, size=size).to(device=device)\\n    else:\\n        return torch.normal(mean=mean, std=std).to(device=device)'),\n",
       " Document(metadata={}, page_content='def sample_cylinder(\\n    radius: float, h_range: tuple[float, float], size: int | tuple[int, ...], device: str\\n) -> torch.Tensor:\\n    \"\"\"Sample 3D points uniformly on a cylinder\\'s surface.\\n\\n    The cylinder is centered at the origin and aligned with the z-axis. The height of the cylinder is\\n    sampled uniformly from the range :obj:`h_range`, while the radius is fixed to :obj:`radius`.\\n\\n    The sampled points are returned as a tensor of shape :obj:`(*size, 3)`, i.e. the last dimension\\n    contains the x, y, and z coordinates of the sampled points.\\n\\n    Args:\\n        radius: The radius of the cylinder.\\n        h_range: The minimum and maximum height of the cylinder.\\n        size: The shape of the tensor.\\n        device: Device to create tensor on.\\n\\n    Returns:\\n        Sampled tensor. Shape is :obj:`(*size, 3)`.\\n    \"\"\"\\n    # sample angles\\n    angles = (torch.rand(size, device=device) * 2 - 1) * torch.pi\\n    h_min, h_max = h_range\\n    # add shape\\n    if isinstance(size, int):\\n        size = (size, 3)\\n    else:\\n        size += (3,)\\n    # allocate a tensor\\n    xyz = torch.zeros(size, device=device)\\n    xyz[..., 0] = radius * torch.cos(angles)\\n    xyz[..., 1] = radius * torch.sin(angles)\\n    xyz[..., 2].uniform_(h_min, h_max)\\n    # return positions\\n    return xyz'),\n",
       " Document(metadata={}, page_content='def convert_camera_frame_orientation_convention(\\n    orientation: torch.Tensor,\\n    origin: Literal[\"opengl\", \"ros\", \"world\"] = \"opengl\",\\n    target: Literal[\"opengl\", \"ros\", \"world\"] = \"ros\",\\n) -> torch.Tensor:\\n    r\"\"\"Converts a quaternion representing a rotation from one convention to another.\\n\\n    In USD, the camera follows the ``\"opengl\"`` convention. Thus, it is always in **Y up** convention.\\n    This means that the camera is looking down the -Z axis with the +Y axis pointing up , and +X axis pointing right.\\n    However, in ROS, the camera is looking down the +Z axis with the +Y axis pointing down, and +X axis pointing right.\\n    Thus, the camera needs to be rotated by :math:`180^{\\\\circ}` around the X axis to follow the ROS convention.\\n\\n    .. math::\\n\\n        T_{ROS} = \\\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\\\\ 0 & -1 & 0 & 0 \\\\\\\\ 0 & 0 & -1 & 0 \\\\\\\\ 0 & 0 & 0 & 1 \\\\end{bmatrix} T_{USD}\\n\\n    On the other hand, the typical world coordinate system is with +X pointing forward, +Y pointing left,\\n    and +Z pointing up. The camera can also be set in this convention by rotating the camera by :math:`90^{\\\\circ}`\\n    around the X axis and :math:`-90^{\\\\circ}` around the Y axis.\\n\\n    .. math::\\n\\n        T_{WORLD} = \\\\begin{bmatrix} 0 & 0 & -1 & 0 \\\\\\\\ -1 & 0 & 0 & 0 \\\\\\\\ 0 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 1 \\\\end{bmatrix} T_{USD}\\n\\n    Thus, based on their application, cameras follow different conventions for their orientation. This function\\n    converts a quaternion from one convention to another.\\n\\n    Possible conventions are:\\n\\n    - :obj:`\"opengl\"` - forward axis: -Z - up axis +Y - Offset is applied in the OpenGL (Usd.Camera) convention\\n    - :obj:`\"ros\"`    - forward axis: +Z - up axis -Y - Offset is applied in the ROS convention\\n    - :obj:`\"world\"`  - forward axis: +X - up axis +Z - Offset is applied in the World Frame convention\\n\\n    Args:\\n        orientation: Quaternion of form `(w, x, y, z)` with shape (..., 4) in source convention.\\n        origin: Convention to convert from. Defaults to \"opengl\".\\n        target: Convention to convert to. Defaults to \"ros\".\\n\\n    Returns:\\n        Quaternion of form `(w, x, y, z)` with shape (..., 4) in target convention\\n    \"\"\"\\n    if target == origin:\\n        return orientation.clone()\\n\\n    # -- unify input type\\n    if origin == \"ros\":\\n        # convert from ros to opengl convention\\n        rotm = matrix_from_quat(orientation)\\n        rotm[:, :, 2] = -rotm[:, :, 2]\\n        rotm[:, :, 1] = -rotm[:, :, 1]\\n        # convert to opengl convention\\n        quat_gl = quat_from_matrix(rotm)\\n    elif origin == \"world\":\\n        # convert from world (x forward and z up) to opengl convention\\n        rotm = matrix_from_quat(orientation)\\n        rotm = torch.matmul(\\n            rotm,\\n            matrix_from_euler(torch.tensor([math.pi / 2, -math.pi / 2, 0], device=orientation.device), \"XYZ\"),\\n        )\\n        # convert to isaac-sim convention\\n        quat_gl = quat_from_matrix(rotm)\\n    else:\\n        quat_gl = orientation\\n\\n    # -- convert to target convention\\n    if target == \"ros\":\\n        # convert from opengl to ros convention\\n        rotm = matrix_from_quat(quat_gl)\\n        rotm[:, :, 2] = -rotm[:, :, 2]\\n        rotm[:, :, 1] = -rotm[:, :, 1]\\n        return quat_from_matrix(rotm)\\n    elif target == \"world\":\\n        # convert from opengl to world (x forward and z up) convention\\n        rotm = matrix_from_quat(quat_gl)\\n        rotm = torch.matmul(\\n            rotm,\\n            matrix_from_euler(torch.tensor([math.pi / 2, -math.pi / 2, 0], device=orientation.device), \"XYZ\").T,\\n        )\\n        return quat_from_matrix(rotm)\\n    else:\\n        return quat_gl.clone()'),\n",
       " Document(metadata={}, page_content='def create_rotation_matrix_from_view(\\n    eyes: torch.Tensor,\\n    targets: torch.Tensor,\\n    up_axis: Literal[\"Y\", \"Z\"] = \"Z\",\\n    device: str = \"cpu\",\\n) -> torch.Tensor:\\n    \"\"\"Compute the rotation matrix from world to view coordinates.\\n\\n    This function takes a vector \\'\\'eyes\\'\\' which specifies the location\\n    of the camera in world coordinates and the vector \\'\\'targets\\'\\' which\\n    indicate the position of the object.\\n    The output is a rotation matrix representing the transformation\\n    from world coordinates -> view coordinates.\\n\\n        The inputs eyes and targets can each be a\\n        - 3 element tuple/list\\n        - torch tensor of shape (1, 3)\\n        - torch tensor of shape (N, 3)\\n\\n    Args:\\n        eyes: Position of the camera in world coordinates.\\n        targets: Position of the object in world coordinates.\\n        up_axis: The up axis of the camera. Defaults to \"Z\".\\n        device: The device to create torch tensors on. Defaults to \"cpu\".\\n\\n    The vectors are broadcast against each other so they all have shape (N, 3).\\n\\n    Returns:\\n        R: (N, 3, 3) batched rotation matrices\\n\\n    Reference:\\n    Based on PyTorch3D (https://github.com/facebookresearch/pytorch3d/blob/eaf0709d6af0025fe94d1ee7cec454bc3054826a/pytorch3d/renderer/cameras.py#L1635-L1685)\\n    \"\"\"\\n    if up_axis == \"Y\":\\n        up_axis_vec = torch.tensor((0, 1, 0), device=device, dtype=torch.float32).repeat(eyes.shape[0], 1)\\n    elif up_axis == \"Z\":\\n        up_axis_vec = torch.tensor((0, 0, 1), device=device, dtype=torch.float32).repeat(eyes.shape[0], 1)\\n    else:\\n        raise ValueError(f\"Invalid up axis: {up_axis}. Valid options are \\'Y\\' and \\'Z\\'.\")\\n\\n    # get rotation matrix in opengl format (-Z forward, +Y up)\\n    z_axis = -torch.nn.functional.normalize(targets - eyes, eps=1e-5)\\n    x_axis = torch.nn.functional.normalize(torch.cross(up_axis_vec, z_axis, dim=1), eps=1e-5)\\n    y_axis = torch.nn.functional.normalize(torch.cross(z_axis, x_axis, dim=1), eps=1e-5)\\n    is_close = torch.isclose(x_axis, torch.tensor(0.0), atol=5e-3).all(dim=1, keepdim=True)\\n    if is_close.any():\\n        replacement = torch.nn.functional.normalize(torch.cross(y_axis, z_axis, dim=1), eps=1e-5)\\n        x_axis = torch.where(is_close, replacement, x_axis)\\n    R = torch.cat((x_axis[:, None, :], y_axis[:, None, :], z_axis[:, None, :]), dim=1)\\n    return R.transpose(1, 2)'),\n",
       " Document(metadata={}, page_content='def make_pose(pos: torch.Tensor, rot: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Creates transformation matrices from positions and rotation matrices.\\n\\n    Args:\\n        pos: Batch of position vectors with last dimension of 3.\\n        rot: Batch of rotation matrices with last 2 dimensions of (3, 3).\\n\\n    Returns:\\n        Batch of pose matrices with last 2 dimensions of (4, 4).\\n    \"\"\"\\n    assert isinstance(pos, torch.Tensor), \"Input must be a torch tensor\"\\n    assert isinstance(rot, torch.Tensor), \"Input must be a torch tensor\"\\n    assert pos.shape[:-1] == rot.shape[:-2]\\n    assert pos.shape[-1] == rot.shape[-2] == rot.shape[-1] == 3\\n    pose = torch.zeros(pos.shape[:-1] + (4, 4), dtype=pos.dtype, device=pos.device)\\n    pose[..., :3, :3] = rot\\n    pose[..., :3, 3] = pos\\n    pose[..., 3, 3] = 1.0\\n    return pose'),\n",
       " Document(metadata={}, page_content='def unmake_pose(pose: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"Splits transformation matrices into positions and rotation matrices.\\n\\n    Args:\\n        pose: Batch of pose matrices with last 2 dimensions of (4, 4).\\n\\n    Returns:\\n        Tuple containing:\\n            - Batch of position vectors with last dimension of 3.\\n            - Batch of rotation matrices with last 2 dimensions of (3, 3).\\n    \"\"\"\\n    assert isinstance(pose, torch.Tensor), \"Input must be a torch tensor\"\\n    return pose[..., :3, 3], pose[..., :3, :3]'),\n",
       " Document(metadata={}, page_content='def pose_inv(pose: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Computes the inverse of transformation matrices.\\n\\n    The inverse of a pose matrix [R t; 0 1] is [R.T -R.T*t; 0 1].\\n\\n    Args:\\n        pose: Batch of pose matrices with last 2 dimensions of (4, 4).\\n\\n    Returns:\\n        Batch of inverse pose matrices with last 2 dimensions of (4, 4).\\n    \"\"\"\\n    assert isinstance(pose, torch.Tensor), \"Input must be a torch tensor\"\\n    num_axes = len(pose.shape)\\n    assert num_axes >= 2\\n\\n    inv_pose = torch.zeros_like(pose)\\n\\n    # Take transpose of last 2 dimensions\\n    inv_pose[..., :3, :3] = pose[..., :3, :3].transpose(-1, -2)\\n\\n    # note: PyTorch matmul wants shapes [..., 3, 3] x [..., 3, 1] -> [..., 3, 1] so we add a dimension and take it away after\\n    inv_pose[..., :3, 3] = torch.matmul(-inv_pose[..., :3, :3], pose[..., :3, 3:4])[..., 0]\\n    inv_pose[..., 3, 3] = 1.0\\n    return inv_pose'),\n",
       " Document(metadata={}, page_content='def pose_in_A_to_pose_in_B(pose_in_A: torch.Tensor, pose_A_in_B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Converts poses from one coordinate frame to another.\\n\\n    Transforms matrices representing point C in frame A\\n    to matrices representing the same point C in frame B.\\n\\n    Example usage:\\n\\n    frame_C_in_B = pose_in_A_to_pose_in_B(frame_C_in_A, frame_A_in_B)\\n\\n    Args:\\n        pose_in_A: Batch of transformation matrices of point C in frame A.\\n        pose_A_in_B: Batch of transformation matrices of frame A in frame B.\\n\\n    Returns:\\n        Batch of transformation matrices of point C in frame B.\\n    \"\"\"\\n    assert isinstance(pose_in_A, torch.Tensor), \"Input must be a torch tensor\"\\n    assert isinstance(pose_A_in_B, torch.Tensor), \"Input must be a torch tensor\"\\n    return torch.matmul(pose_A_in_B, pose_in_A)'),\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 105/105 [01:44<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0555299   0.00410058  0.00766323 ... -0.04854472  0.1010896\n",
      "  -0.07439523]\n",
      " [ 0.05165645 -0.03046053 -0.00414181 ... -0.04216816  0.06255618\n",
      "  -0.01578519]\n",
      " [-0.03924909 -0.02091024 -0.01859647 ... -0.12762715  0.02407106\n",
      "  -0.00186439]\n",
      " ...\n",
      " [-0.000875    0.06802201  0.00077574 ... -0.07184713  0.00267993\n",
      "  -0.03409529]\n",
      " [ 0.06395874  0.03100758 -0.03739776 ... -0.01544104  0.05010741\n",
      "  -0.05373923]\n",
      " [ 0.01896854 -0.07728996  0.01581057 ... -0.00960813 -0.08471295\n",
      "  -0.04199051]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector embeddings using sentence transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(\n",
    "    [doc.page_content for doc in documents], \n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next create supabse account vector db store this in that and do similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
